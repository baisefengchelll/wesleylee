标题,发布时间,正文
它是AI的终极答案吗？类脑芯片简史,2019-10-31 19:57:38,"今年8月，有个消息轰动了中国科技界，尤其是AI圈。由中国科研团队研发的“天机”芯片登上了《自然》杂志封面。相关文章展示了清华大学施路平团队研发的世界首款异构融合类脑芯片，它既可支持脉冲神经网络又支持人工神经网路，并且公布了利用“天机芯片”完成自行车自动驾驶的实验视频。

640?wx_fmt=png
这件事给投资界、产业界的直接影响，是在近段时间“类脑芯片”和“类脑计算”相关的投融资、并购与创业公司突然多了起来。“类脑”相关的会议活动也突然增加。虽然说“类脑热”还远远谈不上，但这个领域的突然升温却是真实可见的。

如果我们把目光放得更远一点，类脑芯片确实在这几年有了大规模的爆发。各大学实验室以及科技巨头纷纷拿出了类脑芯片产品，也有不少专家学者认为，人工智能要经历简单人工智能、深度人工智能、通用人工智能三个阶段。而今天的深度学习代表了第二阶段的开始，类脑计算则是通用智能大门的钥匙。

事已至此，可能给大众的感觉是，类脑芯片已经是注定的未来，人类已经借由它找到了通向强人工智能的门径。

然而果真如此吗？类脑芯片是否就是AI的终极答案，今天还埋藏着太多不确定性。而想要客观认识类脑芯片的未来，我们可能必须要把时间倒回一些，先理解它的过去。

一段人类认识神经与大脑的过去。

神经行为学：AI之外的另一条路

从人类的大脑和智慧中，抽取提炼某种技术，是一件源远流长的工作。能不能让机械像人类一样识别、判断和思考，最终发展出了今天的AI。

而在另一项“兄弟研究”里，却一步步发展出了今天的类脑芯片——换言之，类脑芯片的起点某种程度上来说跟AI没啥关系。因为它类的是青蛙的脑。

早在16世纪，达芬奇就在手稿中分析过无头青蛙也能活的现象，某种程度上来说他发现了生物电和中枢神经系统的秘密。但是我们知道达芬奇手稿近世才被披露，所以这个发现就像他很多惊天发明一样变成了“达芬奇的秘密”。

1786年，伽格尼发现了青蛙挂在金属栅栏上腿会抽动的现象，继而一步步建立了早期生物电学。沿着青蛙们以高贵牺牲精神开拓的道路，人类逐渐发现了生物电和神经系统的奥秘。即生物的神经运转，是依靠生物电刺激神经元节点，最终实现了大脑控制机体的网状神经结构。

640?wx_fmt=png
由这个结构开始，神经学界很自然就会思考另一个问题：既然动物是依靠神经元来传递信息、进行控制的，那么这种控制是如何发生的呢？

围绕这个问题，人类在20世纪开始漫长的，对神经传递、神经动力的研究，并在1963年完成了神经行为学的术语概念确认。这个学科中，研究者从生物、解剖、神经反射等多个角度提出了关于神经元的行为学模型。其中很多关于神经元计算的讨论，甚至早于AI概念的提出。

我们知道，今天人工神经网络是AI的基石，但人工神经网络的提出，其实只是上世纪70年代，AI和计算机学界对神经元研究的一次借鉴，主要是模仿了神经元分层处理的特征。它的基础还是坐落在统计学和控制论的概念上。

但随着AI和现代计算的不断发展，作为“兄弟学科”的神经行为学自身也在进步。于是就有人联想到了，能不能直接整体移植神经元系统，在现实世界里，把类似动物大脑中神经元行为的动力机制变成一种运算机制？

之所以要这么干，主要还是临近21世纪，人类发现冯诺依曼架构不断抵近极限。一种从根儿上不同于经典计算的计算架构，或许是最一劳永逸的解决办法。量子计算是一种解决方式，而全仿生神经元行为学的解决方案则是另一种——这一种在大部分时候就被简称为类脑计算。

事实上，类脑计算中除了神经元行为学的仿生计算，也还要其他计算方式。但今天，毫无疑问模仿神经元行为是最成功的一种，于是我们今天看到的二者大体是可以划等号的。

毕竟人脑肯定是最好的计算机，加上想发展AI，那么类似人脑结构的计算方式显然极具魅惑。于是类脑计算在众多新计算形式中天然占据着加分项，而又过了几十年，摩尔定律的极限愈发明显的今天，类脑计算也确实拿出了一些成绩。

类脑计算：比特之外的另一条路

想要了解芯片化的类脑计算之前，我们还要先了解两个东西：SNN和人造突触。

上面咱们说过了，人工神经网络（ANN），本质上还是一种基于统计学递归原理所构建的计算架构。那么想要搞类脑计算，就需要一种更仿生大脑神经元运作的计算架构。这种架构应该体现出人脑计算的高效、精准和连续性，从而对存储分离的冯诺依曼架构提出挑战。

是不是有这种东西呢？还真有。

这就是今天类脑芯片们的基础检验标准：脉冲神经网络SNN。1952年，发现了神经学的功能的离子学说和突触电位的诺贝尔医学奖得主，艾伦·劳埃德·霍奇金爵士提出了脉冲神经网络这种神经行为学模型。

SNN的价值在于，它描述了神经元之间的电位是如何产生和流动的，它认为神经元之间的交换主要靠“神经递质”来产生化学放电，从而在神经网络中实现复杂和可变的神经系统交互。

这一发明来到了计算世界，就变成了一种高度模仿神经元的计算架构。它用发生脉冲的仿生来模拟神经元电位，构成了一种独特的网络结构。 今天，SNN已经在很多领域，比如低功耗和通用处理能力证明了自己的优秀。

但是对于很多说SNN一定是ANN的进化，是下一代神经网络，这个说法有失偏颇。事实上，SNN的出现并不比ANN晚。说白了要有用早就用了，真正让它停留在实验室中的，还是缺乏实际的任务处理能力。但就像大规模并行计算重新激活了沉睡几十年的ANN一样，SNN的未来谁又说得准呢？

类脑计算的另一个关键点，是计算节点的问题。我们知道，比特计算的节点是晶体管的导电开关。而类脑计算则要求模拟出与人类神经元相似的计算节点，来实现非比特计算的另一条路。这也就是说，我们需要人造神经突触。

今天关于如何模拟，或者制造人工突触，已经有相当多的探索。但整体而言新材料还有这样那样的问题，能够量产的类脑芯片，基本还是用电路模拟人造突触的方式来实现类脑计算。这样做对工艺要求很高，生产效率地下，其实并非长久之计。

沿着这两条路，人类慢慢就摸到了类脑芯片的大门。

2011年，IBM发布了TrueNorth芯片，这也是人类用电路模拟神经行为学的开端。2014年TrueNorth更新了第二代，功耗达到了平方厘米消耗 20 毫瓦，印证了类脑芯片的低功耗价值，也在一些AI任务上印证了类脑芯片的实际工作能力。

而紧随其后的，想想也知道应该是英特尔。2017年，英特尔发布了类脑芯片Loihi，其拥有13万个人造突触。今年7月，英特尔发布了号称业界首个大规模神经形态计算系统Pohoiki Beach。这个系统由64块Loihi组合而成，已经可以在自动导航、陆续规划等需要高效执行的AI任务中带来高于GPU的功耗和处理能力。

640?wx_fmt=png
除此之外，业界比较出名的类脑芯片还有高通的Zeroth，以及一些高校实验室和创业公司发明的芯片。吃瓜群众一致表示，这个场子现在就缺谷歌了。

事实上，某种程度上看2019年是类脑芯片爆发应用潜力的一年。无论是中国的天机，还是英特尔和IBM的类脑芯片，都已经在今天被证明了在低功耗和超高速反应上，具有值得期待的效果。这可能给AI领域的一些相关任务，比如非监督学习、快速定位、路径规划上带来帮助。

但是客观来说，类脑芯片并不是完全成熟的。虽然主流科技公司纷纷布局，中国浙大的“达尔文”芯片、清华的“天机”芯片都已经在路上。但类脑芯片距离真正确立产业价值，从实验室步入现实世界，还有很长的路要走。

已知的，未知的：类脑芯片的今天

类脑芯片到底是什么？是人类的朗基努斯枪，还是唐·吉歌德面向风车的宣言？或许我们真的没有必要在今天就给出答案。计算史上从来不是每一次尝试都必须成功，同时很多失败也具有伟大的价值。

综合来看，今天类脑芯片的发展至今，已经可以明确它的几大优势和特性，也就是类脑芯片的光明面：

1、像人脑一样的存算一体，打破了存储计算分离的架构，这是类脑计算的核心突破。

2、功耗极低，并且不会因计算任务的架构复杂化而功耗激增。这终于让计算耗电和散热两大难题找到了新的方向。

3、可能更适合SNN代表的类神经元计算架构，在未来AI发展之路上想想无限。并且具备架构灵活，阵列化计算效率不衰减等等优点。

当然，最根本的优势在于，类脑计算可以绕开比特编程和摩尔定律。在算力极限面前，是跟量子计算一样都是人类的主要救生船。

但是光明面的背后当然就是阴影，也要确实看到的是，类脑芯片在今天还有极大的不确定性，尤其是有一些基础问题无从解答。

640?wx_fmt=png
比如类脑芯片的任务性处理能力差、算力水平过低。第一代TrueNorth甚至无法处理任何有价值的任务。虽然经过几年的发展，类脑计算可以处理的任务越来越多，但是要看到这些任务都有严苛的先决条件。对于绝大部分计算目标来说，类脑芯片都表示带不动。

另一方面，用电子电路模拟人造突触，是极其不划算的一件事。它要花费极高的工艺与技术成本，来实现效率并不高的神经元模拟。所以面向未来，更多人认为一定要找到可以代替经晶体管的，属于类脑计算的新材料——但是这个材料是什么，如何才能做到像硅晶片一样便宜，今天都是未知数。

另一方面，适配类脑计算的架构、算法、编程方案等等也处在广泛的空白期。总体来看，类脑芯片今天就像一片新的开发区，附近有机场，有铁路，但其他东西都还停留在开发方案上。

尤其在我们身边，还要特别警惕一件事，那就是类脑芯片虚假繁荣带来的危险。AI火了之后，更未来更AI的技术成为投融资与政府扶持热点，是一件很自然的事。但类脑计算和类脑芯片，事实上还有非常远的路要走。今天在产业中讨论它，很多时候都是漫无边际的非理性畅想。

笔者曾经参加过一些地方产业组织牵头举办的类脑芯片活动，现场讨论莫衷一是，产学各界代表完全没有在统一的技术逻辑上展开对话。乱拳打死老师傅模式的盲目发展类脑，很可能最终留下“遇事不决，量子力学”般的一摊浆糊。

回到类脑芯片的真实发展路径，今天的类脑芯片，本质上还处在有太多不确定性的实验室探索阶段。它的进步在真实发生，中国也确实站在非常具有想象力的起跑线上，但想让类脑芯片为世界贡献些什么，我们可能还要拿出更多，更多的，以及更多的耐心。

不知道大家看过电影《富春山居图》没有？那是一部划时代的烂片，但它主题曲的几句歌词，非常适合放在这里作为结尾：

“反正你的亲吻无凭无证

就随天机而死天意而生

只要答案，不要问”"
灵汐科技：类脑计算芯片——脑科学研究与后摩尔时代算力突破的新起点,2022-01-31 17:37:23,"只要大脑的奥秘尚未大白于天下，宇宙将仍是个谜。”

——Santiago Ramón y Cajal （1906年诺贝尔生理学或医学奖获得者）

近百年来，围绕着脑科学领域已经颁发了近20项诺贝尔奖，但人类至今仍然难以准确说出究竟大脑还有多少未解之谜等待着去征服。对大脑结构和功能的探索，已经成为当下最具挑战性也是最有意义的前沿科学难题。

说其最有意义，是因为大脑的解密已经与我们每个人的生活息息相关。一方面，许多因为大脑而引起的如老年性痴呆、抑郁、自闭症等疾病目前仍无较好的治疗方案，甚至还不能对病理有清晰、准确的掌握；另一方面，如人工智能、脑机接口等正在改善人类生活的科学研究，其发展路径也高度依赖于对人脑秘密的探索程度。

正是基于以上原因，自进入二十一世纪以来，世界发达国家如日本、美国和欧盟等纷纷开启了“脑计划”，投入大量资金、人力，以求能快速解开大脑的秘密。脑科学与脑认知，已经被多国视为人类最后的科学尖端，是科研领域 “皇冠上的明珠”。

如今， “中国脑计划”的启动，使中国成为继美国、欧盟和日本之后，第四个推出脑科学研究计划的国家。中国脑计划提出“一体两翼”的发展策略， “以脑认知原理为主体，以类脑计算与脑机智能、脑重大疾病诊治为两翼，搭建关键技术平台，抢占脑科学前沿研究制高点”——这在《“十三五”国家科技创新规划》中被明确提到。

在这“一体两翼”中，类脑计算正在成为近年来在国际与国内都非常火热的一个研究方向。通俗的说，类脑计算指的是：通过借鉴于人脑的基本原理，来改进、升级甚至变革我们传统的计算模式。对于中国“脑计划”的发展与突破，类脑计算正在发挥着“尖兵”的作用。

那么，类脑计算将对哪些细分领域起到重要作用？如何实现类脑计算的落地与应用？中国目前有哪些领先于世界的相关技术？

带着这些问题，镁客网专访了中国智慧城市产业与技术创新战略联盟理事、北京科委&中关村管委会技术专家、北京灵汐科技有限公司CMO华宝洪，与其进行了深入的交流。

“小试牛刀”的类脑计算，后摩尔时代的破题之道

今年5月14日，国家科技体制改革和创新体系建设领导小组第十八次会议在京召开，本次会议不仅由国务院副总理刘鹤主持会议并讲话，还专题讨论了面向后摩尔时代的集成电路潜在颠覆性技术。这说明，对于即将到来的后摩尔时代所要面临的新挑战，已经成为了国家重点关心的方向。

那么，什么是面向后摩尔时代的集成电路潜在颠覆性技术？会议并没有对外完全透露。但华宝洪告诉记者，和学界的不少观点一样，“类脑计算”应该就是会议中所提到的潜在战略性与颠覆性技术之一。

所谓摩尔定律，即“在价格不变的情况下，每18-24个月，集成电路可容纳的晶体管数翻一倍，性能提升一倍”。而随着半导体技术的不断突破，受制于芯片内部信号的有效传递已经接近物理尺度的极限，目前想要在5nm以下再提升工艺水平已经是极为尖端的技术难题，而成本、市场与需求之间的平衡，也极大可能难以为继。

这也就意味着，算力的发展将遇到较大的瓶颈，而与之相关度较高的如人工智能等产业，即将迎来潜在的冲击。

以人工智能举例，在第三次发展浪潮后，传统意义上的“三驾马车”——算法、算力、大数据，为人工智能这几年如火如荼的发展插上了翅膀。但后摩尔时代的计算技术面临的挑战，给人工智能未来的发展蒙上了一层阴影。

华宝洪表示，对于人工智能等高度依赖芯片能力的产业来说，传统意义上的单纯依靠堆大数据、高算力或将成为过去式，要想有新的突破，就必须有新的思路。

“以自动驾驶这一细分领域举例，现在市面上各家都在提升算力、算法，不断用大量的图像数据来训练模型，但依然还是面临着无法突破的瓶颈。人工智能发展的“三驾马车”，现在已经面临发展瓶颈，要解决后摩尔时代的挑战，我认为还是要看类脑计算”。

目前学界的不少观点也认为，传统的“三驾马车”+冯诺依曼架构硬件所支撑的人工智能之路，正在越走越窄。清华大学类脑计算研究中心主任施路平教授曾表示，“基于碳基已经发展出现有的人类通用智能，基于硅芯片已经发展出强大的机器智能，借鉴脑科学的基本原理发展类脑计算，支撑人工通用智能发展，是完全有可能的。”

由此看来，类脑计算将有可能成为一把突破传统AI计算瓶颈、打开通用人工智能大门的钥匙，从而也就破解了后摩尔时代所带来的新挑战。不仅如此，华宝洪认为，对后摩尔时代人工智能方向的支持，只能算是类脑计算的“小试牛刀”，其“用武之地”实际上更为宽广。

脑仿真和脑科学研究，是类脑计算更大的“用武之地”

“目前大多数发达国家脑计划的目的，并不仅单纯为了去解决人工智能的问题，而且也在关注和探索：如何去解决人类脑健康（包括精神疾病、脑疾病等领域）所面临的困难。”华宝洪这样告诉记者。

屡屡在各国“脑计划”中提到的类脑计算，首要是解决脑科学、脑认知方面的问题；其次是解决类脑智能计算的问题；而后才涉及到人工智能、脑机接口等前沿技术的支撑助力。可以说，类脑计算这把“牛刀”还是为了加速助力解开大脑的秘密。

华宝洪对记者表示，过往的对脑科学领域的研究，经常会使用到一些小白鼠、猴子等动物进行实体实验，单纯的走医学途径去了解大脑中的神经元、神经脉冲等功能结构和信息传递机制等。尽管结果直观，但也存在着不少弊端：伦理层面上，世界范围内都在呼吁珍爱实验动物；技术层面上，由于动物个体的差异性等因素，存在着实验成本高、结果难复现等问题。

正是在这种情况下，类脑计算有了发挥空间。

“现在我们提供了另外一种手段，通过利用类脑计算芯片搭建大规模的脑仿真平台，可以支持促进对生物脑动力学模拟等领域的研究。借助由类脑计算芯片构建的类脑服务器以及类脑计算集群，不远的未来我们将可以模拟十亿、百亿甚至千亿级神经元规模的生物脑网络活动。”

而通过类脑计算所实现的大规模脑仿真，可以大大降低脑科学领域实验的成本，提升大规模生物脑仿真的速度，同时其可复现性的特点也足以保证实验结果的客观准确性。

有了大规模脑仿真，未来一些让人类“头疼”的问题就有望更快地得到解决。例如帕金森、抑郁等脑疾病的病理的探究、相关药物的治疗效果试验，都有可能通过脑仿真平台去进行测试。同时像脑机接口和前文提到的人工智能等前沿技术，也可以被类脑计算“赋能”。

华宝洪认为，“我们（类脑计算的研究）最终的目的是为了服务于人类的健康，这比类脑计算带给工业界的社会意义可能更为直观。而且，通过脑仿真和类脑计算，我觉得给大家的脑科学科研路径和方法上打开了一个新的想象空间，这是最重要的一点。”



双脑驱动、异构融合——征服世界顶级学刊的中国类脑芯片

芯片，是类脑计算落地应用的重要载体。目前，像IBM的TrueNorth、德国的BrainScales和英特尔的Loihi等，都是国际上较为知名的类脑芯片，并已经应用在大规模脑仿真等脑科学相关的项目中。那么，中国的类脑芯片，表现如何？

2019年8月1日，清华大学类脑计算中心主任施路平团队的科研成果——上一代类脑计算芯片 “天机芯”登上世界顶级学术期刊《Nature》的封面位置。据统计，这不仅是《Nature》创刊130余年历史中，仅有的十余篇华人一作的主刊封面论文之一，同时还实现了中国在芯片和人工智能两大领域中，在《Nature》期刊上论文零的突破。

“异构融合类脑芯片”究竟强在哪？

记者发现，深度学习加速器(支持人工神经网络)和类脑芯片(支持脑神经网络)是当前人工智能芯片代表性的两个主流方向，而目前的其他家芯片只能支持其中一类神经网络。北京灵汐科技今年成功量产了新一代类脑计算芯片-领启KA200，这款以“双脑驱动、异构融合”思路作为指导研发出的商用类脑芯片，则打破了这一禁锢。

“何为双脑驱动？通俗的说，就是把计算机的刚性和人脑的弹性结合起来”。

华宝洪对记者表示，计算机适合处理的是确定的、静态的、不变的场景，它的特点是快速准确，表现特点为“刚性”。人脑擅长处理的是近似的、不稳定和多变的场景，表现特性为“弹性”。

“KA200类脑芯片的设计思路，就是希望能够把计算机这种快速、准确的特点，和人脑在抗噪音、稀疏处理和时空相关性等特点做融合，最终打造出具有双脑驱动、异构融合的类脑芯片”。

“KA200”类脑芯片的优势是多方面的，突破了冯·诺依曼传统架构，采用众核并行，存算一体和分布式计算等核心技术。单芯片支持25万神经元和2500万个突触，其计算能效比高、扩展性好，可以同时高效地支持传统人工神经网络和脉冲神经网络，助力和支撑人工通用智能的发展。



八年磨一剑的灵汐，赛道上砥砺前行的狂奔者

任何科研成果从学界转向业界，都是惊险的一跳。这对于手握世界顶级类脑计算技术的灵汐科技团队来说，同样如此。

毕竟，无论是国际还是国内，类脑芯片的研发都是一项长期而艰苦的工作，尤其当灵汐科技甩开了传统的思维与框架，力图走出一条颠覆性的路线时，就要迎接更多的挑战和艰辛。

从2013年承建国家项目，2018年成立灵汐科技，到2019年科研成果登上Nature封面,再到2021年商用类脑芯片产品正式量产落地，整个团队已经走过8年的时光，其规模也逐渐增长到了200人以上。

华宝洪告诉记者，最近几年团队规模进入快速发展阶段。但更重要的是——在最新研发的KA200芯片量产成功后，灵汐团队已经完成首批客户签单，实现了营收。

“目前我们正在和国内众多的客户和合作伙伴进行对接测试和商业落地合作，涉及脑科学，安防、车路协同、智慧工控、新基建等多个行业方向。”

现在，灵汐科技推出类脑芯片，类脑计算板卡，类脑计算服务器等硬件，还配套提供面向传统人工智能和脑科学研究的全套软件工具栈。

华宝洪认为，“灵汐科技给合作伙伴提供的是基础的软硬件能力，我们非常愿意跟各个合作伙伴一起去探讨各个行业应用的结合，让异构融合、双脑驱动的类脑芯片给客户创造更大的价值。”

“在当下，脑科学和类脑智能已经成为国家“十四五规划”中的重点新兴领域，也是国家层面大力扶持的领域。灵汐科技希望能够在后摩尔时代，既能有效拓展客户在原有深度学习方面形成的能力，又能在目前逐渐升温的脑科学、类脑智能计算等领域里，为大家开辟了一个新的发展空间”。

宝剑锋自磨砺出。八年磨一剑的灵汐科技团队，没有盲从追随主流的传统框架，而是选择了在异构融合类脑芯片的这条“一个人的赛道”上狂奔。未来中国在脑科学、脑认知领域的进展与突破，会因灵汐科技而带来多少惊喜？让我们拭目以待。"
IBM公布神经计算机芯片Truenorth详细资料,2017-09-01 15:23:00,"ZD至顶网服务器频道 04月08日 新闻消息： IBM日前公布了旗下Truenorth的详细发展计划。Truenorth是一款基于人脑神经形态混合信号的计算机芯片。Truenorth构造师在2016年物理设计大会（ISPD物理设计国际研讨会）上描述了 Truenorth芯片的架构、评估板系列、参考系统和软件生态系统。2016年物理设计国际研讨会是计算机械协会（ACM）举办的讨论下一代芯片的会议，会议得到英特尔、IBM、Cadence、Global Foundries、IMEC、Oracle、Synopsys、TSMC、Altera、Xilinx和其他全球明星级芯片制造商的赞助。

IBM公布神经计算机芯片Truenorth详细资料从第一个微处理器到今天Supercomputers-on-a-chip（超级单片机芯片），其时钟速度和功耗都在稳步增加（图的右上），而IBM Truenorth设计师表示，我们其实应该朝相反的方向走，要沿着更低时钟速度和更低功耗那条曲线走，最终降低到人脑等级的10Hz、20瓦（图的左下）。

IBM在ISPD上描述了称之为大脑式计算机启示的概念，并希望这个词在各个领域成为大家熟知的名字，包括各种应用，如超级智能物联网（IoT）、超智能汽车、超智能相机、超智能无人驾驶飞机、超智能医疗设备，当然还有超智能超级计算机。

IBM公布神经计算机芯片Truenorth详细资料

IBM的大脑式架构从图的左上角开始，先研究人类大脑的皮层神经元结构，再往右边伸展，进入结构性的、功能性及最后的物理实现。同样，更复杂的神经形态大脑式启示亦是从左上向下，从核心概念到单芯片到多芯片模块。

IBM低功率神经电路设计师Filipp Akopyan在会上做了题为 “IBM Truenorth的设计和工具流程：一款含一百万个神经元的超低功耗可编程神经突触（Neurosynaptic）芯片” 的邀请演讲发言，他描述IBM的硬件、软件和支持生态系统的发展。

IBM公布神经计算机芯片Truenorth详细资料

IBM开发了自己EDA工具，可与Cadence、Synopsis、 Spice及其他软件一起使用，幻灯片里列出了理由。

Akopyan表示，IBM Truenorth芯片的目标是网络边缘和大数据解决方案，这些应用里有海量实时数据需要采用超低功耗设备进行处理，Truenorth芯片里含54亿个低成本晶体管神经突触芯片，但消耗的功率仅为700毫瓦，最适合处理此类应用。

IBM公布神经计算机芯片Truenorth详细资料

Truenorth芯片与众最大不同的地方为，芯片同时具有传统的同步部分（接口和时序）和异步部分（大脑式架构），其设计流程必须分开但又必须同时进行，如图中所示。

Akopyan表示， “目前我们的移动设备产生海量的数据，现在是交给云计算机处理，而Truenorth可以置于网络边缘，在数据进入网络边缘时对其进行智能处理和归纳，只将重要的东西发往云里。但计算资源往高功率方向的发展是错的，应该朝低功率方向走，最好是小到生物级别。”

IBM公布神经计算机芯片Truenorth详细资料神经芯片同步和异步部分协同仿真的必要性，IBM用流程图做了说明。

IBM从一个神经形态内核着手，许多神经树突（输入）和轴突（输出）连在一起。每个神经元都可以向另一个神经元发出（称之为“尖峰”的）信号，信号可为一或零（电压脉冲或“尖峰”）。芯片上的神经元可以发信号给同一芯片上的其他神经元或是接受来自同一芯片其他神经元的信号，这种信号方式有其优越性，原因不仅是因为稀疏的本地通信和人脑内的通信方式类似，而且信号延迟也更小。神经元也可以向不在同一芯片甚至不在同一块板上的其他神经元发信息，但延迟明显更大。

IBM公布神经计算机芯片Truenorth详细资料Truenorth芯片的所有部件有关人脑式架构的实现在这个图里有详细的说明。

Truenorth的功耗为70毫瓦，很关键的一点，主要是使用了异步逻辑，整个芯片的功耗在神经元未打开前为零，而神经元只是在与其他神经元通信时才会打开。从结构上来看，同步的54亿个晶体管会消耗50到100瓦，而异步架构的功率则非常小。

为了使神经元构成任意至任意的连接结构，芯片上有一个庞大的交叉开关，用于将芯片上54个亿晶体管的神经元连在一起。Akopyan称此交叉开关是“世界上最大的之一”。

IBM公布神经计算机芯片Truenorth详细资料

IBM的第一个电路板层次解决方案用了单一的Truenorth芯片。DARPA和Lawerence Livermore国家实验室分别开始在他们的捕食者无人机 和核管理程序里使用此芯片。

独特的工具

IBM用了多家供应商的工具，包括Cadence、Synopsys和Spice，IBM也打造了一些自己的EDA工具，以支持Truenorth部分同步和部分异步架构的协同设计。IBM创建的工具名为CoSim，名字的意思是在芯片上不同的部分、在不同的工具软件里同时进行模拟。

Akopyan表示，“我们用了不同的模拟工具软件设计Truenorth芯片不同的部分。利用CoSim工具可以进行混合工具流程，对Truenorth进行定制的协同模拟。”

IBM公布神经计算机芯片Truenorth详细资料IBM的第一款16-芯片板的买主是Lawerence Livermore国家实验室。Lawerence Livermore国家实验室用这款板模拟美国日益老化的核武库管理。

如此得到的是一个64x64、一共4096个神经突触内核的中央阵列，每一个神经突触内核含256个神经元和64K内存突触，共100万个神经元和256万个突触，用了54亿个晶体管，消耗功率仅70毫瓦。此外，这些内核可以平铺无限扩展，内核在将来的芯片得到扩展后可以用于扩展神经处理器。目前的芯片采用三星28纳米低功耗工艺。

IBM公布神经计算机芯片Truenorth详细资料

IBM的长期计划是推出越来越大的大脑式神经形态计算机型号，用目前的芯片可打造的最大的神经形态计算机的神经元数目可达人类大脑的百分之一。

为了向一众物联网厂商、军用无人机制造商、超级计算机制造商等潜在客户 展示Truenorth芯片，IBM推出了单芯片独立板机，一个盘上有16块板，全机48块板，每块板含16枚 Truenorth芯片，IBM计划以后推出64、256、1024和4096枚芯片的盘，后者的神经元数目将达到人脑的1%。

IBM公布神经计算机芯片Truenorth详细资料

除了神经形态大脑式计算机本身之外，IBM还创建了一个含设计软件、驱动、分销渠道和平台的完整生态系统。

IBM还创建一个软件开发和应用程序开发的生态系统。"
芯片的原理应用与分类,2018-01-17 11:49:56,"芯片介绍

　　指内含集成电路的硅片，体积很小，常常是计算机或其他电子设备的一部分。芯片(chip)或称微电路(microcircuit)、微芯片(microchip)、集成电路(英语：integrated circuit, IC)，在电子学中是一种把电路(主要包括半导体设备，也包括被动组件等)小型化的方式，并通常制造在半导体晶圆表面上。前述将电路制造在半导体芯片表面上的集成电路又称薄膜(thin-film)集成电路。另有一种厚膜(thick-film)混成集成电路(hybrid integrated circuit)是由独立半导体设备和被动组件，集成到衬底或线路板所构成的小型化电路。

　　


（图片来源：凯利讯半导体）
　　什么是芯片，芯片有什么作用

　　芯片

　　如果把中央处理器CPU比喻为整个电脑系统的心脏，那么主板上的芯片组就是整个身体的躯干。对于主板而言，芯片组几乎决定了这块主板的功能，进而影响到整个电脑系统性能的发挥，芯片组是主板的灵魂。

　　芯片组(Chipset)是主板的核心组成部分，按照在主板上的排列位置的不同，通常分为北桥芯片和南桥芯片。北桥芯片提供对CPU的类型和主频、内存的类型和最大容量、ISA/PCI/AGP插槽、ECC纠错等支持。南桥芯片则提供对KBC(键盘控制器)、RTC(实时时钟控制器)、USB(通用串行总线)、Ultra DMA/33(66)EIDE数据传输方式和ACPI(高级能源管理)等的支持。其中北桥芯片起着主导性的作用，也称为主桥(Host Bridge)。

　　芯片组的识别也非常容易，以Intel 440BX芯片组为例，它的北桥芯片是Intel 82443BX芯片，通常在主板上靠近CPU插槽的位置，由于芯片的发热量较高，在这块芯片上装有散热片。南桥芯片在靠近ISA和PCI槽的位置，芯片的名称为Intel 82371EB。其他芯片组的排列位置基本相同。对于不同的芯片组，在性能上的表现也存在差距。

　　


（图片来源：凯利讯半导体）
　　除了最通用的南北桥结构外，目前芯片组正向更高级的加速集线架构发展，Intel的8xx系列芯片组就是这类芯片组的代表，它将一些子系统如IDE接口、音效、MODEM和USB直接接入主芯片，能够提供比PCI总线宽一倍的带宽，达到了266MB/s;此外，矽统科技的SiS635/SiS735也是这类芯片组的新军。除支持最新的DDR266，DDR200和PC133 SDRAM等规格外，还支持四倍速AGP显示卡接口及Fast Write功能、IDE ATA33/66/100，并内建了3D立体音效、高速数据传输功能包含56K数据通讯(Modem)、高速以太网络传输(Fast Ethernet)、1M/10M家庭网络(Home PNA)等。



　　芯片的应用

　　与PCR技术一样，芯片技术已经开展和将要开展的应用领域非常的广泛。生物芯片的第一个应用领域是检测基因表达。但是将生物分子有序地放在芯片上检测生化标本的策略是具有广泛的应用领域，除了基因表达分析外，杂交为基础的分析已用于基因突变的检测、多态性分析、基因作图、进化研究和其它方面的应用，微阵列分析还可用于检测蛋白质与核酸、小分子物质及与其它蛋白质的结合，但这些领域的应用仍待发展。对基因组DNA进行杂交分析可以检测DNA编码区和非编码区单个碱基改变、确失和插入，DNA杂交分析还可用于对DNA进行定量，这对检测基因拷贝数和染色体的倍性是很重要的。

　　用于DNA分析的样品可从总基因组DNA或克隆片段中获得，通过酶的催化掺入带荧光的核苷酸，也可通过与荧光标记的引物配对进行PCR扩增获得荧光标记DNA样品，从DNA转录的RNA可用于检测克隆的DNA片段，RNA探针常从克隆的DNA中获得，利用RNA聚合酶掺入带荧光的核苷酸。

　　对RNA进行杂交分析可以检测样品中的基因是否表达，表达水平如何。在基因表达检测应用中，荧光标记的探针常常是通过反转录酶催化cDNA合成RNA，在这一过程中掺入荧光标记的核苷酸。用于检测基因表达的RNA探针还可通过RNA聚合酶线性扩增克隆的cDNA获得。在cDNA芯片的杂交实验中，杂交温度足以除DNA中的二级结构，完整的单链分子(300-3000nt)的混合物可以提供很强的杂交信号。对寡核苷酸芯片，杂交温度通常较低，强烈的杂交通常需要探针混合物中的分子降为较短的片段(50-100nt)，用化学和酶学的方法可以改变核苷酸的大小。

　　


　　不同于DNA和RNA分析，利用生物芯片进行蛋白质功能的研究仍有许多困难需要克服，其中一个难点就是由于许多蛋白质间的相互作用是发生在折叠的具有三维结构的多肽表面，不像核酸杂交反应只发生在线性序列间。芯片分析中对折叠蛋白质的需要仍难达到，有以下几个原因：第一，芯片制备中所用的方法必需仍能保持蛋白质灵敏的折叠性质，而芯片制备中所有的化学试剂、热处理、干燥等均将影响到芯片上蛋白质的性质;第二，折叠蛋白质间的相互作用对序列的依赖性更理强，序列依赖性使得反应动力学和分析定量复杂化;第三，高质量的荧光标记蛋白质探针的制备仍待进一步研究。这些原因加上其它的问题减慢了蛋白质芯片检测技术的研究。



　　人脑芯片

　　几十年来，科学家一直“训练”电脑，使其能够像人脑一样思考。这种挑战考验着科学的极限。IBM公司的研究人员18日表示，在将电脑与人脑结合在一起的研究道路上，他们取得了一项重大进展。

　　这家美国科技公司研制出两个芯片原型，与此前的PC和超级计算机采用的芯片相比，这些芯片处理数据的方式与人脑处理信息的方式更为接近。这两个芯片是一项为期6年的项目取得的一项具有里程碑意义的重大成就。共有100名研究人员参与这一项目，美国政府的国防高级研究计划局(DARPA)提供了4100万美元资金。IBM的投资数额并未对外公布。

　　两个芯片原型提供了进一步证据，证明“平行处理”日益提高的重要性。平行处理具体是指电脑同时处理多个任务。多任务处理对渲染图片和处理大量数据非常重要。迄今为止，这两个芯片仅用于处理一些非常简单的任务，例如操控一辆仿真车穿过迷宫或者玩《Pong》。它们最终走出实验室并应用于实际产品可能需要10年或者更长时间。

　　日前，由瑞士、德国和美国的科学家组成的研究小组首次成功研发出一种新奇的微芯片，能够实时模拟人类大脑处理信息的过程。这项新成果将有助于科学家们制造出能同周围环境实时交互的认知系统，为神经网络计算机和高智能机器人的研制提供强有力的技术支撑。

　　以前的类似研究都局限于在传统计算机上研制神经网络模型或在超级计算机上模拟复杂的神经网络，而新研究的思路是：研发在大小、处理速度和能耗方面都可与真实大脑相媲美的电路。研究小组成员基尔克莫·因迪韦里表示：“我们的目标是直接在微芯片上模拟生物神经元和突触的属性。”

　　做到这一点面临的主要挑战，是配置由人造神经元组成的网络，让其能执行特定的任务。研究小组现在已经成功地攻克了这一“碉堡”，他们研发出一种被称为“神经形态芯片”(neuromorphic chips)的装置，能够实时执行复杂的感觉运动任务，并借助这一装置，演示了一个需要短期记忆力和依赖语境的决策能力的任务，这是认知测试所必需的典型特征。

　　


（图片来源：凯利讯半导体）
　　研究小组把神经形态神经元与利用神经处理模块——相当于所谓“有限自动机”的网络相结合。有限自动机是一个用来描述逻辑过程和计算机程序的数学概念。行为可以表示为有限自动机，由此以自动化的方式转给神经形态硬件。因迪韦里说：“网络连接模式非常类似于在大脑中发现的结构。”

　　由于神经形态芯片可以实时处理输入的信息并作出回应，有关专家认为这项技术将有望走向实用化，从而允许机器人在复杂环境中，在不受人类远程遥控的情况下实现自动作业。

　　这项技术的采用还将有望在未来让计算机能够在有部件损坏的情况下继续运作，就像人类的大脑那样，每天损失数以百万计的脑细胞，但是其整体的思维能力却仍然继续正常运转。

　　欧盟、美国和瑞士目前正在紧锣密鼓地研制模拟大脑处理信息的神经网络计算机，希望通过模拟生物神经元复制人工智能系统。这种新型计算机的“大脑芯片”迥异于传统计算机的“大脑芯片”。它能运用类似人脑的神经计算法，低能耗和容错性强是其最大优点，较之传统数字计算机，它的智能性会更强，在认知学习、自动组织、对模糊信息的综合处理等方面也将前进一大步。

　　不过也有人表示了担忧：装上这种芯片的机器人将来是否会在智能上超越人类，甚至会对人类造成威胁?

　　不少科学家认为，这类担心是完全没有必要的。就智能而言，目前机器人的智商相当于4岁儿童的智商，而机器人的“常识”比起正常成年人就差得更远了。美国科学家罗伯特·斯隆日前说：“我们距离能够以8岁儿童的能力回答复杂问题的、具有常识的人工智能程序仍然很遥远。”日本科学家广濑茂男也认为：即使机器人将来具有常识并能进行自我复制，也不可能对人类造成威胁。值得一提的是，中国科学家周海中在1990年发表的《论机器人》一文中指出：机器人并非无所不能;它在工作强度、运算速度和记忆功能方面可以超越人类，但在意识、推理等方面不可能超越人类。另外，机器人会越来越“聪明”，但只能按照制定的原则纲领行动，服务人类、造福人类。

　　


　　与IC的关系

　　芯片，英文为Chip;芯片组为Chipset。芯片一般是指集成电路的载体，也是集成电路经过设计、制造、封装、测试后的结果，通常是一个可以立即使用的独立的整体。“芯片”和“集成电路”这两个词经常混着使用，比如在大家平常讨论话题中，集成电路设计和芯片设计说的是一个意思，芯片行业、集成电路行业、IC行业往往也是一个意思。实际上，这两个词有联系，也有区别。集成电路实体往往要以芯片的形式存在，因为狭义的集成电路，是强调电路本身，比如简单到只有五个元件连接在一起形成的相移振荡器，当它还在图纸上呈现的时候，我们也可以叫它集成电路，当我们要拿这个小集成电路来应用的时候，那它必须以独立的一块实物，或者嵌入到更大的集成电路中，依托芯片来发挥他的作用;集成电路更着重电路的设计和布局布线，芯片更强调电路的集成、生产和封装。而广义的集成电路，当涉及到行业(区别于其他行业)时，也可以包含芯片相关的各种含义。

　　芯片也有它独特的地方，广义上，只要是使用微细加工手段制造出来的半导体片子，都可以叫做芯片，里面并不一定有电路。比如半导体光源芯片;比如机械芯片，如MEMS陀螺仪;或者生物芯片如DNA芯片。在通讯与信息技术中，当把范围局限到硅集成电路时，芯片和集成电路的交集就是在“硅晶片上的电路”上。芯片组，则是一系列相互关联的芯片组合，它们相互依赖，组合在一起能发挥更大的作用，比如计算机里面的处理器和南北桥芯片组，手机里面的射频、基带和电源管理芯片组。

　　


（图片来源：凯利讯半导体）
　　电脑芯片

　　如果把中央处理器CPU比喻为整个电脑系统的心脏，那么主板上的芯片组就是整个身体的躯干。对于主板而言，芯片组几乎决定了这块主板的功能，进而影响到整个电脑系统性能的发挥，芯片组是主板的灵魂。

　　芯片组(Chipset)是主板的核心组成部分，按照在主板上的排列位置的不同，通常分为北桥芯片和南桥芯片。北桥芯片提供对CPU的类型和主频、内存的类型和最大容量、ISA/PCI/AGP插槽、ECC纠错等支持。南桥芯片则提供对KBC(键盘控制器)、RTC(实时时钟控制器)、USB(通用串行总线)、Ultra DMA/33(66)EIDE数据传输

　　方式和ACPI(高级能源管理)等的支持。其中北桥芯片起着主导性的作用，也称为主桥(Host Bridge)。

　　芯片组的识别也非常容易，以Intel440BX芯片组为例，它的北桥芯片是Intel 82443BX芯片，通常在主板上靠近CPU插槽的位置，由于芯片的发热量较高，在这块芯片上装有散热片。南桥芯片在靠近ISA和PCI槽的位置，芯片的名称为Intel 82371EB。其他芯片组的排列位置基本相同。对于不同的芯片组，在性能上的表现也存在差距。



　　芯片组

　　除了最通用的南北桥结构外，芯片组正向更高级的加速集线架构发展，Intel的8xx系列芯片组就是这类芯片组的代表，它将一些子系统如IDE接口、音效、MODEM和USB直接接入主芯片，能够提供比PCI总线宽一倍的带宽，达到了266MB/s;此外，矽统科技的SiS635/SiS735也是这类芯片组的新军。除支持最新的DDR266，DDR200和PC133 SDRAM等规格外，还支持四倍速AGP显示卡接口及Fast Write功能、IDE ATA33/66/100，并内建了3D立体音效、高速数据传输功能包含56K数据通讯(Modem)、高速以太网络传输(Fast Ethernet)、1M/10M家庭网络(Home PNA)等。

　　


　　应用

　　计算机芯片

　　如果把中央处理器CPU比喻为整个电脑系统的心脏，那么主板上的芯片组就是整个身体的躯干。对于主板而言，芯片组几乎决定了这块主板的功能，进而影响到整个电脑系统性能的发挥，芯片组是主板的灵魂。

　　芯片组(Chipset)是主板的核心组成部分，按照在主板上的排列位置的不同，通常分为北桥芯片和南桥芯片。北桥芯片提供对CPU的类型和主频、内存的类型和最大容量、ISA/PCI/AGP插槽、ECC纠错等支持。南桥芯片则提供对KBC(键盘控制器)、RTC(实时时钟控制器)、USB(通用串行总线)、Ultra DMA/33(66)EIDE数据传输方式和ACPI(高级能源管理)等的支持。其中北桥芯片起着主导性的作用，也称为主桥(Host Bridge)。



　　生物芯片

　　与PCR技术一样，芯片技术已经开展和将要开展的应用领域非常的广泛。生物芯片的第一个应用领域是检测基因表达。但是将生物分子有序地放在芯片上检测生化标本的策略是具有广泛的应用领域，除了基因表达分析外，杂交为基础的分析已用于基因突变的检测、多态性分析、基因作图、进化研究和其它方面的应用，微阵列分析还可用于检测蛋白质与核酸、小分子物质及与其它蛋白质的结合，但这些领域的应用仍待发展。对基因组DNA进行杂交分析可以检测DNA编码区和非编码区单个碱基改变、确失和插入，DNA杂交分析还可用于对DNA进行定量，这对检测基因拷贝数和染色体的倍性是很重要的。



　　人脑芯片

　　几十年来，科学家一直“训练”电脑，使其能够像人脑一样思考。这种挑战考验着科学的极限。IBM公司的研究人员18日表示，在将电脑与人脑结合在一起的研究道路上，他们取得了一项重大进展。

　　这家美国科技公司研制出两个芯片原型，与此前的PC和超级计算机采用的芯片相比，这些芯片处理数据的方式与人脑处理信息的方式更为接近。这两个芯片是一项为期6年的项目取得的一项具有里程碑意义的重大成就。共有100名研究人员参与这一项目，美国政府的国防高级研究计划局(DARPA)提供了4100万美元资金。IBM的投资数额并未对外公布。

　　两个芯片原型提供了进一步证据，证明“平行处理”日益提高的重要性。平行处理具体是指电脑同时处理多个任务。多任务处理对渲染图片和处理大量数据非常重要。迄今为止，这两个芯片仅用于处理一些非常简单的任务，例如操控一辆仿真车穿过迷宫或者玩《Pong》。它们最终走出实验室并应用于实际产品可能需要10年或者更长时间。

　　日前，由瑞士、德国和美国的科学家组成的研究小组首次成功研发出一种新奇的微芯片，能够实时模拟人类大脑处理信息的过程。这项新成果将有助于科学家们制造出能同周围环境实时交互的认知系统，为神经网络计算机和高智能机器人的研制提供强有力的技术支撑。

　　以前的类似研究都局限于在传统计算机上研制神经网络模型或在超级计算机上模拟复杂的神经网络，而新研究的思路是：研发在大小、处理速度和能耗方面都可与真实大脑相媲美的电路。研究小组成员基尔克莫·因迪韦里表示：“我们的目标是直接在微芯片上模拟生物神经元和突触的属性。”

　　


（图片来源：凯利讯半导体）
　　研究小组把神经形态神经元与利用神经处理模块——相当于所谓“有限自动机”的网络相结合。有限自动机是一个用来描述逻辑过程和计算机程序的数学概念。行为可以表示为有限自动机，由此以自动化的方式转给神经形态硬件。因迪韦里说：“网络连接模式非常类似于在大脑中发现的结构。”

　　由于神经形态芯片可以实时处理输入的信息并作出回应，有关专家认为这项技术将有望走向实用化，从而允许机器人在复杂环境中，在不受人类远程遥控的情况下实现自动作业。

　　这项技术的采用还将有望在未来让计算机能够在有部件损坏的情况下继续运作，就像人类的大脑那样，每天损失数以百万计的脑细胞，但是其整体的思维能力却仍然继续正常运转。

　　欧盟、美国和瑞士目前正在紧锣密鼓地研制模拟大脑处理信息的神经网络计算机，希望通过模拟生物神经元复制人工智能系统。这种新型计算机的“大脑芯片”迥异于传统计算机的“大脑芯片”。它能运用类似人脑的神经计算法，低能耗和容错性强是其最大优点，较之传统数字计算机，它的智能性会更强，在认知学习、自动组织、对模糊信息的综合处理等方面也将前进一大步。

　　不过也有人表示了担忧：装上这种芯片的机器人将来是否会在智能上超越人类，甚至会对人类造成威胁?

　　


　　不少科学家认为，这类担心是完全没有必要的。就智能而言，目前机器人的智商相当于4岁儿童的智商，而机器人的“常识”比起正常成年人就差得更远了。美国科学家罗伯特·斯隆日前说：“我们距离能够以8岁儿童的能力回答复杂问题的、具有常识的人工智能程序仍然很遥远。”日本科学家广濑茂男也认为：即使机器人将来具有常识并能进行自我复制，也不可能对人类造成威胁。值得一提的是，中国科学家周海中在1990年发表的《论机器人》一文中指出：机器人并非无所不能;它在工作强度、运算速度和记忆功能方面可以超越人类，但在意识、推理等方面不可能超越人类。另外，机器人会越来越“聪明”，但只能按照制定的原则纲领行动，服务人类、造福人类。



　　其他芯片

　　调制与侦测器技术突破，硅光子芯片互连应用指日可待。

　　高速光通信在过去30几年来的发展下，已经成为有线高速信息传输的标准。在2000年受到美国经济泡沫化及网络市场对带宽需求不如预期的影响下，光通信产业与客户端的拓展曾经沉寂一段时间。过去除政府单位或具大型网络建置的企业外，一般终端使用者直接享受高比特率传输的机会并不高。虽然目前高速光通信应用的领域仍以远距离的骨干网络服务为主，但根据目前主流产学论坛的评估，个人客户端传输比特率将在2015年与2023年分别提升至1Gbit/s与10Gbit/s。



　　芯片工作原理

　　BIOS程序是由芯片工厂使用特殊的方法烧录进去的，以前的BIOS芯片中的内容只能读不能改，一且烧录进去，用户只能验证写入的资料是否正确，不能再作任何修改。后来，芯片慢慢从PROM(Programmable ROM，可编程ROM)、EPROM(Erasable Programmable ROM，可擦除可编程ROM)、EEPROM(Electrically-Eiasable Programmable Read-Only Memory，电可擦除可编程只读内存)，一路升级到系统之家如今的FLASHROM(快擦写存储芯片)，我们已经可以通过很多种方法针对FLASHROM进行数据的修改，甚至很多主板厂商还提供了BIOS升级的程序和网站。

　　


（图片来源：凯利讯半导体）
　　1.而猖獗一时的CIH病毒就是利用了FLASHROM芯片这一特征，轻松实现对BIOS芯片内的数据进行破坏，进而导致了主板不能使用的故障。2000年左右的主板维修中，经常需要使用编程器对CIH病毒破坏的BIOS芯片进行数据重写，现在这种病毒已经基本上绝迹了。

　　FLASHROM芯片的封装形式一般有DIP、PLCC，TSOP等，但仅仅只是封装不同而已，对于同一种型号的芯片，不论是什么封装形式，其管脚的功能都是一一对应的，如图所示是29c0101MFlashRom系列BIOS芯片的32个引脚的定义。

　　2.不同的BIOS芯片引脚的定义也是不同的，但是所有的引脚的功能定义都会分为如下几个部分：

　　*A0A17：地址线*CE#：片选信号*DQ0DQ7：数据线*NC：空脚

　　* OE#：数据允许xp输出信号端(低电平有效)

　　* VDD：芯片供电电压(3.35V)

　　* VPP：编程电压(3.3V、5V、12V)

　　* VSS(GND)：接地线

　　* WE择读写信号控制端，由南桥发出(高电平允许读)。

　　BIOS芯片有4种总线，即：ISA、Intel HUB、LPC、SPI，如图所示是BIOS芯片在Intel HUB总线下的工作电路图(适用于815等Intel主板)。

　　3.BIOS芯片使用3.3V的电压，它通过一个电感传输到芯片的第一脚。FWH0~FWH3引脚为总线的数据和地址线，FWH4脚为周期控制引脚，此引脚由系统下载南桥直接控制。当CPU发出寻址指令时，南桥会要求BIOS芯片进行初始化操作(由INIT#引脚负责)，当INIT#引脚由3V电压信号转为低电平信号后，BIOS芯片便开始自检等操作。



　　分类

　　晶体管发明并大量生产之后，各式固态半导体组件如二极管、晶体管等大量使用，取代了真空管在电路中的功能与角色。到了20世纪中后期半导体制造技术进步，使得集成电路成为可能。相对于手工组装电路使用个别的分立电子组件，集成电路可以把很大数量的微晶体管集成到一个小芯片，是一个巨大的进步。集成电路的规模生产能力，可靠性，电路设计的模块化方法确保了快速采用标准化IC 代替了设计使用离散晶体管。

　　


　　IC

　　对于离散晶体管有两个主要优势：成本和性能。成本低是由于芯片把所有的组件通过照相平版技术，作为一个单位印刷，而不是在一个时间只制作一个晶体管。性能高是由于组件快速开关，消耗更低能量，因为组件很小且彼此靠近。2006年，芯片面积从几平方毫米到350 mm^2，每mm^2可以达到一百万个晶体管。

　　第一个集成电路雏形是由杰克·基尔比于1958年完成的，其中包括一个双极性晶体管，三个电阻和一个电容器。

　　根据一个芯片上集成的微电子器件的数量，集成电路可以分为以下几类：

　　小规模集成电路

　　SSI 英文全名为 Small Scale Integration, 逻辑门10个以下 或 晶体管 100个以下。

　　中规模集成电路

　　MSI 英文全名为 Medium Scale Integration, 逻辑门11~100个 或 晶体管 101~1k个。

　　大规模集成电路

　　LSI 英文全名为 Large Scale Integration, 逻辑门101~1k个 或 晶体管 1,001~10k个。

　　超大规模集成电路

　　VLSI 英文全名为 Very large scale integration, 逻辑门1,001~10k个 或 晶体管 10,001~100k个。

　　甚大规模集成电路

　　ULSI 英文全名为 Ultra Large Scale Integration, 逻辑门10,001~1M个 或 晶体管 100,001~10M个。

　　GLSI 英文全名为 Giga Scale Integration, 逻辑门1,000,001个以上 或 晶体管10,000,001个以上。

　　而根据处理信号的不同，可以分为模拟集成电路、数字集成电路、和兼具模拟与数字的混合信号集成电路。

　　


　　制造

　　芯片制作完整过程包括芯片设计、晶片制作、封装制作、成本测试等几个环节，其中晶片制作过程尤为的复杂。

　　精密的芯片其制造过程非常的复杂 首先是芯片设计，根据设计的需求，生成的“图样”



　　1、 芯片的原料晶圆

　　晶圆的成分是硅，硅是由石英沙所精练出来的，晶圆便是硅元素加以纯化(99.999%)，接着是将这些纯硅制成硅晶棒，成为制造集成电路的石英半导体的材料，将其切片就是芯片制作具体所需要的晶圆。晶圆越薄，生产的成本越低，但对工艺就要求的越高。



　　2、晶圆涂膜

　　晶圆涂膜能抵抗氧化以及耐温能力，其材料为光阻的一种。



　　3、晶圆光刻显影、蚀刻

　　该过程使用了对紫外光敏感的化学物质，即遇紫外光则变软。通过控制遮光物的位置可以得到芯片的外形。在硅晶片涂上光致抗蚀剂，使得其遇紫外光就会溶解。这时可以用上第一份遮光物，使得紫外光直射的部分被溶解，这溶解部分接着可用溶剂将其冲走。这样剩下的部分就与遮光物的形状一样了，而这效果正是我们所要的。这样就得到我们所需要的二氧化硅层。

　　


　　4、掺加杂质

　　将晶圆中植入离子，生成相应的P、N类半导体。

　　具体工艺是是从硅片上暴露的区域开始，放入化学离子混合液中。这一工艺将改变搀杂区的导电方式，使每个晶体管可以通、断、或携带数据。简单的芯片可以只用一层，但复杂的芯片通常有很多层，这时候将这一流程不断的重复，不同层可通过开启窗口联接起来。这一点类似多层PCB板的制作原理。 更为复杂的芯片可能需要多个二氧化硅层，这时候通过重复光刻以及上面流程来实现，形成一个立体的结构。



　　5、晶圆测试

　　经过上面的几道工艺之后，晶圆上就形成了一个个格状的晶粒。通过针测的方式对每个晶粒进行电气特性检测。一般每个芯片的拥有的晶粒数量是庞大的，组织一次针测试模式是非常复杂的过程，这要求了在生产的时候尽量是同等芯片规格构造的型号的大批量的生产。数量越大相对成本就会越低，这也是为什么主流芯片器件造价低的一个因素。



　　6、封装

　　将制造完成晶圆固定，绑定引脚，按照需求去制作成各种不同的封装形式，这就是同种芯片内核可以有不同的封装形式的原因。比如：DIP、QFP、PLCC、QFN等等。这里主要是由用户的应用习惯、应用环境、市场形式等外围因素来决定的。



　　7、测试、包装

　　经过上述工艺流程以后，芯片制作就已经全部完成了，这一步骤是将芯片进行测试、剔除不良品，以及包装。"
AI 芯片的分类及技术,2021-05-27 05:49:27,"AI 芯片的分类及技术
人工智能芯片有两种发展路径：一种是延续传统计算架构，加速硬件计算能力，主要以 3 种类型的芯片为代表，即 GPU、 FPGA、 ASIC，但 CPU依旧发挥着不可替代的作用；另一种是颠覆经典的冯·诺依曼计算架构，采用类脑神经结构来提升计算能力，以 IBM TrueNorth 芯片为代表。

传统 CPU
计算机工业从 1960 年代早期开始使用 CPU 这个术语。迄今为止， CPU 从形态、设计到实现都已发生了巨大的变化，但是其基本工作原理却一直没有大的改变。
通常 CPU 由控制器和运算器这两个主要部件组成。传统的 CPU 内部结构图如图 3 所示，从图中我们可以看到：实质上仅单独的 ALU 模块（逻辑运算单元）是用来完成数据计算的，其他各个模块的存在都是为了保证指令能够一条接一条的有序执行。这种通用性结构对于传统的编程计算模式非常适合，同时可以通过提升 CPU 主频（提升单位时间内执行指令的条数）来提升计算速度。
但对于深度学习中的并不需要太多的程序指令、却需要海量数据运算的计算需求，这种结构就显得有些力不从心。尤其是在功耗限制下， 无法通过无限制的提升 CPU 和内存的工作频率来加快指令执行速度， 这种情况导致 CPU 系统的发展遇到不可逾越的瓶颈。
在这里插入图片描述
▲传统 CPU 内部结构图（仅 ALU 为主要计算模块）
2. 并行加速计算的 GPU
GPU 作为最早从事并行加速计算的处理器，相比 CPU 速度快， 同时比其他加速器芯片编程灵活简单。
传统的 CPU 之所以不适合人工智能算法的执行，主要原因在于其计算指令遵循串行执行的方式，没能发挥出芯片的全部潜力。与之不同的是，GPU 具有高并行结构，在处理图形数据和复杂算法方面拥有比 CPU 更高的效率。
对比 GPU 和 CPU 在结构上的差异， CPU大部分面积为控制器和寄存器，而 GPU 拥有更ALU(ARITHMETIC LOGIC UNIT，逻辑运算单元)用于数据处理，这样的结构适合对密集型数据进行并行处理，CPU 与 GPU 的结构对比如图所示。程序在 GPU系统上的运行速度相较于单核 CPU往往提升几十倍乃至上千倍。
随着英伟达、AMD 等公司不断推进其对 GPU 大规模并行架构的支持，面向通用计算的 GPU(即GPGPU，GENERAL PURPOSE GPU，通用计算图形处理器)已成为加速可并行应用程序的重要手段。
在这里插入图片描述

▲CPU 及 GPU 结构对比图（引用自 NVIDIA CUDA 文档）
GPU 的发展历程可分为 3 个阶段， 发展历程如图所示：
第 一 代 GPU(1999 年 以 前 ) ， 部 分 功 能 从 CPU 分 离 ， 实 现 硬 件 加 速 ， 以GE(GEOMETRY ENGINE)为代表，只能起到 3D 图像处理的加速作用，不具有软件编程特性。
第二代 GPU(1999-2005 年)， 实现进一步的硬件加速和有限的编程性。
1999 年，英伟达发布了“专为执行复杂的数学和几何计算的” GeForce256 图像处理芯片，将更多的晶体管用作执行单元， 而不是像 CPU 那样用作复杂的控制单元和缓存，将 T&L(TRANSFORM AND LIGHTING)等功能从 CPU 分离出来，实现了快速变换，这成为 GPU 真正出现的标志。
之后几年，GPU 技术快速发展，运算速度迅速超过 CPU。2001 年英伟达和 ATI 分别推出的GEFORCE3 和 RADEON 8500，图形硬件的流水线被定义为流处理器，出现了顶点级可编程性，同时像素级也具有有限的编程性，但 GPU 的整体编程性仍然比较有限。
第三代 GPU(2006年以后)， GPU实现方便的编程环境创建， 可以直接编写程序。 2006年英伟达与 ATI分别推出了 CUDA(Compute United Device Architecture，计算统一设备架构)编程环境和 CTM(CLOSE TO THE METAL)编程环境， 使得 GPU 打破图形语言的局限成为真正的并行数据处理超级加速器。
2008 年，苹果公司提出一个通用的并行计算编程平台 OPENCL（OPEN COMPUTING LANGUAGE，开放运算语言），与 CUDA 绑定在英伟达的显卡上不同，OPENCL 和具体的计算设备无关。
在这里插入图片描述

▲GPU 芯片的发展阶段
目前， GPU 已经发展到较为成熟的阶段。谷歌、 FACEBOOK、微软、TWITTER 和百度等公司都在使用 GPU 分析图片、视频和音频文件，以改进搜索和图像标签等应用功能。此外，很多汽车生产商也在使用 GPU 芯片发展无人驾驶。不仅如此，GPU 也被应用于VR/AR 相关的产业。
但是GPU也有一定的局限性。深度学习算法分为训练和推断两部分， GPU 平台在算法训练上非常高效。但在推断中对于单项输入进行处理的时候，并行计算的优势不能完全发挥出来。
3. 半定制化的 FPGA
FPGA 是在 PAL、 GAL、 CPLD 等可编程器件基础上进一步发展的产物。用户可以通过烧入 FPGA 配置文件来定义这些门电路以及存储器之间的连线。
这种烧入不是一次性的，比如用户可以把 FPGA 配置成一个微控制器 MCU，使用完毕后可以编辑配置文件把同一个FPGA 配置成一个音频编解码器。因此，它既解决了定制电路灵活性的不足，又克服了原有可编程器件门电路数有限的缺点。
FPGA 可同时进行数据并行和任务并行计算，在处理特定应用时有更加明显的效率提升。对于某个特定运算，通用 CPU 可能需要多个时钟周期； 而 FPGA 可以通过编程重组电路，直接生成专用电路，仅消耗少量甚至一次时钟周期就可完成运算。
此外，由于FPGA的灵活性，很多使用通用处理器或 ASIC难以实现的底层硬件控制操作技术，利用 FPGA 可以很方便的实现。这个特性为算法的功能实现和优化留出了更大空间。
同时 FPGA 一次性成本(光刻掩模制作成本)远低于 ASIC，在芯片需求还未成规模、深度学习算法暂未稳定， 需要不断迭代改进的情况下，利用 FPGA 芯片具备可重构的特性来实现半定制的人工智能芯片是最佳选择之一。
功耗方面，从体系结构而言， FPGA 也具有天生的优势。传统的冯氏结构中，执行单元（如 CPU 核）执行任意指令，都需要有指令存储器、译码器、各种指令的运算器及分支跳转处理逻辑参与运行，而 FPGA 每个逻辑单元的功能在重编程（即烧入）时就已经确定，不需要指令，无需共享内存，从而可以极大的降低单位执行的功耗，提高整体的能耗比。
由于 FPGA 具备灵活快速的特点， 因此在众多领域都有替代 ASIC 的趋势。 FPGA 在人工智能领域的应用如图所示。
在这里插入图片描述

▲FPGA 在人工智能领域的应用
4. 全定制化的 ASIC
目前以深度学习为代表的人工智能计算需求，主要采用 GPU、 FPGA 等已有的适合并行计算的通用芯片来实现加速。在产业应用没有大规模兴起之时，使用这类已有的通用芯片可以避免专门研发定制芯片（ASIC） 的高投入和高风险。
但是，由于这类通用芯片设计初衷并非专门针对深度学习，因而天然存在性能、功耗等方面的局限性。随着人工智能应用规模的扩大，这类问题日益突显。
GPU 作为图像处理器， 设计初衷是为了应对图像处理中的大规模并行计算。因此，在应用于深度学习算法时，有三个方面的局限性：
第一，应用过程中无法充分发挥并行计算优势。深度学习包含训练和推断两个计算环节， GPU 在深度学习算法训练上非常高效，但对于单一输入进行推断的场合， 并行度的优势不能完全发挥。
第二，无法灵活配置硬件结构。 GPU 采用 SIMT 计算模式，硬件结构相对固定。目前深度学习算法还未完全稳定，若深度学习算法发生大的变化， GPU 无法像 FPGA 一样可以灵活的配制硬件结构。
第三，运行深度学习算法能效低于 FPGA。
尽管 FPGA 倍受看好，甚至新一代百度大脑也是基于 FPGA 平台研发，但其毕竟不是专门为了适用深度学习算法而研发，实际应用中也存在诸多局限：
第一，基本单元的计算能力有限。为了实现可重构特性， FPGA 内部有大量极细粒度的基本单元，但是每个单元的计算能力(主要依靠 LUT 查找表)都远远低于 CPU 和 GPU 中的 ALU 模块。
第二、 计算资源占比相对较低。 为实现可重构特性， FPGA 内部大量资源被用于可配置的片上路由与连线。
第三，速度和功耗相对专用定制芯片(ASIC)仍然存在不小差距。
第四， FPGA 价格较为昂贵，在规模放量的情况下单块 FPGA 的成本要远高于专用定制芯片。
因此，随着人工智能算法和应用技术的日益发展，以及人工智能专用芯片 ASIC产业环境的逐渐成熟， 全定制化人工智能 ASIC也逐步体现出自身的优势，从事此类芯片研发与应用的国内外比较有代表性的公司如图所示。
在这里插入图片描述

▲早期人工智能专用芯片（包括类脑芯片） 研发情况一览
深度学习算法稳定后， AI 芯片可采用 ASIC 设计方法进行全定制， 使性能、功耗和面积等指标面向深度学习算法做到最优。
5. 类脑芯片
类脑芯片不采用经典的冯·诺依曼架构，而是基于神经形态架构设计，以 IBM Truenorth为代表。 IBM 研究人员将存储单元作为突触、计算单元作为神经元、传输单元作为轴突搭建了神经芯片的原型。
目前， Truenorth 用三星 28nm 功耗工艺技术，由 54 亿个晶体管组成的芯片构成的片上网络有 4096 个神经突触核心，实时作业功耗仅为 70mW。
由于神经突触要求权重可变且要有记忆功能， IBM 采用与 CMOS 工艺兼容的相变非挥发存储器（PCM）的技术实验性的实现了新型突触，加快了商业化进程。"
施路平教授：面向人工通用智能的类脑计算,2020-01-13 16:50:23,"在2019年10月31日的北京智源大会“智能体系架构与芯片专题论坛”中，清华大学类脑计算研究中心主任施路平教授为我们分享了题为《面向人工通用智能的类脑计算》的主题演讲。

在演讲中，施路平将天机芯背后的机理——类脑计算，作为实现通用人工智能的解决方案，即将脑的基本原理同现有计算机体系结构相整合，从而实现了：既充分挖掘了现有计算机系统利用时间复杂性处理结构化问题的优势，同时整合了脑计算中利用时空复杂性的优势，来解决通用人工智能中的“非结构化”和“不确定性”问题。

回顾2019年的中国智能芯片领域，施路平团队“天机芯”荣登自然杂志封面无疑是最具影响力的事件之一，而施路平的这次演讲，也是他去年为数不多的公开亮相中，阐释天机芯、类脑计算原理以及相关研究心路历程较为详细的一次，在2020年新年已来之际，我们特别整理了这次演讲的精彩要点，希望为大家思考类脑芯片在未来的发展脉络提供有益的参考。接下来，让我们以施路平的演讲逻辑和要点为线索，步入类脑计算的世界吧。



图1：北京智源大会上的施路平

整理：小蔡

编辑：王炜强

01

人工通用智能

是未来的发展方向



图2：柯洁与Alphago围棋对弈（左）Project Debater对战世界级辩手（右）

众所周知，人工智能正在加速发展。从2017年Google的人工智能围棋程序AlphaGo碾压人类顶尖棋手柯洁[1]，到2018年IBM的人工智能辩论程序Project Debater击败来自以色列的世界级辩手Dan Zafrir和Noa Ovadia[2]，人们已经开始担心：人工智能是否会超越人类智能？但施路平指出，尽管人工智能已取得很大成绩，仍然存在很多的瓶颈，离赶超人类智能还有漫长的距离。主要原因在于，目前的人工智能成果大都是“狭隘”的，即面向特定任务，只能解决特定问题，而且须满足张钹院士所总结的5个约束条件：充足的数据、确定性问题、完备的知识、静态的环境、单一的系统。 这 5 个条件里任何一个条件不满足，目前的人工智能做起来就非常困难了。施路平举例说，如果让一个智能机器人从这里出去，如果不进行事先编程，它是做不到的，为什么？因为人用了几年的时间建立起这些概念：“在哪里”、“怎么出去”、“走门还是窗”等，都与我们人类的通用智能有关。因此，施路平指出，人工智能的未来发展必将从“狭隘”的弱人工智能走向更具鲁棒性与通用性的人工通用智能（Artificial General Intelligence，简称AGI）。



图3：人工智能的挑战与未来方向

关于人工通用智能，据编辑了解，它来源于早期人工智能研究者提出的“制造可与人类相媲美的思维机器”这一终极目标，研究内容主要包括科学理论与工程技术两个方面，即关于人工通用智能的基础理论、形式化模型与计算机实现等。自上个世纪50年代以来，有不少野心勃勃的研究项目试图在不同的抽象层次上实现这一目标，但都失败了，其中包括通用问题求解器（General Problem Solver）、日本的第五代机系统（5thGeneration Computer Systems）等。后来，主流人工智能研究逐渐避离以通用智能系统为目标，转向为特定领域和特定问题寻求解决方案并取得不少研究成果。但这些人工智能成果大都依赖于事先编程与人工采集训练数据来掌握某种特定任务，并没有太多其他的能力。直到2008年左右才又陆续出现一些具有相似研究愿景的人工智能团队，如Google的子公司DeepMind。

施路平认为，DeepMind是目前在人工通用智能方面做得最好的人工智能公司。该公司强调智能系统的通用性特质，试图用10年时间打造可媲美人类智能的人工通用智能框架，赋能各行各业。公司创始人戴密斯·哈萨比斯(Demis Hassabis)是一个难得的多学科复合型人才，17岁进入剑桥大学攻读数学、计算机科学双学位，之后又进入伦敦大学攻读神经科学博士。通过创始人的背景和视野，我们也不难理解缘何DeepMind公司从一开始就采用了脑科学、神经科学与计算机科学交叉融合的思路，来推动人工智能算法与系统研究，并取得了突破性的研究成果。



图4：Deepmind创始人戴密斯·哈萨比斯(DemisHassabis)

关于发展人工通用智能的方法，施路平介绍说，目前主流研究思路有两种：一是从计算机科学的角度出发，借鉴大脑的分层处理机制与学习训练特性，基于冯诺依曼体系结构的人工神经网络与深度学习方法；二是从脑科学研究出发，采用微纳光电器件模拟生物神经元和突触的信息处理特性，采用神经形态芯片与脉冲神经网络替代冯诺依曼体系结构，基于众核分布并行与存储处理一体化架构的类脑计算方法。相对而言，目前前者是研究热点，但后者在施路平看来更具优势，意义更深远。



图5：类脑计算的优势

施路平认为，与计算机方法相比，类脑计算方法在能耗、多路并行与处理不确定性问题等方面具有更大的优势。目前生物大脑的解析步伐正在加快，未来10年有望实现高等动物全脑解析。而在类脑计算机实现方面，纳米级别的人造突触和人造神经元的出现,有望掀起一场计算机的新革命。最近图灵奖得主、计算机体系结构宗师DavidPatterson与John Hennessy在Communicationsof the ACM上发表长文指出“在摩尔定律走向终点的同时，体系结构正在闪耀新的活力——以TPU为代表的领域特定架构(Domain Specific Architectures, DSA) 兴起...未来将是计算机体系结构的黄金十年。”[3]施路平坚信，类脑计算将成为人工通用智能的基石。



图6：图灵奖得主、计算机体系结构宗师David Patterson与John Hennessy

02

类脑计算

是人工通用智能的基石

类脑计算的概念最早是由加州理工Carver Mead于1989年出的，但当时摩尔定律持续发展（基于冯诺依曼架构的处理器主频与性能持续增长），相关研究并没有引起太多关注。直到2004年单核处理器主频停止增长，人们开始转向多核，寻求非冯诺依曼架构的替代技术，类脑计算才开始引起关注。随后，世界各国相继开展脑计划研究并取得一系列阶段性成果。

2004年，美国Stanford大学教授Kwabena Boahen(Carver Mead的学生)研制出基于模拟电路的类脑芯片Neurogrid[4]；2005年欧盟启动FACETS项目，德国海德堡大学牵头研制基于模拟混合信号(AMS)神经形态芯片；同年，英国曼彻斯特大学基于ARM开始研制支持脉冲神经网络的多核超级计算机SpiNNaker[5]。美国DARPA启动SyNAPSE项目，支持IBM与多家合作单位联合研发类脑芯片模拟大规模仿生神经网络；2008年惠普公司实现忆阻器(memristor)原型，能够模拟神经突触功能,并展示了首个忆阻器与硅材料的混合电路, 全球人造突触热潮兴起。

2011年欧盟启动BrainScaleS[6]项目研发大规模并行类脑计算机，后来又启动了人类大脑计划HBP，该计划包括6个平台：神经信息学平台、医学信息学平台、脑仿真平台、高性能计算平台、类脑计算平台与神经机器人平台等。2014年IBM的 SyNAPSE项目推出TrueNorth芯片[7]，包含54亿个半导体，功耗只有 70mW，比半导体数量相当的传统CPU功耗低5000倍左右。实现的一个用于视觉对象检测的应用系统,包含300万个神经元,功耗只有200mW。2016年3月欧盟宣布把刚刚建成的两套类脑计算机通过互联网对外开放使用，以支持神经微回路模拟以及在机器学习和认知计算中应用类脑原理的相关研究。这两套系统即德国海德堡大学的BrainScaleS系统[7]和英国曼彻斯特大学的SpiNNaker系统[6]。



图7：类脑计算系统

施路平指出，如果从历史上往回看，2016年实际上是类脑计算机发展的元年，全球有三套类脑计算系统上线，但每个系统都采用了不同的研究思路与研究架构。由于这个领域正处于飞速发展的阶段，目前还没有形成公认的技术方案。因此，从做研究的角度来说，现在是最好的研究切入点。



图8：典型类脑计算研究团队

不理解人脑凭什么可以制造出类脑计算机？施路平分享经验说，“关于这个问题我们思考了很久，后来我们得到答案“。计算机是把一个多元空间的信息转换成“010101”这样一种信息，用计算来解决，那么计算的CPU主频越来越快，换句话说，用的是时间复杂度的问题，当你缩维度的时候相关性会丢失。我们不知道脑的基本原理，但我们知道一个神经元接一千到一万个神经元，换句话说，我们在这里把信息扩充了，把相关性增强了，我们用的是空间复杂度。另外我们脑还有脉冲来编码，引进了时间的因素，我们还利用了时空复杂度，所以我们设想保持现在的计算机所有的优点，保持时间复杂度的同时，增加一块类脑芯片，即增加了空间复杂度。如果我们以这种观点来看现在的技术就会发现，现在的神经网络加速器，是面向深度人工神经网络，利用的就是空间复杂度；而向脑一样工作的神经形态计算，面向的是脉冲神经网络，利用的是时空复杂度，那何不把两种结合起来？这就是2019年 8 月 1 日发表在《Nature》上的“天机芯”类脑计算架构，即融合脑科学与计算机科学的异构融合类脑计算架构。[8]

、



图9：基于空间复杂性和时空复杂性芯片路线


03

天机芯—融合脑科学

与计算机科学的新型类脑计算架构



图10：天机芯片单片（左）和5x5阵列扩展板（右）

施路平介绍说，深度学习加速器 (支持人工神经网络) 和类脑芯片(支持脉冲神经网络)已成为当前人工智能芯片代表性工作的两个主流方向，但由于算法和模型的巨大差别，当前人工智能芯片均只支持人工神经网络或者脉冲神经网络，难以发挥计算机和神经科学两个领域的交叉优势。“天机芯”作为世界首款异构融合类脑芯片，采用了多层次异构融合的类脑计算芯片架构，用现有计算机处理结构化信息，采用类脑计算处理非结构化信息，整体设计既体现了脑与电脑的异构融合、计算与存储的异构融合，又体现编码了的多样性、精确与近似的统一等人脑思维特性，可同时高效运行计算机科学和神经科学导向的绝大多数神经网络模型，支持异构网络的混合建模，形成浑然一体的空域和时空域协调调度系统，既能降低能耗，提高速度，又能保持高准确度。相比于IBM的TrueNorth 芯片，TrueNorth的面积为 430平方毫米，内置 100 万个模拟神经元。“天机芯”的面积为14.4平方毫米，内置4万个神经元，密度提升了20%，速度提高至少10倍，带宽提高至少100倍，能更好地满足人工智能计算的需求。



图11：天机异构融合类脑计算架构

基于“天机芯”的处理能力，清华类脑研究中心研制了一辆自主行驶自行车。这辆自行车借助模型和算法，不仅可以识别语音指令、实现自平衡控制，还能对前方行人进行探测和跟踪，并自动过障、避障。这辆自行车的语音识别使用的是脉冲神经网络，目标和障碍探测使用的是机器学习中的卷积神经网络。施路平介绍说，“传统的人工智能芯片只能支持单一类别的模型，很难要让这些多模态的模型实时高效交互”。兼容多种算法后，芯片不仅可以提高信息处理速度，还能大幅度降低功耗。同时，仅用一个芯片，就可以在自主行驶自行车系统中同时处理多种算法和模型，实现实时目标检测、跟踪、语音控制、避障和平衡控制，可以为学界提供了一个发展人工通用智能的平台和思路。施路平表示，目前只是初步的尝试。“天机芯”还有非常大的优化改进空间，团队正在为此而努力，希望相关研究能为面向人工通用智能计算平台的进一步发展起到促进作用。



图12：清华类脑研究中心研制基于“天机芯”的自主行驶自行车

同时，施路平也回顾了“天机芯”计算架构研究过程中遇到的挑战。施路平说，做类脑计算研究，真正的挑战既不是科学也不是技术，而是如何用多学科融合推动研究。因为它的跨度很大，从计算机转到脑科学，完全是“两条道上跑的车”。比如搞计算机的人研究深度学习，虽然到现在也不理解为什么它这么有用，但是深度学习确实能够解决问题，就可以用它来开公司、解决问题、赚钱；搞脑科学的人则不同，他们的研究道路，是用最先进的仪器发现新现象、拿诺贝尔奖等。换句话说，两者属于不同的文化、不同的语言、不同的目标，所以很难强行合在一起。因此，施路平在组建清华大学类脑计算研究中心时组合了脑科学、计算机、微电子、电子、精仪、自动化、材料等七个院系形成团队，而且七年时间就做了一件事情就是“融合，融合，再融合”。



图13：清华大学类脑计算研究中心组织结构

后记：类脑研究的发展与前景

关于未来的打算，施路平表示他们会做一个“类脑云脑”。关于“类脑云脑“与云计算的差别，施路平解释说，云计算是把很多的技术整合起来，而“类脑云脑”是面向人工通用智能的。因为大家知道，人工通用智能的研究，从根本上来讲，不同于把很多的窄人工智能加在一起。其主要研究思路是把类脑的弹性和计算机的刚性结合起来，把数据驱动和知识推动结合起来，把通用知识和推理结合起来，当然这是一个非常有挑战的长期的研究。但施路平坚信，在碳基上已经发展出来现有人类智能，基于硅芯片已经发展出强大的机器智能，一旦实现人类的全脑解析，采用类脑计算构建人工通用智能是完全没有障碍的。[9]


参考文献

[1]. AlphaGoZero横空出世，人机围棋大战正式收官：以后不会再有这样的战争了https://36kr.com/p/5098333

[2]. “AI首次在辩论上赢了人类高手，但“舌头”还不够灵活”https://www.roboticschina.com/news/2018062017AI.html

[3]. JohnL. Hennessy and David A. Patterson. 2019. A new golden age for computerarchitecture. Commun. ACM 62, 2 (January 2019), 48-60. DOI: https://doi.org/10.1145/3282307

[4]. B VBenjamin,…,K Boahen, Neurogrid: A Mixed-Analog-Digital Multichip System forLarge-Scale Neural Simulations,Proceedings of the IEEE, vol 102, no 5, pp699-716, 2014.

[5]. S B Furber et al. TheSpiNNaker project. Proc. IEEE 102 652–65, 2014

[6]. K. Meier. A mixed-signal universalneuromorphic computing system. 2015 IEEE International Electron Devices Meeting(IEDM), pp. 4.6.1-4.6.4, 2015.

[7]. P A Merolla et al. A million spiking-neuronintegrated circuit with a scalable communication network and interface. Science345 668–73, 2014.

[8]. Pei, J., Deng, L., Song, S. et al. Towardsartificial general intelligence with hybrid Tianjic chip architecture. Nature572, 106–111 (2019) doi:10.1038/s41586-019-1424-8

[9]. 清华施路平：碳基上能够实现的智能，硅基上也一定能实现https://new.qq.com/omn/20191104/20191104A0BD9U00.html"
清华“天机”芯片登上Nature封面！七大院系参与研发，全球首款异构融合类脑芯片,2019-08-03 17:20:46,"今天，一辆来自清华的无人驾驶自行车登上了Nature的封面。



这辆自行车不仅可以平衡自身，还可以绕过障碍物，甚至可以响应简单的声音命令。





自行车能够按照声音命令改变方向或调整速度




自行车检测并跟踪移动的人，并在必要时避开障碍物


这辆自行车能够如此平衡、顺利的自主运行，靠的是自行车背后的大脑。它采用了一种名为“天机（Tianjic）”的新型计算机芯片，用于实时物体检测，跟踪，语音识别，避障和平衡控制。



这只来自清华的团队也凭借Tianjic芯片，登上了8月1日发布的最新一期Nature封面。



这也是中国的人工智能芯片，首次登上Nature。







论文的通讯作者、清华大学精密仪器系教授、类脑计算中心主任施路平教授表示，虽然这还是非常初步的一个研究，但或许能够推动通用人工智能（AGI）计算平台的进一步发展。





本次的论文作者来自清华大学、北京灵汐科技、北京师范大学、新加坡理工大学和美国加州大学圣塔芭芭拉分校等机构。



论文链接：

https://www.nature.com/articles/s41586-019-1424-8





计算机科学+神经科学双导向，构建更普遍的通用平台



开发通用人工智能有两种主要方法：一种是计算机科学导向，植根于神经科学，并试图构建与大脑非常相似的电路。另一种是和神经科学导向。以计算机科学为基础，并使用计算机来执行机器学习算法。



因为在制剂和编码方案的基本差异，这两种方法依赖于不同的和不兼容的平台，延缓了AGI的发展。



因此，通用人工智能的发展亟待一个支持更普遍的、基于计算机科学的人工神经网络以及神经科学启发的模型和算法的通用平台。





两种方法的结合促进 AGI发展


这款天机（Tianjic）芯片则集成了两种方法，以提供混合、协同平台。



天机芯片采用多核架构，可重构构建模块和采用混合编码方案的流线型数据流，不仅可以适应基于计算机科学的机器学习算法，还可以轻松实现脑启动电路和多种编码方案。





“天机”芯片示意图


通过资源复用，天机芯片只需百分之三的额外面积即可同时运行计算机科学和神经科学导向的绝大多数神经网络模型，支持异构网络的混合建模，形成时空域协调调度系统，发挥它们各自的优势，既能降低能耗，提高速度，又能保持高准确度。



天机芯片同时具有多个功能核心，可轻松地重新配置，使其能够适应机器学习算法和大脑启发电路。研究人员通过将其中一个芯片整合到无人驾驶的自行车中来证明这种方法的潜力，这种自行车可以实现自我平衡，通过语音控制并且可以检测和避开障碍物。





芯片评估建模示例


在论文中，该团队表示，仅使用一个芯片，就可以在无人驾驶自行车系统中同时处理多种算法和模型，实现实时物体检测、跟踪、语音控制、避障和平衡控制。





基于天机芯片的自动驾驶自行车多模态集成示例图


跨学科组队，七个院系共同参与，七年磨一“芯”



在7月30日的电话新闻发布会中，论文通讯作者、清华大学精密仪器系教授施路平介绍了论文的研究思路。研究团队在接受媒体的采访时表示，从2012年孕育这项研究开始，团队遇到的最大挑战不来自于科学、也不来自技术，而是在于学科的分布不利于解决当前的问题。



因此，研究团队成立了七个院系组成的类脑计算研究中心，覆盖脑科学、计算机、微电子、电子、精仪、自动化、材料等学科。



团队成员之一，加州大学圣塔芭芭拉分校博士后邓磊表示，在芯片方面，遇到的最大挑战是如何实现深度和高效的融合。计算机科学导向和神经科学导向是目前流行的两类神经网络模型，这两种模型的语言、计算原理、信号编码方式、应用场景都有很大不同，所以需要的计算架构和存储架构大相径庭，甚至设计的优化目标都很不一样。一些深度学习加速器和神经形态芯片，基本上都是独立的设计体系，因此深度融合并不简单。



深度融合不是深度学习加速模块和神经形态模块简单的拼合，难点在于每部分的比例难以确认，因为现实中的应用复杂多变。而且，如果构建异构的混合模型，可能还需要在两个模块之间添加专门的信号转换单元，这又会有很多额外成本，所以，如何设计一套芯片架构兼容两类模型，可以灵活的配置同时又具有高性能，是团队芯片设计中的一大挑战。



2015年，施路平团队设计出第一代“天机芯”，经不断改进设计，2017年第二代“天机芯”问世。相比于当前世界先进的IBM的TrueNorth 芯片，2017年流片成功的第二代“天机芯”密度提升20%，速度提高至少10倍，带宽提高至少100倍，灵活性和扩展性更好。



也正如MIT科技评论报道所说，“该芯片暗示了中国在开发自己的芯片设计能力方面取得的进展。中国研究人员表明，他们可以制造专门的AI芯片以及任何芯片。”



类脑可以超越人脑吗？



其实早在3年前，谷歌就曾在愚人节那天发布过一辆理想中的自动驾驶自行车。



在谷歌的想象中，这辆“自行车”不仅平衡力超高?







还能够自动通过红绿灯路口，自主导航找到你的位置。







但作为“愚人节视频“发布，也说明了这一技术的难点和不易实现。



而在今天，清华施路平团队终于初步实现了这一想象。这样的黑科技似乎也带着一丝科幻色彩，让人畅想AGI到来的那天。



在接受采访时，施路平教授表示，类脑能否超越人脑的问题，其实和电脑是否能超越人脑的问题类似。



电脑在某些方面其实早就超过了人类，其精准快速的运算能力、强大的记忆让我们叹为观止。然而，目前在很多智能的层次，计算机和人脑还是有相当大的距离。 特别是对于不确定性的问题，比如学习、自主决策等领域。



计算机会逐渐缩小差距，至于最后能否全面超过人脑，施路平教授觉得从技术的层面来看会越来越多，“因为计算机的发展有一个特点，就是它从不退步，它一直往前走。但是我相信我们人是有智慧的，我们会在发展的过程当中来逐渐的完善我们对于研究领域的一个理解，来把控它的风险，因为我相信人们之所以对这个问题重视，是因为我们担心会不会像科幻电影说的那样毁灭人类。”



关于AGI是否会超越人类智慧，吴恩达在AI For Everyone课程中也表示，完全的AGI的出现可能还需要几十甚至上百年，从时间上来说，我们也不需要多度担心。



吴恩达在AI For Everyone课程中谈AGI发展"
数电基础-数字电路芯片种类,2023-01-11 10:23:53,"数字电路芯片主要用于计算机和逻辑控制领域，它的工作原理是通过晶体管控制电流的“开”和“关”，来表达数据或信息的“1”和“0”，或者表达逻辑判断的“是”与“非”，所以数字电路也称为开关电路或者逻辑电路。数字电路主要是由工作在开关状态的晶体管组成的。因此，数字电路的规模大小由其中的晶体管多少来分类。数字电路芯片主要包括以下7类。

1.逻辑电路(包括与门、或门、非门、锁存器、移位器、计数器、编码器、译码器、选择器、比较器、运算器等)：逻辑电路芯片国际通用系列有74系列、40系列、54系列、厂家兼容系列、非系列专用电路等。以74系列为例，它的功能型号超过97种，每种型号再加上输入输出数、电源、功耗、速度等不同，又可衍生出4倍以上的品种，总共多达400多个品种。这么多系列加起来，仅仅逻辑电路芯片就已经非常繁杂了。

但是，逻辑电路品种再多，基本上是由与门、或门和非门电路组合而成的，因此，上述系列的电路也称为组合逻辑电路。与门电路用于“几个输入条件同时存在才有结果，否则就无结果”的判断；或门电路用于“几个输入条件只要有一个存在就有结果，都不存在就无结果”的判断；非门电路用于“输入条件存在就无结果，输入条件不存在就有结果”的判断。这些判断和处理组合起来，就可以处理非常复杂的控制和运算问题。

理论上，数量庞大的逻辑电路芯片可以实现目前所有复杂芯片的功能，例如中央处理器(CPU)、微控制器(mcu)、片上系统(SoC)等，更甚者可以实现一个复杂系统的功能，例如电脑、交换机等。只是印刷电路板(PCB)上将要安装成千上万，甚至更多的芯片。早期的电子产品都是这么干的，但今天不需要这么干了。因为今天芯片的集成度很高，许多自成系统的逻辑电路可以集成在芯片内部，一个芯片可以实现很复杂的功能，例如CPU，也可以实现一个完整的系统，例如SoC。所以，今天没有人愿意用大量小芯片实现大系统，因为那样做出来的系统体积很大、可靠性很差、成本也很高。

今天的逻辑电路芯片用量已经不大了。就像盖房子可以全部用砖瓦，也可以用一些大型构件，辅助以少量砖瓦，砖瓦用量自然就少了。今天的逻辑电路芯片仅用在小电子产品中，或者用在大系统的通用大芯片之间的连接电路中。

2.通用处理器(CPU、GPU、DSP、APU等)：通用处理器是由海量逻辑电路组成的，它包含了控制、存储、运算、输入输出等部分，形成了一个完整的数据和信息处理系统。它是规模最大、结构最复杂的一类数字电路芯片。(按照通用处理器芯片上可以集成100多亿只晶体管计算，通用处理器大致包含了30多亿个与门、或门、非门电路)。因此，通用处理器被归类为巨大规模集成电路。

通用处理器芯片的特点是按照摩尔定律不断迭代，不断推陈出新，形成了若干个产品系列。例如Intel和AMD的X86系列、IBM的PowerPC系列、MIPS的嵌入式CPU系列和ARM RISC系列等数十个系列。每个系列已生产了20～30多个芯片型号，每个型号的市场平均寿命在2年左右。

通用处理器被称为电子产品和信息系统的大脑和中枢。中央处理器(CPU)用于管理、调度和控制电子产品和信息系统各组成部分协调高效工作；图像处理器(GPU)接受CPU管理，但可以独立管理、调度和控制有关图像显示、图形处理的事务；数字信号处理器(DSP)也接受CPU管理，但可以独立完成大量、成批和规整的数据和信息的快速运算和处理；随着人工智能技术快速发展，传统的CPU结构不能适应人工智能系统对信息存储、运算和推理的要求，新型处理器结构创新产品应运而生，百花齐放。这就是人工智能处理器(APU)，代表产品如 IBM公司的 TrueNorth、高通公司的Zeroth、谷歌公司的TPU、微软公司的Brainwave、寒武纪公司的Cambricon-1A、燧原科技的邃思DTU等。

3.存储器(SRAM、DRAM、PROM、Flash等)：存储器是用于存储数据和信息的芯片。其中，可细分为静态存储器(SRAM)、动态存储器(DRAM、LPDDRX)、可编程只读存储器(PROM)、闪速存储器(Flash)和嵌入式存储器(Embedded Memory)等。

SRAM用于电子产品中存储数据，在通电过程保持数据不变，断电后数据丢失；DRAM在通电过程中通过定时刷新保持数据不变，断电后数据丢失；Flash在通电过程中保持数据不变，断电也不丢失；PROM一旦用特殊手段写入数据后，不论通电与否都不会丢失。前两种称为易失性存储器，后两种称为非易失性存储器。以上这几种存储器可以封装成独立的存储器芯片，也可以设计在CPU、MCU、SoC之中,也被称为嵌入式存储器。

根据用途不同，要选择使用不同的存储器，例如，台式电脑断电后数据一般保存在硬盘上，所以大量采用DRAM(DDR、LPDDRX等)，而手机为了永久保持数据(如通信录、照片、音视频等)，就要大量使用Flash芯片。

4.单片系统(SoC)：单片系统就是把一个电子系统全部集成到一颗芯片中。只要给SoC芯片加上电源和少量外部电路，就可以实现一个完整的电子产品或系统的功能。例如音视频播放器(MP4)、汽车导航仪、手机等都可以用一个SoC芯片加少量外部元器件来实现。SoC芯片内部一般由CPU核、嵌入式存储器、I/O接口(按键、触控、USB、WiFi……)等部分组成。SoC芯片是面向具体应用领域而设计的专用系统级芯片，例如用在医疗设备、汽车电子、抄表系统、智能手机、智慧电视等领域，都有适合该领域应用的SoC芯片。SoC芯片不像CPU芯片那样可以跨领域通用，只能在本应用领域内使用。

5.微控制器(MCU)：微控制器通常也称为单板机或单片机，它是简化版的通用处理器(CPU)。简化体现在几个方面，包括处理字宽、处理器和指令架构、内存大小、时钟速度等。MCU一般用在较简单的、小型的电子产品或系统中，实现简单的控制和数据处理任务，但在大型系统中，也可以用许多MCU完成复杂的控制任务。MCU芯片的应用面十分广泛，从小到阳台定时浇花器、电饭锅、电冰箱等的控制，中到仪器、仪表、工业自动化生产线等的控制，再到大型应用例如高铁、飞机等的系统控制等。

以MCU或SOC芯片为核心搭建的电子系统也称为嵌入式系统，MCU和SOC也被称为嵌入式微处理器。

MCU的种类特别多，主要产品多达70 多个系列，500多个品种。例如MCS-51系列、PIC系列、STM32系列、MSP430系列、TMS系列、AVR系列、STC系列等。仅MCS-51系列，按一个机器周期有几个时钟周期T划分为几个规格，12T的芯片有8051、8031、AT89C51、8032等；6T的芯片有STC89系列等；4T的芯片有80C320、W77E58等；1T的芯片有STC系列等。同时又有不同的厂家和品牌，芯片型号非常多。

6.定制电路(ASIC) ：如果用户不想使用通用芯片，而是按自己的应用要求定制一款芯片，这种芯片就称为全定制芯片。二代身份证芯片就是典型的ASIC例子。有些整机厂商为自己的产品定制ASIC，避免采用通用芯片，一是为了保护产品的技术细节和诀窍，二是ASIC会更加适合自己产品的需要，三是只要产品能上量，就可以摊薄ASIC高昂的定制费用。

7.可编程逻辑器件(PLD) (包括PLD、PAL、GAL、FPGA等)：前面第1～6类芯片被称固定逻辑电路芯片，它们从代工厂生产出来后，功能就被固定下来，不能再进行任何大的改变。如果需要完善和升级，就要先修改设计，再交由代工厂重新生产。修改和重新生产的成本是很高的，只有需求量很大的芯片才按照固定逻辑电路的模式进行开发。需求数量少、有更新和升级可能的芯片，需要按照可编程逻辑器件的模式进行开发。

可编程逻辑器件(PLD)由工厂生产出来后，其功能还没有确定，需要设计人员按需求进行编程后，芯片才能表现出想要的功能。而且某些种类的PLD芯片还可以进行多次编程，十分适合要对芯片的功能进行完善和升级的应用场合，例如通信设备、移动通信基站等。

可编程逻辑器件在编程之前属于通用芯片，厂家可以批量生产，满足不同领域的应用需求。而在编程之后就变成了专用芯片，只满足某个具体领域的特殊应用。因此，PLD芯片也称为半定制芯片。

目前应用最广的是现场可编程门阵列(FPGA)，它特别适合用在用量不大，或者用量较大，但需要不断完善和升级的应用场合。它在通信、安防监控、自动控制、人工智能、军工与航天等领域，以及芯片设计的原型验证、算法与嵌入式系统开发等方面都有着广泛应用。

有人喜欢把ASIC和PLD进行对比，因为ASIC和编程后的PLD都是专用定制芯片。但它们有以下区别，一是前者是交由设计者和制造厂去定制，后者是自己编程定制；二是前者生产处理后，芯片功能不能改动，后者经编程后，芯片功能还可以完善和升级。三是前者生产定制代价很高，需要产品上量才行，后者定制成本很低，适合在小批量产品上使用。"
传统芯片与AI芯片,2022-02-14 06:04:03,"传统芯片与AI芯片
Ai芯片是现在热门的领域，与传统的芯片相比，算法和架构有着很大的差异，给市场带来的无限的创造空间，让很多的不可能变为可能。
Ai芯片特定的算法比传统芯片更具优势
Ai芯片是具有Al算法的专业芯片，传统的芯片来运行Ai算法，性能会很低，不具备处理能力。在手机终端，执行人脸识别、智能Ai美颜、语音识别，都必须要经过Ai算法开发的GPU来执行。Ai算法与传统芯片的算法有很大的区别，通过卷积、全连接、网络残差这些类型，再用加法和乘法运算，如果确定了运算图形的尺寸，可以确定运算的总数。、
在这里插入图片描述

Ai芯片NPU单元功能强大，需要大量数据支撑
Ai芯片内置了网络载体NPU，每秒的运算速度达到了1000万次，比传统芯片快30倍，处理图片的速度每分钟2000张，普通芯片只能处理90张。因为Ai芯片就是应用在数据中心的云端和消费者的终端，对数据有很高的要求，通过大量的数据运算来完成各种任务。
在这里插入图片描述

Ai芯片比传统芯片更加智能化，模拟人脑运算机制
传统的芯片应用的软件都是程序编程，有固定的运行模式，通过执行指令来计算。Ai芯片是模仿人脑的神经，基本的控制系统模拟了人脑运行机制，不用人为解决运算能力而编写大量固定的程序。传统CPU计算是指令的模式，需要上千条指令来完成，Ai芯片只要一个指令就可以完成运算任务。
Ai芯片更加智能化的运算能力颠覆了传统芯片的功能，取代传统芯片的地位，让Ai物联网、人工智能得到更好的发展。
AI芯片系统架构
下面是这个问题的回答核心啦。
传统CPU结构中，除了数据运算，还需要执行数据的存储与读取、指令分析、分支跳转等命令。AI算法通常需要对海量数据进行处理，使用CPU执行算法，将会花费大量的时间，在数据指令的读取分析上，计算效率非常低。
在这里插入图片描述

随着AI产业的发展，业界出现了4种AI芯片架构。以冯·诺依曼传统计算架构为基础，用于加速硬件计算能力为主，有GPU、FPGA、ASIC 3种类型为代表，另外是颠覆冯·诺依曼架构，采用类脑神经结构独立设计，提升计算能力。下面详细展开4种不同类型的架构。
在这里插入图片描述

第一种是GPU，通用的图像处理单元。GPU采用SIMD单指令多数据流的方式，也就是一条指令操作多个数据，拥有数量众多的计算单元和超长的图形图像处理流水线，刚发明的时候主要处理图像领域的并行加速运算，由于GPU内部，大部分的晶体管可以组成各类专用电路、多条流水线，使得GPU的计算速度远高于CPU，并拥有了更加强大的浮点运算能力，可以缓解深度学习算法的训练难题，释放AI潜能，广泛使用在深度学习算法领域。值得注意的一点是，GPU缺乏复杂的运算逻辑单元，必须由CPU进行调度。
在这里插入图片描述

英伟达作为GPU巨头，占据了七成的GPU和AI市场份额。在近几年的GTC大会上，CEO黄仁勋满嘴都是AI，可见AI对GPU发展的重要性不言而喻。
在这里插入图片描述

第二种是FPGA，通俗地理解就是，可以把硬件设计重复烧写在可编程存储器里面，使FPGA芯片可以执行不同的硬件设计和功能，所以被叫做「现场可编程逻辑阵列」。FPGA将指令锁定在硬件架构上，然后使用硬件指令流运行数据，简单理解就是将AI的计算架构用硬件电路实现出来，然后持续的将数据流输入系统，并完成计算。与GPU不同的是，FPGA可以同时拥有硬件流水线并行和数据并行处理能力，适用于以硬件流水线方式处理数据流，因此非常适用于AI推理阶段，相对于CPU与GPU有明显的性能或者能耗优势。
在这里插入图片描述

目前使用FPGA用于设计AI芯片国内有深鉴科技、微软Catapult项目。深鉴科技在2018年，以3亿美元的价格卖给FPGA巨头赛灵思。
由于FPGA编程难度大，对开发者要求高，于是出现了ASIC，专用集成电路，主要是实现AI特定算法，要求定制的芯片。所谓定制，就是专门为AI算法，设计出来的架构，可以有助于提高芯片性能和功耗比，缺点是电路设计定制，导致相对开发周期长，不能扩展以外，好处就是在功耗、可靠性、芯片体积、性能等方面都有巨大的优势。
在这里插入图片描述

自从2016年Google发布基于ASIC架构的第一代TPU后，华为的昇腾系列芯片、寒武纪、比特大陆、地平线等厂商纷纷入局，即使AI算法发展迅猛，但是基于ASIC的AI芯片仍然是今天的主流。
有人说，真正的人工智能芯片，未来发展的方向，会不会是类脑芯片呢？最后来谈一下，类脑芯片展，类脑芯片直接基于神经形态架构设计，用于模拟人脑功能进行感知方式、行为方式和思维方式的计算。但是研发难度巨大。
2014年，IBM推出第二代TrueNorth芯片，采用28nm工艺技术，里面包括了54亿个晶体管和 4096个处理核，相当于100万个可编程神经元，以及2.56亿个可编程突触，芯片的工作方式类似于人脑的神经元和突触之间的协同。
在这里插入图片描述

AI芯片的产业链
现在聊到AI芯片，肯定离不开AI芯片在产业链中的位置。从芯片的整体产业环节来看，最上游的是芯片设计，中游是制造和封测，最后是下游的系统集成及应用。但是具体是怎么分工的吗？AI芯片产业链环节中，这里以华为昇腾AI产业为例。
在这里插入图片描述

首先是上游，昇腾910芯片采用的是ASIC专用集成电路，基于达芬奇架构来实现的，达芬奇架构这款IP呢，则是通过华为海思设计的，所以海思是达芬奇这款IP的设计商。
设计完之后，就到中游，分别是AI芯片的晶圆制造和封装测试，但晶圆不仅是在封装时测试，制造后会有一次测试，封装后再进行一次。而现在大部分芯片的制造都依赖于台湾的TSMC，就是大名鼎鼎的台积电，还有中芯国际SIMC等芯片制造商。
最后是AI产业的下游，下游主要为系统集成及应用为主，华为昇腾AI产业作为AI系统集的主要集成商提供昇腾Atlas服务器，然后联合开发商，也就是俗称的ISV，来提供上层的AI解决方案。
AI芯片未来发展趋势
最后就是AI芯片的发展趋势啦，无论是华为昇腾产品的达芬奇架构、英伟达的Tensor Core、还是Google，深度学习需要海量数据进行计算，内存带宽的制约，已经成为整个系统的性能瓶颈。第二个就是海量内存和计算单元，访问频繁切换，导致整体功耗很难降下去。最后便是随着AI产业的快速变化，硬件如何适配算法是个难题。
不妨来预测一下AI芯片的4大发展趋势。
未来10年是加速计算架构变革的新十年。在计算存储一体化方面，把计算单元和存储单元放在一起，使得AI系统的计算和数据吞吐量增大，还能显著地降低功耗。会不会出现一种新型非易失性存储器件，就是在存储阵列里面加上AI计算功能，节省数据搬移操作呢？现在硬件计算能力大于数据读取和访问性能，当计算单元不在是瓶颈，如何减少存储器的访问延时，将会成为下一个研究方向。
通常，离计算越近的存储器速度越快，每字节的成本也越高，同时容量也越受限，因此新型的存储结构将应运而生。
在这里插入图片描述

第二个趋势就是，稀疏化计算。随着千亿、到万亿网络模型提出，模型越来越大，但实际不是每个神经元，都能有效激活，这个时候稀疏计算，可以高效减少无用能效。特别是在推荐场景和图神经网络的应用，稀疏已经成为常态。
例如，哈佛大学就该问题提出了优化的五级流水线结构，在最后一级输出了触发信号。在Activation层后对下一次计算的必要性进行预先判断，如果发现这是一个稀疏节点，触发 SKIP信号，避免乘法运算的功耗，以达到减少无用功耗的目的。
在这里插入图片描述

第三个趋势是支持更加复杂的AI算子。在标准的SIMD基础上，CNN的特殊结构复用，可以减少总线的数据通讯，Transformer结构对大数据在计算和存储之间切换，或者是在NLP和语音领域经常需要支持的动态shape，都需要合理地分解、映射这些不同复杂结构的算子，到有效的硬件上成为了一个值得研究的方向。
最后就是更快的推理时延和存储位宽。随着苹果、高通、华为都在手机芯片SoC上面推出AI推理硬件IP，近年来在手机SoC上，又引入可学习功能。未来如何在手机SoC上执行更快是业界很关注的一个点，包括经常看视频的抖音、bilibili，都需要对视频进行AI编解码，基于ISP进行AI影像处理。另外在理论计算领域，神经网络计算的位宽从32bit到16bit，出现了混合精度到目前8bit，甚至更低的比特数，都开始慢慢进入实践领域。
AI芯片，最后决定成败的因素会什么呢？应该选择，英伟达的GPU的硬件架构、华为达芬奇架构、Google TPU的脉动阵列架构呢？
总的来说，在ZOMI的观点来看，芯片架构的选择应该服务于整个芯片项目的成功，是很多因素博弈的结果。英伟达今天能在AI领域占据着头部市场，是归功于底层硬件架构，还是完善的软硬件生态呢？这个问题呀，觉得大家应该看得清楚。"
"专用神经网络处理器芯片,神经网络芯片的单片机",2022-08-26 16:54:06,"TCL电视神经网络处理器是什么?
npu。1、tcl电视专攻NPU（神经网络处理器）的海思Hi3516DV300芯片是神经网络处理器。2、神经网络处理器，也就是通常说的AI处理器。

它可以是手机更聪明，可以学习用户的用机习惯，在拍摄照片的时候，可以根据被拍摄物体匹配最适合的相机参数。

谷歌人工智能写作项目：神经网络伪原创



手机芯片NPU是什么？
嵌入式神经网络处理器（NPU）采用“数据驱动并行计算”的架构，特别擅长处理视频、图像类的海量多媒体数据好文案。

简介cpu=centerprocessingunitsnpu=neural-networkprocessingunitsnpu不是测试的项目，是网络处理器，可以把这个认为是一个组件（或者子系统），有的时候也可以称呼为[2]NPU协处理器。

嵌入式神经网络处理器（NPU）采用“数据驱动并行计算”的架构，特别擅长处理视频、图像类的海量多媒体数据。

推进过程2016年6月20日，中星微数字多媒体芯片技术国家重点实验室在北京宣布，已研发成功了中国首款嵌入式神经网络处理器（NPU）芯片，成为全球首颗具备深度学习人工智能的嵌入式视频采集压缩编码系统级芯片，并取名“星光智能一号”。

这款基于深度学习的芯片运用在人脸识别上，最高能达到98%的准确率，超过人眼的识别率。该芯片于今年3月6日实现量产，目前出货量为十几万件。

该实验室执行主任、中星微首席技术官张韵东在接受采访时表示，装备了神经网络处理器的芯片应用在监控摄像头上，摄影头由“眼睛”升级为“带有大脑的眼睛”，这是全球首次。

“数字多媒体芯片技术”国家重点实验室成立于2010年，依托于北京中星微电子有限公司，由科技部批准建立。

据资料显示，中星微电子有限公司于1999年由原国家信息产业部直接投资创立，是专攻芯片技术的公司中的“国家队”，其研发的“星光系列芯片”曾打破国际市场上无“中国芯”的局面。

人工智能的落地　“星光智能一号”是一款嵌入式NPU。神经网络处理器NPU（NeuralProcessingUnit）还未被熟知，却是芯片领域热门的技术。

它与冯诺依曼架构中的CPU处理器相对，采用的是“数据驱动并行计算”这种颠覆性的新型架构。

如果将冯诺依曼架构处理数据的方式类比成单车道，那么“数据驱动并行计算”是128条多车道并行，可以同时处理128个数据，利于处理视频、图像类的海量多媒体数据。

在业内，单位功耗的计算性能，也就是性能功耗比，被用来衡量处理器架构的优劣。

据该实验室执行主任、中星微首席技术官张韵东介绍，“星光智能一号”的性能功耗比在传统的冯诺依曼架构上“至少提高了两三个数量级”，也就是几百倍。高功耗是很多顶尖人工智能技术被诟病的。

IBM20世纪的“深蓝”和谷歌2016的AlphaGo因其需要由巨大的数据计算支撑，前者使用超级计算机，后者使用服务器集群，无法脱离恒定温度和湿度的机房。

AlphaGo下一盘棋光电费就需要3000美元。张韵东将它们称之为“一场科学实验”，离技术落地、投入应用还有较远的距离。这凸显了嵌入式NPU的小型化、低功耗和低成本优势，加快人工智能技术落地应用。

例如无人机对摄像头的重量和功耗有很高的要求，否则会影响起飞和续航能力。

而“星光智能一号”只有普通邮票大小，重量仅几十克，它的诞生让诸多监控摄像头等小型设备有了人工智能化的可能，迈出了人工智能从神秘的机房，跨向生活应用的一步。

手机的CPU GPU NPU各是什么？
你好，很高兴为你解答:1.中央处理器（centralprocessingunit，简称CPU）作为计算机系统的运算和控制核心，是信息处理、程序运行的最终执行单元。

CPU自产生以来，在逻辑结构、运行效率以及功能外延上取得了巨大发展。

2.图形处理器（英语：graphicsprocessingunit，缩写：GPU），又称显示核心、视觉处理器、显示芯片，是一种专门在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上做图像和图形相关运算工作的微处理器。

GPU使显卡减少了对CPU的依赖，并进行部分原本CPU的工作，尤其是在3D图形处理时GPU所采用的核心技术有硬件T&L（几何转换和光照处理）、立方环境材质贴图和顶点混合、纹理压缩和凹凸映射贴图、双重纹理四像素256位渲染引擎等，而硬件T&L技术可以说是GPU的标志。

GPU的生产商主要有NVIDIA和ATI。3.嵌入式神经网络处理器（NPU）采用“数据驱动并行计算”的架构，特别擅长处理视频、图像类的海量多媒体数据。

麒麟810处理器怎么样
麒麟810处理器不错的。

麒麟810芯片采用业界领先的7nm工艺制程，旗舰级的A76大核，在应用启动/图库滑动及各种应用体验及游戏体验上均优于麒麟970芯片，并且由于7nm制程带来的功耗优势，续航体验更好。

并且麒麟810集成华为自研的神经网络处理器（NPU），各种AI应用体验更好更快。

新型神经网络芯片会对科技领域乃至整个世界产生什么巨大影响
一、与传统计算机的区别1946年美籍匈牙利科学家冯·诺依曼提出存储程序原理，把程序本身当作数据来对待。

此后的半个多世纪以来，计算机的发展取得了巨大的进步，但“冯·诺依曼架构”中信息存储器和处理器的设计一直沿用至今，连接存储器和处理器的信息传递通道仍然通过总线来实现。

随着处理的数据量海量地增长，总线有限的数据传输速率被称为“冯·诺依曼瓶颈”——尤其是移动互联网、社交网络、物联网、云计算、高通量测序等的兴起，使得‘冯·诺依曼瓶颈’日益突出，而计算机的自我纠错能力缺失的局限性也已成为发展障碍。

结构上的缺陷也导致功能上的局限。例如，从效率上看，计算机运算的功耗较高——尽管人脑处理的信息量不比计算机少，但显然而功耗低得多。

为此，学习更多层的神经网络，让计算机能够更好地模拟人脑功能，成为上世纪后期以来研究的热点。

在这些研究中，核心的研究是“冯·诺依曼架构”与“人脑架构”的本质结构区别——与计算机相比，人脑的信息存储和处理，通过突触这一基本单元来实现，因而没有明显的界限。

正是人脑中的千万亿个突触的可塑性——各种因素和各种条件经过一定的时间作用后引起的神经变化（可变性、可修饰性等），使得人脑的记忆和学习功能得以实现。

大脑有而计算机没有的三个特性:低功耗（人脑的能耗仅约20瓦，而目前用来尝试模拟人脑的超级计算机需要消耗数兆瓦的能量）；容错性（坏掉一个晶体管就能毁掉一块微处理器，但是大脑的神经元每时每刻都在死亡）；还有不需为其编制程序（大脑在与外界互动的同时也会进行学习和改变，而不是遵循预设算法的固定路径和分支运行。

）这段描述可以说是“电”脑的最终理想了吧。注：最早的电脑也是模拟电路实现的，之后发展成现在的只有0、1的数字CPU。

今天的计算机用的都是所谓的冯诺依曼结构，在一个中央处理器和记忆芯片之间以线性计算序列来回传输数据。这种方式在处理数字和执行精确撰写的程序时非常好用，但在处理图片或声音并理解它们的意义时效果不佳。

有件事很说明问题：2012年，谷歌展示了它的人工智能软件在未被告知猫是什么东西的情况下，可以学会识别视频中的猫，而完成这个任务用到了1.6万台处理器。

要继续改善这类处理器的性能，生产商得在其中配备更多更快的晶体管、硅存储缓存和数据通路，但所有这些组件产生的热量限制了芯片的运作速度，尤其在电力有限的移动设备中。

这可能会阻碍人们开发出有效处理图片、声音和其他感官信息的设备，以及将其应用于面部识别、机器人，或者交通设备航运等任务中。

神经形态芯片尝试在硅片中模仿人脑以大规模的平行方式处理信息：几十亿神经元和千万亿个突触对视觉和声音刺激物这类感官输入做出反应。

作为对图像、声音等内容的反应，这些神经元也会改变它们相互间连接的方式，我们把这个过程叫做学习。神经形态芯片纳入了受人脑启发的“神经网路”模式，因此能做同样的事。

人工智能的顶尖思想家杰夫·霍金斯（JeffHawkins）说，在传统处理器上用专门的软件尝试模拟人脑（谷歌在猫实验中所做的），以此作为不断提升的智能基础，这太过低效了。

霍金斯创造了掌上电脑（PalmPilot），后来又联合创办了Numenta公司，后者制造从人脑中获得启发的软件。“你不可能只在软件中建造它，”他说到人工智能，“你必须在硅片中建造它。

”现有的计算机计算，程序的执行是一行一行执行的，而神经网络计算机则有所不同。现行的人工智能程式，基本上都是将大大小小的各种知识写成一句一句的陈述句，再灌进系统之中。

当输入问题进去智能程式时，它就会搜寻本身的资料库，再选择出最佳或最近解。2011年时，IBM有名的Watson智能电脑，便是使用这样的技术，在美国的电视益智节目中打败的人类的最强卫冕者。

（神经网络计算机）以这种异步信号发送（因没有能使其同步的中央时钟而得名）处理数据的速度比同步信号发送更快，以为没有时间浪费在等待时钟发出信号上。

异步信号发送消耗的能量也更少，这样便满足了迈耶博士理想的计算机的第一个特点。如果有一个处理器坏了，系统会从另一路线绕过它，这样便满足了迈耶博士理想的计算机的第二个特点。

正是由于为异步信号发送编程并不容易，所以大多数计算机工程师都无视于此。然而其作为一种模仿大脑的方式堪称完美。

功耗方面：硬件方面，近年来主要是通过对大型神经网络进行仿真，如Google的深度学习系统GoogleBrain，微软的Adam等。但是这些网络需要大量传统计算机的集群。

比方说GoogleBrain就采用了1000台各带16核处理器的计算机，这种架构尽管展现出了相当的能力，但是能耗依然巨大。而IBM则是在芯片上的模仿。

4096个内核，100万个“神经元”、2.56亿个“突触”集成在直径只有几厘米的方寸（是2011年原型大小的1/16）之间，而且能耗只有不到70毫瓦。

IBM研究小组曾经利用做过DARPA的NeoVision2Tower数据集做过演示。

它能够实时识别出用30帧每秒的正常速度拍摄自斯坦福大学胡佛塔的十字路口视频中的人、自行车、公交车、卡车等，准确率达到了80%。

相比之下，一台笔记本编程完成同样的任务用时要慢100倍，能耗却是IBM芯片的1万倍。

Ref:Amillionspiking-neuronintegratedcircuitwithascalablecommunicationnetworkandinterface.PaulA.Merollaetal.Science345,668(2014);DOI:10.1126/science.1254642因为需要拥有极多数据的Database来做training以及需要极强大的计算能力来做prediction，现有的一些Deeplearning如AndrewNg的GoogleBrain、Apple的Siri等都需要连接网络到云端的服务器。

二、争议：虽然深度学习已经被应用到尖端科学研究及日常生活当中，而Google已经实际搭载在核心的搜寻功能之中。但其他知名的人工智能实验室，对於深度学习技术的反应并不一致。

例如艾伦人工智慧中心的执行长OrenEtzioni，就没有考虑将深度学习纳入当前开发中的人工智慧系统中。

该机构目前的研究是以小学程度的科学知识为目标，希望能开发出光是看学校的教科书，就能够轻松应付各类考试的智能程式。

OrenEtzioni以飞机为例，他表示，最成功的飞机设计都不是来自於模仿鸟的结构，所以脑神经的类比并无法保证人工智能的实现，因此他们暂不考虑借用深度学习技术来开发这个系统。

但是从短期来看，情况也许并没有那么乐观。首先芯片的编程仍然是个大问题。芯片的编程要考虑选择哪一个神经元来连接，以及神经元之间相互影响的程度。

比方说，为了识别上述视频中的汽车，编程人员首先要对芯片的仿真版进行必要的设置，然后再传给实际的芯片。

这种芯片需要颠覆以往传统的编程思想，尽管IBM去年已经发布了一套工具，但是目前编程仍非常困难，IBM团队正在编制令该过程简单一点的开发库。

（当然，如果我们回顾过去编程语言从汇编一路走来的历史，这一点也许不会成为问题。）其次，在部分专业人士看来，这种芯片的能力仍有待证实。

再者，真正的认知计算应该能从经验中学习，寻找关联，提出假设，记忆，并基于结果学习，而IBM的演示里所有学习（training）都是在线下的冯诺依曼计算机上进行的。

不过目前大多数的机器学习都是离线进行的，因为学习经常需要对算法进行调整，而IBM的硬件并不具备调整的灵活性，不擅长做这件事情。

三、人造神经元工作原理及电路实现人工神经网络人工神经网络（artificialneuralnetwork，缩写ANN），简称神经网络（neuralnetwork，缩写NN），是一种模仿生物神经网络的结构和功能的数学模型或计算模型。

神经网络是一种运算模型，由大量的节点（或称“神经元”，或“单元”）和之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activationfunction）。

每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重（weight），这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。

而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。Ref：Wikipedia：人工神经网络电路原理神经递质的分泌反过来又是对动作电位刺激的反应。

然而神经元在接收到这些神经递质信号中的一个后便不会再继续发出动作电位。当然，它们会逐渐累加至一个极限值。

在神经元接受了一定数量的信号并超过极限值后----从根本上讲是一个模拟进程----然后它们会发出一个动作电位，并自行重置。

Spikey的人造神经元也是这么做的，当它们每次受到激发时都会在电容中累积电荷，直至达到限值，电容再进行放电。具体电路结构和分析之后有机会的话再更新。

现阶段硬件的实现方式有数电（IBM、Qualcomm）、模电、数模混合（学界）、GPUs等等，还有各种不是基于硅半导体制程制作的神经元等的device方面的研究。

四、历史Neuromorphicengineering由老祖宗CarverMead提出卡福·米德是加州理工学院的一名工程师，被公认为神经形态计算机之父（当然还发明了“神经形态学”这个词）神经形态芯片的创意可以追溯到几十年前。

加州理工大学的退休教授、集成电路设计的传奇人物卡弗·米德（CarverMead）在1990年发表的一篇论文中首次提出了这个名称。这篇论文介绍了模拟芯片如何能够模仿脑部神经元和突触的电活动。

所谓模拟芯片，其输出是变化的，就像真实世界中发生的现象，这和数字芯片二进制、非开即关的性质不同。后来这（大脑研究）成为我毕生的工作，我觉得我可以有所贡献，我尝试离开计算机行业而专注大脑研究。

首先我去了MIT的人工智能研究院，我想，我也想设计和制作聪明的机器，但我的想法是先研究大脑怎么运作。而他们说，呃，你不需要这样做，我们只需要计算机编程。而我说，不，你应该先研究大脑。

他们说，呃，你错了。而我说，不，你们错了。最后我没被录取。但我真的有点失望，那时候年轻，但我再尝试。几年后再加州的Berkley，这次我尝试去学习生物方面的研究。我开始攻读生物物理博士课程。

我在学习大脑了，而我想学理论。而他们说，不，你不可以学大脑的理论，这是不可以的，你不会拿到研究经费，而作为研究生，没有经费是不可以的。我的天。

八卦：老师说neuralnetwork这个方向每20年火一次，之前有很长一段时间的沉寂期，甚至因为理论的不完善一度被认为是江湖术士的小把戏，申请研究经费都需要改课题名称才能成功。

（这段为小弟的道听途说，请大家看过就忘。后来看相关的资料发现，这段历史可能与2006年GeoffreyE.Hinton提出深度学习的概念这一革命性工作改变了之前的状况有关。

）五、针对IBM这次的工作：关于SyNAPSE美国国防部先进研究项目局的研究项目，由两个大的group组成：IBMteam和HRLTeam。

Synapse在英文中是突触的意思，而SyNAPSE是SystemsofNeuromorphicAdaptivePlasticScalableElectronics的简称。

Cognitivecomputing:NeurosynapticchipsIBMproducesfirstworkingchipsmodeledonthehumanbrain另一个SyNAPSE项目是由IBM阿尔马登实验室（位于圣何塞）的达尔门德拉·穆德哈负责。

与四所美国大学（哥伦比亚大学，康奈尔大学，加州大学默塞德分校以及威斯康辛-麦迪逊大学）合作，穆德哈博士及其团队制造了一台神经形态学计算机的原型机，拥有256个“积分触发式”神经元，之所以这么叫是因为这些神经元将自己的输入累加（即积分）直至达到阈值，然后发出一个信号后再自行重置。

它们在这一点上与Spikey中的神经元类似，但是电子方面的细节却有所不同，因为它们是由一个数字储存器而非许多电容来记录输入信号的。"
专家谈计算神经科学与类脑人工智能的关系,2022-10-18 11:33:06,"探索智能、意识的人脑机理，认识人的行为和情感，创新脑疾病诊断与治疗，是二十一世纪科学的前沿领域。这也是目前“十三五”上海市科研发展的一个讨论热点。脑科学是一个重大广阔的领域，本文仅涉及神经网络的计算及其对信息技术前景的影响。
智搜（Giiso）公司成立于2013年是国内领先的“人工智能+资讯”领域技术服务商，在大数据挖掘、智能语义、知识图谱等领域都拥有国内顶尖技术。同时Giiso旗下研发产品包括编辑机器人、写作机器人等人工智能产品！凭借雄厚的技术实力，公司成立之初，就获得了天使轮投资，并在2015年8月获得了金沙江创投500万美元pre-A轮投资。

众所周知，科技创新对社会经济的进步起着巨大的作用。计算机的快速发展使信息技术产业成为现代经济的重心之一。然而，现有计算机仍难以完成许多对人类大脑来说轻而易举的复杂任务。所以，新一代信息技术产业很关注对于大脑功能和神经网络的研究，希望在理解其工作机制的基础上设计出新型的“人脑式电脑和机器人”。今年3月，百度董事长兼首席执行官李彦宏在人大、政协两会上提出了“中国大脑”提案，呼吁一个新技术革命，并希望以此“带动整个民族创新能力的提升”。



对基础研究的投入将带来脑科学、脑科技的突飞猛进

中国要在信息技术产业上成为世界领军人，应该大力发展神经科学，带动下一个信息技术产业革命。在美国，硅谷一些公司（高通（Qualcomm）,IBM等）已开始向这一新兴的“脑科技”（NeuroTech）产业进军。同时，发展神经科学也具有战略意义。美国国防部高级研究计划局（DARPA，DefenseAdvancedResearchProjectsAgency)、高级情报研究计划署(IARPA，IntelligenceAdvancedResearchProjectsActivity)，以及海军等军事部门在神经科学领域已投入大量资金研发最新的神经技术。

但是，在基础研究让我们真正了解大脑的生物学原理之前，这些应用都缺乏坚实的基储难于发展。就好像一个国家如果不重视固体物理研究，不可能在国际上成为芯片和电脑技术创新的领军人。所以，最重要的是对神经科学基础研究提供长期稳定的资金支持。即使美国情报高级研究计划署这样以应用为目标的机构，目前也主要资助“大脑皮层网络的机器智能”（MachineIntelligencefromCorticalNetworks，MICrONS）的基础研究【1】。该项目“旨在逆向大脑的运算法则，彻底改变机器学习”，并且“参与者将利用他们对大脑表征、转换和学习规则的深入研究和理解，来创造能力更强的类脑机器学习算法。”这样的规划正是基于长远的战略考虑。



学科交叉推动新兴的计算神经科学发展

“类脑人工智能”至今最成功的例子，是“深度学习”(deeplearning)【2】。“深度学习”植根于对大脑视觉系统的研究。视觉系统由很多“层”神经网络组成（因此叫“深度网络”）。神经信号经第一层处理后送至第二层，经第二层进一步处理后送至第三层，以此类推。层与层之间的网络连接是通过学习训练而形成的（故名“深度学习”）。深度学习系统在完成某些任务上（比如二维物体识别）已接近人的能力。然而目前这个理论还有相当大的局限。例如，深度网络模型通常只有“前馈”连接（从第一层到第二层、第二层到第三层，等等），而人脑的神经系统有很多“反馈”连接（从第三层回到第二层，等等），比如视觉注意力就来自于从高级“控制”脑区到初级视觉脑区的反馈信号。训练深度网络的学习算法目前也十分有限，需要千万张图来训练网络。人们对视觉注意力、抉择、学习等认知功能的大脑神经网络机制的研究方兴未艾。发展脑科学基础研究，将促进“深度学习”等类脑智能技术的蓬勃发展。

计算神经科学是脑科学中新兴的、跨领域的交叉学科【3，4】。它把实验神经科学和理论研究联系在一起，运用物理、数学以及工程学的概念和分析工具来研究大脑的功能。各种新实验技术的快速发展，给我们带来了海量数据。但指数增长的实验数据，并不保证带来指数增长的知识。就像物理学一样，只有当理论的发展与实验同步时，我们才能找到大脑运作的基本规律。因此侧重于理论和模型的计算神经科学与实验神经科学的互动，将会对认识大脑工作机制起到十分关键的作用。大脑是一个异常复杂的动力学系统，具有多种在不同时空层次上的反馈机制，定量分析和计算模型上深入解析是至关重要的。这也就是为什么理论和计算神经科学成为了美国的“脑计划“中的七大优先研究方向之一:“严谨的理论，模型建造和统计分析，使我们对于复杂的，非线性的大脑功能有深入的了解，这是仅凭直觉无法做到的。为了推动理论和数据分析的发展，我们必须加强来自多学科的实验科学家和理论科学家的合作，如统计学，物理学，数学，工程以及信息科学等。”【5】。

信息科学与现代神经科学真正结合起来，是一个很大的挑战，计算神经科学是二者之间的桥梁，将在类脑计算、人工智能和脑机接口（Brain-MachineInterface）的发展中起关键作用。可以说，没有世界一流的计算神经科学，就不可能有世界一流的类脑智能技术创新。反过来，机器学习（包括深度学习）能引进新的方法来处理脑科学数据，新的思路来模拟脑功能。因此，计算神经科学与信息科学应密切合作，对脑科学与类脑人工智能重大前沿问题进行联合攻关。



应鼓励更多的理工人才进入脑科学

我们需要利用物理学、数学的方法研究上千亿神经元组成的复杂结构及其非线性动态行为；需要用理论和数学模型从基因、到神经元、到神经网络、到脑系统的多个层次来研究，以解释认知功能；需要用新的信息科学的工具分析和解读实验中获得的海量数据；需要用生物医学工程的技术来制造脑机接口，使脊髓损伤和运动残疾的病人能够用脑电信号控制智能假肢。在这些方面，中国都具有很大的潜力优势，而发展计算神经科学是关键。

计算神经科学在我国还相对薄弱，但我们拥有大批杰出的理工科人才，尤其是年轻人。他们若有机会进入脑科学，计算神经科学的快速发展便指日可待。这不仅会培养基础研究的下一代领军人，同时会带出一大批类脑智能工程技术人才。举一个最近的例子，伦敦大学学院的两位计算神经科学博士后创办的DeepMind公司，创业后不久后便被谷歌以4亿美元（约25亿人民币）收购。近年来，谷歌、Facebook等公司极力招计算神经科学的毕业生，这种需求会越来越大。

培养新一代计算神经科学家，可考虑几种方式。在美国，斯隆基金会（SloanFoundation）与斯沃茨基金会（SwartzFoundation）从90年代中便开始建立理论神经科学中心，现在总共有11个斯隆－斯沃茨（Sloan-Swartz）中心，分布在各个大学。这些研究中心重点放在吸引培养有物理、数学或其他计算科学背景的研究生和博士后，帮助他们成功地转入计算神经科学领域。迄今这些中心培养出的一百多名学者已在世界各地的著名学府担任教授，使计算神经科学领域日趋成熟。中国应考虑建立几个理论与计算神经科学中心。另外，各强势大学应打破学科的界线，开本科和研究生的计算神经科学课，条件成熟后可建有关的专业。这不仅让物理学和数学学科的青年人有机会学习神经科学，也让实验科学家有机会学习建模的数学知识。暑期学校是另一种有效的途径。美国海洋生物实验室从1988年起办计算神经科学暑期学校，训练了大批从数理化、工程、信息科学转到脑科学的年轻人。我们在国内也于2010年起步创办了计算和认知科学的暑期学校【6】，培养这方面的跨学科的年轻人才。还有，为了建立一个新的领域，必须有讨论会以便于各领域的专家和学生定期集会、交流和开展合作，这对计算神经科学这样的交叉学科尤为重要。因此，我们需要有条件举办定期的会议和暑期学校。



展望计算神经科学与类脑智能的未来

“十三五”规划，为我国发展这个新的交叉领域带来了良机。我们的目标应该包括：第一，发展海量数据分析的方法，为处理脑数据(包括脑环路和联结组学数据、高密度成像数据、大规模认知功能测量和脑疾病研究数据)服务。第二，发展理论和计算神经学，尤其是大尺度的认知（包括记忆、决策、语言）功能脑系统计算模拟、跨层次（分子，细胞，多尺度网络，动态系统和行为）机制的分析。第三，发展行为实验的测量方法，包括对脑疾病患者认知功能的定量测量及其标准化。第四，发展计算神经学与类脑人工智能的交叉研究和应用，包括深度学习算法、智能机器人等。第五，将人才从数理化、工程、信息科学吸引到神经科学来，培养受过良好定量训练的新一代脑科学家。第六，建立专门支持跨科学研究的项目。第七，在海量数据共享、跨学科的研究和人才培训等方面开展国际合作。这一切都需要改革资助经费和管理运行的方式，还需要我们支持跨单位、跨学科的协同创新。

大脑是宇宙中最复杂的系统之一，近年来研究技术的大突破为人类认识大脑开创了新纪元。但我们对脑功能、尤其高级认知功能的神经网络机制的研究才刚刚起步，实验神经科学必须与理论和计算神经学携手共进。计算神经科学也是脑科学与人工智能两个领域之间的必要桥梁，这些领域的互动和协同创新将极大推动未来的信息科技、脑科技以及下一代超级计算机的发展。（(作者为上海纽约大学副校长、纽约大学神经科学教授，原耶鲁大学神经科学终身教授。)）"
处理器芯片和异构计算,2020-03-13 21:39:06,"芯片
集成电路—可以分为模拟电路、数字电路
模拟电路：用于对模拟信号的转换、放大、传输等
数字电路：用数字信号完成逻辑运算，常见的会有逻辑门、触发器等

当然也可以两者混合，将模拟电路和数字电路集成在单个芯片上，做出数模转换器或模数转换器。

华为有一系列自研的芯片
在这里插入图片描述

处理器
CPU-中央处理器，用于解释计算机指令、处理计算机软件中的数据。

是一个大规模的集成电路，是计算机的控制核心、运算核心，可以理解为人类的大脑
CPU主要包括了寄存器、运算器和控制器。
在这里插入图片描述
输入设备接收数据
交给运算器，
运算器把数据交给寄存器缓存排队
当排到的时候，控制器发送指令给寄存器和运算器，
寄存器把数据发给运算器
运算器把处理完成的数据发送给控制器，
控制器发送控制信号给输出设备。

处理器架构
主要分为两类CISC和RISC

其中CISC是以intel和AMD为首的复杂指令集
RISC是以ARM、IBM为首的精简指令集
在这里插入图片描述

主流的计算架构
X86、ARM、Power
在这里插入图片描述

我们主要关注x86和ARM架构
单独拿出来比较一下
在这里插入图片描述

异构运算
将不同体系架构不同指令集的计算单元组合使用，最大化发挥各个处理器的性能，可以将最合适的任务交给最擅长的计算单元（计算单元包括CPU、GPU等）
在这里插入图片描述

GPU
图形处理器
在这里插入图片描述
CPU -ALU计算核心
GPU-并发能力强，对任务分割交给多个核心运算
CPU作为通用处理器，兼顾计算和控制，其处理器系统是SISD(Single Instruction Single Data）型。GPU主要擅长做类似图像处理的并行计算，其处理器系统是SIMD(Single Instruction Multiple Data)型

FPGA-可编程逻辑阵列，适用于不规则数据的并行计算
ASIC-用于计算加速，不可编程，适用于数据并行计算

达芬奇架构
在这里插入图片描述

总结一下异构计算
首先涉及到的计算单元
CPU：主频高，核数有限；逻辑控制&算术运算单元、大量缓存；管理和调度任务。
GPU：高并发：几千核并行；强浮点能力；高显存带宽：芯片内置HBM存储。
FPGA：主频低但集成大量计算单元；流水线并行和数据并行；硬件编程和加速，特定应用IP核。
ASIC：针对某一场景优化的专用处理单元；硬件基本不可编程；多个IP集成，高性价比和能效比。"
"专用神经网络处理器芯片,神经网络芯片概念股",2022-08-14 15:06:14,"1、苏州科达：苏州科达科技股份有限公司是领先的视讯与安防产品及解决方案提供商，致力于以视频会议、视频监控以及丰富的视频应用解决方案帮助各类政府及企业客户解决可视化沟通与管理难题。
2012年，公司整体改制为股份有限公司；2016年12月1日，公司在上海证券交易所主板挂牌上市。

2、佳都科技：佳都科技（PCI）创立于1986年，总部位于中国广州，在中国30多个区域设有分公司或办事处，员工超过2000人，拥有科学家研发团队，设立了佳都科技全球人工智能技术研究院和交通大脑研究院，建设或参与建设2个国家联合实验室、1个国家企业技术中心、4个省级工程技术中心。

3、千方科技：北京北大千方科技有限公司是由北京大学作为法人股东之一，以留学归国科技人员、清华大学和北京大学的教授、博士、硕士为主要技术力量，与北京大学地学院全面合作组建的高新技术企业、软件企业。

公司在交通领域的业务取得了快速的发展，在交通信息化建设的基础上，又拓展了交通信息服务和交通出行媒体运营等多方面的业务。

4、卫宁健康：公司成立于1994年，是国内第一家专注于医疗健康信息化的上市公司，致力于提供医疗健康卫生信息化解决方案，不断提升人们的就医体验和健康水平。

卫宁健康通过持续的技术创新，自主研发适应不同应用场景的产品与解决方案，业务覆盖智慧医院、区域卫生、基层卫生、公共卫生、医疗保险、健康服务等领域。

5、神思电子神思电子是国内著名的身份识别解决方案提供商和服务商，也是公安部认证的居民身份证阅读机具定点生产企业。

6、科大讯飞科大讯飞主要从事智能语音及语言技术研究、软件及芯片产品开发、语音信息服务及电子政务系统集成等等。7、中科曙光中科曙光是国内高性能计算领域的领军企业，也是亚洲第一大高性能计算机厂商。

主要从事研究、开发、生产制造高性能计算机、通用服务器及存储产品，并围绕高端计算机提供软件开发、系统集成与技术服务等等。8、浪潮信息浪潮是中国最早的IT品牌之一，它是中国领先的云计算、大数据服务商。

拥有云数据中心、云服务与大数据、智慧城市和智慧企业四大业务群组。浪潮服务器也位居中国市场第一、全球前三。

什么是神经网络计算机?
许多新型电子计算机不仅拥有高速的计算功能，而且还能模拟人脑的某种思维活动，就是说，拥有某些智能化的功能。然后，如果严格来鉴定一下，它们离真正的人脑思维功能实在差得太远了，而且有许多本质的差异。

主要表现在人脑拥有高度的自我学习和联想、创造的能力，以及更高级的寻找最优方案和各种理性的、情感的功能。

神经网络计算机就是通过人工神经网络，模仿人的大脑判断能力和适应能力、可并行处理多种数据功能的计算机。它可以判断对象的性质与状态，并能采取相应的行动，而且可同时并行处理实时变化的大量数据，并引出结论。

生物的神经网络是通过树突和轴突连接起来的神经元的网络。神经信号在神经元之间传递，帮助人产生思考和记忆。人工神经网络是一种模仿生物神经网络而建立的运算模型，由大量的节点（或称神经元）之间相互联接构成。

每个节点代表一种特定的输出函数，称为激励函数。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。

网络的输出依照网络的连接方式、权重值和激励函数的不同而不同。而网络自身通常都是对自然界的某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。

生物神经网络（左）、人工神经网络（右）与生物神经网络相似，人工神经网络也可以通过训练提高自身判断和处理的性能。

其原理是向该神经网络输入足够多的样本，通过一定的算法来调整网络的结构，即权重值，使得网络的输出与预期值相符。经过训练的神经网络可以像人脑那样进行判断和预测，并且能同时接受几种信号并进行处理。

譬如，它能去辨别一个签名的真伪。它不是凭签名的图像是否相像来判断，而是根据本人在签名时，笔尖上的压力随时间的变化以及移动的速度来判断。

目前，神经网络计算机的主要用途是识别各种极其细微的变化和趋势，并发出信号。它已经被用来控制热核聚变反应、监督机器的运行、挑选苹果，甚至预测股市行情。

npu是什么意思啊
npu是嵌入式神经网络处理器。NPU是神经网络处理单元，在电路成模拟模拟人类神经元和突触。实行人工智能运算，产出一种新的能以人类智能相似的方式作出反应的运算芯片。

手机正常运行离不开SoC芯片，SoC只有指甲盖大小，却“五脏俱全”，其集成的各个模块共同支撑手机功能实现，如CPU负责手机应用流畅切换、GPU支持游戏画面快速加载，而NPU（神经网络处理器）就专门负责实现AI运算和AI应用的实现。

扩展资料：在手机SoC中，NPU扮演最聪明的角色，并直接影响手机AI能力的强弱。

2017年底，华为推出的麒麟970第一次引入了NPU（神经网络处理器）概念，让SoC也具备了更强的本地（端侧）AI运算能力（类似于“硬解”），执行效率可以秒杀CPU的“软解”。

至此，NPU单元就与AI画上了等号。

不同品牌的SoC，对AI单元的命名和实现方法略有不同，除了高通骁龙AIEngine引擎之中没有独立的NPU单元以外，联发科在HelioP60/P90引入的NeuroPilotAI技术最早也是通过多个单元协同计算（APU+CPU+GPU)。

参考资料来源：百度百科-NPU。

npu神经网络处理单元什么意思
。

cpu=centerprocessingunitsnpu=neural-networkprocessingunitsneural神经神经网络处理部分2016年6月20日，中星微数字多媒体芯片技术国家重点实验室在北京宣布，已研发成功了中国首款嵌入式神经网络处理器（NPU）芯片，成为全球首颗具备深度学习人工智能的嵌入式视频采集压缩编码系统级芯片，并取名“星光智能一号”。

这款基于深度学习的芯片运用在人脸识别上，最高能达到98%的准确率，超过人眼的识别率。该芯片于今年3月6日实现量产，目前出货量为十几万件。

该实验室执行主任、中星微首席技术官张韵东在接受采访时表示，装备了神经网络处理器的芯片应用在监控摄像头上，摄影头由“眼睛”升级为“带有大脑的眼睛”，这是全球首次。

“数字多媒体芯片技术”国家重点实验室成立于2010年，依托于北京中星微电子有限公司，由科技部批准建立。

据资料显示，中星微电子有限公司于1999年由原国家信息产业部直接投资创立，是专攻芯片技术的公司中的“国家队”，其研发的“星光系列芯片”曾打破国际市场上无“中国芯”的局面。

人工智能的落地　“星光智能一号”是一款嵌入式NPU。神经网络处理器NPU（NeuralProcessingUnit）还未被熟知，却是芯片领域热门的技术。

它与冯诺依曼架构中的CPU处理器相对，采用的是“数据驱动并行计算”这种颠覆性的新型架构。

如果将冯诺依曼架构处理数据的方式类比成单车道，那么“数据驱动并行计算”是128条多车道并行，可以同时处理128个数据，利于处理视频、图像类的海量多媒体数据。

在业内，单位功耗的计算性能，也就是性能功耗比，被用来衡量处理器架构的优劣。

据该实验室执行主任、中星微首席技术官张韵东介绍，“星光智能一号”的性能功耗比在传统的冯诺依曼架构上“至少提高了两三个数量级”，也就是几百倍。高功耗是很多顶尖人工智能技术被诟病的。

IBM20世纪的“深蓝”和谷歌2016的AlphaGo因其需要由巨大的数据计算支撑，前者使用超级计算机，后者使用服务器集群，无法脱离恒定温度和湿度的机房。

AlphaGo下一盘棋光电费就需要3000美元。张韵东将它们称之为“一场科学实验”，离技术落地、投入应用还有较远的距离。这凸显了嵌入式NPU的小型化、低功耗和低成本优势，加快人工智能技术落地应用。

例如无人机对摄像头的重量和功耗有很高的要求，否则会影响起飞和续航能力。

而“星光智能一号”只有普通邮票大小，重量仅几十克，它的诞生让诸多监控摄像头等小型设备有了人工智能化的可能，迈出了人工智能从神秘的机房，跨向生活应用的一步。择自NPU_百度百科。

手机上的CPU GPU NPU单元，都是什么意思
手机上的CPUGPUNPU单元的意思如下：1、CPUCPU是通用处理器，是计算单元、控制单元和存储单元。

CPU的结构主要包括运算器（ALU,ArithmeticandLogicUnit）、控制单元（CU,ControlUnit）、寄存器（Register）、高速缓存器（Cache）和它们之间通讯的数据、控制及状态的总线。

2、GPUGPU是图形处理器，全称为GraphicsProcessingUnit。GPU最初是用在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上运行绘图运算工作的微处理器。

专门用于图形计算渲染，也就是一般用于游戏。也可以跑AI部分算法。3、NPUNPU是神经网络处理器，是基于神经网络算法与加速的新型处理器总称。NPU专用AI加速计算。

如中国科学院计算所出品的diannao系列，为深度学习而生的专业芯片。扩展资料：手机上的CPUGPUNPU单元的作用：1、CPU作为手机的核心部件，可以做常用指令计算和各种算法。

2、GPUGPU中有很多的运算器ALU和很少的缓存cache，缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为线程thread提高服务的。

如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram。3、NPU它模仿生物神经网络而构建，由若干人工神经元结点互联而成。

神经元之间通过突触两两连接，突触记录了神经元间联系的权值强弱。每个神经元可抽象为一个激励函数，该函数的输入由与其相连的神经元的输出以及连接神经元的突触共同决定。

为了表达特定的知识，使用者通常需要（通过某些特定的算法）调整人工神经网络中突触的取值、网络的拓扑结构等。该过程称为“学习”。在学习之后，人工神经网络可通过习得的知识来解决特定的问题。"
【AI简报20210827期】AI芯片逐步落地智能教育硬件市场！用AI设计芯片会成为未来趋势吗？...-CSDN博客,2021-08-27 18:37:10,"嵌入式AI硬件

1. 集成全球最大AI芯片，Cerebras发布全球首个人类大脑规模的AI解决方案
原文：

https://new.qq.com/omn/20210826/20210826A06VH900.html

image-20210827105333208
8月25日凌晨，曾推出“全球最大”的AI芯片Wafer Scale Engine（以下简称“WSE”）的芯片初创公司CerebrasSystems，宣布推出了世界上第一个人类大脑规模的AI解决方案——CS-2 AI计算机，可支持超过120万亿参数规模的训练。相比之下，人类大脑大约有100万亿个突触。此外，Cerebras还实现了192台CS-2 AI计算机近乎线性的扩展，从而打造出包含高达1.63亿个核心的计算集群。

资料显示，Cerebras成立于2016年，迄今在14个国家拥有超过350位工程师，2019年Cerebras推出了世界最大AI芯片WSE，2020年Cerebras又推出了新一代的7nm的WSE-2，晶体管数量达到2.6万亿个，震惊业界。

根据官方公布的数据，WSE-2与上一代一样，依然是基于一整张12吋晶圆制造，面积依然是462.25平方厘米，但是制程工艺由台积电16nm工艺提升到了7nm工艺，这也使得WSE-2的晶体管数量提高到了2.6万亿个，同时他的人工智能内核数量也达到了85万个，打破首代WSE 处理器创造的世界纪录。无论是核心数还是片上内存容量均远高于迄今性能最强的GPU。

2. 嘉楠科技与闪现智能合作，AI芯片逐步落地智能教育硬件市场
原文：

https://baijiahao.baidu.com/s?id=1708971331548164815&wfr=spider&for=pc

img
集微网消息，据网易科技报道，日前，嘉楠科技与全年龄段智慧教育解决方案提供商闪现智能合作，并且已经进入产品开发阶段。

闪现智能基于勘智K210芯片开发了多款智能教育硬件产品，包括AI教育核心板、传感器拓展板和自动驾驶拓展小车等，将逐步落地西安高校和职教智能硬件市场。通过K210芯片和板载资源，教育核心板可支持离线实现机器学习、形状识别、动物识别、戴口罩识别、交通标志识别、智能家居、指南针、水平仪、环境监测和成语填充等多项功能。

据了解，勘智K210芯片采用RISC-V处理器架构，具备视听一体、自主IP核与可编程能力强三大特点，支持机器视觉与机器听觉多模态识别，可广泛应用于智能家居、智能园区、智能能耗和智能农业等场景。

3. AI芯片公司「苹芯科技」获近千万美元Pre-A轮融资，打造存内计算加速单元
原文：

https://baijiahao.baidu.com/s?id=1708938332878219718&wfr=spider&for=pc

AI芯片研究领域的苹芯科技已完成红点中国领投，真格基金、红杉资本跟投的近千万美元Pre-A轮融资，据悉，本轮融资将主要用于芯片研发相关工作。苹芯科技的天使轮股东包括普华资本，红杉资本等。

苹芯科技成立于2021年2月，公司总部位于北京，是一家基于存算一体技术的打造面向AI加速器芯片的创业公司，提供基于存算一体技术的用于提升深度学习计算性能的硬件单元和相关IC设计服务。公司在北京、台湾、新加坡等地设有研发团队及办公室，希望利用各地资源优势，推动全球化发展战略。

其产品主要用于可穿戴设备、无人机摄像头、安防领域、机器人领域、智能家居等低能耗、长待机的场景。

传统的芯片架构基于冯诺依曼架构，计算单元和存储单元物理上分离，存储单元的带宽速度低于计算单元，限制了系统的整体性能；此外，数据传输过程会带来延迟和性能消耗，且这种消耗能达到70%~90%以上。随着AI算法的持续升级与AI应用的持续普及，AI领域迫切需要性能更强、功耗更低、成本更低的芯片。

4. 特斯拉发布D1 AI芯片：500亿晶体管、400W热设计功耗
原文：

https://baijiahao.baidu.com/s?id=1708794510144411423&wfr=spider&for=pc

img
近日的特斯拉AI日活动上，特斯拉公布了最新的AI训练芯片“D1”，规模庞大，令人称奇。

该芯片采用台积电7nm工艺制造，核心面积达645平方毫米，仅次于NVIDIA Ampere架构的超级计算核心A100(826平方毫米)、AMD CDNA2架构的下代计算核心Arcturus(750平方毫米左右)，集成了多达500亿个晶体管，相当于Intel Ponte Vecchio计算芯片的一半。

其内部走线，长度超过11英里，也就是大约18公里。

5. IBM发布首款7纳米AI芯片，加码人工智能基础设施竞争
原文：

https://www.jiemian.com/article/6530725.html

img
当人工智能（AI）加速芯片已成为大型科技公司标配后，老牌科技公司IBM亦给出回应。在本周举行的芯片行业会议HotChips上，IBM正式公布新款处理器“Telum”，Telum是IBM首款具有芯片上AI加速功能的处理器，用于IBM下一代Z系列大型机和LinuxONE服务器。

IBM Telum包含8个处理器核心，频率超过5GHz，每个核都由重新设计的32MB专用2级缓存支持。该处理器采用三星7纳米制程工艺，并且单芯片内采用17层金属连接，来完成高密度电路互连，总线长可达约30公里。

为了支持AI加速处理性能，新处理器面积为530平方毫米，集成多达225亿个晶体管，并拥有全新的分支预测、缓存，支持多芯片一致性互连，性能提升超过40%。

此外，IBM Z Telum处理器还采用双向环形互连拓扑结构，带宽接近320GB/s。三级缓存所有核心共享，通过二级缓存与核心相连，平均延迟达到12纳秒（1纳秒等于10的负9次方秒）。

IBM称，新处理器通过芯片内深度学习推理(Inference) ，帮助即时解决金融客户解决欺诈等问题，从而不需要将数据转移至芯片外。AI计算大致分为两个层面，首先是对模型进行训练（training），整个过程可能耗时数天或数周；之后是训练出的模型做出推理。

实际上，在芯片内集成和加强AI能力，IBM与英特尔思路一致，两者均强调增强芯片的AI计算能力作为卖点。例如英特尔推出的第三代至强服务器芯片，在深度学习方面增加了最新深度学习加速指令集，提高AI处理速度和模型计算精度。

IBM称，其人工智能硬件研究中心花了3年的时间研发打造出Telum处理器。打造出第一款结合AI加速推论功能的处理器芯片，用于大型机上时，可以协助银行解决诈骗、高频交易等金融难，预计2022年上半年采用该芯片的大型机系统将会推出。

AI热点新闻

6. 千倍设计效率提升，EDA终极形式，用AI设计芯片会成为未来趋势吗？
链接：

https://www.163.com/dy/article/GID7KKQF05312NX9.html

数字经济正从数字化迈向智能化新阶段，未来十年，智能经济将成为产业发展的核心。在百度创始人、董事长兼首席执行官李彦宏看来，“产业智能化在每一个领域都会发生。”物联网、云计算等技术的发展方向全部指向了智能化，“人工智能+”逐渐成为产业结构升级的驱动力。在智能化产业趋势下，利用人工智能进行芯片设计也逐渐成为行业研究的热门方向。

早在2020年4月，谷歌AI团队便描述了一种基于机器学习的芯片设计方法。Google Brain总监Jeff Dean指出，某些情况下，机器学习会做出比人类更加精准的决策，例如规划芯片中的电路布局。该技术能够极大地缩减研发设计周期，自动生成芯片布局方案，并且在功耗、性能和芯片面积（PPA）等关键参数指标上都有不俗的表现。

而前段时间，据Wired报道显示，三星（Samsung）下一代Exynos处理器将采用AI进行芯片设计，人工智能设计软件（DSO.ai）由新思科技（Synopsys）提供。这些芯片在设计完成并量产后，将会运用到三星智能手机、平板电脑中，并且有部分还将供货给国产手机厂商。这就意味着，未来我们将有机会用上搭载由人工智能进行芯片设计的智能终端产品。

事实上，在利用AI设计芯片方面，三星只能算后来者。谷歌TPU（张量处理单元）采用人工智能优化；英伟达利用人工智能技术生产GPU和云计算TPU平台。这无疑都在传递同一个信号，即芯片设计即将走向人工智能时代。

7. 自动驾驶落地 上汽成立智能驾驶公司
链接：

https://www.163.com/dy/article/GICTO8T50527AQ4N.html

8月26日，上汽集团宣布正式成立商用车智能驾驶科创公司上海友道智途科技公司，将以港口、厂区等特定场景作为L4级自动驾驶技术和L3自动驾驶技术的应用切入点，也就是说，L4和L3级自动驾驶技术，将会在港口与厂区落地。

img
此次，上汽集团商用车智能驾驶科创公司计划落地在中国（上海）自由贸易试验区临港新片区，打造“技术+产品+运营”的全新业务模式。不久的未来，是不是遍地都是威震天、擎天柱了呢？我们拭目以待。

8. 人工智能破解蛋白质结构可能引发医学革命
原文：

https://www.bbc.com/zhongwen/simp/57969896



人工智能被用来预测人体产生的几乎每一种蛋白质的结构。

这一重大科研发展能够加速推进研发治疗疾病的新药物，还能用在其他许多地方。

蛋白质是所有生物有机体的组成部分，我们体内的每一个细胞都充满蛋白质。

了解蛋白质的结构和形状对医药研究的进步至关重要，但是到现在为止，我们只研究出少数蛋白质的构造。

研究人员用一个名为“AlphaFold”的人工智能软件来预测人体和其他有机体的35万种蛋白质的结构。

人类蛋白质如何组成是由我们体内的基因组决定，也就是人类细胞核里面的遗传物质DNA，能够决定大约两万种蛋白质。

“AlphaFold”的人工智能软件是谷歌旗下的DeepMind人工智能公司开发的，该公司共同创办人兼首席执行官哈萨比斯（Demis Hassabis）表示，这是迄今为止人类蛋白质最完整、最准确的图像。

人工智能破解生物学最大谜团之一的意义所在

新冠疫情：人工智能算法能“听咳嗽声音辨识新冠病毒

“我们认为这代表了人工智能对科学知识进展所做的最大贡献。” 他表示，这是人工智能有助于社会的最佳范例，未来还会有更多激励人心的发展。

有趣的开源项目

9. 首个移动端AI虚拟人像加速方案！这研究将让科幻般VR离你更进一步
原文：

https://baijiahao.baidu.com/s?id=1694900288211624038&wfr=spider&for=pc

img
斯皮尔伯格指导的电影《头号玩家》相信大家都不陌生，电影中高度成熟发达的VR（虚拟现实）技术是每一位科技爱好者都会憧憬的场景，谁又不想在这个光怪陆离的世界里来一场惊心动魄的冒险之旅呢～

在众多VR技术的难题当中，如何让高质量VR应用在资源及功耗受限的移动设备上部署成为可能是一大关键。

而近日，来自UIUC（伊利诺伊大学厄班纳香槟分校）和FRL（Facebook Reality Labs）Research的研究人员发表了题为《F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar Decoding》的论文，为移动终端设备部署新一代高拟真VR人像应用提供硬件加速器设计方案。

img
论文链接：https://arxiv.org/abs/2103.04958

10.给图片打「马赛克」可骗过AI视觉系统，阿里安全新研究入选ICCV 2021
原文：

https://mp.weixin.qq.com/s/hw65luLEaQ1pI0e0SuZovQ

来自阿里安全人工智能治理与可持续发展实验室（AAIG）等机构的研究者提出了一个新的机制来生成对抗样本，即与增加对抗扰动相反，他们通过扔掉一些不可察觉的图像细节来生成对抗样本。这项研究成果已被 AI 顶会 ICCV 2021 收录。

人类拥有很强的抽象能力和联想力，例如一个有几块积木拼成的乐高玩具，小朋友也能轻易认出其中描述的场景 （人开着小车）。甚至几个像素，玩家也可以轻易认出这是一个戴着帽子的小人 （超级玛丽奥）。

image-20210827105254922
尽管我们期望模型能具有和人相当的能力，但是「抽象能力」对于模型来说，在当前显然还是一个相当具有挑战性的任务。但相反的，如果我们从对抗样本的角度来考虑：存不存在一种可能，如果我们去掉图片中一些对模型来说关键而微小的特征，模型就无法再正确识别这些图片？感兴趣的大家可以查看原文，文章的创新一定会给大家提供非常好的思路。

11. ICCV 2021 | Transformer再助力！用CWT进行小样本语义分割
原文：

https://mp.weixin.qq.com/s/SSvtl1cy4eS8OSdvigkABg

本文是对发表于计算机视觉领域的顶级会议 ICCV 2021的论文“Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer（简而优：用分类器变换器进行小样本语义分割）”的解读。

该论文由英国萨里大学Centre for Vision, Speech and Signal Processing (CVSSP)发表，针对小样本语义分割问题，提出一种更加简洁的元学习范式，即只对分类器进行元学习，对特征编码解码器采用常规分割模型训练方式。元学习训练后的Classifier Weight Transformer使分类器可以动态地适应测试样本，从而提高分割准确率。

image-20210827105234735
Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer

论文：https://arxiv.org/abs/2108.03032

代码：https://github.com/zhiheLu/CWT-for-FSS

12. 李飞飞团队给机器人造了一个“模拟厨房”：洗切炒菜一条龙训练！人类还能VR监管
原文：

https://mp.weixin.qq.com/s/HgcyVs8eQfrQ4tWsFUo02g

李飞飞团队的机器人模拟训练场2.0版本来了！这个拥有超过8000个交互式场景的模拟环境iGibson，再次发生了进化！

image-20210827103451144
进化之后的iGibson 2.0，核心就一句话：机器人们别抓小球儿了，来做家务吧！像是模拟环境中增加的温度、湿度、切片等多种物理状态，环境信息已经完事具备了，亟待各位机器人来个洗、切、炒菜一条龙服务。

image-20210827103609367
人类还能通过VR进入模拟环境，给机器人示范下如何做一个标准的家务，那么现在，如果你已经迫不及待的想看看具体的内容，请点击原文连接。

聊点技术

13. 手撕非极大值抑制算法NMS
原文：

https://oldpan.me/archives/write-hard-nms-c

非极大值抑制算法（Non-maximum suppression, NMS）是有anchor系列目标检测的标配，如今大部分的One-Stage和Two-Stage算法在推断(Inference)阶段都使用了NMS作为网络的最后一层，例如YOLOv3、SSD、Faster-RCNN等。

当然NMS在目前最新的anchor-free目标检测算法中(CornerNet、CenterNet等)并不是必须的，对这种检测算法提升的精度也有限，但是这并不影响我们学习NMS。

NMS的本质是搜索局部极大值，抑制非极大值元素，在目标检测中，我们经常将其用于消除多余的检测框(从左到右消除了重复的检测框，只保留当前最大confidence的检测框)：

image-20210827105038709
NMS有很多种变体，这里介绍最为常见的Hard-NMS，我们通常所说的NMS就是指Hard-NMS，还有另外一种NMS叫做Soft-NMS，是Hard-NMS的变体，两者的代码几乎相同，只需要改动一个地方。

Hard-NMS

Hard-NMS就是我们传统意义上的NMS，也是最常用的NMS算法。

因为NMS主要用于消除多余的检测框，那么消除的标准是什么，我们使用IOU作为标准来进行演示，IOU的原称为Interp over Union，也就是两个box区域的交集比上并集，下图可以方便理解：

image-20210827105055470
具体介绍可以看这里：深度学习中IU、IoU(Interp over Union)的概念理解以及python程序实现。[https://oldpan.me/archives/iu-iou-interp-over-union-python]

因为我们要手撸么，所以废话不多说，直接开始看代码，首先使用Pytorch来看一篇：

 1def hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):
 2    """"""
 3    Args:
 4        box_scores (N, 5): box的集合，N为框的数量，5即4(位置信息)+1(可能为物体的概率)
 5        iou_threshold: 我们用IOU标准去除多余检测框的阈值
 6        top_k: 保留多少个计算后留下来的候选框，如果为-1则全保留
 7        candidate_size: 参与计算的boxes数量
 8    Returns:
 9         picked: 经过nms计算后保留下来的box
10    """"""
11    scores = box_scores[:, -1]                # 首先我们取出box中的最后一个元素也就是当前box检测到物体的概率
12    boxes = box_scores[:, :-1]                # 取出box中的四个坐标(左上、右下)
13    picked = []  
14    _, indexes = scores.sort(descending=True) # 按照降序排列所有的物体的概率，得到排序后在原数组中的索引信息 indexes
15    indexes = indexes[:candidate_size]        # 只保留前 candidate_size 个 boxes 其余的不考虑了
16    while len(indexes) > 0:
17        current = indexes[0]                  # 每次取出当前在 indexes 中 检测到物体概率最大的一个 
18        picked.append(current.item())         # 将这个最大的存在结果中
19        if 0 < top_k == len(picked) or len(indexes) == 1:
20            break
21        current_box = boxes[current, :]       # 当前第一个也就是最高概率的box
22        indexes = indexes[1:]                
23        rest_boxes = boxes[indexes, :]        # 剩下其余的box
24        iou = iou_of(                         # 将当前的box与剩下其余的boxes用IOU标准进行筛选
25            rest_boxes,
26            current_box.unsqueeze(0),
27        )
28        indexes = indexes[iou <= iou_threshold]# 保留与当前box的IOU小于一定阈值的boxes，
29
30    return box_scores[picked, :]
看了上面的代码，我们可以知道大概的流程：

选取这类box中scores最大的那一个，记为current_box，并保留它(为什么保留它，因为它预测出当前位置有物体的概率最大啊，对于我们来说当前confidence越大说明当前box中包含物体的可能行就越大)

计算current_box与其余的box的IOU

如果其IOU大于我们设定的阈值，那么就舍弃这些boxes（由于可能这两个box表示同一目标，因此这两个box的IOU就比较大，会超过我们设定的阈值，所以就保留分数高的那一个）

从最后剩余的boxes中，再找出最大scores的那一个(之前那个大的已经保存到输出的数组中，这个是从剩下的里面再挑一个最大的)，如此循环往复

至于上述代码中iou_of部分：

 1def area_of(left_top, right_bottom) -> torch.Tensor:
 2    """"""Compute the areas of rectangles given two corners.
 3
 4    Args:
 5        left_top (N, 2): left top corner.
 6        right_bottom (N, 2): right bottom corner.
 7
 8    Returns:
 9        area (N): return the area.
10    """"""
11    hw = torch.clamp(right_bottom - left_top, min=0.0)
12    return hw[..., 0] * hw[..., 1]
13
14
15def iou_of(boxes0, boxes1, eps=1e-5):
16    """"""Return interp-over-union (Jaccard index) of boxes.
17
18    Args:
19        boxes0 (N, 4): ground truth boxes.
20        boxes1 (N or 1, 4): predicted boxes.
21        eps: a small number to avoid 0 as denominator.
22    Returns:
23        iou (N): IoU values.
24    """"""
25    overlap_left_top = torch.max(boxes0[..., :2], boxes1[..., :2])
26    overlap_right_bottom = torch.min(boxes0[..., 2:], boxes1[..., 2:])
27
28    overlap_area = area_of(overlap_left_top, overlap_right_bottom)
29    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])
30    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])
31    return overlap_area / (area0 + area1 - overlap_area + eps)
手撕NMS

手撕代码用什么撕，当然是用C++撕，这才爽么！

直接看代码，其中使用了OpenCV库中的Point2f结构体：

  1// 这是一个模板函数，接受一个已经排好序的vector，然后降序返回其索引
  2template <typename T>
  3vector<int> sort_indexes(const vector<T> &v) {
  4
  5
  6    vector<int> idx(v.size());
  7    iota(idx.begin(), idx.end(), 0);
  8    sort(idx.begin(), idx.end(),
  9         [&v](int i1, int i2) {return v[i1] > v[i2];});
 10    return idx;
 11}
 12// 这就是我们的NMS函数 输入的坐标已经标准化，所有数值的范围为 0-1
 13/*
 14* numBoxes：窗口数目
 15* points：窗口左上角坐标点
 16* oppositePoints：窗口右下角坐标点
 17* score：窗口得分
 18* overlapThreshold：重叠阈值控制
 19* numBoxesOut：输出窗口数目
 20* pointsOut：输出窗口左上角坐标点
 21* oppositePoints：输出窗口右下角坐标点
 22* scoreOut：输出窗口得分
 23*/
 24
 25int nonMaximumSuppression(int numBoxes, const vector<Point2f>& points,
 26                          const vector<Point2f>& oppositePoints, const vector<float>& score,
 27                          float overlapThreshold,
 28                          int *numBoxesOut, vector<Point2f>& pointsOut,
 29                          vector<Point2f>& oppositePointsOut, vector<float>& scoreOut)
 30{
 31
 32    const float eps = 1e-5;
 33    int i, j, index;
 34    float* box_area = (float*)malloc(numBoxes * sizeof(float));  // 定义窗口面积变量并分配空间
 35    vector<int> indices;
 36    int* is_suppressed = (int*)malloc(numBoxes * sizeof(int));   // 定义是否抑制表标志并分配空间
 37
 38    // 初始化indices、is_supperssed、box_area信息
 39    for (i = 0; i < numBoxes; i++)
 40    {
 41        indices.push_back(i);
 42        is_suppressed[i] = 0;
 43        box_area[i] = ((oppositePoints[i].x - points[i].x + eps) *
 44                       (oppositePoints[i].y - points[i].y + eps));
 45    }
 46
 47    // 对输入窗口按照分数比值进行排序，排序后的编号放在indices中
 48    indices = sort_indexes(score);
 49
 50    for (i = 0; i < numBoxes; i++)                // 循环所有窗口
 51    {
 52        if (!is_suppressed[indices[i]])           // 判断窗口是否被抑制
 53        {
 54            for (j = i + 1; j < numBoxes; j++)    // 循环当前窗口之后的窗口
 55            {
 56                if (!is_suppressed[indices[j]])   // 判断窗口是否被抑制
 57                {
 58                    float x1max = max(points[indices[i]].x, points[indices[j]].x);                     // 求两个窗口左上角x坐标最大值
 59                    float x2min = min(oppositePoints[indices[i]].x, oppositePoints[indices[j]].x);     // 求两个窗口右下角x坐标最小值
 60                    float y1max = max(points[indices[i]].y, points[indices[j]].y);                     // 求两个窗口左上角y坐标最大值
 61                    float y2min = min(oppositePoints[indices[i]].y, oppositePoints[indices[j]].y);     // 求两个窗口右下角y坐标最小值
 62                    float overlapWidth = x2min - x1max + eps;            // 计算两矩形重叠的宽度
 63                    float overlapHeight = y2min - y1max + eps;           // 计算两矩形重叠的高度
 64                    if (overlapWidth > 0 && overlapHeight > 0)
 65                    {
 66                        float overlapPart = (overlapWidth * overlapHeight) / box_area[indices[j]];    // 计算重叠的比率
 67                        if (overlapPart > overlapThreshold)          // 判断重叠比率是否超过重叠阈值
 68                        {
 69                            is_suppressed[indices[j]] = 1;           // 将窗口j标记为抑制
 70                        }
 71                    }
 72                }
 73            }
 74        }
 75    }
 76
 77    *numBoxesOut = 0;    // 初始化输出窗口数目0
 78    for (i = 0; i < numBoxes; i++)
 79    {
 80        if (!is_suppressed[i]) (*numBoxesOut)++;    // 统计输出窗口数目
 81    }
 82
 83    for (i = 0; i < numBoxes; i++)                  // 遍历所有输入窗口
 84    {
 85        if (!is_suppressed[indices[i]])             // 将未发生抑制的窗口信息保存到输出信息中
 86        {
 87            Point2f temp_xy(points[indices[i]].x, points[indices[i]].y);
 88            Point2f temp_Opxy(oppositePoints[indices[i]].x, oppositePoints[indices[i]].y);
 89            pointsOut.push_back(temp_xy);
 90            oppositePointsOut.push_back(temp_Opxy);
 91            scoreOut.push_back(score[indices[i]]);
 92        }
 93    }
 94
 95    indices.clear();
 96    free(box_area);         // 释放box_area空间
 97    free(is_suppressed);    // 释放is_suppressed空间
 98
 99    return 0;
100}
好了，代码撕完了。

原文还提供了soft-NMS的手撕菜单，感兴趣的同学可以点击原文连接对比查看两者之间的区别。"
RICS-V和ARM与X86三种芯片架构分析,2019-09-23 14:14:23,"早在10多年前，我们对芯片的理解是停留在IT时代，当时候主要是计算机的发展带来的芯片的发展，到了最近10年的发展，越来越多的设备开始有芯片，有操作系统，有数据存储。特别智能手机的出现，带动了移动设备的快速发展，而芯片就是在这个时候迅猛发展的。
在这里插入图片描述
　　那么什么是X86，是英特尔创造的，性能非常好，特别适用于电脑和笔记本，能够高效运行各种软件和执行各种指令Arm当时是被英特尔压制，当移动互联网快速到来的时候，ARM架构机会来了，移动设备体积小，电池是自带的，不能够外界电源，要求设备续航能力很强，刚好ARM架构就满足了这一个条件。

手机，平板，POS机，，还包括穿戴的手表手环芯片，都是用ARM架构做的，主要原因就是续航能力很强。

ARM在随着移动互联网大放异彩。那么Risc-V又是怎么回事呢?

RISC-V基金会于2015年由硅谷相关公司发起并成立，至今已有150多个企业或单位加入，包括谷歌、华为、英伟达、高通、麻省理工学院、普林顿大学、印度理工大学、中科院计算所等。目前RISC-V由基金会统一维护。

美国国防高级研究计划局(DAPRA)已连续多年通过专项方式支持RISC-V指令集的研究和实用化。2017年，印度政府大力资助基于RISC-V的处理器项目，RISC-V成为印度的事实国家指令集。

为啥中国特别重视Rise-V的研发呢?

其实主要是RISC-V是开放开源的，还有几个优点

1、RISC-V是后发技术，能总结前人经验，所以代码更简洁。

2、RISC-V是开源，授权费用低很多，没有专利困扰，也不用担心被美国卡脖子。

3、RISC-V是没有后面

4、RISC-V不需要向后兼容，因为是新出来的，不需要向X86一样兼容第一代第二代之类的问题。

5、RICS-V功耗更低，体积更小

从技术角度来说，RICS-V具备：成本、性能、简洁性、架构和具体实现分离、增长空间、程序大小，以及易于编程/编译/链接。相对于传统的x86和Arm架构，RISC-V具有三个优势， 即精简、模块化和可扩展性。

架构图对比

严格来说RISC-V也是ARM架构的一种，整个架构类似，反而和X86架构有很大区别，引用一下别人的总结

ARM架构图

下图所示的是ARM构架图。它由32位ALU、若干个32位通用寄存器以及状态寄存器、32&TImes;8位乘法器、32&TImes;32位桶形移位寄存器、指令译码以及控制逻辑、指令流水线和数据/地址寄存器组成。

1、ALU：它有两个操作数锁存器、加法器、逻辑功能、结果以及零检测逻辑构成。

2、桶形移位寄存器：ARM采用了32&TImes;32位的桶形移位寄存器，这样可以使在左移/右移n位、环移n位和算术右移n位等都可以一次完成。

3、高速乘法器：乘法器一般采用“加一移位”的方法来实现乘法。ARM为了提高运算速度，则采用两位乘法的方法，根据乘数的2位来实现“加一移位”运算;ARM高速乘法器采用32&TImes;8位的结构，这样，可以降低集成度(其相应芯片面积不到并行乘法器的1/3)。

4、浮点部件：浮点部件是作为选件供ARM构架使用。FPA10浮点加速器是作为协处理方式与ARM相连，并通过协处理指令的解释来执行。

5、控制器：ARM的控制器采用的是硬接线的可编程逻辑阵列PLA。

6、寄存器
在这里插入图片描述
在这里插入图片描述
　　当然，这个架构图并不是所有的都是如此，根据不同的主板，平台，架构是略有差别的比如说，目前很多主板已经将北桥集成到CPU当中，将南桥集成为PCH，但大致的框架还是如此的。下面对这个架构图上的各个内容分别进行一些简介:

1：CPU，大家都不陌生的名词，中央处理器，计算机的核心大脑。

2：北桥(North Bridge Chipset)：北桥是电脑主板上的一块芯片，位于CPU插座边，起连接作用。

3：南桥芯片(South Bridge)是主板芯片组的重要组成部分，一般位于主板上离CPU插槽较远的下方，PCI插槽的附近，这种布局是考虑到它所连接的I/O总线较多，离处理器远一点有利于布线。

4：内存是计算机中重要的部件之一，它是与CPU进行沟通的桥梁。计算机中所有程序的运行都是在内存中进行的，因此内存的性能对计算机的影响非常大。

5：显卡(Video card，Graphics card)全称显示接口卡，又称显示适配器，是计算机最基本配置、最重要的配件之一。

6：显示j接口

7：网卡是工作在链路层的网络组件，是局域网中连接计算机和传输介质的接口，不仅能实现与局域网传输介质之间的物理连接和电信号匹配，还涉及帧的发送与接收、帧的封装与拆封、介质访问控制、数据的编码与解码以及数据缓存的功能等。

8：声卡的基本功能是把来自话筒、磁带、光盘的原始声音信号加以转换，输出到耳机、扬声器、扩音机、录音机等声响设备，或通过音乐设备数字接口(MIDI)使乐器发出美妙的声音。

9：SATA(Serial Advanced Technology Attachment，串行高级技术附件)是一种基于行业标准的串行硬件驱动器接口，是由Intel、IBM、Dell、APT、Maxtor和Seagate公司共同提出的硬盘接口规范。

10：硬盘是电脑主要的存储媒介之一，由一个或者多个铝制或者玻璃制的碟片组成。碟片外覆盖有铁磁性材料。

11：总线

生态体系比较

ARM在中国影响力巨大。在芯片处理器内核方面，由于中国长期缺乏自主的通用处理器内核，几乎都是购买国外的ARM处理器内核，形成了习惯。RISC-V在中国的传播相比别的国家显得非常缓慢。

ARM和X86都有丰富而成熟的生态体系。这个是RICS-V最弱的地方，但物联网互联万物时代的到来，会给它带来充沛的生命力。

总而言之，X86是PC时代，带电源的设备用的架构，ARM是有钱单位用的移动设备芯片架构，RICS-V是新一代移动硬件设备的芯片架构，但RICS-V是刚刚开始，是个婴儿，要长成巨人还要一大段时间。"
"大脑神经网络是如何形成的,大脑自组织神经网络",2022-08-24 15:59:31,"人的大脑装入多少知识，生理前后会发生怎样变化？
大脑装入知识在于记忆，而记忆的关键是神经元突触形成和重构，因此记忆分近期记忆和远期记忆，而记忆又有不断的加强或者逐渐遗忘，生理的变化在于神经元突触。

对比人类和其它物种，人类在于拥有十分强的远期记忆，因此可以将一辈子获得知识总结下来传递给后代，经过数百万年的时间，人类总结了大量的自然知识，形成了庞大的知识体系，现代的各类学科已经远远不是一个人就能研究透的。

而相比而言，其他哺乳动物近期记忆较为明显，猪可以在短时间内掌握人类的训练，敲下盆它们就知道食物来了，可放归野外一段时间后这种记忆会慢慢消退。

人类的记忆过程也是经历了刺激-短期记忆-重现-远期记忆，也就是经历某件事后也难以形成远期记忆，但是人类可以较短的时间内不断强化相关记忆，最终成为终生都难以忘怀的记忆。

阿尔兹海默症就是一种人类记忆逐渐退化的疾病，而记忆的遗失是从近期记忆慢慢展开的，到最后远期记忆也越来越模糊，最终成为“老年痴呆”。

在解剖结构上，尽管还有很多难解的谜题，但是目前已经可以确定记忆的形成和神经元让突触的形成和重构有关。

神经元的构造是一个树突、轴突、神经末梢组成，树突中有细胞核，轴突主要承担神经传导，而神经末梢是轴突的分末部分，可以分出多个，每个头部都膨大形成凹陷的构造，可以和其它的细胞的树突形成神经元突触，是信息从一个神经元传递到另一个神经元的基础，由此构建了十分复杂的大脑神经元网络。

而一些大脑退行性变化或者大脑部分组织受损，记忆也会受到一定影响，也说明人类记忆和神经元之间的联系有密切关系。

生理构造方面的变化主要是神经元的数量和神经突触的数量，人类大脑总共有接近1000亿的神经元，相互之间可以形成大约100万亿的神经元突触，大脑中装入知识胜利生理变化的主要方面是神经元突触数量和类型等的变化，物质分泌方面肯定也会有神经递质的量的变化，但是其中还有大量未解之谜，人类大脑的复杂性即使现代人类也还不能完全观测，毕竟大脑由一颗颗的物质微粒构成。

前不久，国外的学者发现，人类大脑脑脊液会在人睡眠后从大脑中的微通道进入脑组织中冲刷脑组织，可以将导致阿尔兹海默症的β淀粉样酶冲刷而出，延缓大脑功能的退化，因此熬夜会影响大脑这一功能，进而影响人大脑的记忆等能力，所以熬夜会变傻，这个时候还在看我文章的朋友们，早点睡觉吧[笑脸]。

上图是阿尔兹海默症患者的大脑，对比正常人，大脑中出现了很多空洞，是神经元的死亡和大脑结构的消退形成的，功能受限记忆渐渐丧失，而这类患者最后死亡也和大脑有关，毕竟人体很多性命攸关的功能都存在于大脑。

谷歌人工智能写作项目：爱发猫



神经是由神经元连接形成的吗？
神经主要由三大系统组成，即脑神经、脊神经、植物神经写作猫。各系统之间以脑神经为中心，分工协同，共同实现心理功能。在这里之所以用神经而省去了系统，是因为神经本身就是个系统概念。

按生理心理学定义，神经是由神经元构成的系统，即神经元系统。其中神经元就是神经这个系统基本的功能结构单位。神经元是生理层次的物质，即特殊的细胞，不妨称为神经细胞。

撇开脑神经元、脊髓神经元、植物神经元的具体差别来看，神经元由细胞体和突起构成。神经细胞突起简称为神经纤维。

神经纤维之间纵横交错，是（神经元）构成神经元网络（即神经）的必要条件，具有信息采集与发送功能，表现为心理层面的刺激与反应；神经细胞体是神经元中基本的信息存储与处理单元；经过初步处理的信息，通过神经纤维按层次传递，直至达到脑神经，进行最后的总处理，然后将处理的结果返回到神经元，最终通过神经元上的反应器执行，产生生理反应。

神经网络的发展趋势如何？
。

神经网络的云集成模式还不是很成熟，应该有发展潜力，但神经网络有自己的硬伤，不知道能够达到怎样的效果，所以决策支持系统中并不是很热门，但是神经网络无视过程的优点也是无可替代的，云网络如果能够对神经网络提供一个互补的辅助决策以控制误差的话，也许就能使神经网络成熟起来1人工神经网络产生的背景自古以来，关于人类智能本源的奥秘，一直吸引着无数哲学家和自然科学家的研究热情。

生物学家、神经学家经过长期不懈的努力，通过对人脑的观察和认识，认为人脑的智能活动离不开脑的物质基础，包括它的实体结构和其中所发生的各种生物、化学、电学作用，并因此建立了神经元网络理论和神经系统结构理论，而神经元理论又是此后神经传导理论和大脑功能学说的基础。

在这些理论基础之上，科学家们认为，可以从仿制人脑神经系统的结构和功能出发，研究人类智能活动和认识现象。

另一方面，19世纪之前，无论是以欧氏几何和微积分为代表的经典数学，还是以牛顿力学为代表的经典物理学，从总体上说，这些经典科学都是线性科学。

然而，客观世界是如此的纷繁复杂，非线性情况随处可见，人脑神经系统更是如此。复杂性和非线性是连接在一起的，因此，对非线性科学的研究也是我们认识复杂系统的关键。

为了更好地认识客观世界，我们必须对非线性科学进行研究。人工神经网络作为一种非线性的、与大脑智能相似的网络模型，就这样应运而生了。

所以，人工神经网络的创立不是偶然的，而是20世纪初科学技术充分发展的产物。2人工神经网络的发展人工神经网络的研究始于40年代初。半个世纪以来，经历了兴起、高潮与萧条、高潮及稳步发展的远为曲折的道路。

1943年，心理学家W.S.Mcculloch和数理逻辑学家W.Pitts提出了M—P模型，这是第一个用数理语言描述脑的信息处理过程的模型，虽然神经元的功能比较弱，但它为以后的研究工作提供了依据。

1949年，心理学家提出突触联系可变的假设，根据这一假设提出的学习规律为神经网络的学习算法奠定了基础。

1957年，计算机科学家Rosenblatt提出了著名的感知机模型，它的模型包含了现代计算机的一些原理，是第一个完整的人工神经网络，第一次把神经网络研究付诸工程实现。

由于可应用于模式识别，联想记忆等方面，当时有上百家实验室投入此项研究，美国军方甚至认为神经网络工程应当比“原子弹工程”更重要而给予巨额资助，并在声纳信号识别等领域取得一定成绩。

1960年，B.Windrow和E.Hoff提出了自适应线性单元，它可用于自适应滤波、预测和模式识别。至此，人工神经网络的研究工作进入了第一个高潮。

1969年，美国著名人工智能学者M.Minsky和S.Papert编写了影响很大的Perceptron一书，从理论上证明单层感知机的能力有限，诸如不能解决异或问题，而且他们推测多层网络的感知机能力也不过如此，他们的分析恰似一瓢冷水，很多学者感到前途渺茫而纷纷改行，原先参与研究的实验室纷纷退出，在这之后近10年，神经网络研究进入了一个缓慢发展的萧条期。

这期间，芬兰学者T.Kohonen提出了自组织映射理论，反映了大脑神经细胞的自组织特性、记忆方式以及神经细胞兴奋刺激的规律；美国学者S.A.Grossberg的自适应共振理论（ART）；日本学者K.Fukushima提出了认知机模型；ShunIchimari则致力于神经网络有关数学理论的研究等，这些研究成果对以后的神经网络的发展产生了重要影响。

美国生物物理学家J.J.Hopfield于1982年、1984年在美国科学院院刊发表的两篇文章，有力地推动了神经网络的研究，引起了研究神经网络的又一次热潮。

1982年，他提出了一个新的神经网络模型——hopfield网络模型。他在这种网络模型的研究中，首次引入了网络能量函数的概念，并给出了网络稳定性的判定依据。

1984年，他又提出了网络模型实现的电子电路，为神经网络的工程实现指明了方向，他的研究成果开拓了神经网络用于联想记忆的优化计算的新途径，并为神经计算机研究奠定了基础。

1984年Hinton等人将模拟退火算法引入到神经网络中，提出了Boltzmann机网络模型，BM网络算法为神经网络优化计算提供了一个有效的方法。

1986年，D.E.Rumelhart和J.LMcclelland提出了误差反向传播算法，成为至今为止影响很大的一种网络学习方法。

1987年美国神经计算机专家R.Hecht—Nielsen提出了对向传播神经网络，该网络具有分类灵活，算法简练的优点，可用于模式分类、函数逼近、统计分析和数据压缩等领域。

1988年L.Ochua等人提出了细胞神经网络模型，它在视觉初级加工上得到了广泛应用。为适应人工神经网络的发展，1987年成立了国际神经网络学会，并决定定期召开国际神经网络学术会议。

1988年1月NeuralNetwork创刊。1990年3月IEEETransactiononNeuralNetwork问世。

我国于1990年12月在北京召开了首届神经网络学术大会，并决定以后每年召开一次。1991年在南京成立了中国神经网络学会。IEEE与INNS联合召开的IJCNN92已在北京召开。

这些为神经网络的研究和发展起了推波助澜的作用，人工神经网络步入了稳步发展的时期。90年代初，诺贝尔奖获得者Edelman提出了Darwinism模型，建立了神经网络系统理论。

同年，Aihara等在前人推导和实验的基础上，给出了一个混沌神经元模型，该模型已成为一种经典的混沌神经网络模型，该模型可用于联想记忆。

Wunsch在90OSA年会上提出了一种AnnualMeeting，用光电执行ART，学习过程有自适应滤波和推理功能，具有快速和稳定的学习特点。

1991年，Hertz探讨了神经计算理论，对神经网络的计算复杂性分析具有重要意义；Inoue等提出用耦合的混沌振荡子作为某个神经元，构造混沌神经网络模型，为它的广泛应用前景指明了道路。

1992年，Holland用模拟生物进化的方式提出了遗传算法，用来求解复杂优化问题。1993年方建安等采用遗传算法学习，研究神经网络控制器获得了一些结果。

1994年Angeline等在前人进化策略理论的基础上，提出一种进化算法来建立反馈神经网络，成功地应用到模式识别，自动控制等方面；廖晓昕对细胞神经网络建立了新的数学理论和方法，得到了一系列结果。

HayashlY根据动物大脑中出现的振荡现象，提出了振荡神经网络。

1995年Mitra把人工神经网络与模糊逻辑理论、生物细胞学说以及概率论相结合提出了模糊神经网络，使得神经网络的研究取得了突破性进展。

Jenkins等人研究光学神经网络，建立了光学二维并行互连与电子学混合的光学神经网络，它能避免网络陷入局部最小值，并最后可达到或接近最理想的解；SoleRV等提出流体神经网络，用来研究昆虫社会，机器人集体免疫系统，启发人们用混沌理论分析社会大系统。

1996年，ShuaiJW’等模拟人脑的自发展行为，在讨论混沌神经网络的基础上提出了自发展神经网络。

1997、1998年董聪等创立和完善了广义遗传算法，解决了多层前向网络的最简拓朴构造问题和全局最优逼近问题。

随着理论工作的发展，神经网络的应用研究也取得了突破性进展，涉及面非常广泛，就应用的技术领域而言有计算机视觉，语言的识别、理解与合成，优化计算，智能控制及复杂系统分析，模式识别，神经计算机研制，知识推理专家系统与人工智能。

涉及的学科有神经生理学、认识科学、数理科学、心理学、信息科学、计算机科学、微电子学、光学、动力学、生物电子学等。美国、日本等国在神经网络计算机软硬件实现的开发方面也取得了显著的成绩，并逐步形成产品。

在美国，神经计算机产业已获得军方的强有力支持，国防部高级研究计划局认为“神经网络是解决机器智能的唯一希望”，仅一项8年神经计算机计划就投资4亿美元。

在欧洲共同体的ESPRIT计划中，就有一项特别项目：“神经网络在欧洲工业中的应用”，单是生产神经网络专用芯片这一项就投资2200万美元。据美国资料声称，日本在神经网络研究上的投资大约是美国的4倍。

我国也不甘落后，自从1990年批准了南开大学的光学神经计算机等3项课题以来，国家自然科学基金与国防预研基金也都为神经网络的研究提供资助。

另外，许多国际著名公司也纷纷卷入对神经网络的研究，如Intel、IBM、Siemens、HNC。神经计算机产品开始走向商用阶段，被国防、企业和科研部门选用。

在举世瞩目的海湾战争中，美国空军采用了神经网络来进行决策与控制。在这种刺激和需求下，人工神经网络定会取得新的突破，迎来又一个高潮。自1958年第一个神经网络诞生以来，其理论与应用成果不胜枚举。

人工神经网络是一个快速发展着的一门新兴学科，新的模型、新的理论、新的应用成果正在层出不穷地涌现出来。

3人工神经网络的发展前景针对神经网络存在的问题和社会需求，今后发展的主要方向可分为理论研究和应用研究两个方面。（1）利用神经生理与认识科学研究大脑思维及智能的机理、计算理论，带着问题研究理论。

人工神经网络提供了一种揭示智能和了解人脑工作方式的合理途径，但是由于人类起初对神经系统了解非常有限，对于自身脑结构及其活动机理的认识还十分肤浅，并且带有某种“先验”。

例如，Boltzmann机引入随机扰动来避免局部极小，有其卓越之处，然而缺乏必要的脑生理学基础，毫无疑问，人工神经网络的完善与发展要结合神经科学的研究。

而且，神经科学，心理学和认识科学等方面提出的一些重大问题，是向神经网络理论研究提出的新挑战，这些问题的解决有助于完善和发展神经网络理论。

因此利用神经生理和认识科学研究大脑思维及智能的机理，如有新的突破，将会改变智能和机器关系的认识。

利用神经科学基础理论的研究成果，用数理方法探索智能水平更高的人工神经网络模型，深入研究网络的算法和性能，如神经计算、进化计算、稳定性、收敛性、计算复杂性、容错性、鲁棒性等，开发新的网络数理理论。

由于神经网络的非线性，因此非线性问题的研究是神经网络理论发展的一个最大动力。

特别是人们发现，脑中存在着混沌现象以来，用混沌动力学启发神经网络的研究或用神经网络产生混沌成为摆在人们面前的一个新课题，因为从生理本质角度出发是研究神经网络的根本手段。

（2）神经网络软件模拟，硬件实现的研究以及神经网络在各个科学技术领域应用的研究。

由于人工神经网络可以用传统计算机模拟，也可以用集成电路芯片组成神经计算机，甚至还可以用光学的、生物芯片的方式实现，因此研制纯软件模拟，虚拟模拟和全硬件实现的电子神经网络计算机潜力巨大。

如何使神经网络计算机与传统的计算机和人工智能技术相结合也是前沿课题；如何使神经网络计算机的功能向智能化发展，研制与人脑功能相似的智能计算机，如光学神经计算机，分子神经计算机，将具有十分诱人的前景。

4哲理（1）人工神经网络打开了认识论的新领域认识与脑的问题，长期以来一直受到人们的关注，因为它不仅是有关人的心理、意识的心理学问题，也是有关人的思维活动机制的脑科学与思维科学问题，而且直接关系到对物质与意识的哲学基本问题的回答。

人工神经网络的发展使我们能够更进一步地既唯物又辩证地理解认识与脑的关系，打开认识论的新领域。

人脑是一个复杂的并行系统，它具有“认知、意识、情感”等高级脑功能，用人工进行模拟，有利于加深对思维及智能的认识，已对认知和智力的本质的研究产生了极大的推动作用。

在研究大脑的整体功能和复杂性方面，人工神经网络给人们带来了新的启迪。

由于人脑中存在混沌现象，混沌可用来理解脑中某些不规则的活动，从而混沌动力学模型能用作人对外部世界建模的工具，可用来描述人脑的信息处理过程。

混沌和智能是有关的，神经网络中引入混沌学思想有助于提示人类形象思维等方面的奥秘。

人工神经网络之所以再度兴起，关键在于它反映了事物的非线性，抓住了客观世界的本质，而且它在一定程度上正面回答了智能系统如何从环境中自主学习这一最关键的问题，从认知的角度讲，所谓学习，就是对未知现象或规律的发现和归纳。

由于神经网络具有高度的并行性，高度的非线性全局作用，良好的容错性与联想记忆功能以及十分强的自适应、自学习功能，而使得它成为揭示智能和了解人脑工作方式的合理途径。

但是，由于认知问题的复杂性，目前，我们对于脑神经网的运行和神经细胞的内部处理机制，如信息在人脑是如何传输、存贮、加工的？记忆、联想、判断是如何形成的？大脑是否存在一个操作系统？

还没有太多的认识，因此要制造人工神经网络来模仿人脑各方面的功能，还有待于人们对大脑信息处理机理认识的深化。

（2）人工神经网络发展的推动力来源于实践、理论和问题的相互作用随着人们社会实践范围的不断扩大，社会实践层次的不断深入，人们所接触到的自然现象也越来越丰富多彩、纷繁复杂，这就促使人们用不同的原因加以解释不同种类的自然现象，当不同种类的自然现象可以用同样的原因加以解释，这样就出现了不同学科的相互交叉、综合，人工神经网络就这样产生了。

在开始阶段，由于这些理论化的网络模型比较简单，还存在许多问题，而且这些模型几乎没有得到实践的检验，因而神经网络的发展比较缓慢。

随着理论研究的深入，问题逐渐地解决特别是工程上得到实现以后，如声纳识别成功，才迎来了神经网络的第一个发展高潮。

可Minisky认为感知器不能解决异或问题，多层感知器也不过如此，神经网络的研究进入了低谷，这主要是因为非线性问题没得到解决。

随着理论的不断丰富，实践的不断深入，现在已证明Minisky的悲观论调是错误的。今天，高度发达的科学技术逐渐揭示了非线性问题是客观世界的本质。

问题、理论、实践的相互作用又迎来了人工神经网络的第二次高潮。目前人工神经网络的问题是智能水平不高，还有其它理论和实现方面的问题，这就迫使人们不断地进行理论研究，不断实践，促使神经网络不断向前发展。

总之，先前的原因遇到了解释不同的新现象，促使人们提出更加普遍和精确的原因来解释。

理论是基础，实践是动力，但单纯的理论和实践的作用还不能推动人工神经网络的发展，还必须有问题提出，才能吸引科学家进入研究的特定范围，引导科学家从事相关研究，从而逼近科学发现，而后实践又提出新问题，新问题又引发新的思考，促使科学家不断思考，不断完善理论。

人工神经网络的发展无不体现着问题、理论和实践的辩证统一关系。

（3）人工神经网络发展的另一推动力来源于相关学科的贡献及不同学科专家的竞争与协同人工神经网络本身就是一门边缘学科，它的发展有更广阔的科学背景，亦即是众多科研成果的综合产物，控制论创始人Wiener在其巨著《控制论》中就进行了人脑神经元的研究；计算机科学家Turing就提出过B网络的设想；Prigogine提出非平衡系统的自组织理论，获得诺贝尔奖；Haken研究大量元件联合行动而产生宏观效果，非线性系统“混沌”态的提出及其研究等，都是研究如何通过元件间的相互作用建立复杂系统，类似于生物系统的自组织行为。

脑科学与神经科学的进展迅速反映到人工神经网络的研究中，例如生物神经网络理论，视觉中发现的侧抑制原理，感受野概念等，为神经网络的发展起了重要的推动作用。

从已提出的上百种人工神经网络模型中，涉及学科之多，令人目不暇接，其应用领域之广，令人叹为观止。不同学科专家为了在这一领域取得领先水平，存在着不同程度的竞争，所有这些有力地推动了人工神经网络的发展。

人脑是一个功能十分强大、结构异常复杂的信息系统，随着信息论、控制论、生命科学，计算机科学的发展，人们越来越惊异于大脑的奇妙，至少到目前为止，人类大脑信号处理机制对人类自身来说，仍是一个黑盒子，要揭示人脑的奥秘需要神经学家、心理学家、计算机科学家、微电子学家、数学家等专家的共同努力，对人类智能行为不断深入研究，为人工神经网络发展提供丰富的理论源泉。

另外，还要有哲学家的参与，通过哲学思想和自然科学多种学科的深层结合，逐步孕育出探索人类思维本质和规律的新方法，使思维科学从朦胧走向理性。

而且，不同领域专家的竞争与协调同有利于问题清晰化和寻求最好的解决途径。纵观神经网络的发展历史，没有相关学科的贡献，不同学科专家的竞争与协同，神经网络就不会有今天。

当然，人工神经网络在各个学科领域应用的研究反过来又推动其它学科的发展，推动自身的完善和发展。

神经元法则
。

大脑皮层灰质探秘——神经元的信号处理法则初探--------------------------------------------------------------------------------作者:来源:生物通发布者:刘斌类别:新闻扫描日期:2004-06-21今日/总浏览:2/178来自USC和Technion医学院（以色列）的研究人员发现一些有助于揭开超复杂的大脑神经元秘密的新线索。

他们的发现刊登在本月的NatureNeuroscience上。这些发现与关于神经元加工处理信息的“算法”的一种被广泛接受的观点相抵触。

“令人吃惊的是经过了100年的现代神经系统科学研究，我们仍然不知道一个神经元的基本的信息处理功能”，USCViterbi工程学院副教授、文章作者BartlettMel说。

由于没有更有说服力的看法，因此通常认为一个大脑细胞对所接受的刺激是线性叠加的，即受到A和B两种共同刺激所产生的兴奋相当于分别受到A和B刺激所产生的兴奋的总和。

“我们证实细胞显然是不按照这个推测来运算的”，Mel说。研究组发现单个神经元中的信息总量取决于细胞表面上接受信息输入的位置。

了解一些有关大脑细胞的知识，将有助于理解这个研究组的工作和其发现的意义。大脑中进行的所以信息加工都是由神经元网络进行操控。这些活细胞具有各种各样的形状和大小尺寸，通常类似树枝状。

一个神经元表面散布着数以千计的神经突触，这些神经突触正是神经元细胞接受外界信号的位点。当受到刺激时，每个神经突触能产生一个小的局部的电压反应。

根据对神经元的传统观点，突触反应沿着神经元中像电缆一样的树突传递下去并叠加。如果总的电压反应足够大就会产生一个电位并沿着细胞的轴突传递、与其它成千上万个神经元进行交流。

“最近的证据显示这个过程并不是那么简单”，Mel说。“输入的信号可能会在神经树突中相互作用并且可能在通往细胞体的路途上发生了转化。

尤其是，神经树突的单个分枝在某种情况下能产生局部电位从而显著扩大了神经树突中的局部区域的突触反应”。研究组着手建立神经元用来合并多个突触信号的算法，从而集中研究这种组成大脑皮层灰质的锥体神经元。

这些实验在以色列的海法由论文的第一作者、Technion的研究生AlonPolsky和JackieSchiller进行。

借助大鼠的大脑皮层组织切片，Polsky和Schiller可以定位单个锥体神经元，进行染色后用细胞外电极刺激靠近树突分支的细胞。

通过1—2个刺激电极对树突的不同位置（例如在相同的或不同的树突分支上）施放电刺激，同时记录细胞体上的电压。研究人员接着比较了分别进行两种刺激时和合并在一起时的细胞体的电压反应。

“Schiller的方法最强有力的一点就是你可以看到你正在刺激哪个部位，这是因为在突触被刺激的地方染料会微弱发光”，Mel说。Mel在USC对实验设计和数据分析进行了远程合作。

“你可以对细胞上的特殊空间位置进行刺激并且观察到不同位置产生的差异。”Mel说。房地产界的关键用词“位置决定一切”，在神经元细胞中也一样。

研究资料显示当两个电极A和B刺激同一个树突分支时出现了三种不同的情况：1.如果细胞对这两种刺激（电极A和B）的总反应低于分支上的局部的电位阈值，这个总和呈线性叠加关系，即A+B。

2.如果两种输入合起来刚刚能超过局部阈值，它们的总和表现为超线性，即大于A+B。3.如果单个输入已经能够超过局部阈值，这个总和是次线性的，即小于A+B。

Mel解释了这个方法的最后一点：“如果两个人每个人都有一根火柴，一起点燃一把火，那么一旦第一根火柴点着火，再加入第二根火柴并不会使这把火两倍的光和热。第二个火柴不会起到什么作用。

”研究人员还发现不同树突分支上的输入总和总是表现出线性叠加关系，即如同点亮了两把火。这些发现支持了一个2003年由Mel的实验室进行的模型研究。

在这个模型中，他和研究生PanayiotaPoirazi推测出锥体神经元的这种行为过程。这是第一个对那些推测的实验验证。“因此，我们现在将神经元想成一个双层模型”，Mel说。

“加工的第一层发生在单个的树突分支中。每个分支独立地添加所接受的信号，然后遵从它自己的局部阈值的非线性原则。在第二层的加工中，来源于所有不同分支的信号在细胞体上进行线性叠加，并帮助决定细胞的总的域值。

”虽然这些结果很有前途，但研究组明白这不是锥体神经元的最终结论。“勿庸置疑，这仍然是个简单的模型”，Mel说。“但这种双层模型显然优于将神经元假设成简单的进行线性加合的模型。那明显与这些数据不符。

”据Mel所说，最终需要解决的另外一个难题是来自神经元的最远端即apicaltuft的突触信号可能会和细胞体近端的基部树突的输入信号以微妙的方式发生相互作用。

“我们现在希望知道我们是否需要将双层模型扩展成三层模型”，Mel说。“可能基部和顶端树突分别按照我们所说的规则运转，但当它们发生相互作用时就会有另外一种非线性的反应发生。

Mel强调说，他们发现的这种锥体神经元的算术规则可能不能适用于大脑中的所有神经元。“还有另外一些具有不同形状、输入、形态和离子通道的神经元存在”，他说。

“根据我们所观察到的神经元，这个问题可能有很多不同的答案。”尽管还有许多工作要做，但新的成像技术、逼真的模型和现代化的实验操作正在使得这项了解大脑的复杂的神经元的任务在总体上变的简单了许多。

最后，Mel说从单个神经元观察到的知识对进一步从整体上了解大脑是至关重要的。“我们常将大脑看成是一部计算机”，他说。

“如果我们想知道这部计算机是如何工作的，我们就必须首先知道它的各个部件是如何工作的。”

什么是激活孩子大脑的最好方式？
很多新手妈妈在宝宝出生后都会特别在意宝宝头围大小。她们总觉得宝宝脑袋大就是聪明，头围越大越好。其实头骨过大、也可能是脑部出现了问题的表现。

父母可以比较以下儿童头围参数表，对照下自己孩子的头围大小是否在正常范围内。那么头围大的孩子真的比头围小的孩子聪明吗，其实这是谣传。

著名的脑科学家、心理学教授洪兰说，在统计学上，脑袋大小和聪明的关系系数只有0.34，只有12%的IQ可以归因到脑体积的大小。

举个例子，我们普通人的脑重大约在1400g，爱因斯坦的脑重才1230g，大文豪屠格涅夫脑重为2012g。脑重差距如此大的二人却都是这么优秀，这说明脑袋大小跟智商高低关系不大。

那究竟是什么造成了人与人之间的智商差距呢？脑科学家研究发现，这与后期大脑发育有关，脑科学家通过观察发现，从出生以后大脑神经元连接就一直在发展。

直到二十岁才趋近于成熟，神经元连接的数量才趋于稳定，所以说大脑发育的程度与后天的经验刺激有关！很多父母都早早地给孩子报了很多兴趣班，希望孩子获得多种技能。从塑造大脑的角度来说，是有一定道理的。

每一种新环境对孩子来说都能够带来不同的体验，加速脑内神经元的建立。给孩子报兴趣班是一种选择，但不是最佳选择。著名脑科学领域的专家洪兰曾经说过：“激活大脑最好的方式，就是运动、阅读和游戏”。

而这三种方法，也被称之为激活大脑的“三把钥匙”。

第一把钥匙：运动研究发现，动作训练能够促进神经元的连接，经常运动和不经常运动的孩子相比，神经元网络会更加丰富，所以爱运动的孩子大脑更聪明（所以四肢发达，头脑简单是误区）。

如果稍微了解过脑科学，家长一定反对孩子体育课天天被语数外霸占。其实，每个人都需要多运动来保证各种激素的平衡，才不会生病。而且运动能使人分泌多巴胺、肾上腺素等激素。这些激素对人体至关重要。

多巴胺不足可能会引起多动；肾上腺素不足引会起注意力不集中。但同时家长也要注意：每个人的天分不同，要根据孩子的协调性，肌耐力等综合考量，选择符合孩子天赋的运动项目，别盲目跟风。

第二把钥匙：阅读说话是本能，而阅读是后天习性。孩子有基本对话能力后，就应该有意识培养他的阅读能力了。一般2岁为宜。洪兰说，人一分钟可以看字788个，可以说264个字，阅读所获得的信息量更多。

著名脑科学家洪兰研究发现，阅读能够刺激大脑的各个区域，文字能够帮助孩子发展所需发展的能力，而绘本中的图画还能提升孩子的审美，绘本给孩子感官的刺激丰富，更有助于大脑的发展。

父母请注意：针对幼儿选择绘本时，建议选择饱和度高的鲜艳的绘本，更有助于刺激各项感官，同时亲子阅读一定要挑选孩子感兴趣的，适合孩子的。符合孩子天性的阅读，才能事半功倍。

第三把钥匙：游戏孩子在快乐的时候，他学习任何东西都比较容易。——斯宾塞（英）只要稍微对脑科学有所了解，父母一定不会反对孩子玩游戏。

在游戏中，大脑会产生一种可以促进神经分叉生长的物质，并且是大脑发育的营养物质。换句话说，孩子游戏中的想象力在不知不觉中使他的大脑变得更加强大！这一点哈佛大学的儿童发展研究中心也早有推论。

在哈佛大学的研究中认为，通过游戏能够给孩子提供丰富的经验刺激，建立更加牢固的单脑神经元连接，而这种连接对于孩子未来学习、行为、健康都有重要作用。

父母需要记住：亲子互动游戏效果最好，同时选择游戏的时候，要满足这几点，趣味性、适龄性、创造性和难易程度的递进！

互联网神经学主要内容是什么
。

互联网与神经学这两个原本距离遥远的领域，关系远比想象的要深入和密切，过去10年建立的相关理论和实践基础，使得这两个领域交叉可以产生出21世纪的新学科-互联网神经学（Internetneurology）。

互联网神经学的定义为：基于神经学的研究成果，将互联网硬件结构，软件系统，数据与信息，商业应用有机的整合起来，从而构建互联网完整架构体系，并预测互联网沿着神经学路径可能产生的新功能和新架构；根据互联网不断产生和稳定下来的功能结构，提出研究设想，分析人类大脑产生意识，思想，智能，认知的生物学基础；研究互联网和人类大脑结构如何相互影响，相互塑造，相互结合，相互促进的双巨系统交叉关系。

中文名互联网神经学外文名Internetneurology一。革命技术引发科学突破每一次人类社会的重大技术变革都会导致新领域的科学革命，大航海时代使人类看到了生物的多样性和孤立生态系统对生物的影响。

无论是达尔文还是华莱士都是跟随远航的船队才发现了生物的进化现象。大工业革命使人类无论在力量的使用还是观察能力都获得的极大的提高。为此后100年开始的物理学大突破，奠定了技术基础。

这些突破包括牛顿的万有引力，爱因斯坦的相对论，和众多科学家创建的量子力学大厦，这些突破都与”力“和”观测“有关。互联网革命对于人类的影响已经远远超过了大工业革命。

与工业革命增强人类的力量和视野不同，互联网极大的增强了人类的智慧，丰富了人类的知识。而智慧和知识恰恰与大脑的关系最为密切.二。

互联网与神经科学的7年研究历史从2008年9月，刘锋与科学院大学彭庚教授在科技论文在线发表论文“互联网进化规律的发现与分析”开始，的7年时间里，共发表论文10篇，专著《互联网进化论》一部对互联网与神经学的关系进行深入探讨。

从神经学的角度分析互联网的成熟结构，将其抽象为一个与人类大脑高度相似的组织结构-互联网虚拟大脑。寻找并定位互联网的虚拟听觉，视觉，感觉，运动神经系统，虚拟中枢神经系统等。

绘制出互联网虚拟大脑结构图，如图1。

图1互联网虚拟大脑结构图同时，我们也对应提出如果脑科学对互联网的未来发展有重要的启发作用，那么不断发展的互联网结构和功能会不会能够作为解开大脑之谜的钥匙，即人脑中在数万年前就已经包含了今天和未来成熟的互联网架构，通过神经生理学和神经心理学领域的研究实验，在人类大脑中是否可以找到Google一样的搜索引擎，Facebook一样的SNS系统，IPv4一样的地址编码系统，思科一样的路由系统。

2012年开始美国等国家的科研人员也开始注意到互联网与脑科学的关系，2012年11月16日，加州大学圣迭戈分校DmitriKrioukov在《ScientificReport》发表的论文“NetworkCosmology”提出互联网与脑神经网络的发展与构造具有高度的相似性。

2015年2月4日来自巴塞尔大学的研究人员报道称，发现大脑中的神经元像一个社会网络一样连接在一起。每个神经细胞都与许多其他的神经细胞相连接，但只有少数彼此非常相似的细胞之间会建立最强有力的联系。

这些研究结果发布在2015年2月4日的《自然》(Nature)杂志上。三。

互联网神经学的提出与研究方向互联网与神经学这两个原本距离遥远的领域，关系远比想象的要深入和密切，过去10年建立的相关理论和实践基础，使得这两个领域交叉可以产生出21世纪的新学科-互联网神经学（Internetneurology），互联网进化论作者刘锋这样定义互联网神经学（Internetneurology）：基于神经学的研究成果，将互联网硬件结构，软件系统，数据与信息，商业应用有机的整合起来，从而构建互联网完整架构体系，并预测互联网沿着神经学路径可能产生的新功能和新架构；根据互联网不断产生和稳定下来的功能结构，提出研究设想，分析人类大脑产生意识，思想，智能，认知的生物学基础；研究互联网和人类大脑结构如何相互影响，相互塑造，相互结合，相互促进的双巨系统交叉关系。

如果以脑科学和互联网为横坐标轴两端，生理学和心理学作为纵坐标的上下两段，互联网神经学将由四部分组成：互联网神经生理学，互联网神经心理学，大脑互联网生理学，大脑互联网心理学，它们之间的交叉部分将形成第五个组成部分-互联网认知科学，他们的关系如图二所示。

。

四，破解人类核心机密--互联网神经学的5个组成部分互联网神经生理学（Internetneurophysiology）重点研究基于神经学的互联网基础功能和架构，包括但不限于互联网中枢神经系统，互联网感觉神经系统，互联网运动神经系统，互联网自主神经系统，互联网神经反射弧，基于深度学习等算法，运用互联网大数据进行图像，声音，视频识别等互联网人工智能处理机制。

互联网神经心理学（Internetneuropsychology.）重点研究互联网在向成熟脑结构进化的过程中，产生的类似神经心理学的互联网现象。

包括但不仅限于互联网群体智慧的产生问题，互联网的情绪问题，互联网梦境的产生和特点，互联网的智商问题等。

大脑互联网生理学（BrainInternetphysiology）重点研究大脑中存在的类似于互联网的功能结构，使得不断发展的互联网成为破解大脑生物学原理的参照系，包括但不仅限于大脑中的类搜索引擎机制，大脑中类互联网路由机制，大脑中的类IPv4/IPv6机制，大脑神经元类社交网络的交互机制，人类使用互联网对大脑生理学结构的重塑影响等。

大脑互联网心理学(BrainInternetpsychology)重点研究互联网对人类大脑在心理学层面的影响和重塑，包括但不仅限于互联网对使用者产生的网瘾问题，互联网对使用者智商影响问题，互联网对使用者情绪和社交关系的影响问题等互联网认知科学（TheInternetincognitivescience）可看做互联网神经生理学，互联网神经心理学，大脑互联网生理学，大脑互联网心理学的组合交叉，重点研究互联网和大脑两个巨系统相互影响，相互塑造，相互结合，互助进化，从而产生智慧，认知，情绪的深层次原理。

五建立在中国诞生的新学科：互联网神经学是在过去10年的研究基础上形成的新科学理论，目标是在生理学和心理学两个层面，将互联网和脑科学这两个领域进行交叉对比，从而为建立完整的互联网架构，预测互联网未来发展趋势，大脑架构和运行机理，以及智能，认知，情感的产生原理提供理论支持。

由于互联网和脑科学都处在高速发展过程中，根据互联网和脑科学出现的新进展，吸收其中的合理成分，使之成为互联网神经学发展的第一个动力。

同时运用互联网神经学理论对互联网的未来进行预测，对大脑中的未知结构和运行机制提出研究设想。根据实验结构对互联网神经学理论进行优化和调整，使之成为理论发展的第二个动力。

大脑各神经元合成神经网然后是怎么合作思考的?而突触越多也越聪明?
1，大脑各神经元合成神经网然后是依靠突触这个结构把各个神经元联系在一起，合作思考的。突触是神经元之间在功能上发生联系的部位，也是信息传递的最关键部位。

在光学显微镜下，可以看到一个神经元的轴突末梢经过多次分支，最后每一小支的末端膨大呈杯状或球状，叫做突触小体，它们可以与多个神经元的细胞体或者树突相接触，形成突触。

这种突触是由突触前膜、突触间隙、突触后膜三部分构成。

由于突触的单向传递，中枢神经系统内冲动的传递就有一定的方向，即由传入神经元传向中间神经元，再由中间神经元传向传出神经元，从而使整个神经系统的活动能够有规律地进行。

中枢神经系统中任何反射活动，都需经过突触传递才能完成。2，突触越多也越聪明这种说法是有一定道理的。

突触越多，神经回路越多，人类的思维和想法就越多，思考的宽度就越大，解决问题的办法也就越多，所以说，这种说法是被认可的。

学习不开窍的原因
作为父母最怕老师说啥话？我最怕老师说，这孩子挺聪明，就是学习不“开窍”！很多家长就期待着，孩子某天能突然收获什么内功心法，打通“任督二脉”就开窍了！

这也不是天方夜谭，很多孩子在高一时候学习成绩平平，到了高三突然如“开挂”一般，从中等生挤进前三名的例子比比皆是！为什么孩子不“开窍”？那想要孩子开窍，我们首先要了解，为啥孩子就是不开窍呢？

有以下2个原因，大脑神经网络没打通和认知构架没建立起来两种。一、大脑神经网络没打通脑科学家表明：一个人的智商是否高，与大脑皮层的神经元连接数量（网络）有着直接关系。

神经元连接的数量与外界环境的刺激有关，刺激越多神经元连接越多，孩子的对大脑越聪明。有些孩子在成长中可能缺乏外界刺激，所以大脑给人感觉不灵活。

二、大脑认知框架没建立起来孩子大脑不开窍，就是因为在不断长大得这个过程中，没有得到不断提升认知的框架，大脑对于知识的储备是零散的，很片面，就是不够具象。

比如孩子一道题讲了很多遍还是不理解，这说明孩子缺乏透过现象看本质的能力，这种能力的提升，需要通过重点量的积累来建立认知框架。那如何才能让孩子开窍？

自我驱动力的觉醒是关键无论是外界刺激，还是认知构架的建立的基础是觉醒自我驱动力，只有孩子主动去探索学习的方法、主动去了解知识，接受的刺激才越多，才能快速建立起认知构架。

自我驱动力是很多家长忽略的能力，小时候家长习惯给孩子选择衣服、鞋子、学校甚至爱好，久而久之人生好像父母的，孩子的积极主动型越来越差，父母还觉得是为孩子好，其实是在帮倒忙。

想要激发孩子的自我驱动力，家长要了解马洛斯需求理论，在这个理论中，曾经提到人类有5种需求：生理需求、安全需求、归属与爱的需求、尊重需求以及自我实现需求。

孩子自然也是如此，很多家长给了孩子生理、安全、归属的需求，却忽视了尊重和自我实现的重要性。居里夫人曾说过：“路要靠自己去走，才能越走越宽”。家长要做的是把权利交给孩子！

父母不要什么事都要大包大揽，对于学习这件事孩子上了一天课，对于知识，哪没听懂，哪没讲透，心里都有一个大概的印象，父母在旁边只能给予建议！

父母可以帮孩子拆解目标，这样对于孩子来说更容易实现，这是给孩子建议，而不是帮孩子做主。来自信任的人的鼓励除了给孩子权利，父母不要吝啬鼓励，来自信任的人的鼓励，也是孩子开窍的另一大因素。

在教育领域有一个心理效应，叫做“罗森塔尔效应”。讲述的是，有两名心理学家，一个叫罗森塔尔，另一个叫雅各布森。

他俩去到了一所学校，这个学校孩子的成绩都很低，于是就对一帮孩子进行智商测试，测试完，罗森塔尔列出一份最聪明孩子的单子（每个人都入选了），在不久以后，奇迹发现了，每个学生的成绩都取得了显著的进步，而且个个自信心爆棚，求知欲旺盛！

这个实验表明：如果想要孩子大脑开窍，父母的鼓励也非常重要，孩子听见对父母的评价，才会继续努力。所以家长们，从今天开始不要帮倒忙，干那些费力不讨好的事儿，时刻记住人生是孩子自己的，你能给的只是建议。"
"专用神经网络处理器的芯片,cpu可以跑神经网络吗",2022-10-21 12:04:36,"苹果产品中的“统一内存(unifie？emory)“与以往的”内存怎么样？
这个统一内存就是把内存、GPU的显存和神经网络处理器的缓存放到一起，通过Fabric和CPU/GPU/神经网络处理器相连。

好处是等于在CPU与GPU间加了个缓存区，它们之间的数据传输更容易;坏处就是三个公用那么大统一内存(比如有16GB)，等于给CPU的内存实际不到16GB，其中有部分不得不分给GPU和神经处理器。

这点做法，就是将以往分布式结构存储，变为现在的System on Chip。片上系统拥有更优异的效率，能够集成更大的Cache，Memory以及定制化需求。

作为SoC的一种优势，节省了平时所需的面积以外，内部总线访问效率能够提升更大。在以往的芯片当中，为了集成度以及空间大小，要做一种均衡型考量，体积大，占用面积大。集成度高，成本高。

工作方式既然是一个集中化的磁盘阵列，那么就支持主机系统通过IP网络进行文件级别的数据访问，或通过光纤协议在SAN网络进行块级别的数据访问。

同样，iSCSI亦是一种非常通用的IP协议，只是其提供块级别的数据访问。这种磁盘阵列配置多端口的存储控制器和一个管理接口，允许存储管理员按需创建存储池或空间，并将其提供给不同访问类型的主机系统。

最通常的协议一般都包括了NAS和FC，或iSCSI和FC。

当然，也可以同时支持上述三种协议的，不过一般的存储管理员都会选FC或iSCSI中的一种，它们都提供块级别的访问方式，和文件级别的访问方式(NAS方式)组成统一存储。

谷歌人工智能写作项目：神经网络伪原创



手机芯片NPU是什么？
嵌入式神经网络处理器（NPU）采用“数据驱动并行计算”的架构，特别擅长处理视频、图像类的海量多媒体数据写作猫。

简介cpu=center processing unitsnpu=neural-network processing unitsnpu不是测试的项目，是网络处理器，可以把这个认为是一个组件（或者子系统），有的时候也可以称呼为[2] NPU协处理器。

嵌入式神经网络处理器（NPU）采用“数据驱动并行计算”的架构，特别擅长处理视频、图像类的海量多媒体数据。

推进过程2016年6 月 20 日，中星微数字多媒体芯片技术 国家重点实验室在北京宣布，已研发成功了中国首款嵌入式神经网络处理器（NPU）芯片，成为全球首颗具备深度学习人工智能的嵌入式视频采集压缩编码系统级芯片，并取名“星光智能一号”。

这款基于深度学习的芯片运用在人脸识别上，最高能达到98%的准确率，超过人眼的识别率。该芯片于今年3月6日实现量产，目前出货量为十几万件。

该实验室执行主任、中星微首席技术官张韵东在接受采访时表示，装备了神经网络处理器的芯片应用在监控摄像头上，摄影头由“眼睛”升级为“带有大脑的眼睛”，这是全球首次。

“数字多媒体芯片技术”国家重点实验室成立于2010年，依托于北京中星微电子有限公司，由科技部批准建立。

据资料显示，中星微电子有限公司于1999年由原国家信息产业部直接投资创立，是专攻芯片技术的公司中的“国家队”，其研发的“星光系列芯片”曾打破国际市场上无“中国芯”的局面。

人工智能的落地 　“星光智能一号”是一款嵌入式NPU。神经网络处理器NPU（Neural Processing Unit）还未被熟知，却是芯片领域热门的技术。

它与冯诺依曼架构中的CPU处理器相对，采用的是“数据驱动并行计算”这种颠覆性的新型架构。

如果将冯诺依曼架构处理数据的方式类比成单车道，那么“数据驱动并行计算”是128条多车道并行，可以同时处理128个数据，利于处理视频、图像类的海量多媒体数据。

在业内，单位功耗的计算性能，也就是性能功耗比，被用来衡量处理器架构的优劣。

据该实验室执行主任、中星微首席技术官张韵东介绍，“星光智能一号”的性能功耗比在传统的冯诺依曼架构上“至少提高了两三个数量级”，也就是几百倍。高功耗是很多顶尖人工智能技术被诟病的。

IBM20世纪的“深蓝”和谷歌2016的AlphaGo因其需要由巨大的数据计算支撑，前者使用超级计算机，后者使用服务器集群，无法脱离恒定温度和湿度的机房。

AlphaGo下一盘棋光电费就需要3000美元。张韵东将它们称之为“一场科学实验”，离技术落地、投入应用还有较远的距离。这凸显了嵌入式NPU的小型化、低功耗和低成本优势，加快人工智能技术落地应用。

例如无人机对摄像头的重量和功耗有很高的要求，否则会影响起飞和续航能力。

而“星光智能一号”只有普通邮票大小，重量仅几十克，它的诞生让诸多监控摄像头等小型设备有了人工智能化的可能，迈出了人工智能从神秘的机房，跨向生活应用的一步。

手机上的CPU GPU NPU单元，都是什么意思
手机上的CPU GPU NPU单元的意思如下：1、CPUCPU是通用处理器，是计算单元、控制单元和存储单元。

CPU的结构主要包括运算器（ALU, Arithmetic and Logic Unit）、控制单元（CU, Control Unit）、寄存器（Register）、高速缓存器（Cache）和它们之间通讯的数据、控制及状态的总线。

2、GPUGPU是图形处理器，全称为Graphics Processing Unit。GPU最初是用在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上运行绘图运算工作的微处理器。

专门用于图形计算渲染，也就是一般用于游戏。也可以跑AI部分算法。3、NPUNPU是神经网络处理器，是基于神经网络算法与加速的新型处理器总称。NPU专用AI加速计算。

如中国科学院计算所出品的diannao系列，为深度学习而生的专业芯片。扩展资料：手机上的CPU GPU NPU单元的作用：1、CPU作为手机的核心部件，可以做常用指令计算和各种算法。

2、GPUGPU中有很多的运算器ALU和很少的缓存cache，缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为线程thread提高服务的。

如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram。3、NPU它模仿生物神经网络而构建，由若干人工神经元结点互联而成。

神经元之间通过突触两两连接，突触记录了神经元间联系的权值强弱。每个神经元可抽象为一个激励函数，该函数的输入由与其相连的神经元的输出以及连接神经元的突触共同决定。

为了表达特定的知识，使用者通常需要（通过某些特定的算法）调整人工神经网络中突触的取值、网络的拓扑结构等。该过程称为“学习”。在学习之后，人工神经网络可通过习得的知识来解决特定的问题。

深度学习 对硬件的要求
之前热衷于学习理论知识，目前想跑代码了发现不知道从何下手，自己电脑上搭建的平台基本就是个摆设，因为跑不起来呀。今天我们就来看看想做深度学习应该怎么下手。

首先了解下基础知识：1、深度学习用cpu训练和用gpu训练的区别（1）CPU主要用于串行运算；而GPU则是大规模并行运算。由于深度学习中样本量巨大，参数量也很大，所以GPU的作用就是加速网络运算。

（2）CPU算神经网络也是可以的，算出来的神经网络放到实际应用中效果也很好，只不过速度会很慢罢了。而目前GPU运算主要集中在矩阵乘法和卷积上，其他的逻辑运算速度并没有CPU快。

目前来讲有三种训练模型的方式：1. 自己配置一个“本地服务器”，俗称高配的电脑。这个选择一般是台式机，因为笔记本的“高配”实在是太昂贵了，同一个价格可以买到比笔记本好很多的配置。

如果是长期使用，需要长期从事深度学习领域的研究，这个选择还是比较好的，比较自由。① 预算一万以内的机器学习台式机/主机配置：② 从李飞飞的课程里，可以看到她的电脑配置，这个配置是机器学习的基本设置。

内存：4X8G 显示卡： 两个NV GTX 1070硬盘： HDD一个， SSD两个③ 配置主机需要了解的参数（在上一篇博客中已经详细介绍了各个参数的含义）：GPU：一个好的GPU可以将你的训练时间从几周缩减成几天，所以选GPU一定要非常慎重。

可以参看GPU天梯榜，都是一些比较新的型号具有很强的性能。在英伟达产品系列中，有消费领域的GeForce系列，有专业绘图领域的Quadro系列，有高性能计算领域的Tesla系列，如何选择？

有论文研究，太高的精度对于深度学习的错误率是没有提升的，而且大部分的环境框架都只支持单精度，所以双精度浮点计算是不必要，Tesla系列都去掉了。

从显卡效能的指标看，CUDA核心数要多，GPU频率要快，显存要大，带宽要高。这样，最新Titan X算是价格便宜量又足的选择。CPU：总的来说，你需要选择一个好的GPU，一个较好的CPU。

作为一个高速的串行处理器，常用来作为“控制器”使用，用来发送和接收指令，解析指令等。

由于GPU内部结构的限制，使得它比较适合进行高速的并行运算，而并不适合进行快速的指令控制，而且许多的数据需要在GPU和CPU之间进行存取，这就需要用到CPU，因为这是它的强项。

内存条：主要进行CPU和外设之间的数据交换，它的存取速度要比硬盘快好几倍，但是价格比较昂贵，通常会和容量成正比。

内存大小最起码最起码最起码要大于你所选择的GPU的内存的大小（最好达到显存的二倍，当然有钱的话越大越好）。在深度学习中，会涉及到大量的数据交换操作（例如按batch读取数据）。

当然你也可以选择将数据存储在硬盘上，每次读取很小的batch块，这样你的训练周期就会非常长。

常用的方案是“选择一个较大的内存，每次从硬盘中读取几个batch的数据存放在内存中，然后进行数据处理”，这样可以保证数据不间断的传输，从而高效的完成数据处理的任务。

电源问题：一个显卡的功率接近300W，四显卡建议电源在1500W以上，为了以后扩展，可选择更大的电源。固态硬盘：作为一个“本地存储器”，主要用于存储各种数据。由于其速度较慢，价格自然也比较便宜。

建议你选择一个较大容量的硬盘，通常会选择1T/2T。一个好的方法是：“你可以利用上一些旧的硬盘，因为硬盘的扩展十分简单，这样可以节省一部分资金。”

为什么要用GPU来训练神经网络而不是CPU?
许多现代神经网络的实现基于GPU，GPU最初是为图形应用而开发的专用硬件组件。所以神经网络收益于游戏产业的发展。

中央处理器（central processing unit，简称CPU）作为计算机系统的运算和控制核心，是信息处理、程序运行的最终执行单元。

CPU自产生以来，在逻辑结构、运行效率以及功能外延上取得了巨大发展。CPU出现于大规模集成电路时代，处理器架构设计的迭代更新以及集成电路工艺的不断提升促使其不断发展完善。

从最初专用于数学计算到广泛应用于通用计算。从4位到8位、16位、32位处理器，最后到64位处理器，从各厂商互不兼容到不同指令集架构规范的出现，CPU 自诞生以来一直在飞速发展。

冯诺依曼体系结构是现代计算机的基础。在该体系结构下，程序和数据统一存储，指令和数据需要从同一存储空间存取，经由同一总线传输，无法重叠执行。

根据冯诺依曼体系，CPU的工作分为以下 5 个阶段：取指令阶段、指令译码阶段、执行指令阶段、访存取数和结果写回。

玩绝地求生用哪款手机好
荣耀Play很不错，屏幕采用6.3英寸大视野全面屏，麒麟970芯片八核 + 微智核i710纳米旗舰处理器， 6GB大内存可选，轻松驾驭大型3D游戏，尽享高帧率畅爽体验。

神经网络处理器（NPU）的存在，让复杂的AI算法快速运行。首款搭载GPU Turbo的手机，性能释放，带来持续稳定高帧率的游戏体验，画面平滑，不抖动，不拖影；畅爽不卡顿。

建议可以登陆华为商城查看产品更多相关参数。

电脑主板上tpu是什么意思
TPU是一颗由华硕自主研发的控制芯片，通过这颗芯片玩家可以在不占用CPU性能的基础上对玩家的CPU通过硬件控制的方式进行超频。

EPU节能引擎,可检测目前的PC负载并实时智能调整功率，以此提供整体系统节能省电的功能。

EPU为组件提供自动相位切换（包括CPU、显卡、内存、芯片组、硬盘及系统风扇），可智能加速及超频以提供最适合的用电量，以此节省电力与成本。

什么手机玩游戏最好用
荣耀Play手机很不的，荣耀首款搭载GPU Turbo的手机，性能释放，带来持续稳定高帧率的游戏体验，画面平滑，不抖动，不拖影；畅爽不卡顿的操作，助你游戏旗开得胜。

10纳米旗舰处理器， 6GB大内存可选，轻松驾驭大型3D游戏，尽享高帧率畅爽体验。神经网络处理器（NPU）的存在，让复杂的AI算法快速运行；3750mAh（典型值）高密度电池，续航更持久。

可以登录华为商城了解更多的手机参数，根据个人的喜好和需求来选择一款。

什么牌子手机适合玩游戏又便宜？
通常玩游戏对手机配置需求较高，X30 Pro&X30、iQOO Neo 855版、NEX 3、X27/X27Pro、S1/S1Pro、iQOO Pro&iQOO/iQOO Neo、NEX双屏版、X23、NEX/NEX旗舰版. 这几款手机拥有强大的硬件配置，所以不管是在运行速度还是处理能力上都得以很好体现。

可以进入vivo官网了解更多~。"
IBM林咏华：AI王牌炼成记——访IBM全球杰出工程师、人工智能系统技术总监、IBM中国研究院林咏华,2018-03-16 00:00:00,"自2016年谷歌的阿尔法狗（AlphaGo）让世界围棋冠军李世石认输的那一刻起，人工智能便再一次成为社会各界关注的焦点。2017年，人工智能更是迅速升温，成为政府、产业界、科研机构以及消费市场竞相追逐的对象。在国家人工智能战略和资本市场的推波助澜下，人工智能的企业、产品和服务层出不穷，创业创新日益活跃。







这一次人工智能浪潮的到来得益于强大的计算能力、先进的算法、大数据、物联网等诸多因素。实体经济数字化、网络化、智能化转型给人工智能带来巨大的历史机遇，展现出极为广阔的发展前景。



实际上，早在2011年，IBM就进行了第一次人机大战，IBMWatson超级计算机在电视节目“Jeopardy”问答游戏中战胜人类选手，赢得那一次在美国的边缘挑战节目。此后，IBM用Watson这个品牌打造了IBM在AI领域的发展战略。



2017年，IBM进一步明确了发展“商业人工智能”的战略，并在电子、能源、教育、汽车、医药等各行业领域取得良好应用成果。近期，大数据周刊专访了IBM全球杰出工程师、人工智能系统技术总监、IBM中国研究院林咏华女士，她让我们了解IBM的AI发展历程，并详细阐述了企业在发展AI过程中面临的种种挑战以及如何应对等难题。



 
看点

01

 
IBM的7年AI之路



IBM在人工智能和认知计算领域有着多年研究和积累，人工智能这个学科最早建立在1956年的达特茅斯会议上，而IBM就是这个会议主要的参与者。多年来，IBM一直在这个领域投入人才和资源进行研究探索，直到IBMWatson超级计算机击败“危险边缘Jeopardy”人类选手而震惊世界。



最近几年，IBM在人工智能领域取得可喜可贺的成绩。最新数据显示，IBM对AI和云服务的投入已收到回报。在2017年10月18日IBM公布的第三季度财报中，以这两个方向为主的战略性业务（StrategicImperatives）营收增速达两位数。第三季度，云服务营收增长了20%。在过去12个月里，云服务营收增长到158亿美元规模，占IBM总营收的20%左右。



林咏华对IBM过去六、七年中在AI领域的发展经历做了详细解读，让我们看清IBM是如何走过AI之路的。“IBM在AI的发展中，我认为是有三条轴线的。第一条是处于中间的AI技术核心中轴线。大家今天看到的许多跟AI技术相关的报道（包括IBM和其他公司），其实都围绕着这条中轴线。在这条中轴线上，IBM做了大量创新和研发。首先，我们的AI技术与产品很大一部分是部署在云平台上面，包括公有云及企业私有云。AI应用带来了巨大的新计算需求，导致GPU、FPGA，甚至TPU这样的硬件加速器技术需要部署在云上面，因此这对云计算技术带来了新的技术需求。”她说。







围绕这条核心中轴线，IBM还做了跟AI相关的多个重要领域的研发，林咏华指出，这类研发包括语音、计算机视觉、自然语言处理，人机对话技术等在AI领域的基础性研究。她认为，IBM只发展这条技术中轴线是不够的，IBM跟其他的互联网公司不一样，很多互联网公司是以消费者作为最终用户，但IBM所面对的是大量的市场企业用户。对于以消费者为主导的市场，需求实际由互联网公司来决定。比如一些基于互联网的应用，消费者只是学习怎么使用它，需求由互联网公司就可以决定。像今天的谷歌、Facebook等互联网公司，考虑在AI上面进行创新的时候，主要考虑的是要推给消费者的应用需要具备哪些AI能力更能吸引个人用户。反之，对于企业为主导的市场，需求往往是由企业来决定。这就意味着IBM面对的大量需求都是由客户决定，因此，对于IBM的挑战是如何把AI中轴线的技术用到不同的企业里，满足各类行业客户的需要。



那么，IBM围绕那条中轴线在做大量AI研发的时候，是如何把AI用到不同的行业里去弥补技术的不足？如何更好地结合产业链上下游？



林咏华表示，如交通、医疗、智能制造、汽车行业等，其实要把AI的能力用到各行各业，中间有大量的技术创新需要弥补。所以，这催生了我们在AI技术中轴线以上的第二条技术发展轴线——行业支撑技术线。这条轴线起到了把上游的行业应用和中游的AI平台、算法等技术进行对接的能力。此外，在一个行业领域中，还需要慎重考虑AI技术如何经济性地去落地。在今天，有许多新型的AI计算，例如针对视频分析的深度学习，耗费大量计算能力。哪怕使用现有最好的GPU芯片，往往需要在每个摄像头的成本之上叠加超过10倍的计算硬件成本。这对一个新技术的落地就会产生很大的阻力。因此，IBM以及其他公司，都正在做大量的芯片研究。这也反映了我们在三条发展轴线中的最后一条，就是处在AI技术中轴线之下的AI硬件基础研究。IBM从2014年就发布的神经元芯片，目前正在研究的量子计算技术，内存计算技术，以及针对深度计算的整个硬件系统的创新，都是在这条轴线上。



谈到与产业链上下游的结合，她认为，产业链包括：下游的硬件厂商，中游的软件厂商以及上游的企业应用。目前，如果对于精度、复杂性、系统性要求不高的话，产业链上、中、下游可以通过松耦合完成融合，比较容易。但如果追求高性能、高精度，就需要产业链的深度融合，包括对中游的软件进行定向的优化和改造，对下游的硬件进行定制化的开发，这样才能够满足上游行业应用的高质量需求。



 
看点

02

 
被争抢的AI风口，其他企业能否享受AI红利？



随着人工智能热度的攀升，各企业趋之若鹜，希望能够借此机会弯道超车的企业，是否具备足够理性的认识？在AI领域技术基础较薄弱的企业，又如何才能享受到AI带来的红利？



对此，林咏华表示，企业想进入AI领域，会希望在自己的业务领域能获得新的增长点，希望借用AI的力量打造出新的产品赢得更多市场份额，希望通过AI把已有的产品提升一个水平，从而更快地击败市场上的竞争对手。所以，对于这些希望利用AI技术的企业而言，有两点非常重要。



第一，找准AI技术在自身业务领域的定位，也就是对未来产品的战略思考。这一点是无论如何不能由别的公司代替企业去思考，因为只有企业自己最了解自身的业务领域、发展机遇和企业现状。



第二，Time-toMarket，时间是十分宝贵的。在目前信息平坦的年代，市场机遇的赛跑就是时间的赛跑。







因此，想要借助人工智能为自身带来红利的企业，需要重点思考和策划上面两个问题。除此之外，林咏华还表示，今天，在与时间赛跑的过程中，企业面临的最大挑战是人才和数据的问题。



如何“借助外力”往往让很多企业犹豫。如果像传统的购买企业服务模式来解决人才问题，对于一些行业或许行不通。而许多行业，他们的数据具有高度保密性，不能把这些数据都交由第三方公司进行数据训练和分析。此外，企业的生产环境所针对的数据类型也会随着时间有所改变。因此，哪怕企业交由第三方公司开发了一个机器学习的模型，是否之后所有在生产环境中的变化都要依赖第三方公司进行调整呢？所以，在引入AI技术的过程中，企业往往会在“人才培养时间过长”和“把控产品”之间犹豫。



基于此，IBM在为企业市场打造的AI平台中引入了“AIforAI”的概念。为企业打造的AI开发平台不仅仅是一套工具或方法论，而是真正把一个“AI大脑”潜入到AI开发平台。IBM在“AI大脑”中率先引入了多种深度学习技术，如迁移学习、自动化机器学习、数据增强等。一方面把IBM研究院多年的机器学习研究成果内嵌到系统中，让AI开发平台更像一个专家系统；另一方面，通过“AIforAI”技术，让这样的平台可以针对企业自己的数据进行自动化学习和优化。通过这些技术的内嵌，企业开发团队在没有深度学习的技术背景下，也能很容易地获得好的机器学习准确率。



关于数据的问题，林咏华认为，过去几年，深度学习的成功是基于海量的互联网开源数据。但这些数据都是互联网的数据，和企业希望解决的问题（如特定病种的医疗影像、产品质量的影像等等）没有直接关系。所以，当需要使用AI技术来赋能自己的业务领域时，企业需要自己准备业务场景的数据集，也需要由具备专业领域知识的人员来标注数据集。因此，准备数据这个过程本身就极耗时间和人力。另外，企业往往会面临数据不足的问题，尤其是需要高度关注的数据类型。例如，在医疗影像中，往往有着各种症状的数据比健康人群的数据更为重要；在生产制造中，有着各种瑕疵问题的数据比质量正常的产品数据更需要关注；在汽车驾驶中，各种恶劣天气路况下的数据比正常天气和光照的时候获得的数据更需要注意。但这些数据往往是小概率情况下的数据。不解决好这个问题，就难以把AI真正用到工业场景中。



对此，IBM研究院专门确立了一系列针对小数据（SmallDataset）的研究，基于迁移学习、数据增强等课题进行深入研发。这些技术也应用到了IBMWatson及企业AI开发平台（PowerAI）上，可以直接帮助企业解决数据的问题。



 
看点

03

 
高校教育应把深度学习作为一门必修课



人才是推动人工智能发展的关键因素，上至国家政府，下至科技企业，AI人才不足成为影响当前行业发展的主要瓶颈。在AI人才队伍建设方面，国家和企业从人才培养体系上也在不断变革。



但是，什么样的AI人才才是真正行业所需？未来的AI人才不仅应具备深度思考与分解问题的能力，具备能够和机器进行对话的能力，还应该能够将自身AI技术与业务很好的融合到一起。



谈到如何培养出将AI技术与业务很好地结合的AI人才，林咏华说：“培养这样的人才主要从两个方面来考虑。一方面，企业通过招聘和自我团队建设，积累自己的AI人才。这些人才在熟知业务的同时，经过企业AI技术培训和实践，争取将技术与业务更好的结合。另一方面，AI之所以最近几年发展这么迅猛，是因为有了深度学习，自2012年之后，深度学习就成为了AI领域的基本技术。从现有教育来看，在高校教育里的很多学科领域（例如能源、电气、医疗、经济等学科），希望能把深度学习作为一门必修的基础科目。这样，当这批人才进入岗位，自然懂得深度学习怎么去用，并能够把高校教育经历与自己的业务很好结合。”"
深入理解CPU和异构计算芯片GPU/FPGA/ASIC （下篇）,2017-03-25 19:40:37,"3.2.1 CPU计算能力分析
这里CPU计算能力用Intel的Haswell架构进行分析，Haswell架构上计算单元有2个FMA(fused multiply-add)，每个FMA可以对256bit数据在一个时钟周期中做一次乘运算和一次加运算，所以对应32bit单精度浮点计算能力为：(256bit/32bit) 2(FMA) 2(乘和加) = 32 SP FLOPs/cycle，即每个时钟周期可以做32个单精度浮点计算。

CPU峰值浮点计算性能 = CPU核数 CPU频率 每周期执行的浮点操作数。已Intel的CPU型号E5-2620V3来计算峰值计算能力为 = 6(CPU核数) 2.4GHz(CPU频率) 32 SP FLOPs/cycle = 460.8 GFLOPs/s 即每秒460G峰值浮点计算能力。

CPU芯片结构是否可以充分发挥浮点计算能力？CPU的指令执行过程是：取指令 ->指令译码 ->指令执行，只有在指令执行的时候，计算单元才发挥作用，这样取指令和指令译码的两段时间，计算单元是不在工作的，如图4所示。



图4 CPU指令执行流程

CPU为了提高指令执行的效率，在当前指令执行过程的时候，预先读取后面几条指令，使得指令流水处理，提高指令执行效率，如图5所示。指令预先读取并流水执行的前提是指令之间不具有相关性，不能一个指令的如何执行需要等到前面一个指令执行完的结果才可以获知。



图5 CPU指令流水执行

CPU作为通用处理器，兼顾计算和控制，70%晶体管用来构建Cache 还有一部分控制单元，用来处理复杂逻辑和提高指令的执行效率，如图6所示，所以导致计算通用性强，可以处理计算复杂度高，但计算性能一般。



图6CPU结构

通过CPU计算性能分析，直接提高计算性能方向为：增加CPU核数、提高CPU频率、修改CPU架构增加计算单元FMA(fused multiply-add)个数。这3个方向中，直接增加CPU核数对于计算能力提升最高，但是带来芯片功耗和价格的增加，因为每个物理核中只有30%的晶体管是计算单元。提高CPU频率，提升的空间有限，而且CPU频率太高会导致芯片出现功耗过大和过热的问题，因此英特尔等芯片制造商目前走多核化的路线，即限制单个微处理器的主频，通过集成多个处理器内核来提高处理性能。修改CPU架构增加计算单元FMA个数，目前英特尔按照“Tick-Tock”二年一个周期进行CPU架构调整，从2016年开始放缓至三年，更新迭代周期较长。

3.2.2 GPU计算能力分析
GPU主要擅长做类似图像处理的并行计算，所谓的“粗粒度并行（coarse-grain parallelism）”。图形处理计算的特征表现为高密度的计算而计算需要的数据之间较少存在相关性，GPU 提供大量的计算单元（多达几千个计算单元）和大量的高速内存，可以同时对很多像素进行并行处理。

图7是GPU的设计结构。GPU的设计出发点在于GPU更适用于计算强度高、多并行的计算。因此，GPU把晶体管更多用于计算单元，而不像CPU用于数据Cache和流程控制器。这样的设计是因为并行计算时每个数据单元执行相同程序，不需要繁琐的流程控制而更需要高计算能力，因此也不需要大的cache容量。



图7 GPU结构

GPU中一个逻辑控制单元对应多个计算单元，同时要想计算单元充分并行起来，逻辑控制必然不会太复杂，太复杂的逻辑控制无法发挥计算单元的并行度，例如过多的if…else if…else if… 分支计算就无法提高计算单元的并行度，所以在GPU中逻辑控制单元也就不需要能够快速处理复杂控制。

这里GPU计算能力用Nvidia的Tesla K40进行分析，K40包含2880个流处理器(Stream Processor)，流处理器就是GPU的计算单元。每个流处理器包含一个32bit单精度浮点乘和加单元，即每个时钟周期可以做2个单精度浮点计算。GPU峰值浮点计算性能 = 流处理器个数 GPU频率 每周期执行的浮点操作数。以K40为例，K40峰值浮点计算性能= 2880(流处理器) 745MHz 2(乘和加) = 4.29T FLOPs/s即每秒4.29T峰值浮点计算能力。

GPU芯片结构是否可以充分发挥浮点计算能力？GPU同CPU一样也是指令执行过程：取指令 ->指令译码 ->指令执行，只有在指令执行的时候，计算单元才发挥作用。GPU的逻辑控制单元相比CPU简单，所以要想做到指令流水处理，提高指令执行效率，必然要求处理的算法本身复杂度低，处理的数据之间相互独立，所以算法本身的串行处理会导致GPU浮点计算能力的显著降低。

3.2.3 FPGA计算能力分析
FPGA作为一种高性能、低功耗的可编程芯片，可以根据客户定制来做针对性的算法设计。所以在处理海量数据的时候，FPGA 相比于CPU 和GPU，优势在于：FPGA计算效率更高，FPGA更接近IO。

FPGA不采用指令和软件，是软硬件合一的器件。对FPGA进行编程要使用硬件描述语言，硬件描述语言描述的逻辑可以直接被编译为晶体管电路的组合。所以FPGA实际上直接用晶体管电路实现用户的算法，没有通过指令系统的翻译。

FPGA的英文缩写名翻译过来，全称是现场可编程逻辑门阵列，这个名称已经揭示了FPGA的功能，它就是一堆逻辑门电路的组合，可以编程，还可以重复编程。图8展示了可编程FPGA的内部原理图。



图8 FPGA内部结构图

这里FPGA计算能力用Xilinx的V7-690T进行分析，V7-690T包含3600个DSP(Digital Signal Processing)，DSP就是FPGA的计算单元。每个DSP可以在每个时钟周期可以做2个单精度浮点计算(乘和加)。FPGA峰值浮点计算性能 = DSP个数 FPGA频率 每周期执行的浮点操作数。V7-690T运行频率已250MHz来计算，V7-690T峰值浮点计算性能 = 3600(DSP个数) 250MHz 2(乘和加)=1.8T FLOPs/s即每秒1.8T峰值浮点计算能力。

FPGA芯片结构是否可以充分发挥浮点计算能力？FPGA由于算法是定制的，所以没有CPU和GPU的取指令和指令译码过程，数据流直接根据定制的算法进行固定操作，计算单元在每个时钟周期上都可以执行，所以可以充分发挥浮点计算能力，计算效率高于CPU和GPU。

3.2.4 ASIC计算能力分析
ASIC是一种专用芯片，与传统的通用芯片有一定的差异。是为了某种特定的需求而专门定制的芯片。ASIC芯片的计算能力和计算效率都可以根据算法需要进行定制，所以ASIC与通用芯片相比，具有以下几个方面的优越性：体积小、功耗低、计算性能高、计算效率高、芯片出货量越大成本越低。但是缺点也很明显：算法是固定的，一旦算法变化就可能无法使用。目前人工智能属于大爆发时期，大量的算法不断涌出，远没有到算法平稳期，ASIC专用芯片如何做到适应各种算法是个最大的问题，如果以目前CPU和GPU架构来适应各种算法，那ASIC专用芯片就变成了同CPU、GPU一样的通用芯片，在性能和功耗上就没有优势了。

我们来看看FPGA 和 ASIC 的区别。FPGA基本原理是在芯片内集成大量的数字电路基本门电路以及存储器，而用户可以通过烧入 FPGA 配置文件来来定义这些门电路以及存储器之间的连线。这种烧入不是一次性的，即用户今天可以把 FPGA 配置成一个微控制器 MCU，明天可以编辑配置文件把同一个 FPGA 配置成一个音频编解码器。ASIC 则是专用集成电路，一旦设计制造完成后电路就固定了，无法再改变。

比较 FPGA 和 ASIC 就像比较乐高积木和模型。举例来说，如果你发现最近星球大战里面 Yoda 大师很火，想要做一个 Yoda 大师的玩具卖，你要怎么办呢？

有两种办法，一种是用乐高积木搭，还有一种是找工厂开模定制。用乐高积木搭的话，只要设计完玩具外形后去买一套乐高积木即可。而找工厂开模的话在设计完玩具外形外你还需要做很多事情，比如玩具的材质是否会散发气味，玩具在高温下是否会融化等等，所以用乐高积木来做玩具需要的前期工作比起找工厂开模制作来说要少得多，从设计完成到能够上市所需要的时间用乐高也要快很多。

FPGA 和 ASIC 也是一样，使用 FPGA 只要写完 Verilog 代码就可以用 FPGA 厂商提供的工具实现硬件加速器了，而要设计 ASIC 则还需要做很多验证和物理设计 (ESD，Package 等等)，需要更多的时间。如果要针对特殊场合（如军事和工业等对于可靠性要求很高的应用），ASIC 则需要更多时间进行特别设计以满足需求，但是用 FPGA 的话可以直接买军工级的高稳定性 FPGA 完全不影响开发时间。但是，虽然设计时间比较短，但是乐高积木做出来的玩具比起工厂定制的玩具要粗糙（性能差）一些（下图），毕竟工厂开模是量身定制。



另外，如果出货量大的话，工厂大规模生产玩具的成本会比用乐高积木做便宜许多。FPGA 和 ASIC 也是如此，在同一时间点上用最好的工艺实现的 ASIC 的加速器的速度会比用同样工艺 FPGA 做的加速器速度快 5-10 倍，而且一旦量产后 ASIC 的成本会远远低于 FPGA 方案。

FPGA 上市速度快, ASIC 上市速度慢，需要大量时间开发，而且一次性成本（光刻掩模制作成本）远高于 FPGA，但是性能高于 FPGA 且量产后平均成本低于 FPGA。目标市场方面，FPGA 成本较高，所以适合对价格不是很敏感的地方，比如企业应用，军事和工业电子等等（在这些领域可重配置真的需要）。而 ASIC 由于低成本则适合消费电子类应用，而且在消费电子中可配置是否是一个伪需求还有待商榷。

我们看到的市场现状也是如此：使用 FPGA 做深度学习加速的多是企业用户，百度、微软、IBM 等公司都有专门做 FPGA 的团队为服务器加速，而做 FPGA 方案的初创公司 Teradeep 的目标市场也是服务器。而 ASIC 则主要瞄准消费电子，如 Movidius。由于移动终端属于消费电子领域，所以未来使用的方案应当是以 ASIC 为主。



3.3平台性能和功耗比较
由于不同的芯片生产工艺，对芯片的功耗和性能都有影响，这里用相同工艺或者接近工艺下进行对比，ASIC芯片还没有商用的芯片出现，Google的TPU也只是自己使用没有对外提供信息，这里ASIC芯片用在学术论文发表的《DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning》作为代表。



从上面的对比来看，能耗比方面：ASIC > FPGA > GPU > CPU，产生这样结果的根本原因：对于计算密集型算法，数据的搬移和运算效率越高的能耗比就越高。ASIC和FPGA都是更接近底层IO，所以计算效率高和数据搬移高，但是FPGA有冗余晶体管和连线，运行频率低，所以没有ASIC能耗比高。GPU和CPU都是属于通用处理器，都需要进行取指令、指令译码、指令执行的过程，通过这种方式屏蔽了底层IO的处理，使得软硬件解耦，但带来数据的搬移和运算无法达到更高效率，所以没有ASIC、FPGA能耗比高。GPU和CPU之间的能耗比的差距，主要在于CPU中晶体管有大部分用在cache和控制逻辑单元，所以CPU相比GPU来说，对于计算密集同时计算复杂度低的算法，有冗余的晶体管无法发挥作用，能耗比上CPU低于GPU。

4 总结与展望
处理器芯片各自长期发展的过程中，形成了一些使用和市场上鲜明的特点。CPU&GPU领域存在大量的开源软件和应用软件，任何新的技术首先会用CPU实现算法，因此CPU编程的资源丰富而且容易获得，开发成本低而开发周期。FPGA的实现采用Verilog/VHDL等底层硬件描述语言实现，需要开发者对FPGA的芯片特性有较为深入的了解，但其高并行性的特性往往可以使业务性能得到量级的提升；同时FPGA是动态可重配的，当在数据中心部署之后，可以根据业务形态来配置不同的逻辑实现不同的硬件加速功能；举例来讲，当前服务器上的FPGA板卡部署的是图片压缩逻辑，服务于QQ业务；而此时广告实时预估需要扩容获得更多的FPGA计算资源，通过简单的FPGA重配流程，FPGA板卡即可以变身成“新”硬件来服务广告实时预估，非常适合批量部署。ASIC芯片可以获得最优的性能，即面积利用率高、速度快、功耗低；但是AISC开发风险极大，需要有足够大的市场来保证成本价格，而且从研发到市场的时间周期很长，不适合例如深度学习CNN等算法正在快速迭代的领域。

讲了这么多，当遇到业务瓶颈的需要异构计算芯片的时候，你是否能够根据业务特性和芯片特性选择出合适的芯片呢？

分析完各类芯片特性，接下来，重点来了！



当今的FPGA有很大的性能潜力，支持深度可变的流水线结构，提供大量的并行计算资源，一个时钟周期内就可以完成非常复杂的功能。FPGA的可编程能力保证了这种器件能够满足应用软件的特殊需求，不存在设计定制协处理器的成本或者延迟问题。FPGA是重新可编程的，它可以在一个芯片中为多种应用提供非常灵活的定制协处理功能。拥有了FPGA，业务就拥有无限可能。同样的半导体技术，既能把处理器的性能发挥到极限，也能使FPGA从简单的胶合逻辑控制器，发展到性能很高的可编程架构。FPGA完全能够满足HPC市场的“4P”需求。

FPGA的内置存储器也有很大的性能优势。例如，片内存储器意味着协处理器逻辑的存储器访问带宽不会受到器件I/O引脚数量的限制。而且，存储器和运算逻辑紧密结合，不再需要采用外部高速存储器缓冲。这样，也避免了大功耗的缓冲访问和一致性问题。使用内部存储器还意味着协处理器不需要其他的I/O引脚来提高其可访问存储器容量，从而简化了设计。

很多人由于FPGA的开发难度大以及开发周期较长而对其持有怀疑态度，好消息是HLS以及OpenCL语言越来越完善，很多应用直接使用这两种高级语言就可以取得较大性能提升。

业界成功案例


为了更好地满足对计算性能的要求，全球的很多大型IT企业都在FPGA的加速硬件上进行了布局和实践。

Intel:

Intel决定以167亿美元收购FPGA生产商Altera。Intel预计到2020年，30%以上的服务器CPU芯片将配备一个FPGA协处理器。

IBM：

IBM和Xilinx联合宣布开展一项多年战略协作，在IBM POWER系统上运用Xilinx FPGA加速工作负载处理技术，以打造更高性能、更高能效的数据中心应用。

微软：

早在2014年，Microsoft就将Altera FPGA运用在其Bing搜索的业务中，使Bing的搜索处理量提升了一倍，搜索时间缩短了29%。2015年，微软进一步将FPGA运用于深度学习领域。2016年，微软体系结构顶级会议Micro上发表的《A Cloud-Scale Acceleration Architecture》显示了其在数据中心体系架构上的勃勃野心。现在，进入微软数据中心的每一个服务器上均带有一块FPGA板卡，其基本的架构如下：



论文中涉及到的应用场景包括：

1.网络加速（例如网络数据包加解密）

2.本地应用加速（Bing加速、DNN延时敏感性业务加速）

3.支持fpga之间通信，fpga计算资源池化，提供Hardware-as-a-Service的概念，将FPGA和服务器解耦。

Facebook：

2016年，Facebook也宣称要同Intel合作用Xeon-FPGA平台进行数据中心的建设。

百度：

国内百度也推出了FPGA版本的百度大脑，运用到线上服务；FPGA版百度大脑已运用于包括语音识别、广告点击率预估模型、DNA序列检测以及无人车等业务中。据了解，应用了该版本百度大脑后，语音在线服务、广告点击率预估模型等的计算性能皆提升了3~4倍。

本文转载自腾云阁，已获得作者授权。"
IBM研究院院长：量子计算“大爆发”将在十年内到,2022-10-19 11:22:55,"摘要：IBM研究院院长达里奥·吉尔（Dario Gil）教授正处于量子计算革命的最前沿，这场革命即将从根本上改变我们的世界。在此次独家专访中，他解释了实现这个梦想的挑战是什么，为什么你需要成为爱因斯坦才能理解量子计算，以及什么时候才能开始使用量子计算。



在IBM发布的照片中，量子计算实验室看起来像一艘宇宙飞船的发动机室：明亮的白色房间，无数的电缆从天花板悬挂到浮动的地板上，上面还有通风口。这些技术细节构成了主角“诞生”的大背景：在金属支架上悬挂着的白色不锈钢桶，看起来像个电热水器。在这些“电热水器”中，一场历史性的革命正在酝酿。

IBM这个计算界的“恐龙”，拥有一个多世纪的历史，正试图赢得有史以来最艰苦，最昂贵和最具潜力的科学竞赛——开发量子计算机，以此来重塑自己。

“我们生活在计算史上最激动人心的时代，”IBM高级副总裁兼公司研究部门负责人Dario Gil在接受采访时说。“我们正在见证一个类似于上世纪40年代和50年代的突破性时刻，当时建造了第一台经典计算机。几周后，这一观点得到了进一步的证实，诺贝尔奖委员会宣布将2022年度物理学奖项授予三名研究人员，他们的研究是量子科学领域发展的里程碑。

Dario Gil在全世界物理学家和计算机工程师中闻名遐迩。他是领导着世界上最先进的量子计算机开发工作的人。9月，当Gil在以色列特拉维夫进行短暂的访问并在IBM会议上发表开幕演讲时，大厅里挤满了以色列顶尖大学的研究人员和政府机构的代表，所有人都被Gil的演讲所吸引。



达里奥·吉尔（Dario Gil）教授（图源： Elad Gershgoren）

46岁的Gil出生于西班牙，后移居美国，在麻省理工学院学习并完成了博士学业，毕业后就开始在IBM从事一系列研发工作。2019年以来，他一直领导着该公司的研究部门，该部门在全球的21个地点拥有3,000名工程师。在他的管理下，2016年，IBM建造了第一台量子计算机，任何人都可以使用其服务：如果你有一个复杂的问题，你可以去IBM量子体验网站，通过云远程访问量子计算机，或许能得到一个答案，听起来很简单。

Gill解释道，“量子计算机不再是一台超级电子计算机，而是一种全新的计算机，尽管它使用与经典计算机相同的二进制方法。从一串串贝壳，到念珠和算盘，再到基于齿轮的机械计算机，电子计算机，最后到现在的量子计算机，这是进化的又一里程碑。从本质上讲，量子计算机是一种自然模拟器，通过它可以模拟自然过程，从而解决以前无法解决的问题。”Gil说，“如果经典计算机是数学和信息的结合，那么量子计算就是物理和信息的结合。”

这种结合，使得以前所未有的速度解决某些类型的问题成为可能。谷歌也在开发量子计算机，它在2019年声称已经达到“量子霸权”——证明了量子计算机比经典计算机在执行某些计算时更加高效。谷歌研究人员展示了量子计算机如何在200秒内完成了经典计算机需要一万年才能完成的计算。此后，这一说法被其他研究人员反驳，他们提出了新算法使得经典计算机执行相同的计算所需时间远少于一万年。即使是谷歌的“量子霸权”并未被彻底证实，但也让世人看到了量子计算机将拥有的巨大潜力。

“量子计算机不会淘汰经典计算机：它们将生活在一起，解决不同的问题，”Gil解释说。“这就像如何从A点到达B点：可以步行、骑自行车、开车或飞行。如果两点之间的距离是50公里，你就不会用飞机，对吧？相应地，这是一种适合经典计算机的模式。量子计算机可以让你飞，甚至飞到月球，而且速度很快。”

量子计算能做什么

“量子计算将有可能破解一系列传统无法解决的问题，从而改变世界。其中许多问题都与能源有关，也有一些问题与新材料的开发有关。我们一般对所用的材料不以为然，但在实际上，历史时代就是由当时主要的材料所定义的——石器时代、青铜时代、铁器时代等。量子计算将帮助我们开发具有新特性的材料，因此已经使用它的第一个部门是工业，特别是汽车工业。

汽车制造商对更好的化学材料感兴趣，希望生产更高效，更耐用的电池。对于普通计算机来说，这是一项艰巨的任务，为了在可接受的时间内完成电池设计，我们必须放弃准确性，只满足于近似的答案，但量子计算可以帮助快速开发最合适的材料，不需要进入实验室进行实验。量子计算机在解决化学问题时的方法效率也适用于制药行业，他们可以初步利用量子计算来预测分子的性质，并以这种方式加快新药开发。在化肥行业，量子计算将开发生产不会损害环境的物质。

此外，量子计算的用途不局限于物质世界。“例如金融行业，量子计算机能够分析场景，预测和管理风险，为公众显著改善各种优化后的投资组合收益。该行业对这种潜在应用非常感兴趣。”



IBM（图源：Shutterstock）

量子计算还将迫使一些行业重新洗牌，信息安全行业首当其冲。现代RSA加密系统基于计算的不对称性，信息加密后，在密文中包括一个256位或1024位的大数，是某两个大质数的乘积。在已知其中一个质数（作为解密用的私钥）的情况下，求出另外一个质数（加密信息）非常容易，但是想要在不知道初始质数（私钥）是什么的情况下破译加密信息，需要将这个乘积分解成两个质数。使用经典计算机去完成这项质因数分解任务需要很多年。”Gil解释说。“对于量子计算机来说，这样的计算可能是几秒钟的事情。

这里对整个加密行业构成了真正的威胁——背后的基础加密逻辑自1970年以来就已建立，然而这一基石突然破裂。“的确，普通计算机破解加密问题需要一万年的时间，量子计算机在瞬间就能解决。这就是为什么量子计算机威胁到了网络空间和加密世界，因为这是所有全球信息安全的基础。这与物理学或自然无关，只是与量子计算机更强大，更快的计算能力有关。

违反所有“直觉规则”的计算机

要理解量子计算机的力量，首先必须理解“量子计算”这一概念。第一步是停止在熟悉的0和1的概念中思考，忘记比特和二进制。理解量子计算的关键是认识到0和1的二分法不再适用：量子计算依赖于一种称为“量子比特”的基本信息单位，而不是比特。量子比特可以同时表达0和1，以及介于两者之间的所有数值。

现在是时候解释量子计算机的基础理论了，这似乎违背了常识。“量子理论使得解释非常非常小的粒子的行为成为可能，”Gil解释说。“在学校里，我们看到一个像行星的原子模型，原子核和电子在四处移动，但在20世纪初，这个模型被证明不很准确。”当马克斯·普朗克（Max Planck）和阿尔伯特·爱因斯坦（Albert Einstein）等物理学家意识到光（直到那时物理学将其视为波）也表现为粒子时，并且该粒子的能量只能用“量子”般的跳跃来描述，即只能取一系列离散的数值时，量子力学就开始发展了。随后的几十年里，这个理论得到了越来越多的发展，并被证明在描述微观粒子世界中的各种现象是有效的。但即使在今天，它的深层含义仍然模糊不清。

例如，粒子同时处于多个位置的概念。根据量子理论，在两点之间移动的粒子会同时处于它们之间的所有可能路径中，这种状态称为“叠加”。这并不是说我们不知道它的确切位置。它并没有在某一个路径中，相反，它同时存在于所有可能的路径里面。换句话说，现实不是确定的，而是概率性的。

这并不是量子理论提出的唯一难题。另一个令人困惑的概念是“纠缠”，在这种情况下，几个粒子具有一个整体的物理属性，并同时（！）对其中一个粒子的变化做出反应，即使它们彼此相距很远。吉尔建议把它看作是掷两枚硬币：任何研究过统计学的人都知道，每个硬币上都有“正”或“背”，不同硬币的概率是独立的。但在量子模型中，如果硬币（在这里代表粒子）纠缠在一起，那么丢出一个硬币的结果，将影响另一个硬币。“爱因斯坦不相信纠缠，讨厌这些模式，”Gil笑着说。

现实是由测量决定的？现实不是绝对的，而是概率性的？即使在无限远的距离上也会同时坍缩的纠缠粒子？如果觉得这些概念难以理解或违反直觉，那么您并不孤单。物理学家尼尔斯·玻尔（Niels Bohr）说：“无论谁遇到量子理论并且没有感到震惊，那就是没有理解它。”玻尔是爱因斯坦的同代人，也是他的伟大克星，他因对量子理论发展作出卓越贡献而获得诺贝尔奖。顺便说一句，爱因斯坦对玻尔的量子理论持保留态度。另一位因对量子理论的贡献而获得诺贝尔奖的物理学家理查德·费曼（Richard Feynman）也说过：“如果你觉得你已经理解了量子理论，那么你还没懂。”

费曼也是量子计算之父：他想模拟粒子的行为，但由于量子力学的概率性，用经典计算机去进行这种模拟要海量的计算时间，因此模拟将变得不可行。“费曼以及像他一样的其他物理学家，认为经典计算领域专注于数学，远离自然，而物理学可以与信息世界建立更多的联系，”Gil解释说。“在1981年的一次历史性演讲中，费曼声称，经典计算机从根本上无法处理粒子模拟，因为自然不是经典的。他说，'如果我们想模拟自然，我们需要一台以量子方式运行来模拟自然的机器。1998年，当第一台量子计算机在英国牛津大学建成时，这一愿景得以实现。”

量子计算机利用我们尚未完全理解的量子理论的神秘性来执行计算操作。在普通计算机中，信息的基本单位是“比特”，它可以具有两个值之一，0或1;使用这些比特可以执行通用计算，尽管其中一些计算可能需要很长时间。在量子计算机中，由于叠加原理，量子比特代表的不是一个绝对值，而是一个值的分布。“你可以把它看作是一个维度更多的问题：1和0只是两极，例如硬币的两面，但它也可能是倾斜的，”Gil解释说。使用统计方法可以检查量子比特的状态并获得有用的结果。这种概率法并不适合每个问题，但在解决某些问题时，它比经典计算机寻找答案的效率要高得多。

“由于纠缠效应，也可能导致量子比特相互影响，”Gil说。由于每个量子比特代表一个完整的解空间，因此每添加一个量子比特都会增加量子比特之间可能连接的数量，并且这些连接呈指数级增长。而在经典计算机中，比特的添加使得解空间随线性增长。目前IBM保持着量子比特的世界：去年它推出了一款具有127个量子比特的量子处理器，今年目标是推出433个量子比特的处理器，明年推出1021个量子比特的处理器。

比外太空还冷3度

这种雄心勃勃的计划看起来自命不凡。事实证明，“建造一台行为像大自然的机器”是一个超级复杂的故事：量子比特对外界的影响非常敏感，这使得量子计算机非常复杂和昂贵。“量子计算机非常强大，但同时也非常脆弱，”Gil解释说：“它利用了世界上发生的物理过程，但我们希望这样的过程是一个独立且受控的系统。但我们的世界在微观尺度上都是连接在一起的，一切都在相互影响。这可能会破坏计算的结果：如果来自外部世界的能量进入计算系统内部，并接触到量子比特，这将使它们失去量子态，从而失去量子计算的独特能力。因此，量子计算机最大的挑战是产生一个与外部世界充分隔离的系统，但又不能太孤立（以便我们可以操纵量子比特）。

IBM已经建造了40台量子计算机，当记者试图了解建造成本时，Gil回避了直接回答，但听听这项工作的难度就可见一斑：“有几种不同的方法来构建量子计算机：IBM选择了一种低温方法，这意味着需要超低温并使用超导体。计算机中的温度接近绝对零度：在其底部的超导芯片部分的温度为-273°C，这要比外太空的温度还低3度，仅比绝对零度高了不到1度。我们知道，温度只能接近绝对零度，但无法达到绝对零度，因为在绝对零度时一切微观粒子将停止运动，甚至没有原子的运动。

量子计算机外部是一个用于制冷和保护的外壳，其形状类似于吊起来的电热水器，内部是超导计算芯片和复杂的制冷、控制线路，Gil和其团队形象地称之为“枝形吊灯”。“在保护层核心有一个带有超导处理器的圆柱体。即使只有一小部分能量粒子进入计算机，也足以破坏结果。“吉尔说。



超级的灵敏度以及由此产生的保护需求，意味着建造量子计算机非常麻烦：在最新的型号中，为了包含越来越多的量子比特，其外壳已经达到了几米的高度。在某种程度上，它让人想起上世纪五十年代的第一代经典计算机，它们看起来像巨大的橱柜。随后那些经典的计算机越来越小，直到今天，我们将数百万倍的计算能力挤入一个简单的智能手机中。但在量子计算机的情况下，我们可能不能指望发生类似的过程：“量子计算机需要独特的条件，但这些条件无法在简单的终端设备中获得，这在可预见的未来不会改变，“吉尔解释道，“我相信量子计算将是一种远程访问的服务，就像我们今天访问云服务一样。它将类似于 IBM 今天已经实现的工作：计算机与我们同在，我们可以访问云端“大脑”并接收答案。自2016年以来我们建造了40台计算机，今天已有20台可供公众使用。全世界大约有50万用户已经利用了这些量子计算机的功能，并且基于这种云量子计算，已经发表了大约一千篇科学论文。

谷歌和微软竞争加剧

IBM并不是唯一一家参与量子计算竞赛的公司，但Gil对自己的领导力充满信心：据他介绍，大多数竞争对手只有整个系统的一部分，而没有一台完整的计算机来解决问题。如前所述，谷歌是这场竞赛的有力竞争者，它还允许远程访问其量子计算服务 Google Quantum AI；微软也努力在其云平台 Azure 上提供类似的服务。

量子计算是“纸上谈兵”的承诺吗？这场革命的理论基础在40年前就已经奠定了，第一个实践证明是在20多年前提出的，该行业多年来一直在不断发声，但我们仍然没有看到为普通人服务的用途。

“如果你回到20世纪40年代，当第一台计算机被发明出来时，你会看到，即使这样，这项新发明的用途和优势也不清楚。那些看到第一台计算机的人说，'哦，太好了，你可以用它来破解战争中加密机器的代码，甚至可以计算弹道导弹的路线，仅此而已。谁会使用它？没有人，'“吉尔笑着说。“同样，量子计算的成功将取决于它的用途：编程的难易程度，用户社区的规模，以及人才的规模。量子革命将由一个社区领导，这就是为什么该领域的教育如此重要：我们需要越来越多的聪明人开始思考“我如何使用量子计算来助推我所在领域的发展”。

“现在开始的是量子计算的普及化阶段，这将允许任何人都能与量子计算机进行通信，而无需成为该领域的高级程序员：也能使用传统经典语言编写的问题或任务来调用量子计算机。这就是为什么我们今天已经看到量子计算能力在普及使用。

“还有许多初创公司实际上并没有致力于建立量子计算机，而是专注于深耕量子计算的应用场景。例如，以色列公司 Quantum Machines，它为量子计算机开发硬件和软件系统，并在去年7月由创新局选中，并成立以色列量子计算中心。这些公司的活动创造了一个全新的生态系统，从而促进了该行业的加速发展，就像今天在经典计算机领域发生的那样。IBM也不会仅仅依靠自己：我们希望从该领域的聪明人的创新中受益。

“我相信量子计算的‘大爆发’将在这十年内发生。我们IBM的目标是在未来三年内展示“量子霸权”。我相信，人工智能的进步与量子计算的结合，将带来英伟达在其市场上制造的那种行业革命（英伟达为游戏计算机开发了独特的处理器，这使其成为最快达到十亿美元收入的芯片公司。）量子计算可以在不同行业产生巨大的价值，尽管这非常困难，但我很清楚，我们将在十年内看到实际的应用。

诺贝尔奖为量子计算开辟了新视野

几十年来，量子计算激发了研究人员的想象力，但直到现在它还没有离开实验室。然而量子计算的愿景正成为一场真正的革命。今年的诺贝尔奖颁发给了量子科学领域的三位专家：法国的阿兰·阿斯佩（Alain Aspect）、美国人约翰·克劳瑟 （John Clauser）和奥地利人安东·蔡林格（Anton Zeilinger），他们因自1970年以来（分别）进行的研究而获奖，他们研究了量子纠缠现象，证明了它的存在并为其技术用途铺平了道路。

将诺贝尔奖授予量子纠缠研究人员，证明量子计算不仅仅是某个物理学派的思想实验，而且对于投资于该领域发展的公司来说是一个决定性的时刻。由于他们所在的世界发生了根本性的变化，他们被迫做出这一努力：近几十年来，计算世界一直按照“摩尔定律”运行，该定律预见到计算机处理器中晶体管的密度将每两年翻一番，这将增加芯片的计算能力。然而，随着行业接近物理极限，之后将无法将更多的晶体管塞到芯片上，开发量子计算机的需求变得迫切。

这些数字也表明市场正在发生一些事情。2020年，量子计算市场规模不到5亿美元，但在2021年底，作为愿景开始实现的信号，研究公司IDC发布了一项预测报告。根据该报告到2027年，量子计算市场规模将达到86亿美元，该领域的投资将达到160亿美元（2020年为7亿美元，2021年为14亿美元）。IBM的CEO阿文德·克里希纳（Arvind Krishna）最近也估计，到2027年，量子计算将成为真正的商业化产业。"
芯片破壁者（六.上）：摩尔定律的一次次“惊险”续命,2020-07-19 19:12:48,"1965年，《电子》杂志在创刊35周年之际，邀请时任仙童半导体公司研究开发实验室主任的摩尔，为其撰写一篇观察评论，预测微芯片工业的前景。

此时，全球半导体产业才刚刚萌芽，英特尔公司都尚未成立，市面上生产和销售的芯片更是屈指可数。

摩尔根据有限的数据大胆提出了一条被后人奉为圭臬的路线图——处理器（CPU）的功能和复杂性每12个月增加一倍，而成本却成比例地递减，也就是有名的摩尔定律。（1975年，摩尔将12个月改为18个月，沿用至今）。

这篇名为“让集成电路填满更多的元件”的报告，就此指导了半导体乃至整个信息产业半个世纪的发展步伐。



就连摩尔本人都没有想到，这个定律的效力是如此持久。

2005年，摩尔直言“Something like this can’t continue forever” ，认为摩尔定律可能在 2010 至 2020 年达到极限而失灵，建立在硅基集成电路上的电子信息技术也将被另外一种技术所代替。

此后十几年，不断挑战半导体产业极限的摩尔定律，也在一次次撞向“天花板”的时候“被死亡”。

关于摩尔定律的唱衰言论层出不穷。2014年国际半导体技术路线图组织宣布，下一份路线图将不再依照摩尔定律。台积电张忠谋、英伟达黄仁勋等挑战者更是“语出不逊”，认定摩尔定律不过是苟延残喘。

显然，一切并没有发生。集成电路芯片向5nm甚至3nm制程进发，依然是英特尔、三星、台积电等半导体厂商孜孜以求的目标。

硬挺到今天的摩尔定律，为何总能被成功“续一秒”，又是哪些黑科技在帮助它一次次“起死回生”？围绕在它身上的传奇和产业竞速到底能续写到什么时候？接下来，我们就一起走进——摩尔定律的惊魂夜。

薛定谔的摩尔定律之死

在抵达一个个惊险刺激的历史现场之前，有必要先跟大家聊聊摩尔定律持续“碰壁”的原因。

摩尔定律的定义，历史上其实被更新过几次，因此也形成了不同的版本和表达。比如：

集成电路上可容纳的晶体管数目，约每隔18个月便增加一倍；

微处理器的性能每隔18个月提高一倍，或价格下降一半；

相同价格所买的电脑，性能每隔18个月增加一倍。

正是沿着这个思路发展，电脑、电话等在强劲的处理器芯片加持之下，才有了低价格、高性能的可能，进而得以应用于社会每个的每个领域，成就了今天无处不在的信息生活，甚至彻底改变了人类的生活方式。



而在过去的几十年里，为了满足摩尔定律，半导体行业算是堵上了自己的尊严：

提升晶体管的密度与性能，成为微处理器按“摩尔定律”进化最直接的方法要在微处理器上集成更多的晶体管，芯片制造工艺不断向天花板逼近，制程节点不断逼近物理极限。

1971年英特尔发布的第一个处理器4004，就采用10微米工艺生产，仅包含2300多个晶体管。

随后，晶体管的制程节点以0.7倍的速度递减，90nm、65nm、45nm、32nm、22nm、16nm、10nm、7nm等等相继被成功研制出来，最近的战报是向5nm、3nm突破。



既然大趋势如此成功，为什么“摩尔定律”还会被屡屡宣判失效、死亡呢？

任何一个对指数有所了解的人，都会明白这种增长要无限地保持下去是不可能的。“增加一倍”的周期都是18个月，意味着每十年晶体管的数量要提高一百倍。

摩尔自己在演讲时也开玩笑说，如果其他行业像半导体这样发展的话，汽车现在应该一升汽油就能跑几十万公里，市中心每小时的停车费可能比劳斯莱斯还要昂贵，还有可能汽车尺寸会缩小到两寸长根本无法载人……

因此，摩尔本人在谈论“摩尔定律”的生命周期时，更同意史蒂芬霍金的说法。后者曾在被问及集成电路的技术极限时，提到了两个限制：

一是光的极限速度，芯片的运行速度距离光速还很远；二是物质的原子本质，晶体管已经很接近原子的直径（0.01纳米到0.1纳米之间）。

也就是说，摩尔定律想要在当下继续发展，工程师们就不得不面临与这两个最基本的自然法则做斗争。听起来是不是一个很艰难的挑战？



体现在具体的产业难题上，就是随着硅片上集成电路密度的增加，其复杂性和差错率也会呈现指数级的上升。

硅材料芯片被广为诟病的便是高温和漏电。集成电路部件发散的热量，以及连线电阻增加所产生的热量，如果无法在工作时及时散发出去，就会导致芯片“罢工”；

此外，晶体管之间的连线越来越细，耗电也就成了大问题。而且导线越细，传输信号的时间也就越长，还会直接影响它们处理信号能力。如果电子能直接穿透晶体管中的二氧化硅绝缘层，就会触发“量子隧穿效应”，完全丧失功能。

要在指甲盖大小的芯片上以亿为单位来雕刻晶体管，难度就像从月球上精准地定位到地球上的一平方米一样，这种原子甚至量子级别的集成电路焊接与生产，就对工艺精密度提出了更高的要求。

一边芯片被要求越做越小，性能越来越高；一边物理限制又需要晶体管之间保持一定的距离，可不为难死工程师了嘛。



同时别忘了，摩尔定律还被附加了经济色彩。除了性能之外，成本/价格的同时下降也被看做是基本要求。

体现到消费级市场，就是用户们在每两年，用更少的钱买到性能更高的电脑、手机产品。

但是，技术研发投入与光刻设备的更新换代，都需要半导体厂商耗费大量的资金。

生产精密程度的不断提升，也需要在制造环节投入更大的人力物力，一代代芯片生产线的设计、规划、调试成本，也在以指数级增长。

以前，生产130nm晶圆处理器时，生产线需要投资数十亿美元，到了90nm时代则高达数百亿，超过了核电站的投入规模。按照IBS 的 CEO Handel Jones 的预测，3nm 芯片的研发成本，甚至将达到 令人发指的40 亿至 50 亿美元。

为了摊薄成本，半导体厂商不得不生产更多的芯片，这又会导致单片芯片的利润回报下降。

很显然，半导体企业不可能长期“既让性能翻一倍，又让价格降一倍”，如果18个月没有收回成本，就要面临巨大的资金压力。



更为残酷的是，受软件复杂性等影响，芯片性能的提升在用户感知度上也越来越弱。

上世纪八九十年代，晶体管数量增加带来的性能加成是明显的。比如奔腾处理器的速度就远高于486处理器，奔腾2代又比奔腾1代优秀得多。

但正如大家所见的，进入21世纪以来，芯片制程越来越小，但用户对性能提升的感知度却不如以往令人惊艳，更新换代的买单欲望也能轻易被控制——等待更具性价比的计算硬件，锁死了摩尔定律的增长周期。

曾几何时，谷歌CEO Eric Schmidt 被问及会不会购买 64 位“安腾”处理器时，对方就表示“谷歌已经决定放弃摩尔定律”，不准备购买这种在当时看来的超级处理器。当然，这一决定被历史证明打脸了。

但也说明，即使厂商完成了前期的烧钱游戏，也未必能在中短线消费市场上完美收官。



总体而言，过去六十多年里，半导体行业的快速发展，正是在摩尔定律的推动下实现的，一代代运算速度更快的处理器问世，让人类彻底走进了信息时代。

与此同时，在芯片焊接和生产已经达到原子级别、接近量子级别的程度之后，摩尔定律也从指导行业进化的“金科玉律”，逐渐变成了捆绑在半导体产业头上的紧箍咒。

想要继续发挥作用，必须付出巨大的成本，让行业举步维艰、苦不堪言的同时，不断被唱衰也就成了摩尔定律的宿命。

性能、价格、市场预期，就如同三体世界里的三个太阳，在半个多世纪的时间里反复炙烤着摩尔定律。

接下来，我们就一起回到几个重要的“碰壁现场”，去看看摩尔定律是如何在一次次瓶颈期“惊险”逃生、鞭策着整个行业继续为之奋斗的。

第一次续命：从MSI到VLSI，

工匠之国日本的崛起

在此前的章节中，我们谈论了以DRAM为代表的VLSI超大规模集成电路的崛起，以及美国、日本在这个技术战场上的世纪战争。

而摩尔定律，既是这场战争必然爆发的推动力，也是产业版图更迭的见证者。

了解历史的人知道， 1975年，在“摩尔定律”发布的十年后，摩尔本人对定律进行了修改，将原本的“12个月翻一倍”改为了“18个月”。



当时，摩尔已经离开仙童，与别人一起创立了英特尔。而技术的挑战也在此时拉开序幕。

1975年，英特尔公司准备推出的一款电荷耦合器件(CCD)存储芯片中，只有3.2万个元件，这比摩尔定律预测的千倍增长整整少了一半。

第一个办法当然是修改定律，将产业周期从12个月延长到18个月。摩尔在一次访谈中曾提及这次修改，不无消极地说，自己的论文只是试图找到以最低成本生产微型芯片的方式——

“我觉得不会有人会按照它（摩尔定律）来制定商业计划 ，可能是因为我还沉浸在第一次预测正确的恐慌当中。我不觉得还会有人关注这个预测。”



翻车的原因在于，摩尔定律提出的1965年，还是小规模集成电路（SSL）时代，芯片内的元件不超过100个。此后，MSI（中规模集成电路）顺利地摆渡了十年，生产技术的进步远远领先于芯片设计，晶体管数量几乎每年都会翻番，完美符合摩尔定律。

但接下来，工程师们认为要在单芯片上集成十万个晶体管，VLSI阶段正式来临。与此同时，DRAM存储器、微处理器CPU等芯片产品的出现，在将芯片复杂度发挥到极致的同时，也让成本的经济性开始引起重视。



当时，美国半导体产业界已经在实验室完成了对VLSI的技术突破，为什么最后却是日本成功上位呢？

因为新时期里，拯救摩尔定律的不是技术上的突破，而是商业价值上的精进。

DRAM是当时最重要的半导体市场消费品，而其制造的关键在于更细 、更密集的电路。面临的挑战在于，随着芯片上元件的增多，晶圆上的随机缺陷影响加大，导致成品率降低，自然提高了芯片的生产成本，也让厂商的收益不那么美好。

必须实现成本下降，才能延续摩尔定律。而日本产业对技术和经济的平衡，在此时发挥了重要的作用。

1976年，日本以举国之力启动了闻名遐迩的超大规模集成电路研究计划。

由通产省技术专家和官员出面，集合了富士通、日立、三菱、日本电气（NEC）和东芝等5家公司，共同设立了VLSI研究所。



日本在进军半导体市场时更注重改进制程，而不是产品上有什么革命性的突破。日本VLSI研究所的目标，就是在微精细加工、工艺技术、元件技术等等课题上尝试提升。

VLSI项目实行了4年，于1980年结束，也确实诞生了丰硕的研究成果，大约有1000项发明获得了专利，这对日本半导体的国际竞争力起到了重大作用。

与此同时，注重制造技术也为日本半导体公司带来了全球竞争优势，虽然不像革命性产品那样引人注目，但价格和质量却成为攻占市场的重要筹码。

当时，业界每两三年便会推出新一代DRAM，存储能力以倍数上升，消费者们也热衷于升级存储条。庞大的市场需求，撞上日本工业界对集成电路的改良，直接从半导体产业大本营——美国手里抢走了不少市场份额。

1982底，日本的第一代超大规模集成电路的64K RAM已经占到国际市场的66％，至此，日本在DRAM制造方面的全球领导地位奠定，也使其成为下一代微芯片的技术领导者。



正是日本在VLSI技术上的发力，让摩尔定律得以继续发扬光大。到了1980年代，摩尔定律已经被看到是“DRAM准则”，随后，微处理器也出现在了曲线上。复杂度（晶体管的数量），以及芯片性能（处理器的操作速度），成为摩尔定律的主要预测对象，摩尔定律也从此时起成为业内公认的标准，不少微处理器和存储器芯片企业根据这一趋势来制定生产计划、参与国际竞争。

制程工艺与经济性的正式融合，让摩尔定律与半导体发展节奏，从80年代中期开始，开始变得密不可分。

接下来，摩尔定律还会遇到哪些“要命”的挑战，英特尔为何被IBM狂打脸，让芯片厂商改抱上游材料、设备厂商“大腿”的原因有哪些？我们的时光旅程将在《芯片破壁者（六.下）》中继续……"
"类人脑芯片服务器,美国IBM类人脑芯片又取得新突破：类人脑芯片对我们又有什么好处？...",2021-08-14,"2017年6月29日，第一届“(2017)世界智能大会”在天津举办。阿里巴巴董事局主席马云亲自来到大会现场，面对台下诸位在座的嘉宾，并发表演讲(关于人工智能的话题)。马云当众有说到：“……很多美国的学者，特别是脑外科学专家进入到了人工智能的研究，并且讲出人脑怎么怎么样，机器要像人脑那样学习，我觉得这是个悲哀，我们人类对大脑的了解不到5%，我们希望机器去学5%，这不是愚蠢吗……”马云当众所说的“机器要像人脑那样学习”，这其实就能和类人脑芯片(人工智能神经网络芯片)联系起来。 
 物理学家组织网近期就在报道：美国IBM(和美国空军实验室有合作)研发的类人脑芯片——“真北”(TrueNorth)，其数据处理能力已相当于有着“6400万个神经元和160亿个突触的类脑功能”，其机器学习能力则超过了目前全球已知的其他任何一个硬件模型。简而言之就是，IBM在研发类人脑芯片方面又取得新的突破。 

 (▲图片：从1993年到2016年，IBM在美国获得的专利授权数量。) 
 有IBM研究人员称：“传统计算机和人类的左脑相似，长于逻辑思维和语言等，‘真北’更像人类的右脑，强在图像识别和感知等。”正是因为“真北”有着独特的设计、IBM研究人员既可以在多个数据集上运行单个神经网络，又可以在单个数据集上运行多个神经网络，从而能高效地将多个数据集上的图片、视频和文本等形式的信息实时转换成计算机能识别的代码。需要补充的是，IBM的终极目标是在未来研发出比人类大脑强10倍的类人脑芯片，其有着100亿个神经元和100万亿个突触，且其功耗仅有1000瓦！毫不夸张地说，在未来，IBM所研发的类人脑芯片是有可能在全球引起一场技术革命的。 

 (▲图片：“真北”由4块芯片板构成，每块芯片板装有16个芯片，从而构成了一个64芯片系统。如果其中一个芯片不能正常工作，其他芯片工作时不会受到影响。) 
 2017年1月，成都高新报报道：在四川，电子科技大学微电子与固体电子学院的刘洋博士及其研究团队研发出了类人脑芯片。2016年12月，刘洋博士曾表示，芯片一旦在生产线上制作出来后，其大小是跟一颗米差不多，这是我国首个类人脑芯片，研究成果(在当时)处于全球领先水平。刘洋博士称这个类人脑芯片“5毫米的体格、发挥的作用却可以和1200台同时运作的电脑相媲美”。 

 (▲图片：电子科技大学刘洋博士。) 
 刘洋博士说：“目前，我们研发的人工智能神经网络芯片实现了人类神经元和突触，使它们也有了人类一样具有学习和记忆能力的‘大脑’。不过，它只有10万余个神经元，与人类大脑神经元相比只是几亿分之一。”前文说到，IBM的类人脑芯片已经有着“6400万个神经元和160亿个突触的类脑功能”。这正好说明了，中国在研发类人脑芯片方面有着巨大的成长空间。事实上，包括清华大学等中国大学也都有在研发类人脑芯片。比如，2017年5月，清华大学微电子所钱鹤、吴华强课题组在《自然通讯》(Nature Communications)在线发表题为《使用电子突触阵列实现人脸识别》(Face Classification using Electronic Synapses)的研究成果，即将氧化物忆阻器的集成规模提高一个数量级，首次实现基于1024个氧化物忆阻器阵列的类脑计算。 

 (▲图片：人类大脑的神经元结构。) 
 要知道，IBM之所以苦苦研发类人脑芯片，目的是为了打破传统计算机的基础——冯·诺依曼架构，而模仿人类大脑成为这一领域的技术突破方向。人类大脑可以快速、低功耗地完成各种学习任务。冯·诺依曼架构存在存储墙瓶颈，现有的计算机没法高效地实现相关算法，功耗成为制约因素。人类大脑的神经元传递信号的速度慢是事实，不过，人类大脑拥有的神经元数量却是太多(大约1000亿个神经元)。每个神经元都通过成千上万个突触和来其他神经元相连，从而构成了复杂的神经网络。人类大脑的突触能够同时、原位地进行记忆和计算，这与冯·诺依曼架构存在着显著的不同。 
 结语：类人脑芯片的商用前景有多光明？当智能手机中植入类人脑芯片后，智能手机不但会成超级计算机，还仿佛有了“生命”的特征。那么，如果类人脑芯片植入到各种各样的设备中呢…… 
 举报/反馈"
世界上最快的AI芯片，是何方神圣？,2023-10-27,"01  

 前言 
 这两天，IBM低调地发了一个新闻，推出了一款类脑芯片“北极”（NorthPole），对比4nm节点实现的Nvidia H100 GPU相比，NorthPole的能效提高了五倍，成为当之无愧是现在世界最强的AI芯片。 
 如此逆天的性能，但在国内，关于这款芯片的新闻却寥寥无几。那么，它究竟是何方神圣？ 
 付斌丨作者 
 电子工程世界（ID：EEworldbbs）丨出品 


 02  

  把脑子装进芯片，就行了？  
 首先，IBM的“北极”NorthPole是一种类脑芯片，我们需要先了解什么是类脑芯片。 
 所谓类脑芯片，顾名思义，就是一种高度模拟人脑计算原理的芯片，基于对现代神经科学的理解，反复思考如何从晶体管到架构设计，算法以及软件来模仿人脑的运算。如果把类脑芯片做得更像人脑，就会被赋予一个新的名字——神经形态计算（Neuromorphic Computing）。 
 人类的思考方法与现在传统的芯片存在许多区别，比如，人没有单独的存储器，没有动态随机存取存储器，没有哈希层级结构，没有共享存储器等等。 
 “存储”和“处理器”错综复杂地深绕在人脑里，在人脑的结构中有“神经元”的存在。在电脑中，以数字化核心相互交流基于事件的信息，叫做脉冲，这点和人脑传递信息的方式相似。 
 人类，便从人脑中，得到灵感，从而创造出这样的芯片，来提升性能，成为真正的“人工智能”，甚至无限接近人类，获得思考能力。 
 当然，人脑很复杂，而且人们所制造出来的IC器件，也存在优势，所以人们最终目标就是把这二者的优势融合起来。 
 总而言之，用人话来解释，类脑芯片，就是结合生物大脑和人造器件各自优点而设计制造出来的一种芯片，而它也会像人一样思考，自我学习。 

 想造一颗这样的芯片，可不是光是变变器件结构的就行的，而是从材料、器件、电路、架构带动算法和应用改变的。一言蔽之，就是集合各种最先进的技术，才能造出这样的芯片。 
 虽然实现路径很多，但奈何这种芯片技术难度太大了，且不说好不好造，设计出来就很难了，所以目前也在开拓阶段，都还无法达到商业化水平。 
 但笔者了解到，之所以类脑芯片还未形成大规模商业化，一是因为设计难题依存，就拿英特尔、IBM都看好的CMOS型，多块全数字异步设计的芯片互联、芯片连接的有效性和时效性以及软件层互连计算、分布式计算和灵活分区等问题都难以解决；二是制造、软件和生态都要完全推翻，虽然硅基晶体管路线部分可复用，但底层不可能完全照搬，这就进一步加剧大规模商业化难度。 
 那，我们为啥费大力气折腾类脑芯片？ 
 类脑芯片实在太香了，笔者了解到，某些情况下，完美的神经形态芯片可以用比传统解决方案低1000倍的能耗来解决问题，这意味着我们可以在固定的功耗预算下，打包更多的芯片来解决更大规模的问题。 
 类脑芯片的超高能效比足够让我们牟足力气研究和突破。拿一个最典型的例子来看，AlphaGo下棋打败了人类，但人类只用了20瓦的大脑能耗，而AlphaGo是2万瓦。 

 神经形态芯片涉及的领域和机会，图源丨Nature Computational Science 
 结构上，目前全世界的类脑芯片基本都一致，都是由神经元计算、突触权重存储、路由通信三部分构成，同时采用与脉冲神经网络（SNN）模型。 
 但依据材料、器件、电路，分为模拟电路主导的神经形态系统（数模混合CMOS型）、全数字电路神经系统（数字CMOS型）、基于新型器件的数模混合神经形态系统（忆阻器是候选技术）三种流派。 
 全球范围内，参与神经形态计算芯片开发的机构主要包括三类：英特尔、IBM、高通等为代表的科技巨头企业，斯坦福、清华为代表的高校/研究机构以及初创企业。 
 根据笔者之前与英特尔研究院对话中获悉，数字CMOS型是目前最易产业化的形式，一方面，技术和制造成熟度高，另一方面，不存在模拟电路的一些顾虑和限制。 
 当然，需要强调的是，数字CMOS型还只是最初阶的类脑芯片，还算不上完全模拟人脑的神经形态器件，只能算是一种借鉴神经形态理念的一种芯片。但光是借鉴人脑，这种芯片就能够碾压世界上任何一种芯片。IBM的NorthPole就是这样的数字CMOS型的类脑芯片。 

 类脑芯片主要类型和研发进度，制表丨电子工程世界 


 03  

  IBM的芯片，什么水平？  
 先说结论，应该是迄今为止，人类水平最高的类脑芯片了，对于类脑芯片研究又上了一个台阶。 
 2008年，IBM就开始研究类脑计算了，2011年有了第一次突破性成果：IBM的第一代神经突触计算机芯片。研究人员制造出两个这样的芯片模型：一颗包含262,144个可编程突触，一颗则包含65,536个可学习突触，通过测试展示其可执行导航、机器视觉、模式识别、关联记忆和分类等简单功能。 
 直到2014年，IBM在《科学》杂志发表了一篇文章，向全世界展示了划时代的技术进展：一个符合DARPA SyNAPSE项目指标的、拥有100万神经元的类脑处理器，即TureNorth，在当时掀起了类脑芯片研究热潮。 
 沉寂8年，“北极”（NorthPole）问世，NorthPole就是建立在IBM最后一颗类脑芯片TrueNorth基础之上，彼时TrueNorth就有这比传统微处理器低四个数量级的能效比，对比一下二者，就能很直观得感受到IBM技术的变化： 
 单个TruNorth芯片包含4096个计算核心，可以实现神经突触和神经元排列的动态映射。IBM TrueNorth系统的一个吸引人的功能是，单个芯片由54亿个晶体管组成，仅消耗70mW的功率密度，仅占传统计算单元的1/10000；NorthPole采用12nm节点工艺制造，在800平方毫米内集成了220亿个晶体管，有256个内核，在8位精度下，每个内核每个周期可执行2048次运算；在4位和2位精度下，运算次数有可能分别增加一倍和四倍。运行基于人工智能驱动的图像识别算法速度是目前市场上同类芯片的22倍，能效是同类芯片的25倍。在不使用最先进工艺的情况下，NorthPole芯片能耗是使用最先进技术的人工智能芯片的1/5。总体而言，NorthPole的速度大约是TrueNorth的4000倍。 

 结构方面，NorthPole与TrueNorth一样，由一个大型计算单元阵列（16×16）组成，每个单元都包含本地内存和代码执行能力。 
 计算资源方面，NorthPole每个单元都经过优化，可执行精度较低的计算，精度从 2 bit到8 bit 不等。为保证执行单元使用，它们不能根据变量值执行条件分支，也就是说，使用者代码不能包含if语句。这种简单的执行方式使每个计算单元都能进行大规模并行执行。在2 bit精度下，每个单元可并行执行8000多次计算。 
 存储方面，所有的内存都被封装在一颗芯片内， 这意味着每个内核都可以轻松地访问芯片上的内存。设备外部来看，NorthPole看起来像是一个主动存储芯片，这有助于将NorthPole集成到系统中。 
 不止如此，这款NorthPole目前采用的是12nm纳米节点工艺制造，目前CPU最先进的技术是3nm，而IBM还在研发2nm纳米节点技术，如果用上2nm，可能IBM的类脑芯片性能还会再提升很大档次。 

 NorthPole的潜在应用主要包括图像和视频分析、语音识别以及Transformer神经网络，这些网络是为ChatGPT等聊天机器人提供支持的大型语言模型 (LLM) 。这些人工智能任务可能会用于自动驾驶汽车、机器人、数字助理和卫星观测等领域。 
 某些应用程序需要的神经网络太大，无法安装在单个NorthPole芯片上。在这种情况下，这些网络可以分解为更小的部分，并分布在多个NorthPole芯片上。 
 而NorthPole的超高能效比，意味着它不需要笨重的液体冷却系统来运行，风扇和散热器就足够了，而它也可以部署在更小的空间中。 


 04  

  国内开始研究类脑芯片了吗？ 
 如此强大的芯片，国内也早已有所布局。 
 国内研究则包括清华大学、浙江大学、复旦大学、中科院等顶级学府和机构，同时近两年不断涌现初创公司，如灵汐科技、时识科技、中科神经形态等。其中以清华大学的天机芯和浙江大学的达尔文芯片最具代表性。 

 具体而言，国内的主要成果包括： 
 清华大学2015年开发的第一代天机芯采用110nm工艺，2017年，第二代天机芯开始取得先进成果，基于28nm工艺制成，由156个功能核心FCore组成，包含约4万个神经元和1000万个突触。相比第一代，密度提升20%，速度提高至少10倍，带宽提高至少100倍，此外，清华大学还自主研发了软件工具链，支持从深度学习框架到天机芯的自动映射和编译。根据清华大学的计划，下一代天机芯将是14nm或更先进的工艺，且功能会强大更多；浙江大学联合之江实验室共同研制的类脑计算机，其神经元数量与小鼠大脑神经元数量规模相当。该计算机包含792颗达尔文2代芯片，支持1.2亿个脉冲神经元、720亿个神经突触，而其典型运行功耗仅为350W~500W；2020年10月，清华大学计算机系张悠慧团队精仪系施路平团队与合作者在《自然》（Nature）杂志发文首次提出“类脑计算完备性”以及软硬件去耦合的类脑计算系统层次结构；2023年9月，中国科学院计算技术研究所尤海航研究员和唐光明研究员带领的研究团队研制了超导神经形态处理器原型芯片“苏轼（SUSHI）”，它是一款基于超导单磁通量子（SFQ）电路的超导计算芯片；2023年10月，由中科南京智能技术研究院自主研发是目前国内规模最大、国际一流的类脑超级计算机服务正式启用，它已实现5亿神经元2500亿突触智能规模，较现有计算系统能效提升10倍以上，核心芯片自主可控。 
 IBM的成果代表着，这项布局未来的技术离我们又进了一步，而目前一些初创公司逐渐形成方案，开始应用。不难预见，在近几年，这项技术商业化将逐步展开，而彼时研究成果也将照进现实。 
 参考文献 
 [1] IBM：A new chip architecture points to faster, more energy-efficient AI.https://research.ibm.com/blog/northpole-ibm-ai-chip 
 [2] IEEE Spectrum：IBM Debuts Brain-Inspired Chip For Speedy, Efficient AI.2023.10.23.https://spectrum.ieee.org/neuromorphic-computing-ibm-northpole 
 [3] Schuman C D, Kulkarni S R, Parsa M, et al. Opportunities for neuromorphic computing algorithms and applications[J]. Nature Computational Science, 2022, 2(1): 10-19.https://doi.org/10.1038/s43588-021-00184-y  
 [4] 清华大学：清华大学计算机系张悠慧团队首次提出“类脑计算完备性”.2020.10.16.https://mp.weixin.qq.com/s/-zZpk1pESZ_q2eDWECsIZg 
 [5] 中科院计算所：计算所研制超导神经形态处理器原型芯片“苏轼（SUSHI）”.2023.9.28.https://mp.weixin.qq.com/s/kUz6hYkI7hvs9a09pDtwhw 
 [6] 南京发布：问天Ⅰ，类脑计算机.2023.10.17.https://mp.weixin.qq.com/s/Mm-Tb-Vrr-uXXu4uKZe9sA 
 -END- 

 往期推荐：点击图片即可跳转阅读 


 
 iPhone 15 Pro Max拆解，附bom表！ 
 


 
 树莓派5，值不值得买？ 
 


 
 RTX4090都不让用了？美国这是玩不起？ 
 


 
 公司里的硬件技术总监CTO，天天都在干些啥？"
AI芯片领域又掀风浪啦！IBM公司“神速”芯片“北极”现世！,2023-10-24,"原创 | 文 BFT机器人  
 
 
 
 
最近美国一家科技巨头公司-IBM又有了新的动态，据了解，IBM公司推出了一款全新的类脑芯片“北极”，据介绍，这一款芯片运行人工智能驱动的图像识别算法的速度是同级类商业芯片的22倍，效能是同类芯片的25倍。“北极”面世就有神速之称的AI芯片，到底有什么特别之处？给芯片界又带来了什么影响呢？本篇文章带你阅读一起了解一下吧！ 
 
据了解，设计“北极”这一款芯片的灵感来源于人脑的工作方式，其设计原理就是将其计算模块与存储信息的模块交织在一起，，允许每个计算核心像访问相邻的存储块一样轻松地访问远程存储块，这大大加快了计算单元和存储单元之间信息交换的速度。 
 
在“北极”之前，IBM公司根据根据这一想法原理曾经研发出名为“真北”的芯片，但是这远远不是IBM公司的最终目标。最终，他们的研发团队研发出“北极”并将这项技术转变为一种可以与当代计算机中使用的硅片技术兼容的数字结构，这项研究的负责人达尔门德拉·莫德哈声称这是一项看待算机体系结构的新方法。 
 
 
 
图片来源：IBM公司 
 
其研究团队还表示：“北极”比现在市场上任何芯片都要更快、更高效，而且还能用于语言识别和自然语言处理的人工智能，但是“北极”也有一定的功能限制，比如：无法执行人工智能训练任务，无法轻松运行更大的人工智能模型等。 
 
研究团队声称要继续研究深入，计划证明多个“北极”芯片是否可支持大型语言模型，争取突破更多可能。虽然现在“北极”还不能被普片应用和商业也应用，但“北极”的基础架构具有创新性，这能做未来的研究中克服不少难点。 
 
随着人工智能越演越烈，芯片领域竞争一直都很激烈。AI芯片作为各个大国在人工智能方面决胜的关键，其研发与推出成为各国和人工智能研究领域的关注点。在人工智能加速发展的过程中，各国的科技巨头公司纷纷推出自研AI芯片产品。我国在近年也推出过不少的AI产品，如寒武纪的思元系列、华为的海思、昆仑芯科技的昆仑芯、阿里的平头哥等，这些产品在AI芯片领域也掀起了不小的波澜。 
 
今天，IBM的“北极”芯片推出同时也引发各界的广泛关注，这一研究的论文已经刊登在《科学》的期刊上“北极”的横空出世，在对全球的人工智能界领域产生深远影响的同时，也将AI芯片市场竞争推向了一个新的高度。它的出现不仅能推动新技术的革命，还能给人工智能、大数据处理等领域提供更为强大的计算力。 
 
END 
作者 | 春花 
排版 | 春花 
审核 | 柒柒 
若您对该文章内容有任何疑问，请与我们联系，将及时回应。"
TrueNorth：IBM的百万神经元类人脑芯片,2015-04-08,"邮票大小、重量只有几克，但却集成了54亿个硅晶体管，内置了4096个内核，100万个“神经元”、2.56亿个“突触”，能力相当于一台超级计算机，功耗却只有65毫瓦。 
 这就是IBM公布的最新仿人脑芯片：TrueNorth。 
 为什么要做TrueNorth？ 
 因为自2008年以来，美国国防部研究机构DARPA给了IBM 5300万美元。TrueNorth是IBM参与DARPA的研究项目SyNapse的最新成果。SyNapse全称是Systems of Neuromorphic Adaptive Plastic Scalable Electronics（自适应可塑可伸缩电子神经系统，而SyNapse正好是突触的意思），其终极目标是开发出打破冯•诺依曼体系的硬件。 
 为什么要打破冯•诺依曼体系？ 
 冯•诺依曼体系是传统计算机的基础。这种体系的特点是存放信息和程序指令的内存与处理信息的处理器是分离的。由于处理器是按照线序执行指令的，所以必须不断与内存通过总线反复交换信息—而这个会成为拖慢速度和浪费能量的瓶颈。尽管后来采用了多核芯片和缓存技术，但是这些只能提高速度而不能降低太多能耗，而且没办法实时处理，因为通信是瓶颈—内存和CPU的大量通信要通过总线进行。因此，近几十年来研究人员一直在致力于寻找突破原有体系的技术。 
 模仿大脑 
 模仿人类大脑是科学家寻求突破的方向。人类大脑的神经元尽管传导信号的速度很慢，但是却拥有庞大的数量（千亿级），而且每个神经元都通过成千上万个突触与其他神经元相连，形成超级庞大的神经元回路，以分布式和并发式的方式传导信号，相当于超大规模的并行计算，从而弥补了单神经元处理速度的不足。人脑的另一个特点是部分神经元不使用时可以关闭，从而整体能耗很低。 

 在了解了人脑的这种机理之后，研究人员开始了在软硬件上对人脑的模拟。在硬件方面，近年来主要是通过对大型神经网络进行仿真，如Google的深度学习系统Google Brain，微软的Adam等。但是这些网络需要大量传统计算机的集群。比方说Google Brain就采用了1000台各带16核处理器的计算机，这种架构尽管展现出了相当的能力，但是能耗依然巨大。 
 而IBM则是在芯片上的模仿。 

 集成度和能效令人印象深刻 
 这种芯片把数字处理器当作神经元，把内存作为突触，跟传统冯诺依曼结构不一样，它的内存、CPU和通信部件是完全集成在一起。因此信息的处理完全在本地进行，而且由于本地处理的数据量并不大，传统计算机内存与CPU之间的瓶颈不复存在了。同时神经元之间可以方便快捷地相互沟通，只要接收到其他神经元发过来的脉冲（动作电位），这些神经元就会同时做动作。 
 2011年的时候，IBM首先推出了单核含256 个神经元，256×256 个突触和 256 个轴突的芯片原型。当时的原型已经可以处理像玩Pong游戏这样复杂的任务。不过相对来说还是比较简单，从规模上来说，这样的单核脑容量仅相当于虫脑的水平。 
 不过，经过3年的努力，IBM终于在复杂性和使用性方面取得了突破。4096个内核，100万个“神经元”、2.56亿个“突触”集成在直径只有几厘米的方寸（是2011年原型大小的1/16）之间，而且能耗只有不到70毫瓦，IBM的集成的确令人印象深刻。 
IBM的TrueNorth芯片结构、功能、物理形态图 
 这样的芯片能够做什么事情呢？IBM研究小组曾经利用做过DARPA 的NeoVision2 Tower数据集做过演示。它能够实时识别出用30帧每秒的正常速度拍摄自斯坦福大学胡佛塔的十字路口视频中的人、自行车、公交车、卡车等，准确率达到了80%。相比之下，一台笔记本编程完成同样的任务用时要慢100倍，能耗却是IBM芯片的1万倍。 
 跟传统计算机用FLOPS（每秒浮点运算次数）衡量计算能力一样，IBM使用SOP（每秒突触运算数）来衡量这种计算机的能力和能效。其完成460亿SOP所需的能耗仅为1瓦—正如文章开头所述，这样的能力一台超级计算机，但是一块小小的助听器电池即可驱动。 
 通信效率极高，从而大大降低能耗这是这款芯片最大的卖点。TrueNorth的每一内核均有256个神经元，每一个神经有分别都跟内外部的256个神经元连接。 
 但是相比之下，人脑有上千亿个神经元，每个神经元又有成千上万的突触，那样一个神经网络就更加无法想象了。IBM的最终目标就是希望建立一台包含 100 亿个神经元和 100 万亿个突触的计算机—这样的计算机要比人类大脑的功都强大10 倍，而功耗只有一千瓦，而且重量不到两升（我们大脑的大小）。 
 应用 
 不过，计算能力并非这种最强大脑的长处，其强项在于认知能力。认知芯片可以执行感知、交互、识别等任务，就像演示中的识别对象一样。IBM预计这种认知能力可以在物联网中充分能力，部署在传感器中处理现实世界的感知问题。使用认知计算机通过传感器网络和微型电机网络不断记录和报告数据如温度，压力，波高，声学和海潮等来监测世界范围内的供水状况。然后，它还可以在发生地震的情况下发出海啸警报。而这样的任务传统计算机根本不可能完成。或者这种芯片也可以部署在它的Watson上面，从而大大提升这台在智力竞赛中战胜人类冠军的超级计算机的能力和能效。 

 因此，有人把IBM的芯片称为是计算机史上最伟大的发明之一，将会引发技术革命，颠覆从云计算到超级计算机乃至于智能手机等一切。 
 前景 
 但是从短期来看，情况也许并没有那么乐观。 
 首先芯片的编程仍然是个大问题。芯片的编程要考虑选择哪一个神经元来连接，以及神经元之间相互影响的程度。比方说，为了识别上述视频中的汽车，编程人员首先要对芯片的仿真版进行必要的设置，然后再传给实际的芯片。这种芯片需要颠覆以往传统的编程思想，尽管IBM去年已经发布了一套工具，但是目前编程仍非常困难，IBM团队正在编制令该过程简单一点的开发库。（当然，如果我们回顾过去编程语言从汇编一路走来的历史，这一点也许不会成为问题。） 
 其次，在部分专业人士看来，这种芯片的能力仍有待证实。 
 在神经网络和机器学习专家看来，DARPA的NeoVision2 Tower的数据集相对比较简单，演示只是识别了5种对象，相对于Google和百度等图像识别使用的是有上百万图像种类上千的ImageNet，芯片在这种测试集的表现如何尚不得而知。 
 再者，真正的认知计算应该能从经验中学习，寻找关联，提出假设，记忆，并基于结果学习，而IBM的演示里所有学习都是在线下的冯诺依曼计算机上进行的。不过目前大多数的机器学习都是离线进行的，因为学习经常需要对算法进行调整，而IBM的硬件并不具备调整的灵活性，不擅长做这件事情。但是IBM并不排除硬件实现的可能性（这家公司在历史上已经表现出极佳的持续改进能力）。但是从事同类研究的LeCun认为，应该开发能实现最新算法的芯片，那种芯片应该不是IBM芯片的样子，而是类似于改良版的GPU（参见微软的Adam）。 
 所以说，IBM的芯片是否真如其名称TrueNorth所述、为计算机的未来找到了真正的方向，目前尚不得而知。但是，至少这款芯片超高集成度和超低能耗已经给人留下了深刻印象。 
IBM集成16块TrueNorth芯片的电路板"
【ScienceAI Weekly】IBM新AI芯片提效25倍；清华大学发AI辅助框架；DeepMind新工具预测220万新晶体,2023-12-09,"「ScienceAI Weekly」是 HyperAI 超神经新创建的半月更栏目，主要从科研成果、企业动态、工具资源、近期活动 4 个维度，收集呈现 ScienceAI 领域近期值得关注的最新动态，以期为长期关注该领域的从业者、爱好者，提供更有价值的圈内资讯和行业启示。 
 
一起来看看本期值得关注的 ScienceAI 进展吧！ 
 
科研成果 
 
GaUDI：用于逆向分子设计的引导扩散模型 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
* 标题：Guided diffusion for inverse molecular design 
 
* 来源：Nature Computational Science 
 
* 领域：分子设计，材料化学 
 
* 作者：以色列理工学院团队与威尼斯卡福斯卡里大学科研团队 
 
* 原文： 
 
Guided diffusion for inverse molecular design | Nature Computational Science 
 
方法简介： 
 
GaUDI 是一个用于逆向分子设计 (inverse molecular design) 的引导扩散模型 (guided diffusion model)，它结合了用于属性预测的等变图神经网络，以及生成扩散模型，通过用针对 47.5 万个多环芳香系统生成数据集的单目标和多目标任务检测，科研人员发现 GaUDI 在有机电子学应用的分子设计中，具有极高的有效性。 
 
GaUDI 改善了条件设计，可以生成具有最优属性、甚至超越原始分布的分子，可以识别出比数据集中更优的分子。除 point-wise target 外，GaUDI 还可以用于 open-ended targets（如最小值或最大值），并在所有情况下，生成的分子有效性都接近 100%。 
 
SurfGen：计算机辅助药物发现新模型 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
* 标题：Learning on topological surface and geometric structure for 3D molecular generation 
 
* 来源：Nature Computational Science 
 
* 领域：分子设计，药物发现 
 
* 作者：浙江大学和碳硅智慧的研究团队 
 
* 原文： 
 
Learning on topological surface and geometric structure for 3D molecular generation | Nature Computational Science 
 
方法简介： 
 
SurfGen 模型以类似于「钥匙和锁」原理的方式设计分子，其包含了两个等变神经网络——Geodesic-GNN 和 Geoatom-GNN，分别捕获口袋表面上的拓扑相互作用，以及配体原子和表面节点之间的空间相互作用。 
 
SurfGen 在许多基准测试中都优于其他方法，其对口袋结构有高敏感性，这使得基于生成模型的方案能够解决突变诱导的耐药性这一棘手问题。 
 
清华大学研究团队发布人工智能辅助框架——DeepSEED 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
* 标题： 
 
Deep flanking sequence engineering for efficient promoter design using DeepSEED 
 
* 来源：Nature Communications 
 
* 领域：合成生物学 
 
* 作者：清华大学研究团队 
 
* 原文： 
 
Deep flanking sequence engineering for efficient promoter design using DeepSEED | Nature Communications 
 
方法简介： 
 
DeepSEED 由两个深度学习模块组成，一个条件式生成对抗网络 (cGAN) 用于捕获显式和隐式模式之间的依赖关系，一个基于 DenseNet-LSTM 的预测模型用来评估启动子性能，并利用遗传算法对这两个模型进行偶联，通过模型迭代实现功能启动子的优化设计。 
 
FIREANN：用于原子系统对外部场响应的通用机器学习模型 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
* 标题： 
 
Universal machine learning for the response of atomistic systems to external fields 
 
* 来源：Nature Communications 
 
* 领域：原子系统，机器学习 
 
* 作者：中国科学技术大学和安徽科技大学团队 
 
* 原文： 
 
https://www.nature.com/articles/s41467-023-42148-y 
 
方法简介： 
 
研究团队提出了一种通用场诱导递归嵌入原子神经网络 (FIREANN) 模型，可以将伪场矢量依赖特征整合到原子描述符中，以表示具有严格旋转等变性的系统-场相互作用。 
 
这种「一体式」方法将偶极矩和极化率等各种响应特性与单个模型中的场相关势能相关联，非常适合在存在电场的情况下，对分子和周期系统进行光谱和动力学模拟。对于周期系统，FIREANN 可以通过仅训练原子力来克服极化的内在多值问题。 
 
企业动态 
 
DeepMind 发布 GNoME，利用深度学习预测 220 万种新晶体 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
GNoME 流程示意图 
 
Google DeepMind 近日在 Nature 上发表的一篇论文显示，借助其发布的深度学习工具 GNoME，科研人员在短时间内就发现了 220 万种新晶体（相当于人类科学家近 800 年的知识积累），其中 38 万种新晶体具备稳定的结构，成为最有可能通过实验合成并投入使用的潜在新材料。 
 
GNoME 全称 Graph Networks for Materials Exploration，是一个用于新材料研发的 SOTA GNN 模型，它利用深度学习，可以在极短时间内预测新材料的稳定性，极大提高了材料研发的速度和效率，展现了利用 AI 大规模开发新材料的潜力。 
 
DeepMind 发布气象预测 AI 模型 GraphCast 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
GraphCast 依据 6h 前及当前天气状况，即可预测未来 6h 天气 
 
日前，Google DeepMind 在官方博客中宣布推出的一款基于机器学习的天气预测模型——GraphCast，在全球 0.25° 的分辨率下，在一分钟内预测未来 10 天的数百个天气变量，显著优于传统气象预报方法。 
 
此外，该模型在预测极端事件方面同样表现良好。与行业黄金标准天气模拟系统——高分辨率预报 (HRES) 相比，GraphCast 在 1,380 个测试变量中准确预测超过 90%。 
 
百图生科与赛诺菲达成大型战略合作 
 
百图生科 (BioMap) 宣布与全球 Top10 的医药跨国公司赛诺菲达成战略合作。根据协议，双方将基于百图生科的生命科学大模型 (Life Science AI Foundation Model)，共同开发用于生物治疗药物发现的前沿模型。此次合作，是生命科学领域首次基于大模型的商业合作，同时也进一步推动了「模型开发」与「AI 药物管线研发」两种商业模式并行发展。 
 
IBM 推出新款 AI 芯片 NorthPole，能效提高 25 倍 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
前段时间，美国 IBM 公司推出了一款类脑 AI 芯片 NorthPole，可大幅提高能源、空间和时间效率，相关论文已刊发于国际顶刊 Nature。据介绍，NorthPole 基于 12 纳米节点处理技术构建，以 ResNet-50 模型为基准，NorthPole 的效率明显高于常见的 12 纳米 GPU 和 14 纳米 CPU，能效提高了 25 倍。 
 
论文地址： 
 
‘Mind-blowing’ IBM chip speeds up AI 
 
玄言生物完成数千万天使轮融资 
 
上海玄言生物科技有限公司此前宣布完成数千万元天使轮融资，由海南仟港汇投资合伙企业独家领投，融资获得资金计划用于世界全新癌症靶点抗体药 INTX-001 的临床前研究，开发下游 IVD 管线，AI 预测重大疾病进程（早筛、转移等）相关的 IVD 产品与软件。 
 
工具资源 
 
AI for Science 开源新项目 Polymathic AI 主要团队曝光 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
Polymathic AI 计划的目标是加速开发针对数值数据集和科学机器学习任务定制的多功能基础模型。 
 
为此， Polymathic AI 汇集了一支由纯机器学习研究人员，以及领域科学家组成的团队，接受世界领先专家组成的科学咨询小组的指导，并由图灵奖得主、Meta 首席科学家 Yann LeCun 担任顾问，受到包括剑桥大学 AI+天文/物理助理教授 Miles Cranmer 在内的多位学术大咖的支持，以期集中精力开发科学数据的基础模型，利用跨学科的共享概念解决 AI for Science 行业难题。 
 
项目开源地址： 
 
https://github.com/PolymathicAI/ 
 
阿里达摩院发布遥感 AI 大模型 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
不久前，阿里达摩院发布了业内首个遥感 AI 大模型 (AIE-SEG)，率先在遥感领域实现了图像分割的任务统一，可实现「万物零样本」的快速提取，可识别农田、水域、建筑物等近百种遥感地物分类，还能根据用户的交互式反馈自动调优识别结果，大幅提升灾害防治、自然资源管理、农业估产等遥感应用的分析效率。 
 
该模型已在 AI Earth 地球科学云平台开放使用。在一些特定场景下，对比传统的遥感模型，其实例提取的准确率可提升 25%，变化检测的准确率可提升 30%。 
 
模型地址： 
 
AI Earth 
 
上海AI实验室开源首个城市级 NeRF 实景 3D 大模型 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
目前，上海 AI 实验室已经正式开源了全球首个城市级 NeRF 实景三维大模型「书生·天际」 (LandMark)，支持在不同应用场景下的落地部署，并提供免费商用。这也是上海 AI 实验室首次公开多分支 GridNeRF 模型核心算法及与之匹配的训练策略。 
 
书生·天际官网： 
 
书生·天际 
 
开源地址： 
 
https://github.com/InternLandMark/LandMark 
 
论文地址： 
 
Grid-guided Neural Radiance Fields for Large Urban Scenes 
 
智源研究院开源 10 亿参数的三维视觉通用模型 Uni3D 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
日前，智源研究院开源发布了 10 亿参数的三维视觉通用模型 Uni3D，其对于衡量视觉通用能力至关重要的分类任务，以及零样本识别、理解、分割任务等，都有超预期表现。据介绍，出品 Uni3D 的智源视觉团队此前曾发布 10 亿通用视觉模型 EVA。而本次三维视觉模型的突破关键是利用了 ViT 技术，将 2D 预训练基础经验升维至 3D。 
 
论文链接: 
 
https://arxiv.org/abs/2310.06773 
 
代码/模型链接： 
 
https://github.com/baaivision/Uni3D 
 
https://huggingface.co/BAAI/Uni3D/tree/main/modelzoo 
 
近期活动 
 
中国药科大学生物医药数据科学行业与专业发展大会 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
2023 年 12 月 8 日至 9 日，中国药科大学 2023 生物医药数据科学行业与专业发展大会 BDSC (The Biomedical Data Science Convention For Industry & Discipline Development) 将于南京举办。 
 
大会以「数据求真 · 智慧医药」为主题，重点关注生物医药与数据科学的多学科交叉发展，深度剖析行业发展趋势，探索以人工智能为代表的数据科学在药物发现、药品生产、质量安全、政策监管、生物医药企业数字化转型升级等业界关注的诸多领域的前沿动态、学术成果以及应用实践。 
 
报名地址： 
 
2023BDSC生物医药数据科学行业与专业发展大会 预约报名-活动-活动行 
 
「未来之后」AI4S 第二届国际高峰论坛 
 
 
 
添加图片注释，不超过 140 字（可选） 
 
2023 年 12 月 14 日，由北京大学深圳研究生院发起的「未来之后」AI4S 第二届国际高峰论坛，将于北京大学深圳研究生院会议中心正式开幕。 
 
活动邀请到了美国科学院院士 David Baker 教授、微软杰出首席科学家刘铁岩博士、中国科学技术大学杰出讲席教授罗毅、美国康奈尔大学化学工程与生物分子工程系终身讲席教授尤峰崎、厦门大学化学化工学院教授程俊，围绕 AI for Science 的发展带来深度分享。 
 
报名地址： 
 
https://mp.weixin.qq.com/s/WObN0eh-t0a8ZRuOQzkxsg"
类脑芯片,2020-04-26,"神经拟态芯片架构 
 
 
1、英特尔神经拟态芯片 Loihi 每个芯片包括了：128个内核，13万个神经元，1.3亿个突触。既可以在深度DNN上，也可以模拟时间序列的SNN。 主要贡献，用于模拟生物嗅觉，识别10种有害的化学药品。 
2、IBM的芯片架构 TrueNorth 每个芯片包括了：4096个内核，100万个神经元，2.56亿个突触。 ==特点是：==功耗只有65毫瓦。 
3、清华大学天机芯片 每个芯片包括了：156个内核，40000个神经元，1000万个突触。 特点是： 支持跑CNN和SNN的异构融合。并且能够支持更多的算法，密度提高10倍，带宽提高100倍。 4、浙江大学达尔文芯片2代 每个芯片包括576个内核，一共是15万个神经元，1000万个突触。 
 
 在这个通向未来计算的前沿研究道路上，既有英特尔、IBM、高通、三星、惠普等科技巨头，也有BrainChip、西井科技、灵汐科技、aiCTX、Numenta、General Vision、Applied Brain Research、Brain Corporation等初创公司。"
IBM展示非冯·诺依曼架构AI芯片NorthPole,2023-10-25,"我们正处于人工智能的“寒武纪大爆发”时期。在过去的十年中，人工智能已经从理论和小型测试发展到企业规模的使用案例。但是，用于运行人工智能系统的硬件虽然越来越强大，但在设计时却没有考虑到当今的人工智能。随着人工智能系统规模的扩大，成本也随之飙升。而摩尔定律，即处理器电路密度每年翻一番的理论，已经放缓。但是，位于加利福尼亚州阿尔马登的 IBM 研究实验室经过近二十年的努力，终于完成了一项新的研究，它有可能彻底改变我们如何高效地扩展强大的人工智能硬件系统。自半导体工业诞生以来，计算机芯片主要遵循相同的基本结构，即处理单元和存储待处理信息的内存是离散存储的。虽然这种结构使得设计更加简单，几十年来一直能够很好地扩展，但它也造成了所谓的冯-诺依曼瓶颈，即在内存、处理单元和芯片内的任何其他设备之间不断地来回切换数据都需要时间和能量。IBM 研究院的达曼德拉-莫德哈（Dharmendra Modha）和他的同事们从大脑的计算方式中汲取灵感，旨在改变这种状况。""莫德哈说：""它开辟了一条与冯-诺依曼架构完全不同的道路""。过去八年来，Modha 一直在研究一种用于神经推理的新型数字人工智能芯片，这就是 NorthPole。它是 Modha在2014年开发的类脑芯片 TrueNorth 的延伸。在对流行的 ResNet-50 图像识别和 YOLOv4 物体检测模型的测试中，新的原型设备比目前市场上的任何其他芯片都表现出更高的能效、更高的空间效率和更低的延迟，速度大约是 TrueNorth 的 4000 倍。NorthPole 芯片取得的第一批令人鼓舞的成果发表在今天的《科学》杂志上。Modha表示，NorthPole是芯片架构的一个突破，它在能源、空间和时间效率方面都有巨大的改进。以ResNet-50模型为基准，NorthPole的效率大大高于普通的12纳米GPU和14纳米CPU。(在这两种情况下，NorthPole 的能效是普通 12nm GPU 和 14nm CPU 的 25 倍（NorthPole 本身就是基于 12nm 节点处理技术构建的）。就每十亿个晶体管每秒所需的解释帧数而言，NorthPole 在延迟和计算所需空间方面也更胜一筹。根据 Modha 的说法，在 ResNet-50 上，NorthPole 的性能优于所有主流架构，甚至优于那些使用更先进技术工艺的架构，例如使用 4nm 工艺实现的 GPU。它是如何做到比现有芯片的计算效率如此之高的呢？NorthPole 最大的不同之处在于，设备的所有内存都在芯片上，而不是单独连接。没有了冯-诺依曼瓶颈，该芯片进行人工智能推理的速度大大快于市场上的其他芯片。NorthPole 采用 12 纳米节点工艺制造，在 800 平方毫米内包含 220 亿个晶体管。它有 256 个内核，在 8 位精度下，每个内核每个周期可执行 2,048 次运算，在 4 位和 2 位精度下，运算次数有可能分别增加一倍和四倍。""Modha说：""这是一个芯片上的整体网络。   
Modha说：""从架构上讲，NorthPole模糊了计算与内存之间的界限。""在单个内核层面，NorthPole表现为接近计算的内存，而在芯片外部，在输入输出层面，它表现为活动内存。这使得 NorthPole 很容易集成到系统中，并大大减轻了主机的负担。但是，NorthPole 的最大优势同时也是一种限制：它只能轻松地从板载内存中提取数据。如果必须从其他地方获取信息，芯片上可能实现的所有加速都会被削弱。通过一种称为 ""扩展""（scale-out）的方法，NorthPole 可以将更大的神经网络分解成适合 NorthPole 模型内存的更小的子网络，并将这些子网络连接到多个 NorthPole 芯片上，从而支持更大的神经网络。因此，虽然一个 NorthPole（或一组 NorthPole 的总和）上有充足的内存，可用于许多对特定应用有用的模型，但这种芯片并不是万能的。""Modha 说：""我们不能在上面运行 **-4，但我们可以为企业需要的许多模型提供服务。""当然，NorthPole 也只能用于推理。这种功效意味着该设备也不需要笨重的液冷系统来运行--风扇和散热片就足够了--也就是说它可以部署在一些相当狭小的空间里。 
NorthPole 的潜在应用尽管对 NorthPole 芯片的研究仍在进行中，但其结构适合新兴的人工智能用例以及更成熟的用例。在测试过程中，NorthPole 团队主要关注与计算机视觉相关的应用，部分原因是该项目的资金来自美国国防部。考虑的一些主要应用包括检测、图像分割和视频分类。但它也在其他领域进行了测试，如自然语言处理（在仅编码器的 BERT 模型上）和语音识别（在 DeepSpeech2 模型上）。该团队目前正在探索将纯解码器大型语言模型映射到 NorthPole 扩展系统。一想到这些人工智能任务，各种天马行空的用例就会涌上心头，从自动驾驶汽车到机器人、数字助理或空间计算。许多需要实时处理大量数据的边缘应用都非常适合 NorthPole。例如，NorthPole 有可能成为将自动驾驶汽车从需要设定地图和路线才能在小范围内运行的机器，转变为能够对罕见的边缘情况进行思考并做出反应的设备。这些边缘情况正是 NorthPole 未来应用的甜蜜点。NorthPole 可使卫星监控农业和管理野生动物种群，监控车辆和货运以减少道路拥堵，安全操作机器人，以及检测网络威胁以确保企业安全。下一步工作这只是 Modha 在 NorthPole 上工作的开始。目前最先进的中央处理器是 3 纳米，而 IBM 本身已经在 2 纳米节点上进行了多年的研究。这意味着，除了基本的架构创新外，NorthPole 还可以在几代芯片处理技术上实现，从而不断提高效率和性能。 
NorthPole大部分成员合照，中为Modha但对 Modha 来说，这只是他过去 19 年职业生涯中的一个重要里程碑。在这段时间里，他一直在研究受大脑启发的数字芯片，他知道大脑是我们已知的最节能的处理器，并一直在寻找以数字方式复制大脑的方法。TrueNorth 的灵感完全来源于大脑中的神经元结构--其中的数字 ""突触 ""就像蜜蜂的大脑一样多。但 2015 年， Modha 坐在旧金山一个公园的长椅上，说他正在思考自己迄今为止的工作。他相信，将传统处理设备的精华与大脑的处理结构结合在一起，一定会有所收获，因为大脑中的记忆和处理都是穿插进行的。莫德哈认为，答案就是 ""大脑启发的计算，但具有硅片的速度""。在接下来的八年里， Modha 和他的同事们一心一意、密不透风地致力于将这一愿景变为现实。他们在阿尔马登默默无闻地工作着，直到今年，他们才就自己的工作发表过任何演讲或论文。每个人都带来了不同的技能和观点，但每个人都通力合作，因此团队的整体贡献远远大于各部分的总和。现在，我们的计划是展示 NorthPole 的能力，同时探索如何将设计转化为更小的芯片生产流程，并进一步探索架构的可能性。这项工作源于一个简单的想法--我们如何才能制造出像大脑一样工作的计算机--经过多年的基础研究，我们终于找到了答案。如今，只有在 IBM 研究院这样的地方才有可能实现这一目标，因为这里有足够的时间和空间来探索计算领域的重大问题，以及这些问题会将我们带向何方。""NorthPole是大脑在硅片镜面上的微弱呈现。"
"ibm类脑计算机,IBM首度公开仿人脑计算机",2021-07-21,"原标题：IBM首度公开仿人脑计算机 

 今日的计算机运算能力很强，即便是简单的计算，每秒钟也能跑数十亿次。IBM近日公布最新的TrueNorth仿人脑芯片，只有几克重、邮票大小，却内置了100 万个模拟神经元以及2.56亿模拟神经“突触”(Synapses)，能力超强相当于一台超级电脑。尽管如此，计算机技术仍无法和人类大脑相比。 
 据美国科技网站Digital Trends 8月19日报导，IBM于8月17日公布最新的TrueNorth仿人脑芯片，它希望可以像人脑一样，能模拟图像，进而采取行动并记忆。就像当人类看到球向自己扑近时，伸手接球，是个非常简单的反射动作。但是，对电脑而言，就必须要有一套非常复杂的程序。 
 IBM自2008年便着手进行TrueNorth芯片计划，希望能发展出以人脑神经元与突触网络(一种自适应可扩展塑性电子神经形态系统)为基础的新一代电脑架构，而非只是专注于简单的4核心或8核心的运算。TrueNorth芯片内置4,096颗核心，数据处理与运算速度更快速，且有效率。 
 TrueNorth芯片内的核心和一般家用电脑的处理器不同，TrueNorth的核心仅在需要时运作，所以功耗很低。所有的核心在沟通时是以事件为导向，也就是说系统可以上下调整、核心可以停止运转，却不受干扰而中断任务。TrueNorth电脑的空间不到2升，功耗只有一千瓦。 
 即使科学家将更多的内核置入处理器，并且设计新的储存形式，计算机技术仍无法和人类大脑相比。人脑约有100亿神经元与100兆突触，相当于96座位于加州的劳伦斯利佛摩国家实验室(Lawrence Livermore National Lab)的“红杉”(Sequioa)超级电脑。除此之外，这些超级电脑模拟运算的速度比人脑还要慢1,500倍。返回搜狐，查看更多 
 责任编辑："
详解两类AI芯片架构和关键技术,2020-01-07,"戳蓝字“CSDN云计算”关注我们哦！ 

 作者 | 架构师技术联盟 
 责编 | 阿秃 
 人工智能芯片目前有两种发展路径：一种是延续传统计算架构，加速硬件计算能力，主要以 3 种类型的芯片为代表，即 GPU、 FPGA、 ASIC，但CPU依旧发挥着不可替代的作用；另一种是颠覆型的经典的冯·诺依曼计算架构，采用类脑神经结构来提升计算能力，这类以IBM TrueNorth 芯片为代表。 
 1、传统 CPU 
 计算机工业从1960年代早期开始使用CPU这个术语。迄今为止，CPU从形态、设计到实现都已发生了巨大的变化，但是其基本工作原理却一直没有大的改变。通常 CPU 由控制器和运算器这两个主要部件组成。传统的 CPU 内部结构图如图所示： 

 传统CPU内部结构图(ALU计算模块) 
 从图中我们可以看到：实质上仅单独的ALU模块(逻辑运算单元)是用来完成数据计算的，其他各个模块的存在都是为了保证指令能够一条接一条的有序执行。这种通用性结构对于传统的编程计算模式非常适合，同时可以通过提升CPU主频(提升单位时间内执行指令的条数)来提升计算速度。  
 但对于深度学习中的并不需要太多的程序指令、 却需要海量数据运算的计算需求， 这种结构就显得有些力不从心。尤其是在功耗限制下， 无法通过无限制的提升 CPU 和内存的工作频率来加快指令执行速度， 这种情况导致 CPU 系统的发展遇到不可逾越的瓶颈。 
 2、并行加速计算的GPU 
 GPU 作为最早从事并行加速计算的处理器，相比 CPU 速度快， 同时比其他加速器芯片编程灵活简单。 
 传统的 CPU 之所以不适合人工智能算法的执行，主要原因在于其计算指令遵循串行执行的方式，没能发挥出芯片的全部潜力。与之不同的是， GPU 具有高并行结构，在处理图形数据和复杂算法方面拥有比 CPU 更高的效率。对比 GPU 和 CPU 在结构上的差异， CPU大部分面积为控制器和寄存器，而 GPU 拥有更ALU(逻辑运算单元)用于数据处理，这样的结构适合对密集型数据进行并行处理， CPU 与 GPU 的结构对比如图 所示。 

 CPU及GPU结构对比图 
 程序在 GPU系统上的运行速度相较于单核 CPU往往提升几十倍乃至上千倍。随着英伟达、 AMD 等公司不断推进其对 GPU 大规模并行架构的支持，面向通用计算的 GPU(即GPGPU，通用计算图形处理器)已成为加速可并行应用程序的重要手段，GPU 的发展历程可分为 3 个阶段： 
 第一代GPU(1999年以前)，部分功能从CPU分离 ， 实现硬件加速 ， 以GE(GEOMETRY ENGINE)为代表，只能起到 3D 图像处理的加速作用，不具有软件编程特性。 
 第二代 GPU(1999-2005 年)， 实现进一步的硬件加速和有限的编程性。1999年，英伟达发布了“专为执行复杂的数学和几何计算的” GeForce256 图像处理芯片，将更多的晶体管用作执行单元， 而不是像 CPU 那样用作复杂的控制单元和缓存，将(TRANSFORM AND LIGHTING) 等功能从 CPU 分离出来，实现了快速变换，这成为 GPU 真正出现的标志。 
 之后几年， GPU 技术快速发展，运算速度迅速超过 CPU。2001年英伟达和ATI 分别推出的GEFORCE3和RADEON 8500，图形硬件的流水线被定义为流处理器，出现了顶点级可编程性，同时像素级也具有有限的编程性，但 GPU 的整体编程性仍然比较有限。 
 第三代 GPU(2006年以后)， GPU实现方便的编程环境创建， 可以直接编写程序。2006年英伟达与ATI分别推出了CUDA (Compute United Device Architecture，计算统一设备架构)编程环境和CTM(CLOSE TO THE METAL)编程环境， 使得 GPU 打破图形语言的局限成为真正的并行数据处理超级加速器。 
 2008年，苹果公司提出一个通用的并行计算编程平台 OPENCL(开放运算语言)，与CUDA绑定在英伟达的显卡上不同，OPENCL 和具体的计算设备无关。 

 GPU芯片的发展阶段 
 目前， GPU 已经发展到较为成熟的阶段。谷歌、 FACEBOOK、微软、 Twtter和百度等公司都在使用GPU 分析图片、视频和音频文件，以改进搜索和图像标签等应用功能。此外，很多汽车生产商也在使用GPU芯片发展无人驾驶。不仅如此， GPU也被应用于VR/AR 相关的产业。 
 但是 GPU也有一定的局限性。深度学习算法分为训练和推断两部分， GPU 平台在算法训练上非常高效。但在推断中对于单项输入进行处理的时候，并行计算的优势不能完全发挥出来。 
 3、半定制化的FPGA 
 FPGA 是在 PAL、 GAL、 CPLD 等可编程器件基础上进一步发展的产物。用户可以通过烧入 FPGA 配置文件来定义这些门电路以及存储器之间的连线。这种烧入不是一次性的，比如用户可以把 FPGA 配置成一个微控制器 MCU，使用完毕后可以编辑配置文件把同一个FPGA 配置成一个音频编解码器。因此， 它既解决了定制电路灵活性的不足，又克服了原有可编程器件门电路数有限的缺点。 
 FPGA可同时进行数据并行和任务并行计算，在处理特定应用时有更加明显的效率提升。对于某个特定运算，通用 CPU可能需要多个时钟周期，而 FPGA 可以通过编程重组电路，直接生成专用电路，仅消耗少量甚至一次时钟周期就可完成运算。 
 此外，由于 FPGA的灵活性，很多使用通用处理器或 ASIC难以实现的底层硬件控制操作技术， 利用 FPGA 可以很方便的实现。这个特性为算法的功能实现和优化留出了更大空间。同时FPGA 一次性成本(光刻掩模制作成本)远低于ASIC，在芯片需求还未成规模、深度学习算法暂未稳定， 需要不断迭代改进的情况下，利用 FPGA 芯片具备可重构的特性来实现半定制的人工智能芯片是最佳选择之一。 
 功耗方面，从体系结构而言， FPGA 也具有天生的优势。传统的冯氏结构中，执行单元(如 CPU 核)执行任意指令，都需要有指令存储器、译码器、各种指令的运算器及分支跳转处理逻辑参与运行， 而FPGA每个逻辑单元的功能在重编程(即烧入)时就已经确定，不需要指令，无需共享内存，从而可以极大的降低单位执行的功耗，提高整体的能耗比。 
 由于 FPGA 具备灵活快速的特点， 因此在众多领域都有替代ASIC 的趋势。FPGA 在人工智能领域的应用如图所示。 

 FPGA 在人工智能领域的应用 
 4、全定制化的ASIC 
 目前以深度学习为代表的人工智能计算需求，主要采用GPU、FPGA等已有的适合并行计算的通用芯片来实现加速。在产业应用没有大规模兴起之时，使用这类已有的通用芯片可以避免专门研发定制芯片(ASIC)的高投入和高风险。但是，由于这类通用芯片设计初衷并非专门针对深度学习，因而天然存在性能、 功耗等方面的局限性。随着人工智能应用规模的扩大，这类问题日益突显。 
 GPU作为图像处理器， 设计初衷是为了应对图像处理中的大规模并行计算。因此，在应用于深度学习算法时，有三个方面的局限性： 
 第一：应用过程中无法充分发挥并行计算优势。深度学习包含训练和推断两个计算环节， GPU 在深度学习算法训练上非常高效， 但对于单一输入进行推断的场合， 并行度的优势不能完全发挥。 第二：无法灵活配置硬件结构。GPU 采用 SIMT 计算模式， 硬件结构相对固定。目前深度学习算法还未完全稳定，若深度学习算法发生大的变化， GPU 无法像 FPGA 一样可以灵活的配制硬件结构。 第三：运行深度学习算法能效低于FPGA。 
 尽管 FPGA 倍受看好，甚至新一代百度大脑也是基于 FPGA 平台研发，但其毕竟不是专门为了适用深度学习算法而研发，实际应用中也存在诸多局限： 
 第一：基本单元的计算能力有限。为了实现可重构特性， FPGA 内部有大量极细粒度的基本单元，但是每个单元的计算能力(主要依靠 LUT 查找表)都远远低于 CPU 和 GPU 中的 ALU 模块。第二：计算资源占比相对较低。为实现可重构特性， FPGA 内部大量资源被用于可配置的片上路由与连线。第三：速度和功耗相对专用定制芯片(ASIC)仍然存在不小差距。第四，：FPGA 价格较为昂贵。在规模放量的情况下单块 FPGA 的成本要远高于专用定制芯片。 
 因此，随着人工智能算法和应用技术的日益发展，以及人工智能专用芯片 ASIC产业环境的逐渐成熟， 全定制化人工智能 ASIC也逐步体现出自身的优势，从事此类芯片研发与应用的国内外比较有代表性的公司如图所示。 

 人工智能专用芯片研发情况一览 
 深度学习算法稳定后， AI 芯片可采用ASIC设计方法进行全定制， 使性能、功耗和面积等指标面向深度学习算法做到最优。 
 5、类脑芯片 
 类脑芯片不采用经典的冯·诺依曼架构， 而是基于神经形态架构设计，以IBM Truenorth为代表。IBM 研究人员将存储单元作为突触、计算单元作为神经元、传输单元作为轴突搭建了神经芯片的原型。 
 目前， Truenorth用三星 28nm功耗工艺技术，由 54亿个晶体管组成的芯片构成的片上网络有4096个神经突触核心，实时作业功耗仅为70mW。由于神经突触要求权重可变且要有记忆功能， IBM采用与CMOS工艺兼容的相变非易失存储器(PCM)的技术实验性的实现了新型突触，加快了商业化进程。 

来源：清华2018人工智能芯片研究报告 

福利 扫描添加小编微信，备注“姓名+公司职位”，加入【云计算学习交流群】，和志同道合的朋友们共同打卡学习！推荐阅读： 
 详解CPU几个重点基础知识爆料：美国宣布限制AI软件出口中国；小米回应设备被谷歌禁用：已修复 Bug，适时上线；Imagination欲重夺GPU市场……腾讯内部鄙视链！！！“一百万行Python代码对任何人都足够了”达摩院 2020 预测：工业互联网超融合来袭！万字长文回望2019：影响区块链行业发展的9大事件 
 真香，朕在看了！"
IBM 研究出一种突破冯·诺依曼瓶颈的芯片,2023-11-10,"导读IBM 的 NorthPole 处理器无需访问外部存储器，从而提高了计算能力并节省了能源。NorthPole 芯片将内存和处理功能结合在一起，从而极大地改进了图像识别和其他计算任务。 
 
 
（图片来源：IBM Corp.） 
加州圣何塞 IBM 的研究人员开发了一种受大脑启发的计算机芯片，可以通过以更少的功耗更快地工作来增强人工智能 (AI)。他们的 NorthPole 处理器芯片消除了频繁访问外部存储器的需要，因此执行图像识别等任务的速度比现有架构更快，同时消耗的电量大大减少。 
“它的能源效率令人惊叹”，巴黎萨克雷大学帕莱索分校的纳米电子学研究员达米安·奎利奥兹 (Damien Querlioz) 说道。他说，这项发表在Science 1上的研究表明，计算和内存可以大规模集成。“我觉得这篇论文将动摇计算机架构中的普遍思维。” 
NorthPole 运行神经网络：简单计算单元的多层阵列，经过编程可识别数据模式。底层接收数据，例如图像中的像素；每个连续层都会检测复杂性不断增加的模式，并将信息传递到下一层。顶层产生一个输出，例如，可以表示图像包含猫、汽车或其他物体的可能性。 
因瓶颈而减慢 
一些计算机芯片可以有效地处理这些计算，但每次计算一层时它们仍然需要使用称为 RAM 的外部存储器。以这种方式在芯片之间传输数据会减慢速度——这种现象被称为冯·诺依曼瓶颈，以数学家约翰·冯·诺依曼命名，他首先构想了基于处理单元和单独内存单元的计算机标准架构。 
冯·诺依曼瓶颈是减缓计算机应用程序（包括人工智能）速度的最重要因素之一。它还导致能源效率低下。研究合著者、IBM 计算机工程师 Dharmendra Modha 表示，他曾经估计在这种架构上模拟人脑可能需要相当于 12 个核反应堆的输出。 
NorthPole 由 256 个计算单元或核心组成，每个计算单元都包含自己的内存。“你正在缓解核心内的冯·诺依曼瓶颈，”Modha 说道，他是 IBM 位于圣何塞的 Almaden 研究中心的类脑计算首席科学家。 
莫达说，这些核心在一个网络中连接在一起，其灵感来自于人类大脑皮层各部分之间的白质连接。这一设计原则和其他设计原则（其中大部分以前就存在，但从未被整合到一个芯片中）使 NorthPole 在图像识别的标准基准测试中以大幅优势击败现有的人工智能机器。尽管没有使用最新、最小型化的制造工艺，但它使用的能量仅为最先进人工智能芯片的五分之一。作者估计，如果 NorthPole 设计采用最新的制造工艺，其效率将比当前设计提高 25 倍。 
在正确的道路上 
但即使是 NorthPole 的 224 MB RAM 也不足以支持大型语言模型，例如聊天机器人 ChatGPT 使用的语言模型，即使在最精简的版本中也会占用数千兆字节的数据。而且该芯片只能运行预编程的神经网络，这些神经网络需要在单独的机器上提前“训练”。但该论文的作者表示，NorthPole 架构在自动驾驶汽车等速度关键型应用中可能很有用。 
NorthPole 使内存单元在物理上尽可能靠近核心中的计算元件。在其他地方，研究人员一直在利用新材料和制造工艺开发更激进的创新。这些使存储单元本身能够执行计算，原则上可以进一步提高速度和效率。 
上个月描述的另一种芯片使用忆阻器进行内存计算，忆阻器是能够在电阻器和导体之间切换的电路元件。“IBM 和我们的这两种方法都有望减少延迟并降低与数据传输相关的能源成本，”忆阻器研究的共同作者、北京清华大学的高斌说道。 
另一种方法是由多个团队（包括位于瑞士苏黎世的独立 IBM 实验室）开发的，通过改变电路元件的晶体结构来存储信息。这些新方法能否在经济上得到推广还有待观察。 
 
 本文地址：https://www.linuxprobe.com/?p=286460"
初创公司即融资上亿，这个“人造超级大脑”赛道为什么不是噱头？,2022-08-19,"杨净 丰色 发自 凹非寺量子位 | 公众号 QbitAI 
 这不前阵子，马斯克扬言已将大脑上传到云端，并与虚拟版本进行交谈。 
 关于人造大脑这事儿，再次引发了热议： 
 人类是否能构建跟人脑一样的机器脑？ 

 事实上这个问题，不光是理念，更已经是一种实践方向——归属于类脑计算的范畴。作为下一代人工智能的“种子选手”，它有望打破传统冯诺伊曼架构，引领新的计算变革。 
 不过发展至今，类脑计算始终呈现出正负两极的评价。 
 一面是业内如火如荼的融资进展。据相关机构预测，2035年类脑计算的市场规模约200亿美元。另一面则是脑机制研究不深入、没法复刻出相仿的神经网络等质疑。 
 到底是口耳相传的噱头，还是实打实的硬科技突破？ 
 借着这一契机来盘一盘类脑计算到底什么来头？ 
 什么是类脑计算 
 与人工智能、机器学习类似，类脑计算目前没有明确的定义。以至于有关它的英文表达，也不止一种： 

Brain-like Computing（仿脑计算）；Brain-inspired Computing（脑启发计算）；Neuromorphic Computing（神经形态计算）…… 

 不过字面拆解来看，类脑计算就是借鉴生物大脑的信息处理机制，以此诞生的一种新型计算形态。 
 与现有计算机相比，生物大脑（以人脑代表）有诸多优势。中科院院士、浙大校长吴朝辉曾撰文，主要有以下几点： 
 功耗低，仅20瓦左右；容错性强，即便少部分神经元死亡，对整体功能影响不大；并行处理信息；神经网络可塑性好，可根据环境变化自主学习和进化。 
 而以神经科学为导向、以大脑为模仿对象的类脑计算，既保留计算机本身优势，又叠加了大脑处理机制的buff，比如低功耗、自主决策学习、并行处理等特点，自然成为引领新一代计算变革的种子选手。 
 近年来，人工智能，尤其是深度学习取得了令人瞩目的成果，在某些方面的表现甚至已经超越了人类。 
 但与自然智能相比，深度学习在效率、功耗以及通用性上仍有一定的局限性，远没有达到真正意义上的智能程度。 
 类脑计算另辟蹊径，于是就成为科学家们的研究重点。 
 但想要实现真正的类脑并非那么容易，即便上世纪末科学家们就已经开始探索。清华大学集成电路学院何虎教授将其形容为珠峰。谁也不清楚，哪一条路会攀上顶峰。 
 目前，类脑计算大体可分成三种探索方向： 

 模拟神经元结构和功能，简单来说就是仿真真实大脑机理，进而探索大脑内部的“运作模式”。 
 最新代表性进展来自北京智源人工智能研究院给出的“智能线虫”——天宝1.0。 

 它完整模拟出秀丽隐杆线虫的神经系统——302个神经元，以及数千个连接，并为它构造了3D流体仿真环境。它可以在其中蠕动前行，并具备简单趋利避害的能力。 
 不过这种逆向工程——从生物体环境提取出抽象的数字模型，存在一定的局限性。 
 一言以蔽之，就是生物大脑本身的复杂度。 
 正如何虎教授所介绍：一方面，大脑环境过于复杂。抽象出的大脑模型，相当于只是简化版。另一方面，结构和功能之间“有壁”。即便成功构建了大脑结构，距离真正实现其功能还有很长的路要走。 
 这一路径目前还停留在学研阶段，在此就不进一步深入展开。 
 核心来看剩下两种路径：模拟神经网络以及开发新型电子设备。更通俗来讲，即软件算法层面，或硬件芯片层面上对大脑机制的模拟。 
 为了便于理解，将类脑计算与当下主流的深度学习作为对比。 

 先来看软件算法层面，生物神经元是以脉冲的形式将信息传递到下一个神经元层，放在类脑计算的研究中，即演化为脉冲神经网络SNN。 
 SNN，相较于DNN，更忠实地模拟大脑神经元和连接电路，其信息载体为脉冲序列，有空间域和时间域两个维度来传递信息，在中科院李国齐教授看来，SNN兼具生物合理性与计算高效性。 

 △ SNN与DNN（ANN）的区别，图源：智源社区《中科院李国齐：一文梳理类脑计算的前世今生》 
 进一步的，北京理工大学杨旭博士分享了类脑算法与传统算法模型之间的不同，核心有三个层面： 
 连接方式不同，稀疏连接与全连接；驱动方式不同：事件驱动与数据驱动；学习方式不同：DNN是从大量数据中总结出规律，而SNN则是因果学习，自适应能力强。 
 这也就导致SNN所表现出的功耗更低，效率更高以及自适应能力更强。 
 但与此同时，也不免有人质疑SNN的有效性。 
 因为关于SNN训练，目前还面临着诸多挑战，包括脉冲神经元中复杂的时空动力过程、脉冲信息不可导、脉冲退化和训练精度损失等，也就进一步导致当前尚未存在一种统一的、且公认有效的算法来训练它。 
 具体举个例子，如杨旭博士所说，比如由于SNN中的脉冲不可微分，DNN中非常成熟的梯度下降法就没法直接应用，但现在由于对大脑机制的理解还不够，就找不到一个能和该方法同样有效的训练方法。 
 处于同一境地的，还有类脑芯片。 
 目前也没有统一的技术方案（此处统一指代的是具有超低功耗的计算芯片）。 
 世界上最早的一款类脑芯片，当属于IBM于2011年研制出的两个具有感知认知能力的硅芯片原型。 
 随后像英特尔、斯坦福、曼大、浙大、清华也都相应推出自己的芯片方案。 

 2019年，第三代天机芯登上Nature封面，再度掀起对类脑芯片的热议。芯片搭载在无人驾驶自行车上，实现了自主决策、实时视觉探测、自动避障等功能。 

 除此之外，另外两种趋势也不容忽视。 
 一种是类脑感知芯片，也叫做神经形态传感器，即对类脑触觉、视觉、听觉等传感器的研究，开发具有高时间分辨率、低延时、低功耗的新型传感器，在机器人、物联网等方面有应用价值。 
 比如三星的动态视觉传感器（DVS），配在数码相机上就能捕捉2000帧的画面，只消耗300毫瓦的电能。 
 另一种则是材料的延伸，开发基于纳米等新材料的芯片，比如像忆阻器、相变存储器、电化学存储器。 
 可以感知到的是，兜兜转转近十年的类脑芯片，目前还市场标准还未统一，应用场景也多样。更多芯片方案还处于自我更新迭代当中。 
 算法如此，芯片如此，背后的核心原因其实也不难理解。 
 一方面是理论知识不够，受限于对大脑机制的了解；另一方面则是工程化难题，从理论落到实际。 
 也正因此，类脑计算相关的质疑始终不少。 
 甚至有人直言：噱头而已。 
 当前行业现状如何？ 
 是不是真的噱头，且来看当前的行业现状。 
 事实上，我们已经可以见到类脑计算商业化的身影。放眼全球，从2013年开始便有相关创企开始冒头，国内则集中爆发于2017-2018年。 
 据不完全统计，全球类脑企业公司已有20家左右，虽然融资轮次多集中于A轮，但各家公司拿到的融资金额少则千万多则上亿，甚至还出现了一家上市企业，来自法国的Brainchip。 
 跟更多前沿产业一样，有几家是直接从相关大学或研究所的类脑研究成果中孵化而来，Brainchip在内包括Innatera、时识科技、灵汐科技、优智创芯等。 

 △ 国外类脑企业代表 



 △ 国内类脑企业代表 
 从这些公司的技术路线上来看，主要有两条路径，恰好也是前面提到实现类脑智能的两种解决思路。 
 一是芯片优先，即在硬件层面上进行对大脑机制的模拟。目前大多数类脑企业都是这个思路。 
 以优先上市的Brainchip为例，他们研发出了世界上第一款商用神经拟态处理器Akida，面向边缘AI计算，去年10月开始量产。今年2月还与奔驰达成合作，用于座舱内的感知和识别。 

 最新融资约4000万元的荷兰企业Innatera，去年推出了一款基于SNN的神经拟态加速器，主要用于语音识别、生命体征监测和雷达等。 
 再比如专注于图像和视频领域的类脑企业PROPHESEE，通过模仿人眼和大脑的工作方式，开发出了一款类脑智能视觉处理器，能够帮助提高自动驾驶、工业自动化、物联网、安防以及AR/VR等领域的识别效率。 
 国内方面的代表，比如时识科技，其产品既包括可达到0.1mW的超低功耗计算芯片，还包括可用于面部检测、实时手势识别、实时目标分类等视觉任务的各类动态视觉类脑感知芯片。 
 做感知芯片的不算少，还包括专注类脑触觉芯片的他山科技（该芯片于去年9月流片），专注类脑嗅觉芯片的中科类脑（主要用于火灾预警等场景）等。 
 灵汐科技的重点是异构融合类脑计算芯片，该类芯片只需12W功耗即可提供32Tops的INT8算力和6Tflops的FP16算力。…… 
 这种以芯片优先的思路，最大好处是可以率先实现类脑的有效性，发挥它的低功耗优点。可以看到，目前这些产品已经大多落地于物联网、边缘计算等场景。 
 不过，这种思路也有它的局限性。我们知道，市面上的每一种产品实际上都是工程落地的问题。 
 但是在工程落地之前，要先把它最根本的物理原理理解清楚，变成算法，然后再去寻找最合适的工程方法，去做芯片，把它变成产品落地。 
 也就是说，芯片其实是为算法服务的。于是乎产业界出现了另一种声音： 

如果连一个有效的算法都没有，相关的硬件和硬件加速又从何谈起呢？ 

 这也恰好是第二种技术路线：以算法优先，然后再以算法定义芯片。 
 事实上，这种方式并不陌生，早在人工智能浪潮开始时，就有一波AI公司走的这条路径，比如旷视、地平线、商汤等。 
 因为用“算法定义硬件”，往往可以实现芯片性能的最大化。 
 像深度学习加速器，就是“算法定义硬件”的典型，当传统的芯片hold不住越来越快的新算法时，我们就通过优化算法来获得计算资源需求和内存需求更小的新模型，让芯片得以“适应”。 
 这种优势延伸到类脑领域，可以让开发出来的类脑算法运行在普通的芯片架构上，让传统芯片也能拥有此前不具备的能力。 
 因此，也有一些企业选择了这条路。 
 优智创芯，就是当前代表。 
 这家公司主要解决的是深度学习中的不可解释性问题，自研了基于SNN的可解释因果学习算法系统（CLAS Causal Learning Algorithm System）。 

 该系统下的因果学习算法最大的特点就是像人脑一样，在学习权值的调节过程中，会根据因果关系去决定权值该增加还是减少—— 
 从而做到并非单纯地去模仿数据，而是去理解数据产生背后的具体过程是什么样的。 
 当然，最后还需要利用强化学习去加强每个因果过程（即前后神经元之间的连接关系）。 
 在此，杨旭博士解释道，通过模仿数据找规律的方式就是现在ANN的工作方式，这种网络对数据样本质量的要求非常高，而后者，在SNN上采取因果学习的方法，就没有这种要求了，甚至可能只需小样本就可以做到智能通用。 
 “就像人类认猫认狗，我们只需要认识路边的几只就知道狗长什么样，不需要把全世界的都看一遍。” 
 对于因果学习的合理性，何虎教授则表示，我们这个世界本身就是一套因果系统，人类文明可以说就是靠着不断去问为什么而往前发展的。就像学生，要真正学会解一道数学题，靠不求甚解地背过程是不可能的，还是需要知道每一步都是如何推理出来，即每一步的因果关系。 
 那么因果学习系统能带来的最大好处是什么呢？ 
 是决策，何虎教授表示。 
 而优智创芯开发的这套因果学习算法一开始就瞄准的正是深度学习中的非完美信息决策问题（以自动驾驶为例，可能会出现的非完美信息就包括物体遮挡，道路交通标志不完整、不准确等情况）。 
 因此，针对该类问题的经典场景之一——打扑克，该公司实现了首个基于SNN的斗地主AI——“智玩”。 

 最终，“智玩”通过了107个人类个体样本不严谨图灵测试，拟人化程度超过80%，再经过人类个体样本训练，个性化程度达到了85%，胜率最高做到了49%，实现了“像人一样玩游戏”的目标。 
 除了“智玩”机器人，优智创芯还利用其自研的CLAS因果学习算法系统设计了类脑芯片。 
 其中，旗舰类脑芯片“思辨1号”对标SpiNNaker，采用28nm工艺，主频为2 GHz，支持RISC-V Vector 1.0指令集，同时支持AI加速（算力可达4TOPS）和类脑计算（SNN因果学习算法），单芯片同时最大可实现100万个神经元运算的同时，功耗不高于2W，性能可以与英特尔Loihi2媲美。 
 除此之外，优智创芯还构建出了基于CLAS因果学习算法系统和类脑芯片组成的整体解方案——“硅脑”全自主无人系统平台。 
 基于功耗小、成本低、具有可解释性以及可以自主灵活决策的特点，该平台聚焦在无人机、无人驾驶、机器人的应用，可以扩展到AIGC、元宇宙、脑科学研究等领域。 
 由该平台衍生出来的K50/K51型SFS全自主无人飞行系统（类脑计算盒子）直接挂载在无人机上即能够实现未知地域且离线状态下的全自主飞行任务，可以用于电力巡线、海岸、植被、轨道交通、矿山、消防等多场景全自主无人飞行巡查，也可用于军事领域的武器突防等。 
 以及衍生出来的C60型SDS全自主无人驾驶系统（类脑计算盒子），正在与多家车企合作验证，相信不久的将来，就会出现正真意义上的L4+级别无人驾驶汽车在城市中自由穿梭。 
 市场规模将达200亿美元 
 综上，我们可以看到，类脑计算并非“束之高阁”，而是已经走出实验室，开始了商业化的摸索。 
 据Yole Development预测，2035年类脑计算市场将占人工智能总收入的15%-20%，市场规模将达到 200亿美元。 
 虽然目前领域还处于发展的早期，面临着诸多待解难题，但已经显现出了势不可挡的趋势。我们认为理由有三。 
 首先，纵观人工智能发展的历史，从ANN到DNN，其实都是基于对大脑的模仿。 
 比如2016年击败围棋世界冠军李世石的AlphaGo，作为一个深度学习神经网络，它所利用的多层训练法就借鉴了一项认知科学的研究结果： 
 人们认识事物并不是通过直接分析，而是依靠一种逐层抽象的认知机制，即首先学习简单的概念，然后用它们去表示更抽象的。 

 △ 基于深度学习的图像识别过程中的逐层抽象过程 
 这种借鉴造就了AlphaGo的成功。 
 当然，诸如AlphaGo此类DNN都还是对大脑功能相对简单和抽象的模仿，存在着各种局限性。 
 第三代神经网络SNN由此诞生，除了神经元和突触状态之外，SNN还将时间概念纳入其中，实现了更高级的大脑生物神经模拟水平，有望打破现有的神经网络在功耗、算力、样本数量和质量等方面的限制。 
 因此，我们说，类脑计算不失一种顺势而为的科技发展趋势。 
 其次，要从当下最火热的通用人工智能（AGI）说起。 
 毫无疑问，现阶段的一些AI技术已经可以在某些特定任务上打败人类，但没法在所有技能上胜出。 
 这就像北京师范大学认知神经科学和学习国家重点实验室研究员万小红博士等所说，人工智能更专业，自然智能更通用。 
 更通用的强人工智能是AI发展的终极目标。就在一个多月之前，图灵奖得主LeCun公布的未来十年研究计划，就将AGI作为核心目标。 

 由于人类智能的核心是大脑，模拟大脑的类脑计算也就成为了实现AGI的一大重要路径。 
 最后，再将目光聚焦到当下，可以说，我们从未像今天这样需要新型计算机。 
 调查显示，全球每三四个月对于算力的需求就会翻一倍，这个增长速度已经远超摩尔定律和Dennard缩放定律。 
 但传统冯·诺伊曼计算架构存算分立的设计，让处理器即使再快也要等内存，算力根本无法得到提高。 
 作为新型计算形态的一种，类脑计算芯片有望打破这一僵局。 
 此外，值得一提的是，虽说目前人类对大脑的研究还远不够透彻，但北京理工大学杨旭博士和北京师范大学万小红博士——两位一个来自计算机科学领域，一个来自认知神经科学，都一致认为： 

这并不会真正妨碍类脑计算向前发展。 

 相反，他们都表示，AI技术的发展反过来还可以促进脑科学的研究，两者其实是相互成就的关系。 
 那么，等到真正的类脑时代来临之时，它将会和传统的人工智能技术并存，还是完全取代后者？又将会给人类社会带来怎样的变革？ 
 这无疑充满了想象的空间。我们拭目以待。 
 最后，结尾再抛给大家两个开放问题： 
 1、你认为类脑智能是否会产生意识？ 
 2、类脑智能是否会像生物大脑一样也会产生遗忘？ 
 欢迎讨论。 
 参考文献：[1]https://news.sciencenet.cn/htmlnews/2022/1/472375.shtm[2]https://www.sohu.com/a/424817554_129720[3]https://www.sgpjbg.com/info/25374.html[4]https://www.ahpst.net.cn/News/show/18405.html[5]https://s3.i-micronews.com/uploads/2021/05/YINTR21214-Neuromorphic-Computing-and-Sensing-2021-Flyer.pdf[6]张臣雄 .《AI芯片：前沿技术与创新未来》"
IBM制造类人脑的计算机,2017-04-03,"1、IBM制造类人脑的计算机 
据国外媒体报道，IBM近日宣布，IBM将与四所大学合作制造一种类似人类大脑的计算机。这四所大学是卡内基梅隆大学、麻省理工学院、纽约大学和伦斯勒理工学院。IBM把这个研究项目与“大数据”联系在了一起。大数据就是以新的方式使用计算机处理大量的结构化的和非结构化的数据以便使大数据更容易访问和更有用。 
 
IBM的这个研究项目的内容包括应用程序如何提高组决策过程、如何将处理器能力和算法用于人工智能、应该如何设计计算机系统以进行更自然的互动、以及深入学习如何在科学中影响自动化的方式识别。 
其实早在今年五月的时候IBM就用“红杉”模拟5300亿个人类的神经元。但结果令人大跌眼镜。与真正的人脑相比，“红杉”完全处于下风。目前，在全球超算排名中，“泰坦”居首位，而“红杉”排名第二，它的运算速度高达16000万亿次浮点运算/秒。 
 
美国斯坦福大学教授、“硅脑”研究实验室主任Kwabena Boahen表示，大脑的运算能力比超级计算机强，并不是说算得更快。也就是说人类的大脑能够处理更为复杂的信息，而在计算速度方面，相较于超级计算机，人脑无法企及。大脑的强项在于大规模的并行化，即所有神经网络可以同时解决多个问题。而现有的计算机平台，无论速度多快，都是顺序操作的，每一步只能解决一个问题。 
 IBM“红杉”超级计算机 
Boahen教授的研究方向就是用硬件和软件来复制大脑的计算能力。他的实验室最近取得的研究成果被称为 Neurogrid(神经网格)，可以模拟100 万个神经元的活动。Neurogrid并不是一台超级计算机，它不能模拟宇宙大爆炸，不能预报飓风，也不能预测流行病，但是它能做到的，却能让它与众不同。 
Neurogrid是首个能实时模拟100万个神经元的模拟平台，换句话说，它模拟人脑1秒钟的活动，仅需要1秒钟，相比之下，“红杉”模拟人脑一秒钟的活动需要1500秒。这使Neurogrid成为研究人类大脑的有力工具，除了了解大脑的正常活动，它还可以研究大脑疾病，例如自闭症、精神分裂等，这些疾病用现有的传统技术都很难模拟。 
Neurogrid共有16块芯片，每块芯片上都有超过 65000个“硅神经元”，这些神经元可以通过80个参数进行编程，以模拟不同类型的神经元。真正让它与传统计算系统区别开来的地方，在于它进行计算和通信的方式。大部分计算机都是通过数字信号来运行的，而大脑神经元活动的推动方式，却更类似于模拟信号。于是Neurogrid在计算时采用模拟信号，通信时采用数字信号，这样的方式更接近大脑。 
除了运算上的优势，Neurogrid在能耗上也遥遥领先。“红杉”的功耗高达8兆瓦，相当于160000个家庭的用电量，每年的电费耗资700万美元。而 Neurogrid功耗为5瓦，仅相当于一个手机充电器。这意味着，Neurogrid这种类型的模拟平台仍然有着巨大的潜力。也许不久后的一天，人们通过它就能真正知晓大脑的奥秘。 >> 
热门产品：三星ES8000 创维E700S 夏普LX640A 海信XT770 惠普2520hc 佳能MP288 TP-Link TL-WR847N JCG JHR-N835R 华为T8950HTC One SC 神舟精盾K790 神舟精盾Q480 
 
2、模拟大脑是目前研究的方向  
人工智能是否真的可以达到人机互恋的程度暂且不提，至少现在模拟大脑是目前CPU研究的一个方向。关于人工智能的研究，以前人机大战已经让世界领略到Waston的智能水平，按照目前不断发展的进度，相信IBM蓝色基因计划将于2019年左右完全模拟人类电脑，届时IBM将用约88万个CPU完全模拟人类大脑。 
 IBM蓝色基因模拟人脑 
届时，各种在科幻小说电影里面的机器也能谈恋爱的场景是否会变为现实，值得拭目以待，Siri也只能算是小儿科产品了。Siri只是依靠强大的数据中心和语音识别配以实时搜索功能，甚至有笑话“印度云”秒速Siri背后是一大堆苹果员工在回复iPhone用户。而IBM蓝色基因计划将是让机器用与人类一样的思考方式运作，而且拥有自我学习和创作能力。 
 认知计算机技术 
早在2008年11月，IBM就传出消息，IBM公司将领导一项由政府资助的联合项目，该研制计划可以模拟人类大脑的电子线路。该研究领域的主要一个部分是“认知计算机技术”，研究将有赖神经生物学家、计算机和材料科学家以及心理学家的通力合作。 
领导该项目的IBM科学家达门德·莫哈说：“大脑具有一种惊人的将跨意识的多重含义信息整合能力，它可以毫不费力的创建时间、空间和物体的种类，以及得出感官数据的相互关系。大脑可以完成各种无与伦比的技艺，令现在的计算机望尘莫及。” 
“认知计算机技术”隶属于IBM认知计算（Cognitive Cumputing）研究项目，通过内存模仿突触、通信模仿轴突、计算模仿神经元的方式，IBM能够让这种芯片模仿人类的大脑工作。John E.Kelly表示通过这种仿生学芯片IBM的仿生学计算机在未来能够模拟20亿个人脑的神经元并实现对认知计算的初步完成，这种仿生学计算机将会是计算技术未来发展的一个重要方向。 
 IBM认知计算（Cognitive Cumputing）研究项目 
到2009年，关于模拟人类大脑的蓝色基因计划正式公布，并在2009年获得美国国家科技创新奖章，当时IBM用147456个Power系列处理器模拟一只猫的大脑。>> 
 
3、IBM建立猫的大脑皮层模拟机构 
 IBM的研究人员建立了猫的大脑皮层模拟机构，其中包括10亿脑细胞和10兆认知神经元突触，信息的传递和连接就在这些神经元中间进行。这具猫脑数字模型的运行速度目前只相当于真猫大脑实际运行速度的1/100。 
 蓝色基因模拟超新星 
除了模拟人类大脑外，2010年4月IBM还用蓝色基因超级电脑模拟出了超新星爆发的极端物理过程。 
 蓝色基因 
IBM研究主管John E.Kelly在墨尔本大学接受采访时表示，IBM目前正在研发的新型仿生芯片，可以模仿人类大脑的运算并能够实现学习和记忆，同时可以触类旁通并实现对知识的创造，这种具有创新能力的设计将会让电脑拥有自我学习和创造的能力。 
 IBM人类仿生芯片 
随着技术的发展与进步，模拟猫脑需要CPU数量已经从当时的147456个缩减为24576个。而147456个Power系列处理器目前已经可以模拟出4.5%的人类大脑。完全模拟人类大脑需要88万个CPU，按照目前的研究进度，蓝色基因计划最快可与8年内完成，即2019年完全模拟出人类大脑。>> 
 
4、人脑+计算机的智能组合  
一些科学家认为，正如宇宙学上存在着一个让所有物理定律都失效的“奇点”一样，信息技术也正朝着“超人类智能”的奇点迈进。计算机科学家雷蒙德·库兹韦尔相信，这个信息奇点即将到来，那时，人工智能将超越人脑，人类的意义将彻底改变；那时人将“不人”，而是与机器融合，成为“超级人类”。 
 机器人时代 
超级人类是否意味着不朽？人是否会与自己制造的机器融为一体，或将自己分拆在机器中上市？其实早在1964年，传媒大师麦克卢汉就在《机器的新娘》中隐喻：人类只是未来机器的性器官(负责生产新机器人)。机器是人的延伸，反之，人也是机器的延伸。 电脑的计算速度正变得越来越快，如果有一天，电脑的运算速度快得令人难以置信，它们掌握人工智能的时刻也会到来，那时电脑将能够模拟人脑、产生意识：人类本身将发生改变。在信息学上，这个时间点也被称为“奇点”。 
 人脑+计算机的智能组合 
一些科学家认为，正如天体物理学上存在着一个让所有物理定律都失效的“奇点”(Singularity)一样，信息技术也正朝着“超人类智能”的“奇点”迈进。《时代》杂志文章指出，发明家、计算机科学家雷蒙德·库兹韦尔相信，信息技术的奇点将在2045年到来，届时，人工智能将超越人脑，人类的意义彻底改变，与机器融合为“超人类”，并借助科技的发展而获得“永生”。 当人工智能超越人类智慧时，人类的身体、思维乃至人类文明都将发生彻底且不可逆转的改变。库兹韦尔相信，这一刻不但无法避免，而且还迫在眉睫。根据他的推算，大约34年之后，人类与机器融合，人类文明即将终结。 
IBM称，需要进行额外的研究以找出支持这些计算模式的系统和流程。这种计算模式将允许在不同的技术领域的系统人员和系统一起工作。IBM希望这种认知计算机（cognitivecomputers）将能够处理自然语言和非结构化数据，像人类一样通过经验进行学习。 
IBM表示，计算机不会取代人类，但是，计算机将作为一个决策支持系统帮助人类作出决策，无论人们在健康医疗、金融或者客户服务等哪一个行业。 
 
 
原文发布时间为：2013-11-17 
 
本文来自云栖社区合作伙伴“大数据文摘”，了解相关信息可以关注“BigDataDigest”微信公众号"
它是AI的终极答案吗？类脑芯片简史,2019-10-31,"今年8月，有个消息轰动了中国科技界，尤其是AI圈。由中国科研团队研发的“天机”芯片登上了《自然》杂志封面。相关文章展示了清华大学施路平团队研发的世界首款异构融合类脑芯片，它既可支持脉冲神经网络又支持人工神经网路，并且公布了利用“天机芯片”完成自行车自动驾驶的实验视频。 

 
 

 

 
 

 这件事给投资界、产业界的直接影响，是在近段时间“类脑芯片”和“类脑计算”相关的投融资、并购与创业公司突然多了起来。“类脑”相关的会议活动也突然增加。虽然说“类脑热”还远远谈不上，但这个领域的突然升温却是真实可见的。 

 如果我们把目光放得更远一点，类脑芯片确实在这几年有了大规模的爆发。各大学实验室以及科技巨头纷纷拿出了类脑芯片产品，也有不少专家学者认为，人工智能要经历简单人工智能、深度人工智能、通用人工智能三个阶段。而今天的深度学习代表了第二阶段的开始，类脑计算则是通用智能大门的钥匙。 

 事已至此，可能给大众的感觉是，类脑芯片已经是注定的未来，人类已经借由它找到了通向强人工智能的门径。 

 然而果真如此吗？类脑芯片是否就是AI的终极答案，今天还埋藏着太多不确定性。而想要客观认识类脑芯片的未来，我们可能必须要把时间倒回一些，先理解它的过去。 

 一段人类认识神经与大脑的过去。 
 
 

 
 

 
 
 神经行为学：AI之外的另一条路 
 

 
 

 
 
 从人类的大脑和智慧中，抽取提炼某种技术，是一件源远流长的工作。能不能让机械像人类一样识别、判断和思考，最终发展出了今天的AI。 

 而在另一项“兄弟研究”里，却一步步发展出了今天的类脑"
吊打IBM的芯片 中国研发出第一款类脑芯片,2017-07-05,"人工智能技术(AI)现在已是火得不要不要的,各种传感器,机器学习,外围设备五花八门,可是,其核心的数据处理硬件架构仍基于经典的冯·诺依曼结构。冯·诺依曼结构已有七八十年历史,这就好比已经直立行走的智人,仍长着一颗猩猩的脑袋。 
 这就出现了冯·诺依曼瓶颈,也就是说CPU再快,也要等内存,因为CPU和内存之间的性能差距越来越大。 
 冯·诺依曼结构 
 冯·诺依曼结构中,计算模块和存储单元是分离的,CPU在执行命令时必须先从存储单元中读取数据。每一项任务,如果有十个步骤,那么CPU会依次进行十次读取,执行,再读取,再执行。.. 这就造成了延时,以及大量功耗(80%)花费在了数据读取上,当然多核、多CPU或一些常用数据的就地存储会一定程度上缓解这些问题,但这种中心处理的架构会限制处理能力的进一步发展,好比诺大个北京城,如果所有的政治、经济、文化活动都集中在市中心,为了生活,人们必须穿城而过,任你路修到二十环,还是一个词,堵死。 
 学术界和工业界出现了向人类大脑学习体系结构的趋势,大脑的处理单元是神经元,内存就是突触。神经元和突触是物理相连的,所以每个神经元计算都是本地的,而且从全局来看神经元们是分布式在工作。 
 神经元和突触 
 前几年轰动全球的IBM的仿人脑芯片TrueNorth也模仿了人类大脑的神经元结构,它的计算效率和可扩展性都远超现在的计算机,并且宣称可用于手势识别、情绪识别、图像分类和对象追踪、实时语音识别等领域。 
 而硅谷密探今天采访的团队AI-CTX,他们的模型基于亿万个可以相互连接的仿生神经元,每个神经元都具有跟人脑神经元类似的电学特性与动态参数,具有简单的运算与存储功能,这些神经元像大脑神经元一样,通过脉冲相互沟通。 
 在现实层面,他们的每个芯片可以搭载百万个神经元,亿万个神经突触,可以自由扩展的芯片网络又如同大脑皮层的不同的区域,分别在不同的层级上, 可以并行的,同时的处理任务,从而指数型地提高了数据处理的效率。又因各个指令及相应数据都存储在同一芯片中,所有的计算资源不会因为等待存储访问而导致浪费,功耗也比现在基于冯·诺依曼硬件架构的处理器低两到三个数量级。 
 看看Alpha Go,下一场棋耗电几千瓦,而李世石,大脑只耗20瓦,所以虽然输了,但人类的尊严还算没有低到尘埃里。.. 
 李世石对阵Alpha Go 
 除了运算速度快,能耗低,团队还有一项核心的IP,就是一种特殊的布线方式,使各芯片之间的交流可以突破物理限制,无限的增加新的芯片于原有网络,这样,运算能力也可无限增加。而传统的冯·诺依曼结构,如果要增加CPU的数量,那么CPU之间的沟通是越发困难的,因为运算能力有其上限。 
 在处理AI领域数据上,AI-CTX团队基于仿生神经网络设计出芯片架构比之冯·诺依曼结构无疑是革命性的,那么市场上是否有用类似方案的对手呢? 
 与IBM TrueNorth芯片的对比,来自eetimes 
 CTO 乔宁很自信的说:“Intel,三星等正在研发的加速器类产品在实时处理动态数据速度上比我们慢,能耗还高一个数量级。而跟IBM的TrueNorth芯片相比,我们的模拟神经元具有更复杂的非线性参数,硅面积只有IBM的0.5%,能耗只有10%,基于我们的芯片架构,每个神经元能轻松建立多达8千个连接,而现有IBM Truenorth 芯片只有256个连接。而且我们第一代原型一出来就已经有订单了!” 
 看他谈吐间一派樯橹灰飞烟灭的架势,我只想说,大神,为国争光啊! 
 AI-CTX的CTO乔宁 
 当然,我们也要在这里指出,三星的加速器是做静态图像处理,TrueNorth是一个通用实验芯片,不专门针对任何应用,虽然已经有不少人在开发相应的芯片应用(识别,追踪,决策)。 
 AI-CTX的芯片不适合处理静态硬盘数据(静态图片及数据),但非常擅长于处理包含时间信息的数据,比如,动态的传感器数据,自然界的自然信号(温度,气压),人体信号(心跳EEG),网络数据,IoT,股票高速决策等数据的处理。 
 AI-CTX目前的主要切入的应用场景包括实时在线(always-on)的应用,比如低功耗的智能便携设备,移动设备,健康检测EEG,IoT设备,网络安全等。还有另一块是高速低功耗应用,比如自动驾驶,手势识别,声音识别等。更长远来看可以用于大数据处理和深度学习等领域。 
 AI-CTX团队在中国国家级创新创业大赛中,从两万个团队中杀出重围,勇夺团体第一。 
 AI-CTX的CEO刘洪杰在大赛现场 
 凭啥这么牛,看看团队就知道了,CEO刘洪杰是苏黎世联邦理工脉冲仿生视觉的美女博士;CTO乔宁是中科院微电子研究所博士,之后又在苏黎世联邦做芯片开发项目的领导人,十几年电路设计从未失败(嗯,东方不败);首席科学家Giacomo Indiveri教授是神经仿生计算方面的世界知名专家;联合创始人Kyan Eng也是有博士学位,多项专利的成功的连续创业者。哎,怪不得IBM的芯片都被吊打。.. 
 密探和AI-CTX CEO刘洪杰(中)、CTO乔宁(右) 
 乔宁博士说,他的梦想是造出有生命的芯片,那一刻,好像有道耶酥光闪过,尤瓦尔·赫拉利的预言在耳边轰然响起。.. 
 本文转自d1net（转载）"
科学家在类脑芯片上实现类似LSTM的功能，能效高1000倍,2022-06-06,"来源：机器学习研究组订阅 

格拉茨技术大学的计算机科学家在 Nature 子刊上发表的一篇论文表明，他们找到了一种在神经形态芯片上模拟 LSTM 的方案，可以让类脑神经形态芯片上的 AI 算法能效提高约 1000 倍。 

 随着智能手机的普及，手机游戏也越来越受欢迎。但视频游戏等程序会大量耗电耗能。与 GPU 等标准硬件相比，基于 spike 的神经形态芯片有望实现更节能的深度神经网络（DNN）。但这需要我们理解如何在基于 event 的稀疏触发机制（sparse firing regime）中模拟 DNN，否则神经形态芯片的节能优势就会丧失。 
 比如说，解决序列处理任务的 DNN 通常使用长 - 短期记忆单元（LSTM），这种单元很难模拟。现在有一项研究模拟了生物神经元，通过放慢每个脉冲后的超极化后电位（AHP）电流，提供了一种有效的解决方案。AHP 电流可以很容易地在支持多节段（multi-compartment）神经元模型的神经形态硬件（例如英特尔的 Loihi 芯片）上实现类似于 LSTM 的功能。 

 滤波器逼近理论能够解释为什么 AHP 神经元可以模拟长短期记忆网络的功能。这产生了一种高能效的时间序列分类方法，让类脑神经形态芯片上的 AI 算法能效提高约 1000 倍。此外，它为高效执行大型 DNN 提供了基础，以解决有关自然语言处理的问题。研究论文近期发表在《自然 · 机器智能》期刊上。 

 论文地址：https://www.nature.com/articles/s42256-022-00480-w 
 曼彻斯特大学的计算机科学家 Steve Furber 评价这项研究称：「这是一项令人印象深刻的工作，可能给复杂 AI 算法（例如语言翻译、自动驾驶）的性能带来巨大飞跃。」 
 研究动机 
 AI 程序通常擅长在数据集中找到特定的模式。例如，在图像识别中，AI 算法首先会找到图像清晰的边缘，然后在拼凑出整体图像时记住这些边缘以及图像的所有后续部分。 
 这种网络的一个共同组成部分是一个被称为 LSTM 的软件单元，它在事物随时间变化时保持一个元素（element）的记忆。例如，图像中的垂直边缘需要保留在内存中，因为软件会确定它是代表数字「4」的一部分还是汽车的门。典型的 AI 系统必须同时跟踪数百个 LSTM 元素。 
 当前在传统计算机芯片上运行的 LSTM 网络非常准确，但是非常耗电。为了处理信息比特，它们必须首先检索存储数据的各个比特，对其进行操作，然后再将它们送回存储，并一遍又一遍地重复这个过程。 
 英特尔、IBM 等芯片制造商一直在尝试一种新的芯片设计方式——神经形态芯片。这种芯片处理信息的方式就像大脑中的神经元网络，其中每个神经元接收来自网络中其他神经元的输入，并在总输入超过阈值时触发。 
 在这种新芯片中，一些相当于神经元的硬件被连接在一起形成网络。AI 程序也依赖于人造神经网络，但在传统计算机中，这些神经元完全由软件定义，需要来回访问存储。 
 这种神经形态芯片同时处理存储和计算，因此更加节能。但要利用这种架构，计算机科学家需要在新型芯片架构上重新研究如何运行 LSTM 等网络。 
 这正是来自格拉茨技术大学的计算机科学家 Wolfgang Maass 等研究者的工作重点。他和他的同事试图复刻人脑中的一种记忆存储机制，这种机制由生物神经网络执行，称为超极化后电位 (AHP) 电流。 
 AHP 神经元放电模式 
 大脑中的神经元在触发后通常会返回到其基线水平并保持静止，直到它再次接收到超过其阈值的输入而被触发。但在 AHP 网络中，神经元放电一次后，会暂时禁止再次放电，这有助于神经元网络在消耗更少能量的同时保留信息。 
 Maass 和他的同事将 AHP 神经元放电模式集成到他们的神经形态神经网络软件中，并通过两个标准的 AI 测试运行他们的网络。第一个挑战是让软件在分割成数百个独立像素的图像中识别手写数字「3」。在这个测试中，他们发现，当在英特尔的神经形态 Loihi 芯片上运行时，他们的算法比在传统芯片上运行的基于 LSTM 的图像识别算法的能效高 1000 倍。 
 在第二项测试中，研究人员给了该网络一个 20 个句子组成的故事，测试它对故事含义的理解。结果，该神经形态装置的效率是传统计算机处理器算法的 16 倍。 
 Maass 指出，第二次测试是在英特尔第一代 Loihi 芯片的 22 个系列上进行的，这些芯片在相互通信时消耗相对较大的能量。该公司已经推出了第二代 Loihi 芯片，每一个都有更多的神经元，他说这将减少 chip-to-chip 通信需求，从而使软件运行更高效。 
 目前，神经形态芯片的商业化案例还是凤毛麟角。因此，这项研究的大规模应用可能不会很快出现。但是艾伦研究所的计算神经科学家 Anton Arkhipov 说，先进的 AI 算法（如 Maass 所展示的算法）可以帮助这些芯片获得商业立足点。「至少，这将加速 AI 系统的构建。」 
 反过来，这又将加速新颖的 AI 应用的出现，如一个更加智能的 AI 数字助理，这个助理不仅可以提示照片中某个人物的名字，还能帮你回忆起你是在哪里认识的这个人，以及你们之间发生了什么故事。 
 Maass 说，通过整合大脑中的其他神经元放电模式，未来的神经形态装置甚至有一天可以开始探索众多神经元放电模式如何共同产生意识。 
 原文链接：https://www.science.org/content/article/microchips-mimic-human-brain-could-make-ai-far-more-energy-efficient 
 未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）大脑研究计划，构建互联网（城市）大脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。每日推荐范围未来科技发展趋势的学习型文章。目前线上平台已收藏上千篇精华前沿科技文章和报告。 
   如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”"
Nature子刊：科学家在类脑芯片上实现类似LSTM的功能，能效高1000倍,2022-05-23,"来源：机器学习研究组订阅 

格拉茨技术大学的计算机科学家在 Nature 子刊上发表的一篇论文表明，他们找到了一种在神经形态芯片上模拟 LSTM 的方案，可以让类脑神经形态芯片上的 AI 算法能效提高约 1000 倍。 

 随着智能手机的普及，手机游戏也越来越受欢迎。但视频游戏等程序会大量耗电耗能。与 GPU 等标准硬件相比，基于 spike 的神经形态芯片有望实现更节能的深度神经网络（DNN）。但这需要我们理解如何在基于 event 的稀疏触发机制（sparse firing regime）中模拟 DNN，否则神经形态芯片的节能优势就会丧失。 
 比如说，解决序列处理任务的 DNN 通常使用长 - 短期记忆单元（LSTM），这种单元很难模拟。现在有一项研究模拟了生物神经元，通过放慢每个脉冲后的超极化后电位（AHP）电流，提供了一种有效的解决方案。AHP 电流可以很容易地在支持多节段（multi-compartment）神经元模型的神经形态硬件（例如英特尔的 Loihi 芯片）上实现类似于 LSTM 的功能。 

 滤波器逼近理论能够解释为什么 AHP 神经元可以模拟长短期记忆网络的功能。这产生了一种高能效的时间序列分类方法，让类脑神经形态芯片上的 AI 算法能效提高约 1000 倍。此外，它为高效执行大型 DNN 提供了基础，以解决有关自然语言处理的问题。研究论文近期发表在《自然 · 机器智能》期刊上。 

 论文地址：https://www.nature.com/articles/s42256-022-00480-w 
 曼彻斯特大学的计算机科学家 Steve Furber 评价这项研究称：「这是一项令人印象深刻的工作，可能给复杂 AI 算法（例如语言翻译、自动驾驶）的性能带来巨大飞跃。」 
 研究动机 
 AI 程序通常擅长在数据集中找到特定的模式。例如，在图像识别中，AI 算法首先会找到图像清晰的边缘，然后在拼凑出整体图像时记住这些边缘以及图像的所有后续部分。 
 这种网络的一个共同组成部分是一个被称为 LSTM 的软件单元，它在事物随时间变化时保持一个元素（element）的记忆。例如，图像中的垂直边缘需要保留在内存中，因为软件会确定它是代表数字「4」的一部分还是汽车的门。典型的 AI 系统必须同时跟踪数百个 LSTM 元素。 
 当前在传统计算机芯片上运行的 LSTM 网络非常准确，但是非常耗电。为了处理信息比特，它们必须首先检索存储数据的各个比特，对其进行操作，然后再将它们送回存储，并一遍又一遍地重复这个过程。 
 英特尔、IBM 等芯片制造商一直在尝试一种新的芯片设计方式——神经形态芯片。这种芯片处理信息的方式就像大脑中的神经元网络，其中每个神经元接收来自网络中其他神经元的输入，并在总输入超过阈值时触发。 
 在这种新芯片中，一些相当于神经元的硬件被连接在一起形成网络。AI 程序也依赖于人造神经网络，但在传统计算机中，这些神经元完全由软件定义，需要来回访问存储。 
 这种神经形态芯片同时处理存储和计算，因此更加节能。但要利用这种架构，计算机科学家需要在新型芯片架构上重新研究如何运行 LSTM 等网络。 
 这正是来自格拉茨技术大学的计算机科学家 Wolfgang Maass 等研究者的工作重点。他和他的同事试图复刻人脑中的一种记忆存储机制，这种机制由生物神经网络执行，称为超极化后电位 (AHP) 电流。 
 AHP 神经元放电模式 
 大脑中的神经元在触发后通常会返回到其基线水平并保持静止，直到它再次接收到超过其阈值的输入而被触发。但在 AHP 网络中，神经元放电一次后，会暂时禁止再次放电，这有助于神经元网络在消耗更少能量的同时保留信息。 
 Maass 和他的同事将 AHP 神经元放电模式集成到他们的神经形态神经网络软件中，并通过两个标准的 AI 测试运行他们的网络。第一个挑战是让软件在分割成数百个独立像素的图像中识别手写数字「3」。在这个测试中，他们发现，当在英特尔的神经形态 Loihi 芯片上运行时，他们的算法比在传统芯片上运行的基于 LSTM 的图像识别算法的能效高 1000 倍。 
 在第二项测试中，研究人员给了该网络一个 20 个句子组成的故事，测试它对故事含义的理解。结果，该神经形态装置的效率是传统计算机处理器算法的 16 倍。 
 Maass 指出，第二次测试是在英特尔第一代 Loihi 芯片的 22 个系列上进行的，这些芯片在相互通信时消耗相对较大的能量。该公司已经推出了第二代 Loihi 芯片，每一个都有更多的神经元，他说这将减少 chip-to-chip 通信需求，从而使软件运行更高效。 
 目前，神经形态芯片的商业化案例还是凤毛麟角。因此，这项研究的大规模应用可能不会很快出现。但是艾伦研究所的计算神经科学家 Anton Arkhipov 说，先进的 AI 算法（如 Maass 所展示的算法）可以帮助这些芯片获得商业立足点。「至少，这将加速 AI 系统的构建。」 
 反过来，这又将加速新颖的 AI 应用的出现，如一个更加智能的 AI 数字助理，这个助理不仅可以提示照片中某个人物的名字，还能帮你回忆起你是在哪里认识的这个人，以及你们之间发生了什么故事。 
 Maass 说，通过整合大脑中的其他神经元放电模式，未来的神经形态装置甚至有一天可以开始探索众多神经元放电模式如何共同产生意识。 
 原文链接：https://www.science.org/content/article/microchips-mimic-human-brain-could-make-ai-far-more-energy-efficient 
 未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）大脑研究计划，构建互联网（城市）大脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。每日推荐范围未来科技发展趋势的学习型文章。目前线上平台已收藏上千篇精华前沿科技文章和报告。 
   如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”"
IBM 发布首个深度学习类脑超级计算平台 IBM TrueNorth,2017-08-02,"IBM TrueNorth。图片来源：federallabs.org 
 
IBM 日前发布了一款用于深度学习的类脑超级计算平台 IBM TrueNorth，其处理能力相当于 1600 万个神经元和 40 亿个神经键，消耗的能量只需 2.5 瓦。将低能耗的类脑处理器应用于深度学习无疑是未来大数据处理创新方法。 
 
劳伦斯·利弗莫尔国家实验室（LLNL）数据科学部门的副主任 Jim Brase 表示，类脑计算与未来高性能计算和模拟发展趋势一致，类脑计算的潜力和机器智能将会改变科研方式。 
 
IBM TrueNorth 的架构基于 IBM 此前发布的 TrueNorth 脉冲神经网络芯片。就在上周举行的 2016 年物理设计国际研讨会（ISPD）上，IBM 展示了其研发的 TrueNorth 的发展现状及规划，包括 TrueNorth 芯片架构、评估板数组、参考设计系统和软件生态架构。ISPD 是 ACM 旗下的会议，这次会议对下一代的芯片进行了讨论。 
 
一台 TrueNorth 处理器由 54 亿个连结晶体管组成，构成了包含100万个数字神经元阵列，这些神经元又可通过 2.56 亿个电突触彼此通信。超过 4 亿比特的本地存储单位（100KB每核）存储突触和神经元参数。在 0.8 伏电压下，一台实时运行的 TrueNorth 处理器功耗仅为 70 毫瓦，每秒可执行 46 千兆突触运算。图片来源：research.ibm.com 
 
TrueNorth 设计师 Filipp Akopyan 在演讲中表示，从第一个微处理器到今天的超级计算机芯片，时钟速度和功耗在稳步的上升，我们应该朝着相反的反向走：更低的时钟速度和更低的功耗，最终低到人类大脑的 10 赫兹、20 瓦的指标。 
  
 
IBM 的类脑架构从上图中的左上角开始，通过研究人类大脑的皮质神经元结构，然后是右边的架构、功能并最终达到物理实现。同样，更复杂的神经形态的灵感，来自于左上角下面的核心理念，从单芯片到多芯片的模块。图片来源：edugeek.net 
  
“今天我们的移动端生产着大量的数据，它们必须通过云端来处理，但是 TrueNorth 可以配置在边缘网络中，数据可以流入并且得到智能的处理，而只需要把重要的项目和总结传送到云端，”Akopyan 在报告中说道：“但是计算资源正在往错误的高功率方向发展，它们最好降低到生物级别的水准。” 
 
TrueNorth 芯片最大的差别是，它既有传统的同步的部分（接口和时序），也有异步的部分（类脑架构），所以流程需要单独但同步的开发。来源：eetimes.com 
 
 
Akopyan 表示，TrueNorth 芯片的目标是边缘网络（Edge-of-the-Net）和大数据解决方案，因此必须通过超低功耗实时处理大量数据。TrueNorth 实现 54 亿个晶体管 70 毫瓦功率的关键是采用了异步的逻辑，芯片除了在特定的神经元被开启并和其他神经元进行通信的时候，功率消耗都为 0。在结构上，如果是采用同步的架构，里面的 54 亿晶体管会消耗 50 到 100 瓦，但异步架构会让功耗降到最低。 
 
为了在结构上实现任何神经元彼此连结，晶体管上有一个巨大的纵横栓开关（crossbar switch）连结 54 亿的神经元。 
  
 
IBM 第一个电路板级别的解决方案，使用了16块单芯片的 TrueNorth。图片来源：research.ibm.com 
  
IBM 现在有一块单芯片电路板，机箱有 16 片和 48 片的电路板，每块电路板有 16 个 TrueNorh 芯片。未来 IBM 还打算做出有着 64、256、1024 和 4096 块芯片的电路板。其中，4096 块芯片的电路板包含的神经突触数量可能达到人脑的1%。 
  
由于借鉴了人脑中神经元细胞的脉冲模型，IBM TrueNorth 与当前冯·诺依曼架构的计算机截然不同。IBM 类脑计算机首席科学家 Dharmendra S. Modha 在介绍 TrueNorth 也表示：如果把当前的冯·诺依曼架构计算机比作人的左脑，是快速处理抽象数据的计算器；那么相比之下 TrueNorth 更像右脑，是通过慢速感知进行模式识别的机器。他们希望利用神经突触可塑性，增强 TrueNorth 规模，创造现场适应性强并且能够在线学习的新一代类脑计算机。 
文章转载自新智元公众号 原文链接"
什么是AI芯片？ 阅读笔记,2022-06-27,"1. 原文连接 
【科普】什么是AI芯片？-硬件十万个为什么 
2. 阅读笔记 
2.1 背景 
AI的三大关键基础要素： 
数据是AI算法的“饲料”算法是AI的背后“推手”算力是基础设施 
算力源于芯片，通过基础软件的有效组织，最终释放到终端应用上，作为算力的关键基础，AI芯片的性能决定着AI产业的发展。 
不同类型的芯片所擅长的领域不同 
AI运算指以“深度学习” 为代表的神经网络算法，需要系统能够高效处理大量非结构化数据（ 文本、视频、图像、语音等） 。 
需要硬件具有高效的线性代数运算能力，计算任务具有：单位计算任务简单，逻辑控制难度要求低，但并行运算量大、参数多的特点。 
对于芯片的多核并行运算、片上存储、带宽、低延时的访存等提出了较高的需求。 
 
针对不同应用场景，AI芯片还应满足：对主流AI算法框架兼容、可编程、可拓展、低功耗、体积及价格等需求。 
2.2 AI芯片对比 
【AI芯片包括：GPU、FPGA、ASIC、类脑芯片】 
GPU 通用性强、速度快、效率高，特别适合用在深度学习训练方面，但是性能功耗比较低。FPGA 具有低能耗、高性能以及可编程等特性，相对于 CPU 与 GPU 有明显的性能或者能耗优势，但对使用者要求高。ASIC 可以更有针对性地进行硬件层次的优化，从而获得更好的性能、功耗比。但是ASIC 芯片的设计和制造需要大量的资金、较长的研发周期和工程周期，而且深度学习算法仍在快速发展，若深度学习算法发生大的变化，FPGA 能很快改变架构，适应最新的变化，ASIC 类芯片一旦定制则难于进行修改。 
2.2.1 Why CPU can’t 
优点：CPU有大量的缓存和复杂的逻辑控制单元，非常擅长逻辑控制、串行的运算 
缺点：不擅长复杂算法运算和处理并行重复的操作。 
2.2.2 GPU (GraphicsProcessing Unit) 
GPU有一定的局限性。深度学习算法分为训练和推断两部分， GPU 平台在算法训练上非常高效。但在推断中对于单项输入进行处理的时候，并行计算的优势不能完全发挥出来。 
优点： 
提供了多核并行计算的基础结构，且核心数非常多，可以支撑大量数据的并行计算拥有更高的浮点运算能力相对于FPGA开发更加简单。 
缺点： 
管理控制能力（最弱）功耗（最高）。 
 
2.2.3 FPGA（Field Programmable Gate Array） 
优点： 
可以无限次编程，延时性比较低，同时拥有流水线并行和数据并行（GPU只有数据并行）、实时性最强、灵活性最高。FPGA可同时进行数据并行和任务并行计算，在处理特定应用时有更加明显的效率提升。对于某个特定运算，通用 CPU可能需要多个时钟周期，而 FPGA 可以通过编程重组电路，直接生成专用电路，仅消耗少量甚至一次时钟周期就可完成运算。由于FPGA的灵活性，很多使用通用处理器或 ASIC难以实现的底层硬件控制操作技术， 利用 FPGA 可以很方便地实现。这个特性为算法的功能实现和优化留出了更大空间。FPGA 一次性成本(光刻掩模制作成本)远低于ASIC，在芯片需求还未成规模、深度学习算法暂未稳定， 需要不断迭代改进的情况下，利用 FPGA 芯片具备可重构的特性来实现半定制的人工智能芯片是最佳选择之一。功耗方面，从体系结构而言， FPGA 也具有天生的优势。传统的冯氏结构中，执行单元(如 CPU 核)执行任意指令，都需要有指令存储器、译码器、各种指令的运算器及分支跳转处理逻辑参与运行， 而FPGA每个逻辑单元的功能在重编程(即烧入)时就已经确定，不需要指令，无需共享内存，从而可以极大的降低单位执行的功耗，提高整体的能耗比。 
缺点： 
开发难度大只适合定点运算价格比较昂贵 
 
2.2.4 ASIC（Application Specific IntegratedCircuit） 
目前以深度学习为代表的人工智能计算需求，主要采用GPU、FPGA等已有的适合并行计算的通用芯片来实现加速。在产业应用没有大规模兴起之时，使用这类已有的通用芯片可以避免专门研发定制芯片(ASIC)的高投入和高风险。 GPU先天缺陷和FPGA的通用无用。 但是随着规模的扩大，ASIC的成本会低于FPGA。 
追求性价比。 
优点:它作为集成电路技术与特定用户的整机或系统技术紧密结合的产物，与通用集成电路相比具有体积更小、重量更轻、 功耗更低、可靠性提高、性能提高、保密性增强、成本降低等优点。 
缺点：灵活性不够，通用性不够。 
主要性能指标：功耗、速度、成本
2.2.5 类脑芯片 
开句玩笑：人类可以想出比人脑更加优秀的AI芯片模型吗？人类有想象力吗？ 
类脑芯片不采用经典的冯·诺依曼架构， 而是基于神经形态架构设计，以IBM Truenorth为代表。IBM 研究人员将存储单元作为突触、计算单元作为神经元、传输单元作为轴突搭建了神经芯片的原型。 
3. 所思所想 
使用何种芯片说到底就是在极致性价比下追求通用和专用的平衡艺术。通用意味着部分无用，专用意味着无法灵活 
在这种民用的技术需要赶超只能弯道超车，军用的却可以追赶，因为知识产权和市场垄断。 
一种技术的理论在提出来时，可能会面临现实的局限（成本，技术，良品率），但是在未来如果你实现了局限的突破，那你就比较方便实现弯道超车，上一个技术的引领者因为自身的惯性和阻力，比较难进行快速的调整。"
2024国外类脑智能企业汇总,2024-07-05,"Intel 
官网: Intel: https://www.intel.com企业介绍: Intel是全球领先的半导体公司，致力于开发高性能计算技术。产品及介绍: 
Hala Point: 这是世界上最大的类脑智能系统，利用Intel的Loihi 2处理器，支持高达20千万亿次操作每秒（petaops），显著提高AI工作负载的效率和可持续性，应用于科学和工程问题解决、物流、智能城市基础设施管理和大型语言模型（LLM）训练。
IBM 
官网: IBM: https://www.ibm.com企业介绍: IBM is a global leader in AI and computing technology. Its research division focuses on various aspects of brain-inspired computing, including neuromorphic chips.产品及介绍: 
TrueNorth: IBM’s TrueNorth is a neuromorphic chip designed to mimic the brain’s neuronal structure. It supports low-power, real-time computing tasks similar to neural networks in biological brains.
SynSense 
官网: SynSense时识科技公司: https://www.synsense.ai企业介绍: SynSense is a neuromorphic computing startup focused on low-power, high-efficiency AI solutions.产品及介绍: 
DYNAP-CNN: DynapCNN is SynSense’s neuromorphic processor for real-time vision and audio processing, designed for edge AI applications.
Cogitat 
官网: Cogitat公司: https://www.cogitat.io企业介绍: Cogitat是一家专注于脑机接口技术的公司，由Imperial College London的研究人员创立，致力于将脑电图（EEG）信号转化为可操作的指令。产品及介绍: 脑机接口用于医疗康复和虚拟现实（VR）应用，帮助恢复运动控制，提升虚拟现实体验的沉浸感。 
BrainChip 
官网: BrainChip公司: https://www.brainchip.com企业介绍: BrainChip is a leading provider of neuromorphic computing solutions.产品及介绍: 
Akida Neural Processor: Akida is a neuromorphic processor designed for edge AI applications, offering low power consumption and high efficiency. It is used in various industries, including automotive, IoT, and cybersecurity.
Numenta 
官网: Numenta公司: https://www.numenta.com企业介绍: Numenta focuses on theoretical neuroscience and its application to AI.产品及介绍: 
HTM（Hierarchical Temporal Memory）: HTM is a brain-inspired computational framework for understanding the neocortex’s algorithm and applying it to create intelligent machines.
Vicarious 
官网: Vicarious公司: https://www.vicarious.com企业介绍: Vicarious is an AI company using the computational principles of the brain to build general intelligence.产品及介绍: 
Recursive Cortical Network (RCN): RCN is Vicarious’s approach to AI, aiming to achieve human-level performance in vision and other cognitive tasks by mimicking the structure and function of the human cortex.
Hewlett Packard Enterprise (HPE) 
官网: 慧与公司(原是惠普公司的一部分): https://www.hpe.com企业介绍: HPE is a global enterprise information technology company.产品及介绍: 
The Machine项目: The Machine is HPE’s ambitious project that integrates memory-driven computing and brain-inspired architectures to handle massive data workloads with high efficiency and speed.
Samsung 
官网: 三星公司: https://www.samsung.com企业介绍: Samsung is a global leader in electronics and technology innovation.产品及介绍: 
Neuromorphic Processor: Samsung is developing neuromorphic processors designed to emulate brain functions, aimed at advancing AI capabilities in consumer electronics and other applications.
Qualcomm 
官网: 高通公司: https://www.qualcomm.com企业介绍: Qualcomm is a leading semiconductor and telecommunications equipment company.产品及介绍: 
Zeroth Platform: The Zeroth Platform is Qualcomm’s neuromorphic computing project that focuses on enabling cognitive capabilities in mobile and embedded devices.
DeepMind 
官网: DeepMind(谷歌旗下): https://www.deepmind.com企业介绍: DeepMind is a pioneer in AI research, focusing on deep learning and reinforcement learning.产品及介绍: 
Neural Turing Machines (NTMs): NTMs are designed to enhance machine learning models’ memory and processing capabilities, inspired by the human brain’s structure.
Fujitsu 
官网: 富士通公司: https://www.fujitsu.com企业介绍: Fujitsu is a global information technology company.产品及介绍: 
Digital Annealer: Digital Annealer is a computing architecture inspired by quantum and neuromorphic computing principles, designed to solve complex combinatorial optimization problems efficiently.
Sony 
官网: Sony索尼公司: https://www.sony.com企业介绍: Sony is a global electronics and entertainment company.产品及介绍: 
Neuromorphic Image Sensor: Sony’s neuromorphic image sensors are designed to process visual information similarly to the human brain, enabling more efficient and intelligent image processing.
Innatera 
官网: Innatera公司: https://www.innatera.com企业介绍: Innatera is focused on building neuromorphic processors for real-time sensory processing.产品及介绍: Innatera’s neuromorphic processors enable ultra-low-power, high-speed processing for applications in IoT, robotics, and autonomous systems. 
 
 
【转载自:】OpenSNN开思通智网 ---- “一起来o站，玩转AGI！” 【官网:】https://www.opensnn.com/ 【原文链接:】https://www.opensnn.com/os/article/10000932 
结束"
IBM研制新一代芯片可模拟大脑认知能力,2011-08-22,"电脑领域的王者IBM（图） 
IBM研究人员日前研制出新一代计算机芯片，可以模拟大脑认知和活动等能力。 
IBM第一代神经突触(neurosynaptic)计算机芯片完全不同于计算机设计与制造的传统理念，它们通过先进算法和硅电路，再现了发生于大脑等生物系统中神经细胞和突触之间的现象。研究人员已经制造出两个这样的芯片模型，目前正在进行测试。 
用这种芯片造出来的系统被称为“认知计算机”，它们无法像传统计算机一样进行编程，但可以通过积累经验进行学习，发现事物之间的相互联系，模拟大脑结构和 突触可塑性。为了做到这一点，IBM将纳米科学、神经科学和超级计算机原理融入一个为期多年的认知计算系统开发项目中。 
IBM及其高校合作伙伴同时还宣布，美国国防高级研究计划局(DARPA)向他们提供了大约2100万美元经费，用以展开SyNAPSE项目第二阶段研发 工作。SyNAPSE项目的目标是开发一种先进系统，不仅可以同时分析来自多个感觉形态的信息，而且还能动态地融入环境进行互动。 
IBM  研究实验室项目关注达曼德拉·摩德哈(Dharmendra  Modha)说：“这是一个重要计划，将探究统治计算机结构五十余年的冯·诺依曼体系以外的领域。计算机的未来应用会对功能性提出越来越高的要求，而当今 计算机体系则无法有效解决这个问题。这些芯片是计算机从计算器转化为学习系统的重要一步。”(轩辰) 
技术帝IBM今天宣布它们正在研发一种具有革命性的新一代计算机芯片，在这种新芯片加持下，终结者T-800同学将不再只是电影人物，而很有可能成为现实。 
IBM将这种芯片称之为neurosynaptic，不同于现有芯片，neurosynaptic通过先进算法和硅电路，模拟发生于大脑等生物系统中神经 细胞和突触之间的现象，进行认知并积累学习。简单来说，就是这种芯片是模拟成一个大脑，并且完成大脑的各种功能。一旦成功，统治计算机结构至今冯·诺依曼 体系将会彻底被颠覆。 
鉴于这种芯片的吸引力实在太大，所以美帝已经着力推进这件事。美国国防高级研究计划局已经投入2100万美元供IBM及其合作高校进行 neurosynaptic第二期的研究工作。或许在不久将来，我们所熟悉那些的科幻电影情节将不再是科幻，嗯，貌似是好还是坏还真难给个定论。 
IBM作为计算机行业的领军企业，已成功驰骋65余载，如今它又再创了一个电子大脑——人工智能仿生芯片。   
Big  Blue今天宣布，在四所大学和国防高级研究计划局（DARPA）的协助下，他们研制了一款模拟大脑信息处理方式的计算机芯片的基础设计。 
IBM的这款仿生芯片，旨在模拟人类大脑功能，具备如今计算机还不能像人一样处理信息的功能如意识、感知、交互、识别等。 
Dharmendra  Modha是DARPA此项目的主要研究者，他同时兼任美国加利福尼亚州圣何塞IBM研究中心的研究员。Dharmendra  Modha在采访中说：“这种芯片，将会成为新一代超级计算机的种子，将结合神经科学和纳米技术。我们今天的计算机其实更像一个大的计算器，而此项研究的 目标，就是让它更像人类的大脑，这是截然不同的。” 

 该芯片必然导致商业智能化，也必将颠覆传统计算模式，用人工大脑的思维来统占和取代信息时代。最终的应用程序将会对商业、政府、科学具有巨大的影像。根据这种芯片设计出来的电脑，可以更好地处理现实世界的问题，这是当今的“计算器”远远达不到的。 
 IBM在2008年9月就曾宣布该项目的启动，并于2009年达到了第一个里程碑。如今，研究人员已成功完成第一期工程，即设计一个基本的计算单元，可以不断重复复制来搭建仿生计算机的初步架构。 
 这种新的计算单位或核心与大脑类似，仿生芯片有“神经元”，或者说是数字信息处理器；“突触”是学习和记忆的基础；“轴突”即数据通路可以连接计算机各组织。 
 这虽然听起来很简单，但与如今的大多数电脑都完全不同。现在被命名为计算机的是建立在冯诺依曼架构的基础之上，特别是在“冯诺依曼瓶颈”之下，IBM的创想更值得人期待。 
 仿生处理器集成内存运行速度并不快，发送数据大概在10Hz左右，远远被今天5000Hz的电脑抛在后面。但是！人类的大脑是无数多任务同时开展的，这是 相当可怕的。同时在各个方向上发送信号，大脑的10亿个神经元和10万亿个连接（突触）加在一起计算，产生的能量是相当巨大的。 
 IBM就希望效仿这一大脑结构创建全新的芯片。 
 该研究小组目前已建立起第一批类似人脑的计算单元，由256个神经元，256×256个突触和256个轴突构成。  换句话说，它已经拥有了处理器，内存和通信的基本架构。  此外这种类人脑结构还有另一个好处，运行功耗低，而且在不使用时还可以实现部分关闭。另外这些新的芯片将不会以传统的方式进行编程。  基于它的认知计算机也有望实现学习经验，寻找相关性，建立假设，记住和学习等能力。  由于他们模仿大脑的“结构和突触可塑性”，因而处理过程是分布式和平行式的，而非集中和串行式。 
 这种IBM人工智能仿生芯片还能重新创建一种类似大脑中发生在神经元和突触之间的“脉冲”现象。  因而其能够处理十分复杂的任务，比如玩Pong游戏。目前已有两个原型芯片被制造出来正在测试。研究人员也即将步入到第2个阶段，创建一个计算机。  目标是创建一个不仅能立刻通过多种感官分析复杂信息，而且能动态修正自身，与环境互动和识别周围发生的事情的计算机。 
 另外除了玩Pong游戏，IBM的团队还测试过该芯片解决导航，机器视觉，模式识别，联想记忆以及分类等问题的能力。最终，IBM将把该计算单元完全融入 到一个完整的硬件和软件的集成系统中去。  Modha说，IBM希望建立一台包含100亿个神经元和100万亿个突触的计算机。这比人类大脑的功都强大10倍以上。  另外Modha还预测，完整的系统只会消耗一千瓦的功率，而且将占据不到两升的量（我们大脑的大小）。  相比之下，目前最快的IBM超级计算机蓝色基因有147,456处理器，内存容量超过144T，有一个巨大的空调柜那么大，消耗超过2兆瓦的电力。 
 对于具体应用方面，IBM说可以使用认知计算机通过传感器网络和微型电机网络不断记录和报告数据如温度，压力，波高，声学和海潮等来监测世界范围内的供水状况。  然后，它还可以在发生地震的情况下发出海啸警报。而这样的任务传统计算机根本不可能完成。 
 据悉，该项目是用DARPA捐赠的2100万美元创建的，包括六个IBM实验室，四所大学（康乃尔大学，威斯康星大学，加州大学和哥伦比亚大学）以及一些 政府研究人员。虽然这个项目比较新，但是IBM自其1956年创建第一台人脑模拟器（512个神经元）以来就一直在从事对类人脑计算机的研究。
 
IBM研究人员说：“如果一切顺利，这将不是5％的飞跃。而是一个巨大的飞跃。  而且到现在为止我们也已经克服了巨大的能够想象到的困难。” 
  

更多精彩内容请关注：IBM虚拟化与云计算官方微博IBM虚拟化与云计算官方网站 

 

 
 

 
转载于:https://blog.51cto.com/vccbook/645927"
打破冯·诺依曼结构，中国的类脑芯片已经来了！,2017-03-13,"打破冯·诺依曼结构，中国的类脑芯片已经来了！ 
 
 尤瓦尔·赫拉利的《未来简史》有不少朋友应该读过了，它英文原著的名称其实是Homo Deus, 意为""神化”，说的就是：我们人类，正在一步步进化为神一般的存在，因为，我们开始复制自身，创造新的智慧生命体。硅谷密探最近釆访到的项目又一次让密探深深感到，未来己来。 
 
 人工智能技术（AI）现在已是火得不要不要的，各种传感器，机器学习，外围设备五花八门，可是，其核心的数据处理硬件架构仍基于经典的冯·诺依曼结构。冯·诺依曼结构已有七八十年历史，这就好比已经直立行走的智人，仍长着一颗猩猩的脑袋。 

 
 
这就出现了冯·诺依曼瓶颈，也就是说CPU再快，也要等内存，因为CPU和内存之间的性能差距越来越大。 
 
冯·诺依曼结构 
冯·诺依曼结构中，计算模块和存储单元是分离的，CPU在执行命令时必须先从存储单元中读取数据。每一项任务，如果有十个步骤，那么CPU会依次进行十次读取，执行，再读取，再执行... 这就造成了延时，以及大量功耗（80%）花费在了数据读取上，当然多核、多CPU或一些常用数据的就地存储会一定程度上缓解这些问题，但这种中心处理的架构会限制处理能力的进一步发展，好比诺大个北京城，如果所有的政治、经济、文化活动都集中在市中心，为了生活，人们必须穿城而过，任你路修到二十环，还是一个词，堵死。 
学术界和工业界出现了向人类大脑学习体系结构的趋势，大脑的处理单元是神经元，内存就是突触。神经元和突触是物理相连的，所以每个神经元计算都是本地的，而且从全局来看神经元们是分布式在工作。 
 
神经元和突触 
前几年轰动全球的IBM的仿人脑芯片TrueNorth也模仿了人类大脑的神经元结构，它的计算效率和可扩展性都远超现在的计算机，并且宣称可用于手势识别、情绪识别、图像分类和对象追踪、实时语音识别等领域。 
 
而硅谷密探今天采访的团队AI-CTX，他们的模型基于亿万个可以相互连接的仿生神经元，每个神经元都具有跟人脑神经元类似的电学特性与动态参数，具有简单的运算与存储功能，这些神经元像大脑神经元一样，通过脉冲相互沟通。 
 
在现实层面，他们的每个芯片可以搭载百万个神经元，亿万个神经突触，可以自由扩展的芯片网络又如同大脑皮层的不同的区域，分别在不同的层级上， 可以并行的，同时的处理任务，从而指数型地提高了数据处理的效率。又因各个指令及相应数据都存储在同一芯片中，所有的计算资源不会因为等待存储访问而导致浪费，功耗也比现在基于冯·诺依曼硬件架构的处理器低两到三个数量级。 
看看Alpha Go，下一场棋耗电几千瓦，而李世石，大脑只耗20瓦，所以虽然输了，但人类的尊严还算没有低到尘埃里... 
 
李世石对阵Alpha Go 
除了运算速度快，能耗低，团队还有一项核心的IP，就是一种特殊的布线方式，使各芯片之间的交流可以突破物理限制，无限的增加新的芯片于原有网络，这样，运算能力也可无限增加。而传统的冯·诺依曼结构，如果要增加CPU的数量，那么CPU之间的沟通是越发困难的，因为运算能力有其上限。 
在处理AI领域数据上，AI-CTX团队基于仿生神经网络设计出芯片架构比之冯·诺依曼结构无疑是革命性的，那么市场上是否有用类似方案的对手呢？ 
 
与IBM TrueNorth芯片的对比，来自eetimes 
CTO 乔宁很自信的说：“Intel，三星等正在研发的加速器类产品在实时处理动态数据速度上比我们慢，能耗还高一个数量级。而跟IBM的TrueNorth芯片相比，我们的模拟神经元具有更复杂的非线性参数，硅面积只有IBM的0.5%，能耗只有10%，基于我们的芯片架构，每个神经元能轻松建立多达8千个连接，而现有IBM Truenorth 芯片只有256个连接。而且我们第一代原型一出来就已经有订单了！” 
看他谈吐间一派樯橹灰飞烟灭的架势，我只想说，大神，为国争光啊！ 
 
AI-CTX的CTO乔宁 
当然，我们也要在这里指出，三星的加速器是做静态图像处理，TrueNorth是一个通用实验芯片，不专门针对任何应用，虽然已经有不少人在开发相应的芯片应用（识别，追踪，决策）。 
AI-CTX的芯片不适合处理静态硬盘数据（静态图片及数据），但非常擅长于处理包含时间信息的数据，比如，动态的传感器数据，自然界的自然信号（温度，气压），人体信号（心跳EEG），网络数据，IoT，股票高速决策等数据的处理。 
AI-CTX目前的主要切入的应用场景包括实时在线（always-on）的应用，比如低功耗的智能便携设备，移动设备，健康检测EEG，IoT设备，网络安全等。还有另一块是高速低功耗应用，比如自动驾驶，手势识别，声音识别等。更长远来看可以用于大数据处理和深度学习等领域。 
AI-CTX团队在中国国家级创新创业大赛中，从两万个团队中杀出重围，勇夺团体第一。 
 
AI-CTX的CEO刘洪杰在大赛现场 
凭啥这么牛，看看团队就知道了，CEO刘洪杰是苏黎世联邦理工脉冲仿生视觉的美女博士；CTO乔宁是中科院微电子研究所博士，之后又在苏黎世联邦做芯片开发项目的领导人，十几年电路设计从未失败（嗯，东方不败）；首席科学家Giacomo Indiveri教授是神经仿生计算方面的世界知名专家；联合创始人Kyan Eng也是有博士学位，多项专利的成功的连续创业者。哎，怪不得IBM的芯片都被吊打... 
 
密探和AI-CTX CEO刘洪杰（中）、CTO乔宁（右） 
资本的嗅觉很敏锐，目前已有几家资本与他们接触了，但名花还没主，有兴趣的投资人可以联系密探。 
乔宁博士说，他的梦想是造出有生命的芯片，那一刻，好像有道耶酥光闪过，尤瓦尔·赫拉利的预言在耳边轰然响起... 
作者：昊鹭 
编辑：严肃"
IBM公布神经计算机芯片Truenorth详细资料,2017-09-01,"ZD至顶网服务器频道 04月08日 新闻消息： IBM日前公布了旗下Truenorth的详细发展计划。Truenorth是一款基于人脑神经形态混合信号的计算机芯片。Truenorth构造师在2016年物理设计大会（ISPD物理设计国际研讨会）上描述了 Truenorth芯片的架构、评估板系列、参考系统和软件生态系统。2016年物理设计国际研讨会是计算机械协会（ACM）举办的讨论下一代芯片的会议，会议得到英特尔、IBM、Cadence、Global Foundries、IMEC、Oracle、Synopsys、TSMC、Altera、Xilinx和其他全球明星级芯片制造商的赞助。 
  从第一个微处理器到今天Supercomputers-on-a-chip（超级单片机芯片），其时钟速度和功耗都在稳步增加（图的右上），而IBM Truenorth设计师表示，我们其实应该朝相反的方向走，要沿着更低时钟速度和更低功耗那条曲线走，最终降低到人脑等级的10Hz、20瓦（图的左下）。 
 IBM在ISPD上描述了称之为大脑式计算机启示的概念，并希望这个词在各个领域成为大家熟知的名字，包括各种应用，如超级智能物联网（IoT）、超智能汽车、超智能相机、超智能无人驾驶飞机、超智能医疗设备，当然还有超智能超级计算机。 

 IBM的大脑式架构从图的左上角开始，先研究人类大脑的皮层神经元结构，再往右边伸展"
"未来计算机基于生物质 模拟人脑,IBM开发人脑模拟芯片 可能改变未来计算机",2021-07-02,"Modha表示，IBM的旗舰超级计算机Watson是线性计算的代表例子，而上述芯片则能够实时识别或“感知”周边环境，类似于人耳和人眼。举例来说，这种芯片可被用来玩乒乓球，它能“看到”球并移动球拍接球。不过，虽然IBM强调这种技术最终可能将被用于眼镜等消费者产品，但这种应用仍有很长一段路要走。 

 IBM去年从美国能源部那里赢得了一份价值3.25亿美元的合同，内容是在2017年以前开发两台先进的超级计算机，分别名为“Summit”和“Sierra”，这两台计算机可能会整合这种新的架构。 
 Modha称，工程师目前所面临的挑战之一是如何复制各种大小的人脑以及用来对其进行操作的程序软件。IBM是从复制蠕虫大小的人脑开始实验的，工程师使用了256个处理器来模拟神经元，然后一直扩大规模至使用100万个处理器，也就是相当于蜜蜂大小的人脑。IBM工程师团队的目标是在明年底以前复制出类似于老鼠大小的人脑，这种芯片将使用2.56亿个处理器来模拟神经元。但即使是达到这一目标，与人脑的1000亿个神经元相比也仍相去甚远。"
聊聊人工智能芯片（一）,2018-05-18,"人工智能芯片主要包括NVidia GPU、Google的TPU、Intel的Nervana、IBM的TreueNorth、微软的DPU和BrainWave、百度的XPU、Xilinx的xDNN、寒武纪芯片、地平线以及深鉴科技的AI芯片等，基本上是GPU、FPGA、神经网络芯片三分天下的趋势，三种芯片各有各自的优劣，都在面向自己独特的细分市场。本章先聊一聊深度神经网络和NVidia GPU的崛起。 
    人工智能的终极目标是模拟人脑，人脑大概有1000亿个神经元，1000万亿个突触，能够处理复杂的视觉、听觉、嗅觉、味觉、语言能力、理解能力、认知能力、情感控制、人体复杂机构控制、复杂心理和生理控制，而功耗只有10~20瓦。 
这是人脑基本神经元和突触： 
     这是人工神经元模拟神经元： 
 
输入模拟神经元输入电信号，权值模拟神经元之间的突触连接，激活函数模拟突触之间的电信号传导。 
早在1981年David Hubel和Torsten Wiesel就发现了人的视觉系统的信息处理是分级的，因此获得了诺贝尔医学奖。如图所示，从视网膜出发，经过低级的V1区边缘特征提取，到V2区识别基本形状或目标的局部，再到高层的目标识别（例如识别人脸），以及到更高层的前额叶皮层进行分类判断等，人们意识到高层特征是低层特征的组合，从低层到高层越来越抽象，越来越能表达语义或者意图。 
深度神经网络模型模拟人脑识别的分层识别过程： 
 
深度神经网络的输入层模拟视觉信号的输入，不同的隐藏层模拟不同级别的抽象，输出层模拟输出的对象分类或者语义等。 
从上图的典型的人脸识别的训练任务来看，按照10层深度神经网络、训练上百万张图片，大概需要30 Exaflops的计算能力，如果还是用CPU来做训练，大概需要训练一年的时间，这显然是无法忍受的速度，亟待需要计算能力更强的人工神经网络芯片出现。 
 
NVidia GPU的崛起 
 
 可能有很多人会问，目前在人工智能领域，NVidia GPU为什么具有无可撼动的霸主地位，为什么AMD的GPU和NVidia GPU性能相差不多，但是在人工智能领域的受欢迎的程度却有天壤之别。 

 我们知道GPU原本就是显卡，它是为游戏和渲染而生的，它里面核心运行单元是shader，专门用作像素、顶点、图形等渲染用的。 

 NVidia在2006年的时候跨时代的推出了统一计算设备架构CUDA(Compute Unified Device Architecture)以及对应的G80平台，第一次让GPU具有可编程性，让GPU的核心流式处理器Streaming Processors(SPs)既具有处理像素、顶点、图形等渲染能力，又同时具备通用的单精度浮点处理能力，NVidia称之为GPGPU（General Purpose GPU），黄教主的野心是让GPU既能做游戏和渲染也做并行度很高的通用计算。 

 
 

 2006年，第一代GPGPU架构G80 


 
 
 G80有16组流式处理器Streaming Processors(SPs)，每组SP里有16个计算核心，一共128个独立的计算核心，单精度峰值计算能力可达330 Gflops，而同期主流的Core2 Duo CPU只有50 Gflops的处理能力，更为重要的是从G80架构开始，GPU开始支持可编程，所有的计算密集型的并行任务都有可能通过程序移植在GPU上运行起来。 
 
 
CUDA的编程模型 
 
 CUDA将GPU的计算单元抽象成3个编程层次：Grids、Blocks和Threads，一个CUDA kernel在执行的前会先把数据和指令传到GPU上，在执行的时候会使用若干个Grids，一个Grid里含有多个Blocks，一个Block里含有多个Threads，调度上一个Block的Threads会调度到一个独立的Streaming Processors上执行，而16/32个Threads称为一个Warp，Warp是GPU上指令调度的最小单元，一个Warp会同时运行在16/32个计算核心上。 
 
 
 
性能增长远超CPU战略 
 
 NVidia从2006年推出Tesla架构以来，不断的更新架构和性能，陆续推出了Femi、Maxwell、Pascal还有最新的Volta架构，基本上保持着2年性能翻1倍的增长态势。 

 而对CPU的性能加速比，在单精度计算能力上保持着遥遥领先的态势，并且拉开的差距越来越大。 



 
 
 深度神经网络+NVidia GPU掀起人工智能浪潮 

深度神经网络+NVidia GPU掀起了业界的人工智能浪潮，不得不说这只是老黄整体战略的一个副产品，谁也没有想到，高性能计算领域的一个分支--人工智能会如此火爆。 
 
2011年，负责谷歌大脑的吴恩达通过让深度神经网络训练图片，一周之内学会了识别猫，他用了12片GPU代替了2000片CPU，这是世界上第一次让机器认识猫。 
 


2015年，微软研究院用GPU加速的深度神经网络，在ImageNet比赛中获得了多项击败人的辨识准确度，这是第一次机器视觉的识别率打败了人眼的识别率(错误率5%)，可以认为是人工智能史上的一个重要里程碑事件。2016年，谷歌旗下Deepmind团队研发的机器人AlphaGo以4比1战胜世界围棋冠军职业九段棋手李世石（AlphaGo的神经网络训练用了50片GPU，走棋网络用了174片GPU），引发了围棋界的轩然大波，因为围棋一直被认为是人类智力较量的巅峰，这可以看做是人工智能史上的又一个重大里程碑事件。"
IBM发明世界首个人造神经元，人工智能的底层硬件基石已完成！,2016-08-06,"from: http://it.sohu.com/20160804/n462610626.shtml 
 

 

 
图片来源：IBM 
编者按：从AlphaGo击败李世石，宣布超级计算机攻克了围棋这一穷举法不可能征服的领域后，人工智能（AI）又成了所有人最热门的话题之一。 
对于不少该领域的科学家而言，人工智能的终极目标之一就是用机器实现人脑的全部功能，而作为人脑的最小细胞单位——神经元，可能会是一个最好的入手点。 
美国当地时间8月3日，IBM官方宣布了他们的最新成果——首个人造神经元，可用于制造高密度、低功耗的认知学习芯片。 
IBM苏黎世研究中心制成了世界上第一个人造纳米尺度随机相变神经元。IBM已经构建了由500个该神经元组成的阵列，并让该阵列以模拟人类大脑的工作方式进行信号处理。 
该技术突破具有重要意义，因为相变神经元具有传统材料制成的神经元无法匹敌的特性——其尺寸能小到纳米量级。此外，它的信号传输速度很快，功耗很低。更重要的是，相变神经元是随机的，这意味着在相同的输入信号下，多个相变神经元的输出会有轻微的不同，而这正是生物神经元的特性。 
 
人造神经元论文的第一作者：托马斯·图玛（Tomas Tuma） 
IBM相变神经元由输入端（类似生物神经元的树突）、神经薄膜（类似生物神经元的双分子层）、信号发生器（类似生物神经元的神经细胞主体）和输出端（类似生物神经元的轴突）组成。信号发生器和输入端之间还有反馈回路以增强某些类型的输入信号。 
 
人造神经元研发团队，图片来源：IBM 
神经薄膜是整个神经元的关键。在生物神经细胞中，起神经薄膜作用的是一层液态薄膜，它的物理机理类似于电阻和电容：它阻止电流直接通过，但同时又在吸收能量。当能量吸收到一定程度，它就向外发射自己产生的信号。这信号沿着轴突传导，被其他神经元接收。然后再重复这一过程。 
在IBM制造的神经元中，液态薄膜被一小片神经薄膜取代。神经薄膜是由锗锑碲复合材料（也称GST材料）制成的，该材料也是可重写蓝光光盘的主要功能材料。锗锑碲复合材料是一种相变材料，即它可以以两种状态存在：晶体态和无定形态。通过激光或电流提供能量，两种状态之间可以互相转变。在不同状态下，相变材料的物理特性截然不同：锗锑碲复合材料在无定形态下不导电，而在晶体态下导电。 
在人工神经元中，锗锑碲薄膜起初是无定形态的。随着信号的到达，薄膜逐渐变成结晶态，即逐渐变得导电。最终，电流通过薄膜，制造一个信号，并通过该神经元的输出端发射出去。在一定的时间后，锗锑碲薄膜恢复为无定形态。这个过程周而复始。 
 
生物神经元与人造神经元对比图，图片来源：IBM 
由于生物体内各种噪声的存在，生物神经元是随机的（Stochastic）。IBM研究人员表示，人工神经元同样表现出了随机特性，因为神经元的薄膜在每次复位后，其状态有轻微的不同，因此随后的晶态化过程略有不同。因此，科学家无法确切地知道每次人工神经元会发射什么信号。 
那么人工神经元到底有何意义？ 
首先，人工神经元采用了成熟的材料，历经几十亿次工作而不损坏（寿命长），体积极小（有报道说是90纳米，但从下图中看应该在300纳米左右，而论文中表示未来有望达到14纳米）。因此，这是一种性能非常棒的器件。 
 
人工神经元网络。图中的银色方块是放大后的相变神经元，该神经元网络还没有配备工业标准的输入输出接口。图片来源：IBM 
其次，人工神经元跟生物神经元的工作方式非常类似。当大批人工神经元组成并行计算机后，它也许可以和人类一样进行决策和处理感官信息。IBM表示，他们的人工神经元技术和目前发展中的另外一种人工神经元器件——忆阻器互为补充。 
目前，IBM制造了10乘10的神经元阵列，将5个小阵列组合成一个500神经元的大阵列，该阵列可以用类似人类大脑的工作方式进行信号处理。事实上，人工神经元已经表现出和人类神经元一样的“集体编码”特性。此外，它的信号处理能力已经超过了奈奎斯特-香农采样定理规定的极限。 
 
　编者注：集体编码：每个神经元有2种状态，可以表示1比特信息，那么N个神经元就可以表示2N比特信息。神经元数量足够多时，能表示的信息量将极其惊人。 
 
IBM研究人员计划构建包含几千个相变神经元的单一芯片，并编写能充分利用相变神经元芯片随机特性的软件。 
视频来源：IBM 
编辑：离子心 
参考：IBM、Nature、Arstechnica 
论文：Nature Nanotechnology, 2016. DOI: 10.1038/nnano.2016.70 (About DOIs)."
跳出框架的类脑芯片，究竟是人脑增强仪还是操纵人类的工具？,2019-11-28,"科技云报道原创。 
美剧《超脑特工》中男主角的大脑中植入了一枚堪比超级计算机的微芯片，令其能在最短的时间内从遍及世界的“电子网格”中获取所需的信息。科幻电影里最常见的桥段，现实中想要实现可能需要几十年甚至几百年，那么脑芯片这门技术，现阶段的发展是怎样呢？​脑芯片的起源是来自于一个著名的“公牛实验”。1963年，何塞·德尔加多（Jose Delgado）医生在妻子和几名助手的帮助下，在没发狂的公牛头骨上安装了立体定位框架，将刺激接收器植入了它们的大脑，试图利用手持收音机上的按钮控制公牛的大脑。 
当一头公牛气势汹汹地朝他冲过来时，他出于求生本能他疯狂地按控制按钮。最后，那只牛在离他几英尺远的地方停了下来。 ​ 由此，何塞·德尔加多开创了一项至今依然具有争议的技术——类脑芯片。简单的说，脑芯片技术就是通过植入电极，用电刺激神经组织，达到操纵心灵的目的。 
打破冯·诺依曼结构 
类脑芯片究竟有何不同？ 
冯·诺依曼结构已有七八十年的历史，这就好比已经直立行走的智人，仍长着一颗猩猩的脑袋。这就出现了冯·诺依曼瓶颈，也就是说CPU再快，也要等内存，因为CPU和内存之间的性能差距越来越大。 
冯·诺依曼结构中，计算模块和存储单元是分离的，CPU在执行命令时必须先从存储单元中读取数据。 
每一项任务，如果有十个步骤，那么CPU会依次进行十次读取、执行、再读取、再执行，这就造成了延时，以及大量功耗（80%）花费在了数据读取上，当然多核、多CPU或一些常用数据的就地存储会一定程度上缓解这些问题。 ​ 学术界和工业界如今出现了向人类大脑学习体系结构的趋势，大脑的处理单元是神经元，内存就是突触。神经元和突触是物理相连的，所以每个神经元计算都是本地的，而且从全局来看神经元们是分布式在工作。 
说到类脑芯片的独特性，首先它不是传统的冯·诺依曼结构，而是专门的类人脑芯片结构。类人脑芯片架构是一款模拟人脑的新型芯片编程架构，这一系统可以模拟人脑功能进行感知、行为和思考。 
这种新型编程架构可以允许开发者为类人脑芯片设计应用程序，未来，它还可以允许计算机科学家开发出一种带有人脑功能的计算机，甚至比人脑更加智能。 
其次，类人脑芯片的存储器不但但是数据存储单元，而是进一步研发成为类似人体的突触，这是迈向人造大脑重要的一步。 
我们知道人脑之所以复杂是因为庞大复杂的神经结构，人脑有上千亿个神经元，几乎所有的生物意识行为都是从这里发起的，所以让存储器模仿突触是实现类人脑芯片不可或缺的创举。 ​ 第三，类脑芯片的能源利用率极高，普通芯片在工作时会伴随大部分的散热，计算机芯片如果没有风扇或者空间制冷技术，几分钟就足以将自己融化。但是，类脑芯片却可以像人脑一样，几乎所有的热量都用于“思考”。 
最后，类脑芯片懂得“学习”，人类大脑之所以厉害就是因为可以不断的学习和接纳新事物，通过机器学习算法，类脑芯片也做到了。从最开始的代码写入存储知识，到现在高通、IBM芯片自主学习获得知识，类脑芯片的环境应变能力正在不断的增强。 
在现实层面，每个芯片可以搭载百万个神经元，亿万个神经突触，可以自由扩展的芯片网络又如同大脑皮层的不同的区域，分别在不同的层级上， 可以并行的，同时的处理任务，从而指数型地提高了数据处理的效率。 
又因各个指令及相应数据都存储在同一芯片中，所有的计算资源不会因为等待存储访问而导致浪费，功耗也比现在基于冯·诺依曼硬件架构的处理器低两到三个数量级。 
类脑芯片争议远未停止 
找到技术发展平衡点才是关键 
本文开头提到的德尔加多医生于1969年出版了一本书，他在《心灵的物理控制：迈向精神文明社会》一文中描述了脑刺激研究，讨论了它的作用。 
但这项技术自诞生起就饱受争议，一名妇女甚至起诉德尔加多和他所服务的学校并索赔100万美元，尽管她和德尔加多未曾谋面。而时至今日，争议并没有停止。 ​ 记忆芯片的使用势必会引起人们对记忆数据不安全的讨论。且不论记忆数据被盗，单纯的个人隐私泄露，也足以引起舆论关注。难以想象记忆被盗后公众的反应，当最后一块遮羞布被彻底粉碎，一场记忆保卫战便可以轰轰烈烈的拉开了。 
如果脑芯片仅仅能增强记忆力，还不是最令人畏惧的，真正让人觉得恐怖的是，我们很难预测这些技术究竟会用在哪些地方，会被哪些有心人利用。 
事实上，对大脑的刺激可以分为侵入式和非侵入式，德尔加多的研究中后期将注意力转移到非侵入性的神经刺激方法上，他发明了一种能够向特定神经区域传递电磁脉冲的环状装置和头盔。 
他在他自己和他女儿身上测试这些装置，实验结果证明可以诱发困倦、警觉和其他状态。 ​ 无独有偶，美军也研发了一种叫作“经颅直流电刺激器”的头盔，带上这个头盔可以让人集中注意力，让你专注于一件事儿。实验表明这个头盔的确有利于士兵战斗，而且还能促进学习。公众没法不担心一旦这种头盔被滥用后，是否会制造超级士兵甚至杀人机器。 
脑芯片究竟是大脑增强仪还是会操纵人类？那些抱有激进态度的脑外科专家是否真的拿到了理想的数据？ 
这一切我们不得而知，但类脑芯片作为一门过于前沿的技术，其研究和应用的讨论仍将持续进行下去，我们相信人类会在脑芯片发展上找到一个制约点和平衡点，因为人类相对机器最大的不同是人类富有更大的想象力。 
【科技云报道原创】 微信公众账号：科技云报道"
IBM类脑超算平台TrueNorth芯片将于本周送往劳伦斯利弗莫尔,2017-07-03,"过去，我们经常听到把芯片当做是计算机的“电子大脑”的说法。不过劳伦斯·利弗莫尔国家实验室(LLNL)却决定与IBM研究院合作，并打造出一个以大脑为启发的超级计算平台。该平台的首个系统，基于IBM TrueNorth这款“神经突触计算芯片”。它的处理能力相当于1600万个神经元和40亿个突触，同时能耗仅为2.5瓦。 

 IBM TrueNorth平台的16颗芯片，将于本周晚些时候被送往劳伦斯利弗莫尔国家实验室。 
 TrueNorth被标为70年来计算机设计的一个基本背离——使用神经元和突触来处理信息，某种程度上类似于大脑（人类右半球的大脑皮层）。 
 尽管这并不是首次尝试，但IBM指出，TrueNorth非常先进，不仅克服了传统中关键的“冯·诺依曼体系结构”瓶颈，还为新硬件的利用而开创了新的思维方式。 
 IBM表示，TrueNorth技术能够打造出操作量达到百亿亿次级（或每秒计算量达10亿x10亿）的计算机，这比当前的P-Flops级计算机还要快五倍，同时体型更小、能耗更低！ 
 其源于DARPA最初携手康奈尔大学开发的“神经形态自适应可塑可扩展电子系统”（SyNAPSE）的一部分，送给劳伦斯·利弗莫尔国家实验室的16颗TrueNorth芯片，每颗都包含了54亿的晶体管、有百万级数字神经元×2.56亿个电子突触。相比之下，人脑中的神经元数量为1000亿级。 
 除了每秒46G的突触操作，IBM表示一颗TrueNorth处理器还仅仅使用了0.8V的电压（功耗70毫瓦）。16颗芯片一起工作的话，等效于1600万个神经元和四十亿个突触，但能耗却比一台平板电脑还要少。 


 本文转自d1net（转载）"
"ibm服务器架构芯片,第五代X架构 会让IBM服务器就是不一样",2021-08-12,"【IT168 专稿】2010年，X86服务器市场将迎来新一轮的平台和系统升级，就芯片而言，会有多款产品出现，包括第一款32纳米服务器处理器Westmere-EP(至强5600系列)、第一款12核芯片AMD皓龙6100、第一款单核心功耗小于6瓦特的X86芯片AMD皓龙4100以及开创X86多路服务器新纪元的Nehalem-EX(至强7500系列)[详见：2010年最值得期待的6款服务器CPU]。 
 eX5：向中低端普及 
 IBM同时作为英特尔和AMD的重要合作伙伴，其System x服务器产品线的升级自然不在话下。不过，我们这里想提的是，芯片平台升级之外的革新。对IBM的X86服务器产品线而言，最值得期待的就是其下一代的eX5架构了，而且值得一提的是，过去，如eX4主要是用在四路、八路这样的高端机型x3850 M2、x3950 M2中，而eX5将会普及到普通双路服务器和刀片服务器中来。 
 比如，未来IBM的刀片服务器通过使用eX5芯片的内存扩展板，单组刀片最大将可以支持到80个DIMM内存条，实现高达640GB的内存容量，这对于虚拟化应用是非常关键的。由于虚拟化用户经常在超过CPU最高计算能力之前耗尽内存，从而影响到一个单一系统上所能够支持的虚拟机数量。目前普通双路系统一般可用内存为96GB到144GB，此前思科推出的UCS通过其所谓的“内存扩展模块”技术可以支持到48个DIMM，将内存容量增加到384GB。内存容量的提升使得每台物理服务器所能托管的虚拟机数量也可以成倍数增加，同时降低了每个虚拟机承担的能耗和制冷成本，用户不再需要仅仅为了更大内存容量去购买更贵的4路服务器。 
 从历史看未来 
 不过，目前IBM还没有公布eX5的具体技术细节，这里我们不妨通过了解IBM X架构的发展历史来看一看它会给服务器系统带来什么样的好处。 
 如果把处理器看作服务器的大脑，那么，芯片组就象是心脏——将处理器和内存、I/O端口及其他可扩展子卡连接起来，让数据得以在整个系统中更加畅快地流动。而IBM X架构的技术核心就主要体现在芯片组上。随着处理器计算性能越来越高，I/O、内存的瓶颈就日益突出了，平衡的系统设计就非常重要。比如，2007年，IBM推出的高端x86服务器System x3850 M2和System x3950 M2，就是通过将X4架构和最新处理器技术结合在一起，从而实现了处理器、内存和I/O方面的均衡的系统设计，使得x3850 M2的性能非常领先——这种均衡设计对于多路多核系统而言极为重要。 
 回顾IBM服务器企业级X-架构的历史[IBM企业级X-架构历史概述]，我们不然看出IBM在服务器整体系统设计上的实力，这跟贴牌、OEM厂商会大有不同。总的来看，X-架构主要着力于解决服务器在按需扩展、I/O加速、高可用、系统性能、可管理性等方面的挑战。 
 自1998年IBM提出并于1999年3月在美国纽约宣布推出“X架构”以来，IBM在x86服务器的研发设计过程中，先后推出了4代EXA架构，最近的一次架构更新就是2007年底推出的X4架构。“X架构”，英文是“X-Architecture”，它是IBM的交叉服务器战略，将中、大型服务器，包括AS/400、RS/6000、S/390上面的一系列领先技术(光通道、远程联接诊断、超级并行交换开关、热插拔和热添加)系统地移植到工业标准服务器上，系统吸收IBM在高可靠企业计算系统方面的技术精华。X架构的技术核心主要体现在芯片组上，2001年推出的代号为“Summit”的芯片组，是第一代“企业级X-架构”的核心。 
 2003年6月，IBM推出了第二代企业级X-架构芯片组，支持Intel 2.8GHz的第二代Xeon MP处理器，主要有以下几方面的改进：采用了配合新至强处理器的XA-32第二代芯片组，该芯片组支持CPU的Xpand-On-Demand按需扩展，最高可到32路;内存不仅支持热插拔，还支持热添加;四级缓存提升到64MB;对远程I/O的支持也更加增强，而且在第二代企业级X-架构中还增加了铜诊断(Copper Diagnostics)技术。 
 2005年3月推出的第三代企业级X-架构芯片组Hurricane 3.0是X3的心脏，它为x366提供了集成处理器和内存控制器，从而显著缩短内存延迟，并帮助客户减少响应时间和提高总体系统性能。Hurricane芯片组独有的关键优势：1. 与上一代产品x365服务器相比，性价比显著提高。2. 切实减少了处理器到内存的延迟，这一点对于企业商业负载来说十分重要。 3. 虚拟XceL4v动态服务器高速缓存可提供智能高速缓存功能，内存延迟极低，以至于不必需物理四级缓存。 4. 具备集成探测过滤器和远程目录功能的嵌入式动态随机访问内存(DRAM)。 5. 3个扩展端口，运行速度均高达6.4GB/秒，在业界速度最快。 6. 高达266MHz的Active PCI-X 2.0，是上一代的两倍，并向后兼容传统PCI和PCI-X。 

 2007年末，IBM终于推出了企业级X架构的第四代芯片组——X4(“Hurricane 4 ”)。相较于上一代的X3架构，X4采用了模块化设计，允许用户将多台基于X4架构的服务器整合起来，形成一个大型的SMP系统，而板载的控制器和多种内存保护技术也确保了服务器的可靠性和可用性。[详见：透视X4架构 解析IBM多路服务器] 
 小结 
 当然，随着AMD和英特尔在CPU设计中纷纷采用超传输直连架构，集成内存控制器，取消北桥芯片，芯片组的地位和作用也正在发生改变。特别是当英特尔在单双路和四路服务器上都引入基于QPI的Nehalem架构之后，服务器厂商在系统设计层面还有哪些可创新之处，是非常值得关注的，或许2010年的IBM eX5会带给我们答案。 
 总之，在X86架构越来越标准化的今天，很多厂商开始寻求标准下的特殊性，为自己寻找有力的市场定位。IBM X4架构延续了成功的X系列优势，即由更高的内存带宽和总线速度，连同嵌入式虚拟化和节能特性，使得它在X86架构中独出一帜。而2010年将出炉的X5架构从高端服务器向中低端产品线的普及，相信会在工业标准服务器市场上独树一帜。"
《智能时代的脑科学与类脑智能研究》读后总结,2024-10-17,"智能时代的脑科学与类脑智能研究 
院刊链接 
名词解释 
1.BPU（类脑处理器）：类脑计算的原理是模拟人脑神经元、突触和互连的结构，来模拟脑的行 为，解决一些认知类的问题。 
2.SNN（脉冲神经网络)：脉冲网络由脉冲神经元连接而成，脉冲神经元输入为脉冲，输出也是脉冲，脉冲神经元内部有电动势v，v在没有接收到任何输入时会随着时间指数衰减到某个稳定的电动势(平衡电压)，而某一时刻接收到输入脉冲时电动势会增加某个值，当电动势增加的速度快过衰减的速度时(如频繁有脉冲输入)，神经元内部的电动势会越来越大，直到达到某个发放阈值后该脉冲神经元会发放脉冲，此后脉冲神经元电动势迅速置为静息电动势，电动势变化过程如下图二所示。电动势变化的规律又称为神经电位动力学。 
3.CNN（卷积神经网络）：一种在计算机视觉领域取得了巨大成功的深度学习模型。它们的设计灵感来自于生物学中的视觉系统，旨在模拟人类视觉处理的方式。 
4.DNN(深度神经网络):一种模拟人脑神经网络结构和功能的计算模型。其基本单元是神经元，每个神经元接收来自其他神经元的输入，通过调整权重来改变输入对神经元的影响。神经网络通过多层的非线性隐藏层，可以实现对复杂函数的逼近，达到万能近似的效果。 
5.C-DNN（互补深度神经网络）： 
6.结构化数据：即行数据,存储在数据库里,可以用二维表结构来逻辑表达实现的数据。 
7.非结构化数据：包括所有格式的办公文档、文本、图片、XML、HTML、各类报表、图像和音频/视频信息等等。 
8.脑科学：是研究人、动物和机器的认知 与智能的本质和规律的科学。 
全文总结 
1.前提：解析人类大脑这一最 为复杂的信息和智能系统，会启迪类脑智能理论和类脑智能技术，即脑科学/神经科学启发的智能理论和技术。 2.物理基础(如：脉冲神经网络智能处理器)为构建大规模类脑智能计算系统奠定了基础，未来类脑超级算力极可能超越人类大脑算力。 
一、智能时代脑科学新领域：类脑智能 
1.智能科技（智能时代的核心科技） 
（1）通过大数据对机器进行训练，使其具有感知、理解、学习和决策等能力。（如：mlp等） 
（2）智能时代生产 力 的 特 征 是 智 能 算 力 ， 超 级 智 能 计 算 系 统 （super intelligent computer）、智能芯片 （intelligent chip） 等 智能计算设施和器件是制造智能算力的关键技术。 
2.类脑智能科技 
（1）“智力蓝图”（intelligence blueprint），其目的是 理解认知等大脑高级功能的物质及结构基础，在防治 认知障碍性疾病的同时，建立一套脑科学的智能理论 和类脑智能科技。 
（2）分为狭义和广义：狭义的是借鉴生物脑的感知行为机制和信息传递机制，构建相关类脑智能算法、模型和系统，通过软硬协同实现机器智能，相比于传统ai计算速度快、能耗少、逻辑分析和推理能力强。广义的包括类脑血管系统等生物脑的非神经元以及神经网络系统。（如：类脑组织工程等） 
（3）目标：在硅基和碳基 上构造类似生物脑的结构和功能/智能，并在智能芯 片、计算机、机器人、脑医学等方面加以应用。 
二、解析脑认知功能网络（数据） 
对人脑感知认知神经网络功能及机 理的研究主要通过脑形态和分子成像、脑机接口等技 术检测神经元及神经环路活动，记录大脑脑区一定时 期内较为完整的神经元动态活动，各类神经递质的存 储、释放、调制等活动过程，解析脑功能的编码与解码机制。（这是物理基础，后面要进行逻辑建模） 
三、开发类脑智能（从建模到应用） 
（1）目前研究方向：① 借鉴人脑存储和计算合二 为一的特点，研发存算一体计算框架和算法模型，突 破“存算墙”的限制；② 实现类脑器件的三维高度集 成，研制出规模化、可扩展的通用存算一体芯片和包 含多种类脑器件的融合类脑芯片；③ 发展适用于类脑 计算的编程语言、工具和编译器；④ 借鉴人脑自主学 习、推理演绎、涌现等高级功能机制，逐步实现机器 的感知智能、自主记忆、推理与抉择、自然语言、多 模态协同感知与社会认知等功能。在智能传感器、便 携式智能信息工具、低能耗高速度和高存储的类脑智 能芯片和计算机等智能系统支撑下，构建类脑智能物联网，实现智能交通、智能医疗、无疆界全方位个性 化服务、智能社会管理和决策体系等新社会形态和 模式。 
（2）目前成果：互 补 深 度 神 经 网 络 （CDNN）、美国 IBM 报道了一种大规模并行、高效能的神经推理架构—Northpole，具有存算一体、主动存储和空间计算的特 点，以及协同优化、高利用率的编程模型、广东省智能院发布了首个以 BPU（BPU 对于非结构化、随机排布、稀疏的数据运算具有更高的处理效率） “天琴芯”和类脑晶圆芯片“天琴芯·海”为技术支 撑的类脑智能计算系统等 
四、结论与展望 
结论：现有AI算力的高能耗以及低效率会阻碍智能技术的应用，类脑智能将打破此壁垒。 
展望：脑科学是智能时代的科技前沿。我 们还可以期待类脑智能超级计算机的算力超过人类大 脑算力，类脑智能算力作为智能时代的新生产力会给 智能科技乃至人类社会带来重大变革。"
基于深度学习的类脑计算,2024-08-18,"基于深度学习的类脑计算（Brain-inspired Computing Based on Deep Learning）旨在模拟人脑的结构和功能，通过借鉴神经科学的原理，构建更智能、更高效的计算模型。类脑计算希望通过模仿大脑的学习和推理机制，推动人工智能（AI）的发展，使得计算系统在解决复杂问题时能够达到或超越人类水平。以下是对这一主题的详细介绍： 
1. 背景与动机 
 人脑的计算能力：人脑以极低的能耗和惊人的效率处理信息，具有强大的学习和推理能力。这种能力远超当前传统计算机和人工智能模型，特别是在处理非结构化数据、理解复杂场景和进行抽象推理等方面。现有深度学习的局限：尽管深度学习在图像识别、自然语言处理等领域取得了巨大成功，但其仍存在诸多局限，如对大数据的依赖、高能耗、解释性差等问题。这些局限激发了对类脑计算的探索，希望通过模拟大脑机制来突破当前AI的瓶颈。
2. 核心思想 
类脑计算基于以下几个关键概念： 
 神经元和突触：模拟大脑中神经元和突触的结构与功能，构建更加接近生物神经网络的人工神经网络。突触可塑性：借鉴大脑中突触的可塑性机制，通过动态调整网络权重和连接，增强模型的学习能力和适应性。并行计算：人脑通过大规模并行计算实现高效的信息处理，类脑计算也借助并行处理能力提高模型的效率和响应速度。自组织特性：大脑能够自发地组织和优化神经网络结构，类脑计算尝试在没有明确监督的情况下自动调整和优化模型。
3. 主要方法 
受启发的神经网络模型 
 脉冲神经网络（Spiking Neural Networks, SNNs）： 
原理：模仿生物神经元通过脉冲（spikes）进行信息传递的过程。SNNs通过脉冲发放时序和频率编码信息，更接近生物神经网络的工作方式。优点：能耗低、计算效率高，尤其适合在神经形态芯片上实现。挑战：训练复杂性较高，需要专门的算法（如STDP）和硬件支持。 卷积神经网络的类脑增强： 
原理：结合大脑的感知机制，对传统卷积神经网络（CNN）进行改进，如引入多尺度特征表示、注意力机制等。应用：提高图像和视频处理任务中的精度和鲁棒性。 类脑的递归神经网络： 
原理：模拟大脑的记忆与循环机制，改进传统递归神经网络（RNN），使其在处理时序数据和长期依赖问题时表现更好。应用：自然语言处理、语音识别等需要处理长序列数据的任务。 
突触可塑性与自适应学习 
 Hebbian学习规则： 
原理：基于“用进废退”的生物学原则，模拟大脑中神经元连接的增强或削弱机制。应用：自适应学习和无监督学习中，提升网络的自主学习能力。 自适应连接强度： 
原理：动态调整网络中的连接强度，模拟大脑中突触可塑性的变化，增强模型的学习与记忆能力。应用：实时学习、连续学习任务，尤其是在环境动态变化的情况下。 
并行计算与神经形态硬件 
 神经形态计算： 
原理：开发模仿生物神经网络工作方式的硬件架构，如IBM的TrueNorth芯片，Intel的Loihi芯片等。应用：大幅提升计算效率，特别是在低功耗和实时处理场景中表现突出。 大规模并行处理： 
原理：模仿大脑的并行处理能力，设计能够支持大规模并行计算的深度学习框架。应用：处理超大规模数据集，适用于复杂任务如图像处理、基因组分析等。 
自组织与动态网络 
 自组织映射（SOM）： 
原理：模仿大脑的自组织特性，在无监督学习中，通过竞争学习算法构建自组织的神经网络结构。应用：用于聚类分析、数据可视化等任务。 动态神经网络： 
原理：模型可以动态调整其结构和参数，模拟大脑在不同情境下的自组织能力。应用：提高模型的适应性和泛化能力，特别是在面对变化环境时。 
4. 应用领域 
 认知计算：模仿人类认知过程，用于开发具有推理、学习和决策能力的智能系统，如智能助理、推荐系统等。机器人控制：通过类脑计算模型，提升机器人在复杂环境中的感知与决策能力，实现更自然的交互与自主导航。医疗诊断：结合类脑计算的强大学习能力，开发智能医疗诊断系统，帮助医生进行复杂病症的分析与判断。大规模数据分析：利用类脑计算的并行处理能力，加速大规模数据集的分析与处理，如基因组数据、气象数据等。
5. 挑战与未来发展 
 硬件支持不足：类脑计算通常需要专门的神经形态硬件，目前这些硬件的开发和普及程度还不够高，限制了类脑计算的应用推广。训练复杂性：类脑模型的训练往往更加复杂，需要新的算法和策略来提高训练效率和模型的可解释性。与神经科学的融合：类脑计算的进一步发展依赖于与神经科学的深入融合，需要更多的跨学科研究来推动该领域的进步。
6. 未来展望 
 跨学科合作：未来类脑计算的发展将依赖于计算机科学、神经科学、物理学等多个学科的紧密合作，通过相互借鉴和融合推动技术进步。神经形态硬件的普及：随着神经形态硬件的不断进步和普及，类脑计算有望在更多实际应用中得到推广，如智能手机、无人驾驶、智能制造等领域。自我进化的人工智能：通过类脑计算模型，开发出能够自我进化、自我优化的人工智能系统，进一步接近人类智能。
基于深度学习的类脑计算代表了人工智能发展的一个重要方向，通过模仿大脑的结构和功能，旨在构建更加智能、高效、灵活的计算系统。这不仅有助于克服当前深度学习模型的局限，还为未来的人工智能发展开辟了新的路径。"
类脑智能与认知计算原理与代码实战案例讲解,2024-08-11,"类脑智能与认知计算原理与代码实战案例讲解 
 
 关键词：类脑智能、认知计算、神经网络、人工智能、深度学习、脑科学、计算神经科学 
 
1. 背景介绍 
在人工智能快速发展的今天，类脑智能和认知计算已成为科技前沿的热点研究领域。这两个领域旨在模仿人脑的结构和功能，创造出更加智能、更具适应性的计算系统。类脑智能致力于构建类似人脑的信息处理系统，而认知计算则聚焦于模拟人类认知过程的计算方法。这些研究不仅推动了人工智能的发展，还为我们理解人类智能提供了新的视角。 
本文将深入探讨类脑智能与认知计算的原理，并通过实际的代码案例来展示这些先进理念如何在实践中应用。我们将从基本概念出发，逐步深入到复杂的算法和模型，最后通过具体的项目实践来巩固理解。 
2. 核心概念与联系 
类脑智能和认知计算虽然是两个独立的研究方向，但它们之间有着密切的联系。类脑智能更多地关注如何构建类似大脑的硬件和软件系统，而认知计算则专注于模拟人类的认知过程和决策机制。两者相辅相成，共同推动了人工智能向更高层次发展。 
以下是类脑智能与认知计算的核心概念及其联系的流程图：</"
英特尔推出人工智能芯片，性能直逼GPU？,2024-04-26,"英特尔正在发布一整套人工智能芯片和系统，希望能在市场上留下一些东西。它的最新条目是一个名为Hala Point的神经形态系统。该系统包括英特尔名为Loihi 2的研究芯片，这是一款奇特的人工智能芯片，其灵感来自人脑的设计和功能。 
“这与基于图形处理单元（GPU）和中央处理器（CPU）的架构所达到的水平相媲美并超过了，”英特尔在其网站上声称。 
值得重申的是：Hala Point是一个研究系统，Loihi 2芯片仍在英特尔的实验室中，尚未上市。 它不会很快取代 Nvidia GPU 的主导地位 。 
相反，它可能是一种新的计算架构的蓝图，在这种架构中，计算会传输到数据。当今的计算设计依赖于数据传输到计算，这造成了瓶颈。 
Hala Point 系统和 Loihi 2 系统追求“稀疏计算”的新概念，美国国防部正在将其作为未来的计算架构进行研究。 
该芯片的架构在处理音频和视频方面更有效，英特尔在上个月发表的一篇研究论文中强调了这一点。 
英特尔神经芯片的前景 
英特尔在 2017 年宣布了其首款神经形态芯片，因为它在扩大制造能力和减小芯片尺寸方面遇到了困境。 
当前的 AI 计算技术涉及将数据移动到计算中，由于存储、内存和处理方面的瓶颈，这被认为对 AI 来说效率低下。Loihi 2 的神经形态设计侧重于在数据点进行高效执行，从而释放带宽和存储空间。 
美国国防部情报高级研究项目活动（IARPA）的项目经理威廉·哈罗德（William Harrod）在超级计算22大会的主题演讲中说，数据的激增使当前的计算模型不可持续，特别是对于后百万兆次级系统。 
Harrod说，IARPA有一个以数据为中心的计算程序，可以“根据数据移动问题而不是ALU的浮点单元来定义计算的未来”。 
该计划名为 AGILE（高级图形智能逻辑计算环境），专注于一种新的架构，将计算转移到数据中，反之亦然，以生成更快的实时结果。 
英特尔、高通和AMD等公司都在推动该计划的新芯片设计和计算机架构。这些公司正在向IARPA提交提案。 
英特尔研究人员上个月发表的一篇研究论文将 Loihi 2 与 Nvidia 的 Jetson Orin Nano 进行了比较。研究人员得出的结论是，Loihi 2 比 Orin Nano 具有功耗和性能优势，但 Nvidia 的芯片在大规模计算时更好。 
硅脑的虚假承诺 
十多年来，芯片制造商和大学一直在开发神经形态芯片。 
这些异国情调的芯片被大肆宣传为计算机带来了更高水平的智能，这是英伟达GPU等人工智能芯片所扮演的角色。当时，IBM和其他公司也在开发神经形态芯片。 
神经形态芯片是概率性的，并且会考虑计算中的不确定性和随机性。这些芯片旨在复制大脑的结构和功能，这本身仍然是一个谜。 
大多数传统的人工智能芯片都基于确定性模型运行，这些模型更精确，并且依赖于机器学习和相关算法。然而，人工智能芯片也可以结合概率模型。 
神经形态芯片中的计算核心扮演神经元的角色，神经元是相互连接的（如突触）。跨突触的神经元计算是并行进行的。低精度计算点通过了解信息中的趋势和关联来得出结论。 
大脑可以通过许多并行工作的神经元来识别猫，而英特尔的芯片也具有类似的功能。 
英特尔的Hala Point将其系统中的计算神经元数量扩大到1.15亿个，这与人脑的规模（约86亿个）相去甚远。但计算能力明显大于其2020年名为Pohoiki Springs的神经形态系统，该系统拥有100亿个神经元。 
英特尔声称，该芯片可以“支持每秒多达 20 万亿次运算，或 20 petaops，在执行传统深度神经网络时，效率超过每秒 15 万亿次 8 位运算/瓦 （TOPS/W）。这些说法尚未得到独立核实。 
神经形态芯片的起源 
英特尔的第一个硅脑项目于2017年问世，但其他芯片开发已经在进行中。IBM正在开发自己的神经形态芯片，而高通正在开发其Zeroth芯片。 
2013年，高通公司发布了Zeroth，首席执行官保罗·雅各布斯（Paul Jacobs）嘲笑了智能手机具有洞察力并预测智能手机用户需求的想法。他的预测花了一段时间才成真，但像谷歌Pixel这样的智能手机现在可以适应用户的需求。 
美国和欧盟在2010年代为类脑芯片提供资金的努力消失得无影无踪。 
从2008年开始，DARPA资助了一个名为Multiphase Synapse（神经形态自适应塑料可扩展电子系统）的项目，该项目涉及IBM，惠普，康奈尔大学，斯坦福大学和其他大学。 
欧盟资助了一项耗资1亿美元的人脑项目，以了解大脑是如何工作的，其中一个组成部分包括开发基于该设计的芯片。人脑计划现已结束。 
IBM仍然是为数不多的开发硅脑的组织之一，去年，它发布了一款名为NorthPole的芯片，该芯片更注重能效，而不是扩展。 
英特尔的神经芯片 
Hala Point 系统包括 Loihi 2 芯片，该芯片采用英特尔 4 工艺制造，每个芯片有 128 个内核。每个芯片包括多达 1 万个数字神经元和 120.2 亿个突触。Loihi <> 的吞吐量是通过促进神经元之间通信的电脉冲实现的。 
 
 
Loihi 2 是英特尔的第二代神经形态研究芯片。（来源：英特尔公司） 
Hala Point 芯片有 1,152 个 Loihi 2 芯片和 1.15 亿个神经元和 128 亿个突触。六机架系统功耗为 2,600 瓦，芯片采用网状设计，具有六个异步并行通道以互连多个 Loihi 2 芯片。 
10Gbps 以太网连接器有助于通信。英特尔表示，网状配置的范围可以从一个到“数千个芯片”。 
英特尔在一篇研究论文中表示：“进一步的发展将使神经形态计算的应用能够克服目前限制人工智能功能在现实世界中实时部署的功耗和延迟限制。 
这些芯片是人工神经网络，可以执行许多并发的音频和视频功能。计算是稀疏的，并且更多地在数据所在的位置执行。 
英特尔的研究人员在论文中写道：“虽然今天的GPU、Tensor处理器和深度学习加速器专注于以非常高的吞吐量进行基于密集矩阵的计算，但神经形态处理器专注于稀疏事件驱动的计算，最大限度地减少活动和数据移动。 
英特尔的研究将其人工神经网络芯片与大约一年前发布的Jetson Orin Nano芯片进行了比较。 
 
 
英特尔 Hala Point 系统由英特尔 Loihi 2 神经形态处理器组成。（出处：英特尔公司） 
该研究论文得出的结论是，与稀疏计算模型上的Jetson芯片相比，Loihi 2芯片上的神经元仅在需要时激活。这样更节能，更好地利用了带宽和其他计算资源。 
然而，当计算核心完全启动并在可比带宽内大规模运行时，Jetson Orin 在大型操作中具有优势。 
芯片适合的位置 
Loihi 2在重组期间幸免于Pat Gelsinger的斧头，因此该公司可能会看到研究的价值。Hala Point部署在桑迪亚国家实验室，该实验室测试各种节能AI的芯片和配置。 
该芯片也是英特尔稳定的人工智能芯片的一部分，其中包括 Gaudi 3 和 Ponte Vecchio GPU。它目前不会对任何 CPU 或 GPU（包括英特尔的产品）构成任何威胁。 
Neuromorphic 是英特尔考虑其未来的另一个芯片概念。使芯片在商业上可行是一项挑战。与量子处理器一样，这些芯片将需要软件框架和算法。"
2023人工智能与类脑计算产业发展研究报告,2024-04-09,"今天分享的是人工智能专题系列深度研究报告：《人工智能专题：2023人工智能与类脑计算产业发展研究报告》。 
（报告出品方：中国赛宝智库） 
类脑计算的基本概念 
研究背景 
近年来，人工智能(Artificial Intelligence，AI)在经济发展、社会进步、国际政治经济格局等方面已经产生重大而深远的影响。现代计算系统对计算资源的需求与日俱增，尤其是日益成为生活一部分的复杂人工智能应用。虽然不是所有数据密集型计算都需要人工智能进行深度学习，但深度学习的部署如此广泛,以至于不得不担心其计算成本。例如，云平台的普及使得人们习惯于近乎即时地获取信息，忽略了A应用背后所消耗的资源。以DeepMind 公司的 AlphaGo 和 AlphaZero 为例，这些复杂的战略游戏需要成千上万的并行处理单元来打败人类玩家,每个单元的消耗约为 200 瓦特!1。因此,如何更好地管理计算资源，降低能源消耗，是未来人工智能发展中需要解决的重要问题之一。 
研究表明，现代计算系统对计算能力的需求不断增长，这种增长远远超过了按照摩尔定律发展取得的进步。英伟达(Nvidia)公司的首席科学家兼高级工程师 Bil Dally表示，2012年11月至2020年5月期间，Nvidia 芯片的性能在人工智能计算中提高了317 倍，远远超出了摩尔定律的预期，同时单元的功耗也随之增长。究其原因，为了AI应用的表现力精益求精，AI模型的复杂度日益攀升，使其计算成本也不断提高。传统的计算解决方案已无法满足长期需求，特别是考虑到极度复杂的深度学习模型所需训练成本时，需要寻找替代新的解决方法。 
 
 
当今大多数与人工智能相关的算法都运行在基于经典冯·诺依曼体系结构的传统的计算系统上(见图2)，例如中央处理单元(CPU)图形处理单元(GPU)和现场可编程门阵列(FPGA)。在执行计算任务时，大量数据需要高速来回传输，处理器花了大部分时间和精力来移动数据，极易造成性能瓶颈并导致显著的面积/功率效率低下。造成问题的主要原因是，计算系统将数据与它们的处理位置分开存储所造成结果。因此，如何过渡到新型架构并建立高效的认知型计算，是十分迫切解决的问题。 
 
 
建立高效的认知型计算是目前人工智能发展中的一个关键挑战。由于深度学习模型所需的训练成本非常高，传统的计算架构已经无法满足长期需求。因此，需要寻找替代新的解决方法来降低计算资源的消耗和能源消耗。其中一个备受关注的解决方案是类脑计算，它采用类似于人类大脑的计算模式来进行高效的认知型计算。 
类脑计算 
经过长期的研究和探索，类脑计算已经成为解决以上问题的一种潜在途径。人类大脑的记忆和处理是高度交织在一起的，以一种完全不同的方式对信息进行编码或直接对信号进行操作。人类大脑以每焦耳皮特运算速度，其超低功耗的认知能力为新型计算机提供了“存在证明”。鉴于人类大脑运行模式的启发，通过存储器和处理器的协同定位由大规模神经元和可塑性突触组成计算以及基于脉冲的通信和信息处理等技术，能够更好的使计算机模拟人类大脑的运作方式。类脑计算在智能机器人、大数据分析等领域具有广泛的应用前景，可以实现更加高效、智能的计算和决策，进而推动人工智能技术的发展。 
Mehonic 的观点表明,类脑计算至少包括三个广泛的研究目标群体，包括神经形态工程(Neuromorphic Engineering，如大脑逆向工程)、神经形态计算(Neuromorphic Computing)和类脑仿生电子设备设计[21。其含义如下: 
神经形态工程：从生物学、物理学、数据、计算机科学和电子工程等领域汲取灵感的跨领域学科。围绕大脑如何利用生物突触和神经元(大脑的基础单位2)完成计算(功能)这一问题，主要研究基于生物神经系统启发人工神经系统(Artificial Neural Systems)的物理架构和设计原则。 
神经形态计算：受人脑结构和功能启发，模拟人脑与世界的交互方式的计算方法。研究旨在模拟生物神经网络结构化的和/或操作(像大脑一样协同定位存储和计算)、生物系统动作电位的电压尖峰3(如SNN4，SpikingNeuralNetwork，脉冲神经网络)。神经形态计算是一个新兴的科学领域，各个团体正在进行研究，包括大学、美国军方和技术公司(如英特尔实验室和 IBM) 
类脑仿生电子设备：支撑类脑计算功能的设备和材料。通过模仿突触和神经元等生物元素，研究实现类脑计算功能的电子和光子设备(如忆阻器，可模拟生物树突的某些功能1)，以扩展神经形态工程和神经形态计算的能力。 
清华大学张悠慧教授等人受传统计算系统层次结构的启发，提出了一个类脑计算系统层次结构，包括软件、编译器和硬件。 
硬件层：包括类脑芯片和架构模块，以抽象神经形态架构作为硬件抽象。具体地，该层包括一个执行基元图(EPG)，作为链接上层的接口,用于描述它能执行的程序。EPG采用控制流-数据流混合表示法，最大限度地提高了对不同硬件的适应性。 
编译层：是类脑计算系统框架的中间层，将程序转换为硬件支持的等效形式。为了提高可行性，张悠慧团队提出了一套主流类脑芯片广泛支持的基本硬件执行基元，并证明了配备这套基元的硬件是神经形态完备的。还实现了一个工具链软件作为编译层的一个实例，以证明层次结构的可行性、合理性和优势。 
软件层：软件指的是编程语言或框架，以及在此基础上构建的算法或模型。在类脑计算系统框架中，包含了统一而通用的软件抽象模型--编程运算符图(POG)，用于适应各种类脑算法和模型设计。POG由统一的描述方法和事件驱动的并行程序执行模型组成，整合了存储和处理功能。POG 描述了类脑程序的内容,并定义了程序的执行方式。由于 POG 是图灵完备的，因此它能最大程度地支持各种应用程序、编程语言和框架。 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
报告来源/公众号：【海选智库】 本文仅供参考，不代表我们的任何建议。海选智库整理分享的资料仅推荐阅读，如需使用请参阅报告原文。"
芯片学习笔记,2024-04-01,"芯片及类脑芯片内容的学习记录） 
 一些混淆概念的介绍普通芯片cpu中的矩阵计算gpu中的矩阵计算 
ai芯片卷积变矩阵相乘ai芯片中的矩阵相乘Nvidia TensorCore矩阵相乘过程Google TPU矩阵相乘过程 
 
类脑芯片忆阻器TureNorthLoihi芯片天机Tianjic芯片苏轼SUSHI芯片（中科院-2023年） 
相关问题 
 
 
 
 仅个人学习记录，有错误还希望被指出 
 
 
 本来只是想了解芯片中矩阵相乘的具体操作过程，看了一堆资料和讯息，就将了解到的记录下来了 
 
一些混淆概念的介绍 
推荐视频和链接： 1、芯片的介绍 2、硬件知识cpu/gpu/tpu/npu相关知识 3、简单的聊聊 NPU ，Tensor core 
CPU的主要计算单元是标量运算，当然后面出来的SSD,AVX指令不包含在里面GPU的主要计算单元是矢量运算，现在一般都是SIMD,然后多个SIMD组成MIMDNPU的主要计算单元是矩阵运算. 矩阵运算器就是tensor core.TPU英文全称Tensor Processing Unit，中文全称为张量处理单元，是深度学习算法的专用芯片。一般情况下，npu和tpu就是ai芯片的核心，cpu和gpu就是简单处理一些计算。但这只是一般情况。 
普通芯片 
1、普通的芯片是遵循冯·诺依曼计算架构的。 2、cpu和并行加速运算的GPU都是属于普通芯片。 3、GPU在早期是为了处理图形数据。gpu相当于是10000个加法题找很多的幼儿园学生去做。效率也就高了很多。 4、cpu/gpu支持矩阵相乘的库：cpu（OpenBLAS、Intel MKL)、gpu（cuBLAS、cuDNN） 
cpu中的矩阵计算 
推荐链接：cpu中的矩阵计算 主要的计算过程就像下面写的代码一样： 
 for (unsigned int i = 0; i < hA; ++i){
for (unsigned int j = 0; j < wB; ++j) {
float C[i][j] = 0;
for (unsigned int k = 0; k < wA; ++k) {
C[i][j] += A[i][k] * B[k][j];
}
C[i][j] = Cij ;
}
 }
123456789 
就是按照最原始的线性代数的方式，不断的从内存中获取值并计算。下图采用的是行优先存储，也就是按照先一行行的存储再进行下一列的，正好符合上述代码。具体计算过程见上述链接，本文只叙述大概的过程。 
gpu中的矩阵计算 
推荐链接：gpu中的矩阵运算、更详细的不同gpu计算方法 gpu中为了实现并行运算，主要的思想还是将矩阵分为多个子矩阵，然后同时计算他们的乘积。具体计算过程见上述链接，本文只叙述大概的过程。 
ai芯片 
1、ai芯片也是遵循冯·诺依曼架构的 2、ai芯片主要用于处理深度学习，专门用于机器学习的芯片。（例如：Google TPU系列、NPU） 3、ai芯片的指标： 1）OPS（Operations Per Second）； 2）MACs（Multiply-Accumulate Operations）； 3）FLOPs（Floationg Point Operations）； 4）MAC（Memory Access Cost） 
卷积变矩阵相乘 
在研究ai芯片矩阵相乘的过程中，要知道卷积变为矩阵相乘也是一个小内容。 例如：下图中为卷积变矩阵相乘的过程，但是途中的1234为位置编号，非数值。
ai芯片中的矩阵相乘 
推荐链接：AI芯片：高性能卷积计算中的数据复用、芯片原理：NPU矩阵乘法加速详解 
Nvidia TensorCore矩阵相乘过程 
简单来说，就是在矩阵相乘的过程中矩阵a和矩阵b中需要相乘的每一个过程都分别计算出来。 
第一步就是先分别计算需要的相乘部分；第二步就是按照矩阵相乘的要求，再进行相乘部分的相加。 下图是博主HaoBBNuanMM 给出的一个解释，很形象。 
图中解释的是A*B+C的过程，1-3步解释的是A*B，第四步是相加。图中不好理解的时候可以倒着推一下过程，A*B的结果矩阵中(i,j)位置上的数是怎么得来的，再去看第2步和第1步。上图只是针对4*4的矩阵过程，如果是8*8。此时并行就需要把大的矩阵转化为小矩阵进行操作计算。如图 
 
总结：简单来说此时的计算就是需要拆分计算过程，优先乘法，再加法。需要添加一个3维的硬件计算单元。（这也是为什么nv要求用户模型中的矩阵大小最好是4的倍数） 
Google TPU矩阵相乘过程 
官网介绍：张量处理单元 (TPU) 是 Google 定制开发的应用专用集成电路 (ASIC)，用于加速机器学习工作负载。如需详细了解 TPU 硬件，请参阅系统架构。Cloud TPU 是一项网络服务，可让 TPU 作为 Google Cloud 上的可伸缩计算资源使用。 
TPU 使用专为执行机器学习算法中常见的大型矩阵运算而设计的硬件，更高效地训练模型。TPU 具有芯片上高带宽内存 (HBM)，让您可以使用较大的模型和批量大小。TPU 可以组成 Pod 组，这样无需更改代码即可纵向扩容工作负载。 
博主HaoBBNuanMM给出的一个计算过程解释，感觉比官网形象。 就是两个矩阵按照特定的方式进入MAC计算阵列，边相乘边相加。
类脑芯片 
1、类脑芯片是不遵循冯·诺依曼架构，采用的是存算一体的方式进行计算和处理数据。 2、现在的类脑芯片研究主要是进行人工智能边缘计算，突破冯·诺依曼架构，提供一种新的创新发展路径。 3、现如今发展的类脑芯片有： 
TrueNorth芯片（IBM-2014年）英特尔Loihi芯片（英特尔-2018）天机Tianjic芯片（清华大学-2019nature）达尔文Darwin芯片（浙江大学和之江实验室-2020年）英特尔Loihi2芯片（英特尔-2021年）苏轼SHSHI芯片（中科院-2023年） 
4、忆阻器也会处理double类型的数据。 5、即使现在有很多框架可以让snn跑在ai芯片上(例如:spikingjelly)，但是实际结果并不具备压倒性的优势。 
忆阻器 
忆阻器是一种存算一体电子突触。忆阻器是一种具有记忆功能的非线性电阻。具体构造解说见：忆阻器芯片介绍 简单来说就是一种电阻，受流过的电荷影响而变化的元件。 
说明： 
忆阻器的电阻值通电结束后是不变的，需要重新输入权重的话，再来个电压给电阻值重新刷新即可忆阻器阵列，其实是一种利用物理规律完成的模拟计算。 
举例说明忆阻器的应用过程： 现在有一个矩阵和一个向量相乘：
然后如下图所示，小圆圈部分是忆阻器元件，我们可以通过在1,2,3号竖线上加电压，分别表示向量中的三个值。让9个小圆圈（忆阻器）的电导和矩阵中的数值对应起来。电流=电导*电压。那么此时横向方向上在电线位置测量一下总电流就是矩阵的结果。
应用： 1、2015年，IBM用一个12*12的阵列实现了三个字母的识别 2、2017年，清华大学吴华强团队用128*8的阵列实现了人脸识别等等 
模拟计算中，计算精度是一个很大的考验。毕竟以二进制为代表的数字计算只需要管有电和没电，所有的状态都转化为0101数字再进行数学运算，对物理精度完全不关心。但是模拟计算中，电压电流就代表数值，每一点微小的变化就会带来计算数字的变化。 那么由此可见，提高模拟计算的精度就变成了一个新的问题。清华大学吴华强团队（研究出全球首个多阵列忆阻器存算一体芯片）的一些改进： 
1、阻值变化不稳定问题 
使用千层饼结构，导电层和不导电层交替叠加，使其成为了一个千层饼结构，并且在千层饼之上又增加了一个热导率低的热交换层。 
2、电流的不稳定 
增加限流电路把忆阻器分为正向权重和负向权重。相同电压下，一个电阻值增加，一个电阻值降低。增加权重，更新正向。减少权重，更新负向。
3、算法 
误差模型。通过压力训练，让AI自己寻找误差规律。 
TureNorth 
推荐视频：truenorth 1M的神经元，每个神经元256个轴突输入。 
计算过程如下图： 横向的线是轴突，用于接受上一层神经元信息。 纵向的是树突，用于每个下层神经元接收信息。 横向和纵向的交叉点就是突触，每个突触会有不同的权重 
过程：来自上层神经元的信息通过轴突再通过突触最后汇聚到下方的神经元。通过激活函数的处理，产生新的脉冲再传递到更下一层的神经元。
整个芯片没有全局的时钟，靠脉冲驱动。也就是说脉冲来了就计算，没有脉冲就休息。但是芯片还是基于CMOS架构。实际采用的还是传统的计算。物理结构如下图。 
数据存储到SRAM中，再采用一个本地时钟进行同步。物理的神经元只有一个，它把时间分成小块，在一个计算电路中依次计算256个神经元的方程，此处没有并行运算。就和传统的cpu一样挨个计算。
Loihi芯片 
218个神经内核和三个嵌入式x86处理器，每个内核有1024个尖峰神经单元。神经元突触的密度比TrueNorth高了三倍。 
天机Tianjic芯片 
速度比TrueNorth提高了10倍，带宽提高了至少100倍。最关键的是把ANN和SNN的计算逻辑合并到了一个计算单元中。通过巧妙的打开和关闭乘法运算单元，同时兼容ANN和SNN。 
苏轼SUSHI芯片（中科院-2023年） 
 
推荐视频：sushi超导神经形态芯片 1、采用超导神经形态芯片。需要极低的超导温度。 2、RSFQ:快速单通量量子 3、Josephson junctions约瑟夫森结 4、半导体中实现snn：硬件模拟出神经元，然后构建加法器，把脉冲权重累加一下做一个判断，超过阈值就输出一个1，然后发送给下一级的神经元。（TrueNorth就是这样实现的） 01脉冲通过晶体管的通断来维持，线路打开，电流通了就是1，关闭就是0。会有一个全局时钟来控制。5、分离器可以复制脉冲
超导中会遇见的问题—————————————————————— 
同步时钟：超导中RSFQ就不可能像半导体一样，脉冲是随机产生的，所以需要一个同步时钟。存储器拖后腿：无法满足快速计算的要求 芯片集成度不够：很难形成大规模 
中科院的创新SUSHI—————————————————————— 状态控制器：完成数据的存储；状态的翻转，会在脉冲来了的情况下触发四种状态：1）当前是0，不翻转；2）当前是0，翻转；3）当前是1，不翻转；4）当前是1，翻转；
状态控制器完成了激活函数的计算。将状态进行单位统一，便于定量分析。例如：500个状态就可以记录整个神经元的放电过程的所有状态，只需要用状态控制器表示这500个状态就可以，就相当于用计数器在记录倒水的次数。 
那不同的状态怎么变为这统一定量下的标准呢？这就采用了超导脉冲神经网络（Superconductuing SNN,SSNN），绕开了传统的权重计算过程。 
传统计算如下，第一个权重是0.3，第二个权重是0.5。
改进就是统一单位，数数量。本来是按照权重计算大小，现在只需要数脉冲个数了。使用分离器将脉冲进行复制。 
网络框架 
横向是轴突，纵向是树突。树突的末尾是神经元。横向和纵向交叉的地方就是突触。 权重的逻辑，就是统一单位定量的地方就在这里。每个突触上设计了一个开关，可以决定更新脉冲到达的时候要不要更新权重。这样就可以并行更新所有需要更新的权重了。 
计算过程就是脉冲沿上一级神经元的轴突横向传过来，在每一个突触上根据权重被复制出不同的份数，然后这些脉冲再沿着树突传播，汇聚到神经元上。根据累计的状态来决定是否需要发射一个新脉冲到下一个神经元。
位片处理切片 为了在较小的规模下运行很大规模的网络，还采用了一种位片处理的方法把网络进行切片。按照神经网络的层分为很多片。这样就可以重复利用计算核心，以时间换空间。 
相关问题 
1、我还是不太清楚，类脑计算中的数值在权重计算时，还是以double类型存在吗？ 2、类脑芯片中处理的数据是ann还是snn（spike neural network）还是都有呢？ 3、类脑芯片的核心是存算一体还是说脉冲计算呢？"
AI芯片资料概述,2024-10-12,"一、背景
1、AI芯片发展背景

AI模型原先只是针对特定应用场景需求进行训练的小模型，小模型通用性差，换到另一个场景可能并不适用，需要重新训练，这其中需要消耗很多成本和时间。而随着数据、算力以及算法的提升，AI技术也有了很大的变化，从过去的小模型发展到现如今大模型的兴起。

大模型就是Foundation Model（基础模型），指通过在大规模宽泛的数据上进行训练后能适应一系列下游任务的模型。现如今很多企业都投身于大模型的研发当中，并且有些企业已经开发出了可以使用的、比较成熟的大模型产品，比如百度的文心一言、智谱华章的智谱清言、百川智能的百川大模型等。而无论是大模型的训练，还是大模型的推理环节，都需要较为庞大的数据量的传输，因此，现如今对AI芯片的需求快速增长，其市场规模也增长显著。

二、简介

AI芯片也被称为AI加速器或计算卡，从广义上讲只要能够运行人工智能算法的芯片都叫作 AI 芯片。但是通常意义上的 AI 芯片指的是针对人工智能算法做了特殊加速设计的芯片。

三、AI芯片分类
1、按照芯片的技术架构分
GPU

全名叫Graphics Processing Unit，即图形处理单元，在传统的冯·诺依曼结构中， CPU 每执行一条指令都需要从存储器中读取数据， 根据指令对数据进行相应的操作。 从这个特点可以看出， CPU 的主要职责并不只是数据运算， 还需要执行存储读取、 指令分析、 分支跳转等命令。深度学习算法通常需要进行海量的数据处理，用 CPU 执行算法时， CPU 将花费大量的时间在数据/指令的读取分析上， 而 CPU 的频率、 内存的带宽等条件又不可能无限制提高， 因此限制了处理器的性能。 而 GPU 的控制相对简单，大部分的晶体管可以组成各类专用电路、多条流水线，使得 GPU 的计算速度远高于 CPU； 同时，GPU 拥有了更加强大的浮点运算能力，可以缓解深度学习算法的训练难题，释放人工智能的潜能。但 GPU 无法单独工作，必须由 CPU 进行控制调用才能工作， 而且功耗比较高。

半定制化的 FPGA

全名叫Field Programmable Gate Array，即现场可编程门阵列，其基本原理是在FPGA芯片内集成大量的基本门电路以及存储器，用户可以通过更新 FPGA 配置文件来定义这些门电路以及存储器之间的连线。与 GPU 不同， FPGA 同时拥有硬件流水线并行和数据并行处理能力， 适用于以硬件流水线方式处理一条数据，且整数运算性能更高，因此，常用于深度学习算法中的推理阶段。不过 FPGA 通过硬件的配置实现软件算法，因此，在实现复杂算法方面有一定的难度。将 FPGA 和 CPU 对比可以发现两个特点， 一是 FPGA 没有内存和控制所带来的存储和读取部分，速度更快， 二是 FPGA 没有读取指令操作，所以功耗更低。 劣势是价格比较高、编程复杂、整体运算能力不是很高。 目前，国内的 AI 芯片公司如深鉴科技就提供基于 FPGA 的解决方案。

全定制化 ASIC

全名叫Application-Specific Integrated Circuit，即专用集成电路，是专用定制芯片，即为实现特定要求而定制的芯片。定制的特性有助于提高 ASIC 的性能功耗比，缺点是电路设计需要定制，相对开发周期长， 功能难以扩展。 但在功耗、可靠性、 集成度等方面都有优势，尤其在要求高性能、低功耗的移动应用端体现明显。谷歌的 TPU、寒武纪的 GPU，地平线的 BPU 都属于 ASIC 芯片。谷歌的 TPU 比 CPU 和 GPU 的方案快 30 至 80 倍，与 CPU 和 GPU 相比， TPU 把控制电路进行了简化，因此，减少了芯片的面积，降低了功耗。

神经拟态芯片

神经拟态计算是模拟生物神经网络的计算机制。 神经拟态计算从结构层面去逼近大脑，其研究工作还可进一步分为两个层次，一是神经网络层面，与之相应的是神经拟态架构和处理器，如 IBM 的 TrueNorth 芯片，这种芯片把定制化的数字处理内核当作神经元，把内存作为突触。 其逻辑结构与传统冯·诺依曼结构不同：它的内存、CPU 和通信部件完全集成在一起，因此信息的处理在本地进行，克服了传统计算机内存与 CPU 之间的速度瓶颈问题。同时，神经元之间可以方便快捷地相互沟通，只要接收到其他神经元发过来的脉冲(动作电位)， 这些神经元就会同时做动作。 二是神经元与神经突触层面，与之相应的是元器件层面的创新。如 IBM 苏黎世研究中心宣布制造出世界上首个人造纳米尺度的随机相变神经元，可实现高速无监督学习。

2、按功能分

根据AI算法步骤，可分为训练（training）和推理（inference）两个环节。训练卡一般都可以作为推理卡使用，而推理卡努努力不在乎时间成本的情况下大部分也能作为训练卡使用，但通常不这么做。训练芯片通常拥有更高的计算能力和内存带宽，以支持训练过程中的大量计算和数据处理。相比之下，推理芯片通常会在计算资源和内存带宽方面受到一定的限制。同时，二者支持的计算精度也通常不同，训练阶段需要高精度计算，因此常用高精度浮点数如：fp32，而推理阶段一般只需要int8就可以保证推理精度。

除了高带宽高并行度外，就片内片外的存储空间而言训练芯片通常比较“大”，这是训练过程中通常需要大量的内存来存储训练数据、中间计算结果以及模型参数。相较而言推理芯片可能无法提供足够的存储容量来支持训练过程。

训练卡

训练环节通常需要通过大量的数据输入，训练出一个复杂的深度神经网络模型。训练过程由于涉及海量的训练数据和复杂的深度神经网络结构， 运算量巨大，需要庞大的计算规模， 对于处理器的计算能力、精度、可扩展性等性能要求很高。目前市场上通常使用英伟达的 GPU 集群来完成， Google 的 TPU 系列 、华为昇腾 910 等 AI 芯片也支持训练环节的深度网络加速。

推理卡

推理环节是指利用训练好的模型，使用新的数据去“推理”出各种结果。与训练阶段不同，推理阶段通常就不涉及参数的调整优化和反向传播了，它主要关注如何高效地将输入映射到输出。这个环节的计算量相对训练环节少很多，但仍然会涉及到大量的矩阵运算。在推理环节中，除了使用 CPU 或 GPU 进行运算外， FPGA 以及 ASIC 均能发挥重大作用。典型的推理卡包括NVIDIA Tesla T4、NVIDIA Jetson Xavier NX、Intel Nervana NNP-T、AMD Radeon Instinct MI系列、Xilinx AI Engine系列等。

四、国内AI芯片
1、AI芯片各参数含义

在查阅国内AI芯片相关资料时，会发现在各个厂商的官网上其AI芯片产品会有很多参数，如表1所示，不同参数的高低代表该芯片会应用于不同的场景，比如高精度、高内存、高带宽可能意味着该芯片适合应用于大模型的训练，而比较低的带宽和内存以及比较低的精度意味着该芯片可能用于大模型的推理环节。由于目前主流的AI处理器无疑是NVIDIA的GPU，并且，英伟达针对不同的场景推出了不同的系列和型号，因此将针对英伟达GPU各个产品的功能和参数详细解释AI芯片各个参数的含义。

表1

目前NVIDIA的GPU产品主要有 GeForce、Data Center/Tesla 和 RTX/Quadro 三大系列，如下图所示，虽然，从硬件角度来看，它们都采用同样的架构设计，也都支持用作通用计算(GPGPU)，但因为它们分别面向的目标市场以及产品定位的不同，这三个系列的GPU在软硬件的设计和支持上都存在许多差异。其中，GeForce为消费级显卡，而Tesla和Quadro归类为专业级显卡。GeForce主要应用于游戏娱乐领域，而Quadro主要用于专业可视化设计和创作，Tesla更偏重于深度学习、人工智能和高性能计算。

三大系列里每一系列又有多种产品，具体如下：

GeForce：

RTX 40系列（4080，4070Ti，4070，4060Ti，4060），

RTX 30系列（3090Ti，3090，3080Ti，3080，3070Ti，3070），

RTX 20系列（2080Ti，2080 SUPER，2080，2070 SUPER，2070），

GTX 16系列（1660Ti，1660 Super，1660， 1650 Super，1650（G5））

NVIDIA RTX/Quadro：

NVIDIA Ampere 架构：NVIDIA A40，NVIDIA A10，NVIDIA RTX A6000

NVIDIA Turing 架构：NVIDIA Quadro RTX 8000，NVIDIA Quadro RTX 6000

数据中心/Tesla：H100（H800），A100（A800)，A40，A30，A16，A10，A2，L40，L40S，L4，V100......常见型号的参数如下：

  其中，A800/H800是针对中国特供版（低配版），相对于A100/H100，主要区别：

A100的Nvlink最大总网络带宽为600GB/s，而A800的Nvlink最大总网络带宽为400GB/s。

H100的Nvlink最大总网络带宽为900GB/s，而A800的Nvlink最大总网络带宽为400GB/s。

  随着美国新一轮的芯片制裁，最新针对中国特供版H20、L20和L2芯片也将推出。

  板卡型号&芯片型号

  板卡通常指的是一个整体的硬件设备，它由多个组件组成，包括主要的芯片（例如CPU、GPU、FPGA等）、电路板、连接器、内存、电源管理等。板卡型号通常描述了整个硬件设备的型号或型号系列，以及其在特定领域或用途中的特性。比如，在图形处理领域，NVIDIA的显卡产品通常由一个板卡组成，其型号可能是类似于RTX 3080、GTX 1660 Ti等。

  芯片则是板卡中的一个关键组件，它是一块半导体晶片，承担着计算、处理、存储等功能。CPU（中央处理器）、GPU（图形处理器）、ASIC（专用集成电路）等都是不同种类的芯片。芯片型号通常指的是这块半导体晶片的型号或系列，描述了其设计、制造和性能特征。比如，Intel的处理器可能有型号如i7-11700K、i9-12900K等。

  联系：板卡通常包含了多个芯片，其中至少会有一个主要的芯片承担着核心的计算或处理任务。这些芯片的型号构成了板卡的关键组成部分。

  区别：芯片型号是指半导体晶片的型号，描述了芯片本身的性能、制造工艺、规格等；而板卡型号是指整个硬件设备的型号，包括了多个组件，描述了整体设备的特性、用途等。

  总体来说，芯片型号是指单个芯片的型号，而板卡型号则是指整个硬件设备的型号，两者之间具有层次关系，板卡中的芯片型号决定了板卡的核心性能。

  如下表所示，NVIDIA公司的一些常用板卡以及其芯片型号。

	

NVIDIA H100

	

NVIDIA A100

	

NVIDIA Tesla V100

	

NVIDIA Tesla P100




GPU型号

	

GH100

	

GA100

	

GV100

	

GP100

  GPU架构

  GPU架构是指图形处理器单元的设计和组织方式，包括处理器核心的数量、缓存结构、内部总线、存储器控制器等方面的设计。这些设计决定了GPU如何执行并行计算任务以及其在处理图形和其他计算密集型工作负载时的性能表现。

  每一代 GPU 都有其独特的架构，随着技术的进步和新功能的引入，GPU架构也会不断演变和改进。例如，NVIDIA GPU架构历经多次变革，从起初的Tesla，到Turing架构，再到Ampere、Hopper，发展史可分为以下时间节点：

  GPU架构的进步通常意味着更高的计算性能、更高的能效比、更好的并行计算能力、新的功能和特性的引入，以及更好的支持各种应用和工作负载。架构的不同特点和优势会影响到GPU在游戏、科学计算、深度学习等领域的表现和适用性。

  对开发者和研究人员来说，了解GPU架构是重要的，因为它有助于优化程序以充分利用GPU的性能，并且能够更好地理解和评估不同GPU型号在不同应用场景下的性能差异。

  接口

	

NVIDIA H100

	

NVIDIA A100

	

NVIDIA Tesla V100

	

NVIDIA Tesla P100




接口类型

	

SXM5/PCIe 5.0

	

SXM4/PCle 4.0

	

SXM2/PCle 3.0

	

SXM/PCle 3.0

  芯片接口类型指的是连接芯片（比如GPU）与主板或其他设备之间的物理接口标准。这些接口标准规定了芯片与系统之间数据传输的方式、速度和通信协议，不同的接口类型有着不同的特点和用途，常见的有PCle接口、SXM接口、OAM接口等，下面简要介绍各个类型：

PCIe（Peripheral Component Interconnect Express）：PCIe是一种常见的高速串行接口标准，用于连接各种设备（如显卡、网卡、存储设备等）到主板上。它提供高带宽和低延迟的数据传输，常用于连接图形卡（GPU）到主板上，通过PCIe插槽进行数据传输。

SXM（Server-Grade System Mezzanine）：SXM是一种特定于服务器级别的芯片连接标准。SXM模块通常是一种插槽式模块，比如NVIDIA的Tesla系列的GPU常常使用SXM连接方式，这种方式通过插槽或者连接器直接与服务器主板相连。

OAM（Omnipath Adapter Module）：OAM是Intel推出的用于高性能计算和数据中心的高速互连标准。它不仅包括了物理连接接口，还涵盖了数据传输的协议和管理等方面。OAM用于连接高性能计算资源（如CPU、GPU等）以及网络设备，提供高速、低延迟的数据传输。

  用途和应用场景：PCIe是通用的外部设备连接标准，SXM更多用于连接服务器级别的GPU加速器，而OAM主要用于高性能计算和数据中心的高速互连。

  连接方式：PCIe通常通过插槽连接到主板，SXM直接插入服务器主板上的插槽，而OAM涉及到高速互连和通信协议。

  功耗

  芯片的最大热设计功率（TDP）是指在特定工作负载下，处理器或其他芯片能够稳定运行时所产生的最大热量。这个指标通常以瓦特（W）为单位表示，下表是一些比较常用的NVIDIA芯片的TDP数据。

	

NVIDIA H100

	

NVIDIA A100

	

NVIDIA Tesla V100

	

NVIDIA Tesla P100




功耗

	

700W/350W

	

250/300/400W

	

250/300/450W

	

250/300W

  可以看到，每一个板卡下对应的TDP有多个，这其实是表示该板卡内芯片以不同的连接方式连接时所导致的实际功耗不一样，比如NVIDIA H100板卡有两种封装形式，分别是SXM、PCle，两种连接方式对应的各种参数对比：

  从上图可以看出，接口类型是SXM的板卡功耗要比接口类型是PCIe的板卡功耗要高得多，那么出现这个现象的原因如下：

  虽然NVIDA GPU-SXM和NVIDA GPU-PCIe这两种卡都能实现服务器的通信，但是实现的方式是不一样的。SXM规格的一般用在英伟达的DGX服务器中，通过主板上集成的NVSwitch实现NVLink的连接，不需要通过主板上的PCIe进行通信，它能支持8块GPU卡的互联互通，实现了GPU之间的高带宽。

  这里说的NVLink技术不仅能够实现CPU和GPU直连，能够提供高速带宽，还能够实现交互通信，大幅度提高交互效率，从而满足最大视觉计算工作负载的需求，因此这种类型接口的板卡功耗就比较高。

  而NVIDA GPU-PCIe就是把PCIe GPU卡插到PCIe插槽上，然后和CPU、同一个服务器上其他的GPU卡进行通信，也可以通过网卡与其他的服务器节点上的设备进行通信，这种就是PCIe的通信方式，但是这种传输速度不快。如果想要和SXM一样，有很快的传输速度，可以使用NVlink桥接器实现GPU和CPU之间的通信，但是和SXM不一样的地方就是它只能实现2块GPU卡之间的通信。也就是说，如果有 2 个 PCIe GPU，那么可以使用 NVLink 桥接器（Bridge）实现互联；如果超过 2 个 PCIe GPU，就无法实现 NVLink 的分组互联，此时只能考虑使用 SXM GPU。基于此GPU连接数量的限制，其工作负载的需求必然比SXM要低，所以其功耗就比SXM板卡低。

  GPU显存

  不难看出，在各个厂商的芯片产品官网上，对应产品的信息中往往都会存在显存信息，接下来会具体介绍显存的各项参数都有什么作用。

GPU的内存硬件存储介质与CPU的类似，主要的区别是设计的结构有所差异。先说一下GPU内存硬件的分类，按照是否在芯片上面可以分为片上(on chip)内存和片下(off chip)内存，片上内存主要用于缓存(cache)以及少量特殊存储单元（如texture）特点是速度快，存储空间小；片下内存主要用于全局存储(global memory) 即常说的显存，特点是速度相对慢，存储空间大，不同于CPU系统内存可扩展的设计，GPU的内存硬件的大小整体都是固定，所以在选择好显卡型号后就会确定好，包括缓存和全局存储。

显存各类信息：

显存容量指的是显存能够存储的数据量，单位是GB，显存容量越大，能够存储的画面就越多，在玩游戏的时候不容易出现卡顿，目前主流显卡已经淘汰了512MB的显存，能够存储的数据也大幅提升了。

显存频率指的是显存工作的频率，单位是MHz，显存频率决定了数据在显存上存取的速度，显存频率越高，就可以更快地存取数据。这里要注意，显存频率和核心频率是两个概念，显存频率指的是显存的工作频率，而核显频率是GPU核心的频率，两者虽然单位一样，但是不能混淆。

显存位宽是指显存在一个时钟周期内能够传输数据的位数，单位是bit。显存位宽越大，瞬间传输的数据量就越大。

显存带宽指的是显示芯片与显存之间的数据传输速率，单位是GB/s。显存带宽是由显存频率，显存位宽决定的，显存带宽＝工作频率×显存位宽/8bit。也就是说，显存带宽和显存频率及显存位宽成正比，频率越高，位宽越大，显卡的显存带宽就越高，存取性能就越强。

片间互联是指在多处理器系统中，各个处理器之间进行数据和控制信号传输的技术

  对于不同的GPU，其显存往往有不同类型，而之所以会分为不同类型，通常都是因为其带宽和显存容量的不同导致其应用场景不同，比如NVIDIA GPU显存就有两种类型，GDDR和HBM，每一种也有不同型号，比如GDDR5、GDDR6、HBM2、HBM2e等。这两种型号的显卡采用不同的工艺制作，其带宽也不同，HBM 显存通常可以提供更高的显存带宽，但是价格也更贵，通常在训练卡上会使用，比如：H100、A100 等，而 GDDR 显存通常性价比更高，在推理 GPU 或游戏 GPU 更常见，比如：T4、RTX 4090 等。

  常见的NVIDIA GPU的显存信息如下：

训练GPU显存信息：

训练卡

	

H100 SXM

	

H100 PCle

	

H200

	

A100 PCle

	

A100 SXM

	

V100 PCle

	

V100S PCle




显存类型

	

HBM3

	

HBM2e

	

HBM3e

	

HBM2

	

HBM2e

	

HBM2

	

HBM2




显存大小

	

80GB

	

80GB

	

141GB

	

80GB

	

80GB

	

32/16GB

	

32GB




显存带宽

	

3.35TB/s

	

2TB/s

	

4.8TB/s

	

1935GB/s

	

2039GB/s

	

900GB/s

	

1134GB/s

推理GPU显存信息：

推理卡

	

L40s

	

L40

	

L4

	

A40

	

A10

	

A30




显存类型

	

GDDR6

	

GDDR6

	

GDDR6

	

GDDR6

	

GDDR6

	

HBM2




显存大小

	

48GB

	

48GB

	

24GB

	

48GB

	

24GB

	

24GB




显存带宽

	

864GB/s

	

864GB/s

	

300GB/s

	

696GB/s

	

600GB/s

	

933GB/s

可以看到，NVIDIA的训练卡一般都是HBM类型的显存，这是因为训练卡通常注重计算性能和大规模并行计算能力。训练卡训练时需要大量的计算核心和显存容量来高效地处理大型数据集上的复杂计算任务，而HBM能够为人工智能训练提供高带宽与高显存。而相对应的NVIDIA的推理卡一般是GDDR类型，这是因为推理卡通常侧重于低延迟、能效和推理性能。它们可能在设计上更加注重功耗效率，并且优化了推理阶段的计算效率，这种时候并不需要和训练卡一样的高带宽和高显存需求，因此如上表所示，推理卡的带宽普遍都比较低。

算力

算力其实就是表格中对应的FP32、FP64、FP16、INT8等参数值，芯片算力越大，每秒能够进行的运算次数就越多，执行计算任务就越快。当涉及到深度学习和计算任务时，FP32、FP16 和 INT8是常用的数据类型，用于表示不同的数值精度和存储需求。

单精度（Fp32）：浮点数使用32位表示，具有较高的精度和动态范围，适用于大多数科学计算和通用计算任务。通常我们训练神经网络模型的时候默认使用的数据类型为单精度FP32。

半精度（Fp16）：浮点数使用16位表示，相对于FP32提供了较低的精度，但可以减少存储空间和计算开销。按照理论来说可以跑机器学习这些任务，但是FP16会出现精度溢出和舍入误差，所以很多应用都是使用混合精度计算的也就是FP16+FP32模式，简单来说FP16其实在图像处理有更大优势点。

双精度（Fp64）：浮点数使用64位表示，提供更高的精度和动态范围。通常在需要更高精度计算的科学和工程应用中使用，相对于单精度，需要更多的存储空间和计算资源。

BFloat16（Bf16）：是一种用于表示浮点数的16位数值格式，特别适用于深度学习中的计算任务。它主要用于表示神经网络中的权重和激活值，针对深度学习模型的训练和推理。bf16 具有和 fp32 相同的 range，但精度（也就是两个最小单位之间的间隔）降低，因此可以节省存储空间并提高计算效率。

固定点数（INT8）：固定点数使用固定的小数点位置来表示数值，可以使用定点数算法进行计算。INT8与FP16、FP32的优势在于计算的数据量相对小，计算速度可以更快，并且能通过减少计算和内存带宽需求来提高能耗。

更高的精度通常意味着更准确的结果，但同时也需要更多的计算资源。所以，在设计芯片时，需要考虑算力和精度之间的平衡，以满足特定应用的需求。

特别的，NVIDIA Ampere架构引入了一种新的格式TF32格式，TF32是一种混合精度格式，用于表示32位浮点运算的一种优化形式，针对深度学习模型的训练和推理。TF32采用了低精度（16位）存储的方式来提高性能，但是在计算过程中会以32位精度进行累积。

大多数AI浮点运算采用16位“半”精度（FP16）、32位“单”精度（FP32），以及面向专业运算的64位“双”精度（FP64），人工智能训练的默认是FP32 ，没有张量核心（Tensor Core）加速度。 NVIDIA Ampere架构引入了TF32的新支持，使AI训练能够在默认情况下使用张量核心，非张量运算继续使用FP32数据路径，而TF32张量核心读取FP32数据并使用与FP32相同的范围，内部精度降低，然后生成标准 FP32输出。 TF32 使用与半精度 (FP16) 数学相同的10位尾数，表明其具有足够的余量来满足AI工作负载的精度要求。TF32采用与FP32相同的8位指数，因此可以支持相同的数值范围。这种组合使TF32成为FP32的绝佳替代品，用于处理单精度数学，特别是深度学习和许多HPC应用程序核心的大量乘法累加函数。

除此之外，Ampere架构还引入了Bfloat16 （ BF16 ）的数据类型，BF16 / FP32混合精度张量核心运算的运行速度与FP16 / FP32混合精度相同。相对来说，在深度学习计算里，范围比精度要重要得多，于是有了BF16，牺牲了精度，保持和 FP32 差不多的范围，而TF32的设计，在于即汲取了BF16的好处，又保持了一定程度对主流 FP32 的兼容，FP32只要截断就是TF32 了。先截断成TF32计算，再转成FP32，对历史的工作几乎无影响。

2、国产AI卡资料汇总"
菜叶子芯酸笔记1：芯片的基本概念、种类、产业链和生产工序,2024-10-13,"相关基本概念 
集成电路 Integrated Circuit IC : 把一定数量的常用电子元件，比如电阻、电容、晶体管等，以及这些元件之间的连线，通过半导体工艺集成在一起的具有特定功能的电路。 
芯片：集成电路行业口语化说法。 
半导体：材料 。集成电路行业是半导体行业（85%）的子集 、器件设备等等。 
微电子：材料+器件+工艺。现在也包括集成电路 也专门专业叫集成电路设计专业。 
几个概念之间的关系：芯片【成品】的本质是在半导体【材料】衬底上制作实现一系列功能的集成电路【技术实现形式】。 
芯片的种类分类及相关有名厂商 
按功能分类 
 
处理器芯片 
CPU芯片 
作用：计算机的“大脑”，用于执行计算、控制和数据处理任务。 
示例：Intel和AMD的x86系列处理器芯片、ARM架构的处理器芯片等。 
国外英特尔、AMD、IBM、高通 
华为海思、龙芯、海光、北京君正 
图形处理器芯片（GPU芯片） 
作用：用于图形渲染、图像处理和并行计算，广泛应用于图形显示、游戏和科学计算等领域。 
英特尔、英伟达、AMD 
壁仞科技、摩尔线程、芯动科技、景嘉微 
DSP数字信号处理器 
作用：用数字信号处理信息的器件 
Microchip、德州仪器、analog devices、ARM 
MCU微控制器 
将计算、存储、接口都集成在一起形成芯片级的计算机  汽车行业常见 
瑞萨、恩智浦、士兰微、复旦微电子、ST、华大半导体 
存储芯片 
作用：用于数据存储，包括内存芯片（RAM、ROM、Flash等）和存储器芯片（硬盘、固态硬盘等）。 
易失性存储器：SRAM    DRAM 
非易失性存储器：ROM FLASH 
巨头垄断：三星、海力士、美光  海外 
长江存储、长鑫存储、复旦微电子、兆易创新、华天科技 
传感器芯片 
作用：用于检测环境中的物理量，如温度、湿度、光照等，广泛应用于物联网设备、汽车等领域。 
CIS （CMOS图像传感器） 
作用：将光学图像转换成电子信号的电子设备 
索尼、三星、豪威科技、格科微 
MEMS （微机电系统） 
在芯片把微电路和微机械集成于一体的系统  手机 比如磁传感器、指纹传感器、环境传感器 
博通、博世、歌尔微电子、敏芯微、士兰微 
Touch触摸触控芯片 
Atmel、比亚迪、晨星半导体、汇顶半导体 
电源芯片 
给ASIC、DSP、CPU、Memory、FPGA及其他数字或模拟负载提供供电 
DC/DC电源芯片：开关电源芯片  直流转换直流的电源 
LDO低压差线性稳压器：稳压器  不同直流电压给不同器件供电 
PMU电源管理单元：高度集成更加便携式的电源管理方案 
芯源系统、TI、英飞凌、东芝、瑞芯微、圣邦微电子、上海贝岭 
通信芯片 
作用：用于网络通信、无线通信和数据传输，包括以太网芯片、Wi-Fi芯片、蓝牙芯片等。 
蓝牙芯片 
近距离无线连接技术、支持声音图像的传输 
高通、赛普拉斯、Dialog、ST、紫光展锐 
无线芯片 
高通、博通、Marvell、联发科、瑞昱、乐鑫 
NB-LOT 窄带物联网芯片远距离无线通信 广域网智能抄表、智慧路灯、智慧停车、共享单车 
高通、华为海思、中兴微、联发科、紫光展锐 
接口芯片 
USB 通用串行总线芯片 
赛普拉斯、恩智浦、全志 
HDMI 高清多媒体接口芯片全数字化视频和声音发送接口 
东芝、亚德诺、全志 
按工作原理分类 
模拟芯片 
作用：处理连续性的光、声音、速度、温度等自然模拟信号。 
举例子，比如声控/红外线控制路灯。 
数字芯片 
作用：实现数字信息传输，包括存储、计算、运算等功能。 
举例子，基本上就是数码产品都是这类芯片。 
数模混合芯片 
作用：同时包含模拟和数字功能的芯片。 
按芯片体系结构分类 【异构计算】 
通过硬件加速，采用专用协处理器的异构计算方式来提升处理性能，使得处理器的性能可以满足应用软件的需求。 
CPU 
CPU作为通用处理器，兼顾计算和控制，70%晶体管用来构建Cache 还有一部分控制单元，用来处理复杂逻辑和提高指令的执行效率，如图所示，所以导致计算通用性强，可以处理计算复杂度高，但计算性能一般。 
 
GPU 
GPU主要擅长做类似图像处理的并行计算，所谓的“粗粒度并行（coarse-grain parallelism）”。图形处理计算的特征表现为高密度的计算而计算需要的数据之间较少存在相关性，GPU 提供大量的计算单元（多达几千个计算单元）和大量的高速内存，可以同时对很多像素进行并行处理。 
图是GPU的设计结构。GPU的设计出发点在于GPU更适用于计算强度高、多并行的计算。因此，GPU把晶体管更多用于计算单元，而不像CPU用于数据Cache和流程控制器。这样的设计是因为并行计算时每个数据单元执行相同程序，不需要繁琐的流程控制而更需要高计算能力，因此也不需要大的cache容量。 
 
FPGA 
FPGA作为一种高性能、低功耗的可编程芯片，可以根据客户定制来做针对性的算法设计。所以在处理海量数据的时候，FPGA 相比于CPU 和GPU，优势在于：FPGA计算效率更高，FPGA更接近IO。 
FPGA不采用指令和软件，是软硬件合一的器件。对FPGA进行编程要使用硬件描述语言，硬件描述语言描述的逻辑可以直接被编译为晶体管电路的组合。所以FPGA实际上直接用晶体管电路实现用户的算法，没有通过指令系统的翻译。 
FPGA的英文缩写名翻译过来，全称是现场可编程逻辑门阵列，这个名称已经揭示了FPGA的功能，它就是一堆逻辑门电路的组合，可以编程，还可以重复编程。图8展示了可编程FPGA的内部原理图。 
 
ASIC 
ASIC是一种专用芯片，与传统的通用芯片有一定的差异。是为了某种特定的需求而专门定制的芯片。ASIC芯片的计算能力和计算效率都可以根据算法需要进行定制，所以ASIC与通用芯片相比，具有以下几个方面的优越性：体积小、功耗低、计算性能高、计算效率高、芯片出货量越大成本越低。但是缺点也很明显：算法是固定的，一旦算法变化就可能无法使用。目前人工智能属于大爆发时期，大量的算法不断涌出，远没有到算法平稳期，ASIC专用芯片如何做到适应各种算法是个最大的问题，如果以目前CPU和GPU架构来适应各种算法，那ASIC专用芯片就变成了同CPU、GPU一样的通用芯片，在性能和功耗上就没有优势了。 
我们来看看FPGA 和 ASIC 的区别。FPGA基本原理是在芯片内集成大量的数字电路基本门电路以及存储器，而用户可以通过烧入 FPGA 配置文件来来定义这些门电路以及存储器之间的连线。这种烧入不是一次性的，即用户今天可以把 FPGA 配置成一个微控制器 MCU，明天可以编辑配置文件把同一个 FPGA 配置成一个音频编解码器。ASIC 则是专用集成电路，一旦设计制造完成后电路就固定了，无法再改变。 
 
 
ref:深入理解CPU和异构计算芯片GPU/FPGA/ASIC - 哔哩哔哩 (bilibili.com) 
按行业用途分类 
数据中心级芯片 
应用：云计算数据中心，包含CPU、GPU、内存、存储控制器、固态硬盘等。 
消费类芯片 
应用：日常使用的电脑、手机等产品。 
工业芯片 
应用：工业产品，需具备稳定性、高可靠性和高安全性。 
车规芯片 
应用：汽车领域，对温度环境要求严格。 
军工芯片 
应用：国防军事工业领域，如卫星通信、制导、精准导航等。 
按工艺制程分类 
芯片还可以根据半导体芯片的工艺制程的演进历史进行分类，例如从早期的10μm工艺到现在的更先进的纳米级工艺。 
7nm工艺  14nm  28nm。工艺制程通常以纳米（nm）为单位来表示，例如7纳米（7nm）、5纳米（5nm）等。这些数字代表了晶体管的关键尺寸，特别是栅极长度。如图。 
 
按集成度分类 
小型集成电路（SSI）、中型集成电路（MSI）、大规模集成电路（LSI）、超大规模集成电路（VLSI）等。 
芯片行业上下游供应链及生产工序 
简单顺序和对应公司 
IC设计 --> 晶圆制造 --> IC封装 --> IC测试 -->成品芯片 
纯芯片设计公司，又称为Design House，Fabless公司。比如英伟达、AMD。 
设计、制造、封装、测试都有涉及的公司，有Intel、TI、三星。 
纯制造的公司，又称为Foundry，TOP1为台积电。 
做封测的公司有日月光等。 
也有做相关制造设备的公司，比如光刻机最有名ASML。 
做芯片材料的公司，掩模版，光刻胶，抛光材料，电子气体，湿电子化学品和靶材等，公司有比如江丰电子、南大广电等等。 
芯片设计 
 
对于客户（ODM，Original Design Manufacturer，原始设计制造商，比如华勤，闻泰做AIoT的，还有很多很多啦）提出需求后，在芯片设计部分，主要分模拟芯片设计和数字芯片设计，这里数字芯片设计，基本上都会用到EDA（Electronic design automation）设计工具，EDA服务供应商做得较为有名的有Synopsys（‌新思科技）、‌Cadence（‌楷登电子）和‌Siemens EDA（原‌Mentor Graphics，被‌西门子收购后更名）；模拟芯片巨头德州仪器（TI，Texas Instrument）和英飞凌，这个陆老师比我熟。成熟的IP模块授权的公司里最为有名为ARM。 
数字IC设计基本流程：设计—验证—RTL freeze—综合—STA（静态时序分析）—DFT—PR（自动布局布线）—Design sign-off。 当然，有时候前端设计里，也需要做一些DFT的工作。对应岗位有系统架构工程师、前端设计工程师DE和验证工程师DV DFT工程师属于左边粉色队列；后端设计工程师PR属于右边蓝色队列。 
 
模拟芯片设计流程：对应的工程师有系统架构设计师、模拟电路设计工程师、模拟版图设计工程师。 
 
两者对比图 
 
晶圆制造流程 
 
首先需要纯净的是半导体晶圆。半导体晶圆是从大块硅锭切割后的结果，而硅锭是从大块具有多晶结构和未掺杂本征材料生长得来。把多晶块转变成一个大单晶，给予正确的定向和适量的N型或P型掺杂，这就是晶体生长。晶体生长主要通过三种方法：直拉法、液体掩盖直拉法和区熔法。这里无论是直拉法还是区熔法，使用的设备均为单晶炉，单晶炉由炉体、热场、磁场、控制装置等部件组成，其中控制炉内温度的热场和控制晶体生长形状的磁场是决定单晶炉生产能力的关键 
这里用到单晶炉。 
然后到切割形状的步骤。晶体生长之后变进入晶圆准备环节，第一步是硅切片加工。硅切片加工的目的在于将硅锭切成一定厚度的薄晶片，切后的参数如晶向偏离度、TTV等精度对后道工序的加工（如研磨、刻蚀和抛光等）起直接作用，主要包括切去两端、硅片定位、精准切割等步骤。 
这里用到切割机、滚圆机、截断机。 
对原料硅片进行清洗工序。在不破坏硅片表面特性的前提下，使用不同的化学品进行前段清洗，去除半导体硅片表面的尘埃颗粒、有机物残留薄膜和吸附在表面的金属离子，以确保后续热氧化层成长的质量。主要方式为将硅片沉浸在清洗机化学液体槽内或使用化学液体喷雾清洗，再使用超纯水清洗，以避免化学液残留。 
这里用到清洗类设备。 
氧化、薄膜沉积步骤。氧化是在800-1250℃高温的氧气和惰性携带气体（N2）下使硅片表面的硅氧化生成二氧化硅膜。热氧化层中重要的闸极氧化层(Gateoxide)与场氧化层(Fieldoxide)即以此方法形成。根据反应气体的不同，氧化工艺通常分为干氧氧化和湿氧氧化两种方式。 干氧氧化制备的二氧化硅结构致密，厚度均匀，对于注入和扩散的掩蔽能力强，工艺重复性强，其缺点是生长速率较慢。这种方法一般用于高质量的氧化，如栅介质氧化、薄缓冲层氧化，或者在厚层氧化时用于起始氧化和终止氧化。 在湿氧工艺中，可在氧气中直接携带水汽，也可以通过氢气和氧气反应得到水汽，通过调节氢气或水汽与氧气的分压比改变氧化速率。湿氧氧化由于反应气体中同时存在氧气和水汽，而水汽在高温下将分解为氧化氢(HO)，氧化氢在氧化硅中的扩散速率比氧快得多，所以湿氧氧化速率比干氧氧化速率高约一个数量级。 
这里用到管式反应炉、快速热处理设备。 
光刻、刻蚀。晶圆表面上的电路设计图案直接由光刻技术决定。首先要在硅片上涂上一层耐腐蚀的光刻胶，随后让强光通过一块刻有电路图案的镂空掩模板照射在硅片上。被照射到的部分(如源区和漏区)光刻胶会发生变质，而构筑栅区的地方不会被照射到，所以光刻胶会仍旧粘连在上面。接下来就是用腐蚀性液体清洗硅片，变质的光刻胶被除去，露出下面的硅片，而栅区在光刻胶的保护下不会受到影响。光刻工艺决定着整个IC工艺的特征尺寸，代表着工艺发展水平。光刻机把图案印上去，然后刻蚀机根据印上去的图案刻蚀掉有图案（或者没有图案）的部分，留下剩余的部分。刻蚀可分为干法刻蚀和湿法刻蚀，它们的区别就在于湿法使用溶剂或溶液来进行刻蚀。 
这里用到光刻机和刻蚀机。最有名为ASML光刻机，真卡脖子技术。 
去胶、离子注入。离子注入是在真空与低温环境将杂质离子加速以高能离子束植入硅片表面所需特定区域，以植入物质的质量与能量控制在硅表面掺入的浓度与范围。离子注入工序后，通过高温退火，消除前期植入造成的晶格缺陷并使注入离子活化，注入离子起施主或受主的作用。在集成电路制造工艺中，离子注入通常应用于深埋层、倒掺杂阱、阈值电压调节、源漏扩散注入、源漏注入、多晶硅掺杂、形成 PN 结合电阻/电容等。掺杂： 高温环境下在硅表面掺入纯杂质原子的过程，从而改变和控制半导体内杂质的类型、浓度和分布，以便建立起不同的电特性区域。采用不同的掺杂工艺,通过扩散作用，将 P 型半导体与 N 型半导体制作在同一块半导体硅片上。 
这里使用了离子注入机、等离子去胶机、清洗设备。 
薄膜沉积。在集成电路制备中，很多薄膜材料由淀积工艺形成。主要包括化学气相 (CVD)淀积和物理气相淀积(PVD)两大类工艺，可以在晶圆表面生成了许多薄膜，这些薄膜可以是绝缘体、半导体或导体。它们由不同的材料组成，是使用多种工艺生长或淀积的。从全球范围看，AMAT在CVD设备和PVD设备领域都保持领先。 
这里使用了CVD设备、PVD设备等。 
抛光。CMP是指利用化学研磨液与机械力使晶圆制造中各层的全面性平坦的方式，以在晶圆的局部与整体各层表面减少凹凸起伏的情况，在已形成图案的硅片上进行化学机械抛光，使之形成整体平面，以减轻多层结构造成的严重不平的表面形态，满足光刻时对焦深的要求。根据对象的不同，CMP工艺主要分为硅抛光、硅氧化物抛光、碳化硅抛光、钨抛光和铜抛光。 
这里用到了抛光机、清洗设备、测量设备等。 
金属化是在硅片表面上制成金属或合金的导体。在硅片上沉积金属以作为电路的内引线的方法有蒸发、溅射、CVD等。金属溅镀就是将金属薄膜沉积在硅片表面的工艺过程。在此工艺中，薄膜主要以物理填充而不是化学反应。它是通过给金属靶材加上直流电，并利用磁场作用将靶材上的金属溅射出去并沉积到晶圆表面。铝是8英寸硅片常用的金属沉积材料，12英寸硅片常用的金属沉积材料是铜，其它的材料包括金、钛、钼、钨、钛钨合金、钯等。 
这里用了物理气相沉积设备、化学气相淀积设备、电镀设备、清洗设备等。 
电学测试及包装。电学测试是用探针对生产加工好的硅片产品功能进行测试，验证每个晶圆是否符合产品规格，检测通过的晶圆即可进行包装入库。 
这里用了晶圆中测测试机、分选机、探针台。 
 
ref:半导体晶圆制造工艺及设备大全！ (zhihu.com) 
晶圆/芯片封装流程 
1、硅片减薄：使用物理手段，如磨削、研磨等；或者化学手段，如电化学腐蚀、湿法腐蚀等，使芯片的厚度达到要求。薄的芯片更有利于散热，减小芯片封装体积，提高机械性能等。 
2、硅片切割：用多线切割机或其它手段如激光，将整个大圆片分割成单个芯片 
3、芯片贴装：将晶粒黏着在导线架上，也叫作晶粒座，预设有延伸IC晶粒电路的延伸脚，用银胶对晶粒进行黏着固定。 
4、芯片互联：将芯片焊区与基板上的金属布线焊区相连接，使用球焊的方式，把金线压焊在适当位置。芯片互联常见的方法有，打线键合，载在自动键合（TAB）和倒装芯片键合。 
5、包封固化：用树脂体将装在引线框上的芯片封起来，对芯片起保护作用和支撑作用。包封后进一步固化。 
6、电镀：在引线条上所有部位镀上一层锡，保证产品管脚的易焊性，增加外引脚的导电性及抗氧化性。 
7、打印：在树脂上印制标记，包含产品的型号、生产厂家等信息。 
8、切脚成型：将导线架上已封装完成的晶粒，剪切分离并将不需要的连接用材料切除，提高芯片的美观度，便于使用及存储。 
9、测试：筛选出符合功能要求的产品，保证芯片的质量可靠性。 
10、包装入库：将产品按要求包装好后进入成品库，编带投入市场。 
ref: 芯片封装基本流程及失效分析处理方法 - 哔哩哔哩 
 
这涉及的工种有芯片制造、封测、流片过程中涉及多个关键岗位，这些岗位共同确保芯片从设计到最终产品的顺利实现。以下是芯片制造、封测、流片过程中所需的主要岗位： 
1.质量QA工程师‌：负责产品实现各阶段的策划和过程控制，包括先期设计开发过程及后续的生产制造过程。处理客户基于产品的相关反应活动，如客户反馈、客户需求、客户满意度等，并跟进产品开发/流片/封装/测试/可靠性及客户使用过程问题，推进制定纠正预防措施，并跟踪验证改善效果‌。 
2‌.设备工程师‌ 和 ‌工艺工程师‌：分别负责芯片制造过程中的设备和工艺的维护与管理，确保生产线的稳定运行和产品质量的稳定‌。 
3‌.良率工程师‌ 和 ‌质量工程师‌：专注于提高产品的良率和保证产品质量，通过分析和改进生产过程中的问题，以提高产品的可靠性和性能‌。 
4‌.封装工程师‌ 和 ‌测试工程师‌：负责芯片的封装和测试工作，确保芯片在封装和测试过程中达到规定的标准和性能要求‌。 
这些岗位共同构成了芯片制造、封测、流片的完整链条，确保从设计到最终产品的每一个环节都能得到专业的处理和监控，从而生产出高质量的芯片产品。 
ref: 一颗芯片的诞生需要哪些岗位人员共同协作？ - 哔哩哔哩"
类脑智能：人造超级大脑,2022-12-29,"脑科学被视为理解宇宙、自然与人类关系的“终极疆域”，人类从未停止对人脑的探索，以及对其运行机制的模仿。人脑以极强的可塑性、通用性、自适应性、自组织性以及低能耗、高效率等，为人工智能技术的发展提供了启发和示范。类脑智能这一新兴学科得以诞生，被认为是后摩尔时代最具发展潜力的颠覆性领域之一。日前，中国科学技术协会完成了《直面人类未来——2021年度人类社会发展十大科学问题解读》一书的编辑出版，本期，我们邀请其中的两位作者，介绍有关类脑智能的研究。 
 《直面人类未来——2021年度人类社会发展十大科学问题解读》由中国科协提供 
 1.全球关注类脑智能研究 
 类脑智能研究具有重大意义，借鉴人脑的信息处理方式，有助于打破冯·诺依曼体系结构束缚，有望引领人工智能从机器智能走向机增强智能，从专用智能走向通用智能。因此，目前世界各主要发达国家和地区都在积极推动类脑智能的发展。该领域全球重大项目密度逐年提升，如欧盟的“人类脑计划”、美国的“推进创新神经技术脑研究计划”以及我国的科技创新2030——“脑科学与类脑研究重大项目”等，而且有谷歌、微软等国际商业机器公司等高科技公司的大力度投入。 
 类脑智能又被称为神经形态计算，旨在模拟人类大脑的形态结构及信息处理机制。一方面，突出生物学基础和硬件导向，以神经形态硬件仿真为核心，开发类脑芯片，模拟神经电路结构和脉冲神经网络，并利用神经脉冲计算原理形成新型物理模式；另一方面，突出感知模式、认知机理、学习能力、记忆存储等功能实现，在脑智能机制研究的基础上，借助机器强大的整合、搜索、计算等能力，实现功能类脑和性能超脑的运行模式。 
 自然选择并不必然意味着智能发展，其中蕴含着对于生存的妥协和生物体依附关系的辅助。因此，人类大脑作为自然进化的产物，仍然存在诸多弊端和局限，并非其所有的运行机制都是科学的和值得被模仿的，其信息处理的方式亦然。诸如记忆容量有限、学习过程缓慢、计算能力个体差异明显等，这就需要梳理明确机脑究竟在哪些方面应该借鉴人脑之所长，在哪些方面恰恰应该发挥机脑与人脑相互协同之作用。 
 在信息处理方面，人脑有几个显著优势：一是能够很好地处理非结构化信息，能够多模态感知并行处理数据，动态过滤和捕捉关键内容，并进行跨媒体融合和自主决策；二是小样本学习和泛化能力强，在知识和资源相对不足的条件下，主体具有强有力的自适应能力，具有强容错性；三是存算一体化，大脑的生物神经元同时具备运算和存储能力，具有一体化运行和超低能耗的特点；四是大脑建模不仅可以通过计算来实现，还具有稀疏性、学习性、选择性和方向性等生物特性，蕴含巨大的信息处理潜能。 
 从信息的获取、处理与存储等角度，以及综合考虑人脑在信息处理方面的显著优势，类脑智能可以划分为类脑感知、类脑记忆和类脑学习等。要最终形成“人造超级大脑”，还有很长的路要走，目前的类脑研究尚处于初级阶段，仅是对大脑的高度抽象和简化。 
 未来研究路径应该对大脑如何进行信息加工加以揭示，阐明复杂行为之下的内在机制实现形式，特别是了解神经信息如何产生感知觉、学习、记忆、决策等认知功能，并思考如何通过机脑实现智能，建立新型的计算结构与智能形态，使其在信息处理机制上“类脑”，在信息处理性能上“超脑”，在认知行为和智能水平上“类人”。 
 2.类脑感知模拟视觉、听觉、触觉和嗅味觉 
 根据人类感知世界的方式，类脑感知可以分为视觉智能、听觉智能、触觉智能和嗅味觉智能。当前的视觉智能、听觉智能处于信息技术领域学术研究与实际应用的最前沿。相比之下，触觉智能和嗅味觉智能处于材料硬件、规模化芯片硬件和类脑芯片结构创新的研究阶段。 
 在基础研究层面，类脑视觉智能已经发展为目前以深度学习为代表的视觉方法。1982年，大卫·马尔的《视觉》一书问世，使得视觉计算理论成为跨界计算机科学与认知神经科学的桥梁。现在，不同的类脑神经网络框架与其他领域的模型，如自然语言处理领域的转换器已经取得最先进的性能。在产业发展层面，视觉智能从上游的光源、镜头、相机，到视觉系统中游的中间算法，再到下游的设备制造和终端应用等已经取得全面的发展。 
 听觉智能作为另外一个发展较为成熟的领域，其主要研究内容以自然语言处理为核心，以语音和文本为载体，对抽象的信息进行表达。在应用层面，语音信号处理技术已广泛应用于虚拟主播、在线通话、智能音箱等。在产业界，自然语言处理技术已经被用于机器翻译、聊天机器人、舆情分析和市场预测等各个领域。 
 触觉智能是相对发展较为缓慢的一类感知技术。为了实现机器人准确地感知世界，需要全方位的视觉智能与触觉智能。2021年诺贝尔生理学或医学奖得主、美国加利福尼亚大学旧金山分校的戴维·朱利叶斯和斯克利普斯研究所的阿德姆·帕塔普蒂安分别独立发现了温度与触觉的受体，该发现揭示了人体皮肤感知温度、压力及疼痛的分子机制，即揭示了外部的温度和机械刺激是如何转化为内部的神经信号的。以该类发现为启发，机器的触觉智能以新材料为切入点，感知环境的温度、压力与湿度等各种信号，通过神经网络等机器学习方法自适应地学习模型参数，使得机器具备感知触觉的能力。 
 嗅味觉智能的研究难点也在感知材料层面，而不是后续的深度学习算法与认知训练层面。人工智能对味觉与嗅觉的识别，进展远远落后于对视觉和听觉的识别。原因之一是视觉与听觉的数据获取相对容易且标注较为简单。而相比之下，嗅觉与味觉的标注非常稀疏，且大部分物品并不会被轻易标注，因为人类不会自己去尝试陌生物品的味道。在基础理论层面，人类对味觉和嗅觉的机理研究并不透彻，目前的认知仅停留在味道与物质分子有关，而对分子之间的联合作用不存在类似视觉的系统认知。因基础理论认知的缺失、感知材料的发展限制，嗅味觉智能尚未在工业界广泛应用。在学术研究层面，IBM研究院于2019年研发的“电子舌头”能在1分钟内识别多种液体，可以用于监测食品安全、工厂质检、疾病诊断、环保检测等领域。2020年，英特尔与康奈尔大学在《自然-机器智能》上联合发表论文，宣布其利用英特尔神经拟态芯片Loihi可以识别10种有害气体。 
 人脑对大量的信息会基于注意力机制进行筛选，并将资源重点用于面向任务的关键信息处理。类脑注意力智能已经被应用于深度学习的各个领域，如视频图像处理、语音识别和自然语言处理等。当前注意力机制发展最为成熟的领域是自然语言处理，它通过编码-解码器来实现对词语的注意权重。 
 3.类脑记忆已用于问答系统、机器翻译 
 生物大脑在进行信息处理时，能够将信息记住一段时间，即大脑具有记忆功能。该功能是生物大脑的重要功能，是大脑进行学习、认知的基础。根据记忆时间的长短，可将记忆分为短时记忆和长时记忆。短时记忆是对当前环境的即时反应，将当前环境数据存储于生物大脑中，从微观角度来看，短时记忆是刺激神经元后的持续性变化。长期记忆是对历史信息的高层次概括，从微观角度来看，是神经元之间的突触连接和强度发生了变化。具有记忆单元的智能体有很好的自适应能力，可以从历史经验中学习，从而更好地发挥作用。 
 受到脑科学的启发，研究人员将记忆模块应用于机器学习模型中。长短期记忆网络就是一种代表性方法。它通过对神经记忆单元进行结构化设计，通过门控模型控制信息的更新。其中，遗忘门控制信息中的哪些部分会被丢弃，输入门控制神经元要更新的信息，输出门控制神经元要输出的信息，一个细胞状态中存储着由遗忘门和输入门共同确定的可记忆的信息。长短期记忆网络方法将信息表示成固定长度的向量化编码，当外部信息量变大时，这种定长的编码方法可能会丢失信息的细节。这导致了长短期记忆网络的记忆能力有限，并不能精确地记住过去的事实。此时的大部分模型都缺乏可以读取和写入外部知识的组件。 
 在此基础上，研究人员开始研究非定长的记忆单元存储信息。Facebook 团队提出的记忆网络就是其中一种。该团队在记忆网络中引入了一个独立的存储器。我们可以通过类比方式来理解这种网络：网络结构可看作计算机中的中央处理器，而独立存储器可看作随机存储器。一个记忆网络由一个记忆数组和四个组件（输入组件、泛化组件、输出组件、回答组件）组成。输入组件负责将输入数据转化为网络内在的向量；泛化组件用来更新记忆数组；输出组件结合输入，从记忆数组中抽取合适的记忆；回答组件负责将输出组件的输出转化为需要的形式。 
 记忆网络虽然解决了定长记忆方法的局限性，但这种方法并不是一种端到端的方法。端到端的记忆神经网络随之出现了。键值记忆网络的提出解决了端到端的神经网络记忆规模不足的问题。以上方法通过非定长记忆编码的方式改善定长记忆方法的缺陷，但随着记忆的增长，这种方法可能会造成信息的冗余。因此，学者又引入了注意力机制来对信息进行动态融合，提取记忆中的重要信息。目前，相关工作已经在问答系统、机器翻译等任务中取得了良好表现。 
 尽管目前的神经网络计算模型对记忆的借鉴已经在应用层面取得了良好的效果，但这些方法仍然是从功能角度去模拟大脑，其进一步的发展还需要对人脑记忆机制和原理进行深入的探索。 
 在类脑记忆方面，现有的人工神经网络并不具备生物机理作为理论支撑。例如，在生物大脑中，记忆遵循着赫布学习律与脉冲时序依赖的突触可塑性等法则。类脑记忆未来的发展方向应该借鉴生物大脑的研究，根据生物大脑的记忆环路结构及相关理论构建多尺度的记忆框架，实现自主记忆。由于不同种类的记忆类型在神经机制和神经环路上差异明显，因此类脑记忆应重点解决对生物大脑的长时记忆、短时记忆、工作记忆等不同种类记忆方法的建模，探索不同记忆的脑机制。除此之外，类脑记忆还应解决如何对记忆进行表示、如何构建层次化记忆、如何对记忆进行去冗余处理、如何快速激活记忆单元等关键问题。 
 4.类脑学习需要融合更多神经生物学理论 
 生物大脑的学习智能来源是其有着超大规模和复杂互联的神经网络，通过模拟生物神经网络从而实现类脑学习是类脑智能的发展思路之一。近年来蓬勃发展的深度神经网络利用生物大脑在进行信息处理时的层次化信息，在诸多领域取得了重大突破。 
 卷积神经网络就是一个很好的例子。它通过对网络结构的改造，使其某些神经元共享相同参数，可以提取输入数据某种不变的特征。但其仍是浅层的初步尝试，对脑信息处理机制的深度借鉴还有很长的距离。 
 神经学研究显示，大脑新皮质的功能实现依赖于其区域连接组成的层次结构。该理论是皮质学习算法的理论基础，认为人脑的智能与新皮质紧密相关。皮质学习算法中的核心部分是分层时间记忆算法，该算法模拟了大脑新皮质结构和功能运作。虽然该模型目前已经在数据处理领域得到了广泛的应用，但只是对大脑皮层对应的功能进行粗略建模，与大脑的真实结构差距甚远。 
 20世纪90年代，神经生物学家发现，在神经元细胞体或轴突附近存在一系列的短脉冲，而且不同的信息在尖脉冲数量和脉冲输出的时间上存在差异。这些研究成果促使了脉冲神经网络的产生。脉冲神经网络被称为第三代神经网络，设计更加接近生物大脑的真实结构，是对真实的生物神经元进行建模。脉冲神经网络目前在模式识别、脑电信号处理、智能感知和控制方面有着良好效果，但在图像或语音处理的效果方面仍然是个未知数。除此之外，脉冲神经网络训练算法的理论发展尚需完善。 
 对于人类来说，学习新的知识并不需要很多标记数据。这是由于人类在学习新任务时，会有其他的先验知识可以用于联想或者类比学习，人类只需要通过少量样本就能快速学习，这就是机器学习和人类学习之间存在的差距。可不可以借鉴人类学习的过程，从而通过其他任务的学习促进当前任务的学习？ 
 元学习就是基于此思想发展而来的，它试图让机器学习更加接近人类思维。元学习又被称为学会学习，是利用以往的经验来指导新任务的学习，使人工神经网络具备学习的能力。元学习尽管取得了一些成绩，但还有自适应性、进化性、可解释性等一系列问题有待研究，其系统理论知识尚需进一步完善。 
 在类脑学习方面，虽然现有的深度神经网络已经模拟了生物大脑信息处理的部分原理，但这只是初步的探索，并没有从更深层次去融合神经生物学的相关理论。目前，在微观层面，神经生物学在神经元和突触的类型、数目及其工作原理等方面都取得了不少进展，发现它们在不同的脑区中差异很大，且能根据实现功能的不同动态变化调整，可以通过深度融合生物大脑的学习机制来提升现有模型。 
 作者：任福继（电子科技大学特聘讲席教授）；李太豪（之江实验室高级研究专家） 
 《光明日报》（ 2022年12月22日 16版）"
苏联芯片史：从电子管到晶体管再到集成电路,2024-08-20,"注：同样的文章，csdn 审核比微信敏感，哪有什么政治偏向，就是审核不通过，只好改个名字。 
 
从电子管到晶体管再到集成电路，苏联是怎么掉队的？ 
原创 寰球大百科 
想必俄罗斯方块这个游戏很多人都玩过，这也是我们 80、90 后记忆最深刻的一款游戏。这款游戏的发明者，是苏联一位名叫阿列克谢・帕基特诺夫的工程师，他是利用上班摸鱼的时间做出来的。由于具有较强的可玩性， 1984 年这款游戏问世后，很快就传到了海外。恰在这时，日本任天堂公司即将发布自己的第一代便携掌机 Game Boy，非常希望拿到俄罗斯方块的版权。于是任天堂公司便派代表前往莫斯科谈判，希望购买该游戏的版权。 
 
▲阿列克谢・帕基特诺夫 
根据当时苏联的法律，帕基特诺夫开发出的游戏属于集体财产，个人无法通过售卖游戏来盈利。因而苏方与任天堂谈判的代表便不是帕基特诺夫，而是苏联安全局和商贸部的几名高级官员。在洽谈中，苏联官员们向任天堂代表提了一个要求，那就是价格好商量，条件则是任天堂要转让游戏机的制造技术，让苏联人民也能玩上俄罗斯方块。这个提议合情合理，但任天堂代表却果断地拒绝了。 
 
因为当时正逢美日芯片大战，两国关系略显尴尬。而在此之前，美国早已经打过招呼了，大致意思就是说，现在在已经到了冷 战的决胜阶段，凡是还想挣美刀的，一律不许向苏联出口技术，即便是民用领域的技术也不行。1987 年，东芝公司不信邪，以 35 亿美元的价格将 4 台民用数控机床卖给了苏联。结果美国里根政府知道后，抓住东芝事件不放，先罚了东芝一笔巨款，接着又警告日本政府，若是再有此类事件发生，小心吃不了兜着走。 
 
迫于压力，日本通产省只得在事后紧急下发通知，要求日企在与苏联合作时必须万分谨慎，不能再让老美抓住小辫子。所以，尽管苏联愿意以白菜价格出售游戏版权，但向来以唯利是图著称的任天堂还是不敢占这个便宜。而在这场交易中，我们也不难发现，80 年代的苏联可以用生产香肠的速度制造各类导弹，但在半导体领域却落魄到了连游戏机都造不好的地步。 
 
比如苏联生产的 “大海战” 街机，由于技术问题，机台没有屏幕，玩家只能透过类似潜望镜一样的装置才能看到游戏画面。而众所周知，苏联是有名的数学强国，苏联也几乎与美国同时间发展半导体产业，可为什么几十年过去了，苏联会在这一领域被对手甩开如此大的差距？梳理历史，我们会发现问题出在了苏联自己身上。 
 ▲苏联大海战街机 
二战期间，同盟国和轴心国都使用了大量飞机轰炸对方的军事目标。经过实战后大家发现，要想打下飞机，就必须精确计算并绘制出射击图表，经查表确定炮口的角度，才能使高射炮射出去的炮弹正中飞行目标。但是这里面的每一个数据都要做几千次的四则运算才能得出来，而十几个人用手摇机械计算机得算上几个月才能完成一份图表。针对这种情况，美国科学家便试图研究通用计算机来提高运算效率。 
 
1946 年 2 月 14 日，经过无数科学家努力，世界上第一台通用计算机埃尼阿克 ENIAC 在美国宾夕法尼亚大学诞生。埃尼阿克这玩意虽然也叫计算机，但比起现在的 PC，它有个 “小小” 的缺点，那就是大到丧心病狂。长度达到 30 米，宽 6 米，高 2.4 米，占地面积约 170 平方米。总共用到了 17468 根电子管、7200 根水晶二极管、1500 个中转、7 万个电阻器、1 万个电容器、1500 个继电器和 6000 多个开关，重达 30 英吨。 
 ▲ENIAC 
据说当时只要埃尼阿克开机，半个费城的电力都会受到影响，因为耗电量实在是太大了。不仅如此，由于用到了大量电子管，埃尼阿克运行时还经常烧电子管，负责维护的工程人员整天啥也不干，净忙着更换电子管了。虽然第一代通用计算机在现如今看来是如此的奇葩，但在当年它却是划时代的产品。埃尼阿克的出现可以让以前需要花上半小时的计算，压缩到短短的 30 秒钟。也正是因为超高效率，所以美国政府才愿意拨款研发这个秘密武器。 
 ▲ENIAC 
不过众所周知的是，冷 战时期的西方国家在苏联面前毫无秘密可言，所以没过多久，苏联就得知消息，并成功研发了欧洲的第一台通用计算机 MESM。相比美国的埃尼阿克，MESM 的算力稍弱，只有前者的六成左右。但因为只用到了 6000 个电子管，MESM 不容易出故障，效率反而更高。最初的电子计算机都是用的电子管技术，不过电子管容易坏，美国当时的一些科学家便开始研究晶体管技术。1947 年 12 月，美国贝尔实验室研制出了晶体管。 
▲MESM 
与电子管相比，晶体管具有运算速度更快、发热量小、功耗低、成本低、体积小等优势。既然晶体管更先进，苏联自然也不会熟视无睹。1950 年，苏联也成功研发出了晶体管。后来的几年时间里，美苏在半导体领域开始你追我赶。1955 年，美国贝尔实验室研制出世界第一台全晶体管计算机 TRADIC，随后苏联也在 1959 年推出了自家的晶体管计算机 M4。自 1960 年代初，虽然美国一直是领跑者，但苏联始终没有被甩下，在晶体管计算机领域与美国基本是同等水准。 
▲TRADIC 
然而，从晶体管到集成电路的演变过程中，苏联却选择了一条错误的路线。由于历史原因，苏联从建国伊始就没有太平过，不是在热战就是在冷 战，这种生存压力使得苏联人有着深深的创伤后应激障碍，总觉得核战争迟早要爆发，造出的武器首先就要扛得住核电磁脉冲。而电子管因为结构简单，比晶体管更皮实，所以在 60 年代，苏联和美国便在集成电路方面走上了截然不同的道路。 
 ▲晶体管 
苏联放弃晶体管，集中精力搞电子管小型化，但埋头搞了几年后，苏联工程师发现电子管这玩意儿它就是造不小。就在苏联人押宝电子管的时候，美国那头一开始其实也是同样的心态。美国政府和军方同样认为晶体管在核爆产生的电子脉冲面前几乎是毫无招架之力，主张要电子管不要晶体管。但美国与苏联有一点不一样，苏联的半导体行业完全由政府主导研发，始终是以国家力量推动的。苏联政府和军方只关心工程师搞出的产品能不能用于战争，不考虑能不能赚钱，能不能下放到民用市场。 
 ▲电子管 
而美美国的半导体产业则是以市场为导向，自由生长。对于美国的消费者而言，晶体管和电子管哪个轻便好用，我就用哪个，至于哪个更适应核战争，这不是我该操心的。对于美国半导体公司而言，军方确实是大客户，但民用市场的规模更大。由于市场的力量，晶体管在美国被造得越来越小，性能也越来越好，原本被认为不适合用于军事的晶体管也渐渐开始被军方接受。1965 年，仙童半导体八叛逆之一，同时也是英特尔公司创始人之一的戈登・摩尔在《电子学》杂志上畅想未来。 
 ▲戈登・摩尔 
他认为，未来集成电路上的晶体管每 18 个月到 24 个月就能增加一倍，单个晶体管的价格会持续下降，芯片计算能力会越来越强。这就是大名鼎鼎的摩尔定律。摩尔当时只是畅想了 10 年的未来，然而摩尔定律仿佛是半导体行业的指路明灯，指导了半导体产业 70 年发展。苏联人是万万没有想到几十年后的晶体管能做到纳米级。越战中期，苏联就发现美军已经开始将半导体集成电路用在了最新的战斗机和导弹上，而这些新武器的实测数据要远优于苏军的武器。 
 
既然已经证明晶体管比电子管好用，苏联军方也只得承认了晶体管的先进性，并试图停止电子管小型化的尝试，开始转向于晶体管和集成电路的研究。然而这时候从头再来已经不可能了，于是苏联人决定摸着鹰酱过河，开始 360 度无死角的山寨美国的芯片。可由于前期发展方向的错误，导致整个苏联的电子技术理论和生产工艺在当时已经与西方严重脱节了。这种鸿沟不是单单靠着山寨就能弥补的，经常是苏联好不容易复刻出来一板芯片，而美国的芯片已经迭代好几轮了。 
 
比如 1989 年，苏联工程师在实验室里成功仿制了 80286 微处理器，制程是 1.5 微米，而这玩意儿已是英特尔在 1982 年也就是七年前推出的产品。随着工艺的发展，逆向工程越来越不好做了，苏联芯片基本跟不上时代。最要命的是，当时苏联已经搞了十几年的电子管，全国有成百上千的工程师、工人和技术人员专注在电子管产业上。若是以晶体管取代电子管，那就意味着有成千上万的相关从业者要失业或调岗。 
 
正所谓 “百万漕工衣食所系”，当时研究电子管的苏联科学家、工程师和工厂，尽管他们知道电子管再怎么发展也无法和晶体管抗衡，但为了保证饭碗，还是形成了一个反晶体管的利益共同体。反正也是花国家的钱，反正也没有盈利需求，就这么混呗，混一天算一天。在这种情况下，苏联电子行业便呈现出了很奇怪的现象，一边是继续浪费大量的人力和物力投入到没有前途的电子管产业上，一边是为了应付上级半公开的科研造假和仿造美国芯片。 
 
1982 年，中情局发现苏联正在大规模窃取西方芯片技术，于是在当年 11 月，美国与西欧各国达成协议，建立了针对苏联的联合科技与经济战略协调机制，对流向苏联的新技术进行全面管控与限制。苏联半导体产业本来就没有市场资金的反哺，现在又没法进行国际技术交流，美国的技术封锁最终导致苏联在 80 年代末期还得指着任天堂转让技术，才能让苏联人民也能玩上俄罗斯方块。 
 
1991 年年底，苏联解体。没有了军事工业的订单，苏联原来生产电路的工厂纷纷破产，半导体行业直接休克。更悲催的是，由于当年苏联领导人搞产业布局时，将半导体和微电子工业基地以及零零散散的工厂刻意分配在了乌克兰、白俄罗斯和波罗地海三国，以致苏联解体后，虽然俄罗斯继承的科研机构比较多，但生产基地却不在手里。巧妇难为无米之炊，现在没有了米，巧妇为了生计只能纷纷出走欧美。 
 
不得不承认，苏联培育的人才还是很厉害的，比如曾在苏联搞 CPU 的彭特科夫斯基移民美国后就成了英特尔的首席工程师，主导了奔腾 3 的架构研发，为英特尔带来了黄金发展时期。随着电子工业被拆得七零八落，以及大量顶尖计算机人才相继流失，自此曾经也算是两分天下的苏联半导体行业再无翻身机会，以至于最终输掉了芯片战争。 
 
修改于 2023 年 11 月 09 日 
 
via: 
 从电子管到晶体管，再到集成电路，硬核苏联为什么会输掉芯片冷 战？寰球大百科 2023 年 11 月 09 日 21:18 江苏 https://mp.weixin.qq.com/s/A6ytUjKSuG7ISjLnbzVi2A
 
苏联芯片消亡真相 
原创 哲空空 蓝钻读书 
引子 
二战末期，希特勒饮弹自杀，美苏两国军队先后踏上德国本土。 
当时，德国有家名叫蔡司的公司，专门为德军生产望远镜、测距仪、瞄准镜和航空摄影机等设备，美国和苏联都将这家公司列为接收德国技术的首要目标。 
美国将军巴顿先人一步，不惜冒着破坏《雅尔塔协定》的风险，率领第三集团军闯入划给苏联的民主德国区域，占领了蔡司所在的耶拿，计划用 600 辆卡车将整个蔡司公司搬到联邦德国。 
苏军收到消息，加速了推进的脚步，匆忙中，美国人只好选择 “拿走大脑”，将蔡司最核心的 100 多名主干人员迁移到联邦德国的奥博科亨，建了个新蔡司。 
苏联人慢了一步，遂将美国人没能带走的东西全部收入囊中。 
1946 年 2 月，乔治・凯南从莫斯科向美国政府发了封 “长电报”，阐述了他对苏联的深入分析，认为美苏冲突不可避免，主张以围堵政策对付苏联。 
与此同时，苏联这边的莫洛托夫也写了一封传回莫斯科的 “长电报”，称美国是一个独断的霸权势力，正在准备赢得一场新的世界大战，苏联必须小心提防。 
不久后，英国前首相丘吉尔在密苏里州发表 “铁幕演说”，指控苏联在 “从波罗的海到亚得里亚海” 之间建立了一道 “铁幕”，呼吁西方世界联合对抗苏联。 
冷 战的序幕拉开了。 
美国拿到蔡司后，令蔡司为一家名为仙童相机的公司生产 3400 个目镜，仙童相机后来成立了 “仙童半导体” 子公司，主营集成电路业务。 
集成电路，即俗称的 —— 芯片。 
第一回 克格勃安插间 谍，莫斯科死磕芯片 
按照克格勃的分类，在外联络人可分为三种。 
位于顶层的是 “特工”，指处于意识形态或经济原因，正式为克格勃工作的这类人；下一层是 “秘密联络人”，他们同情苏联的事业，愿意为之秘密提供帮助；再下一层是大量担任公职的公开联络人，他们可根据现实需要，与前两种人接触或进行掩护。 
为苏联芯片事业立下汗马功劳的的乔尔，属于 “秘密联络人”。 
乔尔的父母是俄罗斯犹 太 人，为了逃离沙 皇迫 害，举家移民到了美国。 
作为移民二代的乔尔，生长于美国布鲁克林贫民区，后被纽约城市学院录取，学习电气工程。乔尔在学校里与一群共产主义者打得火热，他们隔三差五饮酒聚会，交流思想，认为苏联是纳粹的克星。 
很快，乔尔认识了一个叫萨兰特的电气工程师，因志趣相投，结为莫逆之交，决心一起为共产主义理想奋斗终身。 
 
乔尔和萨兰特 
20 世纪 30 年代，二人找到了组织，加入苏联情报集团，在组织的指示下，乔尔和萨兰特进入美国西部电器公司，从事机密雷达和其他军事系统的研究工作。 
乔尔和萨兰特在美国尖端技术公司潜伏了十几年，掌握了新武器系统中关于电子设备的专业知识。 
20 世纪 40 年代末，联邦调查局开始瓦解苏联在美国的情报网，罗森伯格夫妇因从事间 谍活动被判处电刑，一时间风声鹤唳，乔尔和萨兰特与联邦调查局斗智斗勇，在身份暴露之前，逃离美国，抵达苏联。 
20 世纪 50 年代末，乔尔和萨兰特殚精竭虑，为苏联制造出第一台电脑 ——“头脑”，很快，这两个出类拔萃的英才被苏联半导体事业的技术专家尤里和苏联无线电领域的高官肖金发现，并将其招致麾下。 
当时，苏联的半导体工厂如雨后春笋般在全国各地建立起来，尤里是半导体领域专家中的专家。 
尤里的大部分青少年时光在中国大连度过，他的父亲是黄海沿岸一家军事医院的军医，随父母回到苏联后，尤里进入莫斯科一所顶级大学，专门研究半导体。 
毕业后，尤里被分配到一家半导体工厂，为苏联的太空计划和军队制造半导体芯片。工厂负责人拿出一块锗，要求尤里在上面制造出含有多个器件的电路，经过许多个不眠之夜的钻研，尤里于 1962 年研制出了属于苏联的集成电路原型。 
那是一段激情燃烧的岁月，尤里与同事们白天在实验室里忙碌，晚上闲暇时，尤里弹吉他为同事们解乏，苏联科学正处在上升阶段，这群年轻人眼里有光。 
每当尤里放下吉它，望向茫茫苍穹，肉眼便能看见在他们头顶上飞行的苏联人造卫星斯普特尼克。 
第二回 美国空军青睐德州仪器，赫鲁晓夫押宝苏联硅谷 
1957 年 10 月 4 日，苏联发射了世界第一颗人造卫星斯普特尼克，该卫星以每小时 18000 英里的速度从西向东环绕地球运行。四年后，苏联宇航员加加林又成为第一个进入太空的人。 
苏联接连震惊世界，成为冷 战格局领跑者，令美国无地自容。 
就在苏联卫星发射三天前，美国仙童公司成立，其创始者诺伊斯和摩尔正犯愁，谁会为集成电路买单，当卫星上天的消息传来，诺伊斯猛拍大腿，路子有了。 
 
诺伊斯 
苏联卫星上天，震动了美国政府，肯尼迪启动紧急计划，以赶上苏联的火箭，并宣布美国将派宇航员登月。 
美苏争霸让仙童公司的芯片有了用武之地，用经济学术语来说就是 —— 有了市场。 
类似仙童的芯片公司，闻到金钱的味道，纷纷出手。 
德州仪器总裁哈格蒂意识到，不只是发射卫星和登月，在不远的未来，芯片将用于美国军方使用的每一件电子产品中。 
哈格蒂通过关系，找到美国国防部，向其负责人介绍了芯片的妙用，随后，美国空军同意赞助德州仪器的芯片研究。 
在此期间，一个叫张忠谋的年轻人，加入德州仪器，成为该公司第一名中国员工。 
1962 年秋，美国空军开始寻找新的计算机来引导其 “民兵 11 号” 导弹，该导弹的设计初衷，是在太空发射核弹头打击苏联。 
 
民兵 11 号 
第一版 “民兵” 导弹因太重，无法保证其从美国西部的发射场击中莫斯科，这版导弹的制导计算机是一个巨大的怪物，基于分立晶体管制造，核弹打击程序必须通过打孔纸带输入计算机。 
哈格蒂向美国空军承诺，使用德州仪器芯片的计算机，可以用一半的重量完成两倍的计算。 
就在美国磨刀霍霍之际，苏联这边也开始了芯片研发。 
赫鲁晓夫致力于从各个领域超越美国，从玉米产量到卫星发射，出身农民的赫鲁晓夫，相比于实验室，更喜欢在农场里指点江山，他对技术一无所知。 
 
赫鲁晓夫 
苏联无线电电子委员会第一副主席肖金、半导体技术专家尤里以及克格勃策反的来自美国的 “秘密联络人” 乔尔和萨兰特，这些人组成了苏联研究芯片的主干班底。 
肖金多次向赫鲁晓夫谈起芯片的重要和神奇，有一次，肖金对赫鲁晓夫说，领导，想象一下，电视可以做成香烟盒那么大。 
1962 年 5 月 4 日，在肖金安排下，赫鲁晓夫访问了位于列宁格勒的苏联芯片研究所。 
为了欢迎领导，萨兰特穿了一套深色西装，乔尔秃顶的头上架着一副金属眼镜，紧张地站在萨兰特身后，在萨兰特主导下，二人向赫鲁晓夫展示了苏联的微电子技术成就。 
赫鲁晓夫试了试一台可以装在耳朵里的小收音机，又摆弄了两下可以打印出他名字的电脑，萨兰特对赫鲁晓夫说，半导体器件将很快用于航天器、工业甚至核弹领域，未来的美苏争霸将是半导体的争霸。 
萨兰特和乔尔将赫鲁晓夫带到一个画架前，画布上描绘了一座城市，这座城市专门生产半导体芯片，市中心有一座 52 层的摩天大楼。 
 
赫鲁晓夫与苏联芯片专家，右图戴帽者为肖金 
几个月后，赫鲁晓夫批了一个大手笔的项目 —— 兴建苏联半导体城市，具体位置在莫斯科郊区，取名为泽列诺格勒，俄语中的意思是 “绿城”。 
这座城市被设计成一个科学圣地，有研究院和大工厂，还有为科研人员及其家属准备的学校、托儿所、电影院、图书馆和医院，市中心是莫斯科电子技术学院，学校的砖墙模仿了美国的大学。 
泽列诺格勒后来被称为 —— 苏联硅谷。 
第三回 智仙童开创历史，莽肖金发号施令 
仙童公司的集成电路上市几个月之后，被克格勃定位为 “第三类间 谍”（有正式公职作掩护，可见光）的苏联工程师特鲁特科出动了。 
苏联人造卫星斯普特尼克绕地球飞行的第三年，特鲁特科作为交换生进入了美国斯坦福大学。 
美苏冷 战期间，并非铁板一块，两个超级大国通过交换生进行交流，特鲁特克是少数几个由苏联甄选并通过美国审查的学生之一，他在斯坦福大学研究美国最先进技术，甚至参加了 “晶体管之父” 肖克利的讲座。 
 
硅谷摩西肖克利 
1955 年，肖克利离开贝尔实验室，创建了肖克利半导体实验室，吸引了一众青年科学家加盟，但很快，肖克利的怪异性格引起员工不满，包括诺伊斯和摩尔（摩尔定律发明者）在内的 8 人一同辞职，转头在帕洛阿尔托建立了仙童半导体公司，史称 “八叛逆”，而帕洛阿尔托后来成为硅谷的中心枢纽。 
 
八叛逆 
言归正传。 
特鲁特科参加肖克利的讲座时，特鲁特克正因八叛逆的事满腹牢骚，特鲁特科拿出一本肖克利的巨著《半导体中的电子和空穴》请他签名。 
肖克利扫了特鲁特科一眼，得知他是苏联人交换生后，大声抱怨道，苏联还没为俄文版《半导体中的电子和空穴》支付版税。 
尽管如此，肖克利还是在书的扉页上，写下 “致苏联朋友特鲁特科” 的签名。 
肖克利和美国政府都不知道，苏联已将交换生和国防工业目标捆绑在一起，特鲁特科这样的交换生的另一个身份是 —— 克格勃特工。 
特鲁特科在美国的一个重要任务是，搞到美国最先进的半导体器件，并测试其质量以及跟踪改进情况，根据特鲁特科搞到的半导体，苏联人发现，美国在芯片领域领先苏联二到四年。 
就在赫鲁晓夫宣布建立 “苏联硅谷” 时，另一名潜伏在宾夕法尼亚州大学的交换生马林回到了苏联，马林的行李箱里装着一个神秘物件 —— 德州仪器制造的 SN-51 芯片。 
肖金将尤里、萨兰特、乔尔等科学家叫到他的办公室，将德州仪器的芯片放到显微镜下，透过显微镜进行观察。 
肖金命令他们，复制它，一一对应，不能有任何偏差，我给你们三个月时间。 
第四回 巴统瓦解复制大计，T 局折戟集成电路 
苏联的 “复制” 战略进行的不太顺。 
1963 年，也就是兴建苏联硅谷的那一年，克格勃成立了一个新的部门 ——T 局，其任务是获取西方设备和技术，为苏联生产集成电路保驾护航。 
20 世纪 80 年代初，克格勃 T 局雇佣了 1000 多人来搜集外国半导体技术。 
苏联驻旧金山领事馆有一个由 60 名特工组成的团队，目标是 “搞定” 硅谷的科技公司，这些特工有的直接偷走芯片，有的从黑市上购买芯片，1982 年，有个被称作 “独眼杰克” 的克格勃，在加利福尼亚州被捕，他被指控将芯片藏在皮夹克里，从英特尔工厂偷走芯片。 
 
1982 年秋，几个罗德岛的渔民从北大西洋水域打渔时，捞到了一个奇怪的金属浮标，该金属浮标被送到军事实验室检验，被确认为苏联的监听设备，里面使用了德州仪器 5400 系列半导体的复制品。 
作为对苏联 “复制战略” 的反制，美国及其盟国通过一个名为 “出口管制统筹委员会” 的组织禁止向苏联等社会主义国家转让包括半导体在内的先进技术，因该总部设在巴黎，又称 “巴统”。 
鉴于此，苏联的半导体工厂多数情况下，只能使用那些质量低下的材料，生产出的芯片也就质量难以达标。 
克格勃可以搞到芯片，却无法知道它是怎么制作的，就像一个厨师得到一个蛋糕成品，却无法知道蛋糕是如何烘培出来的，而芯片的制造工艺比烘培蛋糕复杂的多。 
芯片制造的每一步都涉及专业知识，如化学物质需要加热到何种程度，光刻胶应曝光多长时间，这些都是美国芯片公司的 “秘方”，很少在特定公司之外共享。 
这些被视为 “秘方” 的专业细节，往往没有行诸于文字，克格勃虽是优秀的情报员，却没有掌握必需的技术知识，只能干着急。 
按照摩尔定律，集成电路上可以容纳的晶体管数量，每过 18 月到 24 个月便会增加一倍，也就是说，芯片处理器的性能大约两年翻一倍。 
 
即便苏联人设法复制了某个芯片设计，仍难免随着时间推移，被远远甩在后面，德州仪器和仙童每年都会推出越来越多的新的芯片设计，苏联疲于奔命的追赶复制的效果犹如牛车追汽车。 
整个苏联芯片的 “复制工程”，就像一个上下密不透风的罐头，它的客户只有一个 —— 苏联军方，机械的复制，死板的订单，官僚的系统，没有自由市场的参与，也就没有任何创新。 
苏联硅谷拥有最好的科学家，搜集技术机密的能力也位列世界翘楚，但苏联硅谷和美国硅谷是两种完全不同的事物，在市场机制下，美国硅谷的从业者可以通过 “跳槽”，获得实际的工厂经验，而苏联硅谷的模式，是通过肖金这样的官僚在办公室里发号施令。 
1985 年，美国中央情 报 局对苏联微处理器进行了解剖研究，发现苏联生产出了英特尔和摩托罗拉芯片的复制品，苏联这些复制品，如钟表一般准确，总是落后美国五年。 
第五回 张忠谋沉潜芯片蓝海，台积电攀上世界之巅 
张忠谋在德州仪器时有个花名 —— 佛陀。 
一位同事回忆，张忠谋习惯静坐着 “驱使” 员工干活，他不会声嘶力竭地发号施令，他只是坐在那里，吸着烟斗，透过烟雾看着你，目光深邃而怪异，令员工心生恐惧，当时流传一个说法，如果你没被张忠谋折磨过，你就没有在德州仪器工作过。 
佛 陀在菩提树下悟到了生 死 轮 回，张忠谋这尊 “佛 陀” 则在时代浪潮之巅的前夜悟到了芯片的未来大势。 
 
1984 年，负责中国台湾半导体项目的官员孙运璇中风，经手术抢救后醒转，开口便问：大型积体电路（大规模集成电路）的计划怎样了？ 
设立大规模集成电路实验工厂，是孙运璇卧病前的拼死一博，该厂是台湾地区的第一座 6 英寸厂，也是台积电的前身。 
孙运璇中风后，俞国华接班，上任不久，即派出 “工研院” 董事长徐贤修赴美，请张忠谋赴台主持芯片大计。 
在中国台湾的土地上，芯片界的佛 陀开始思考。 
此时的台湾地区，半导体方面，无甚可夸，既无顶级芯片设计公司，也无广阔市场，而半导体是全球性产业，唯有做到世界一流，方有立足之地。 
如果要做一家像德州仪器那样的传统半导体公司，已然错过了最好的时机，唯有找到传统半导体公司的最大弱点，然后从中切入，方能开创新天地，那么，传统半导体公司的最大弱点是什么呢？ 
张忠谋嚼着槟榔，狂饮咖啡，带着这个触碰到时代痛点的追问度过了好几个不眠的夜晚，佛陀的灰色脑细胞全负荷运转，力图在如波斯名贵地毯的时代缝隙中找到一个可落手之处，然后将手插进去用力再用力直至掀翻整个半导体行业的旧格局。 
 
当夜枭在台北的梧桐树上滋哇乱叫，当槟榔西施穿着暴露的衫裙在大街上肆意卖笑，经过几个昼夜不眠不休的佛陀穿过地狱般的冥思苦想终于在暖黄色的清晨之光中顿悟。 
在 20 世纪 80 年代，所有的半导体公司都遵循 “垂直模式”，自己亲力亲为，搞定设计和制造的全流程，卧榻之上，不容他人插手，当时所有半导体公司大佬都信奉超威公司老板桑德斯说过的一句话 —— 好汉都有晶圆厂。 
如此一来，整个半导体行业的门槛和护城墙就高耸入云端了，入行者若不趁个亿万身家，几乎无法拿到参与芯片业的门票，更别提撼动旧有秩序。 
上世纪八十年代，一方面是入行的门槛极高，另一方面像乔布斯那样的天才如火山爆发般喷薄而出，尽管苹果手机彼时尚未诞生，尽管疯狂的天才们尚未大展拳脚，但作为时代之骄子，他们必然要在未来岁月与芯片领域撞个满怀，在思想之海中冲浪的佛陀提前看到了未来，也看到了台积电在未来芯片大时代中的位置所在。 
就在上穷碧落下黄泉的苦苦思索之后，张忠谋猛然想起卡弗・米德于 1979 年写的一本书，作者在书中断言，芯片设计应该跟制造分离，也就是说，当设计独立出来时，应当有与之配套的制造企业。 
喝光咖啡无觅处，得来全不费功夫。 
张忠谋对李国鼎说，我们要创办一家纯代工的晶圆厂，这样的工厂在全世界还没有先例，但是你别怕，我已经看到了未来，代工厂一定会是未来的趋势。 
 
张忠谋先人一步，看到未来，而未来正如他所设想。 
进入 21 世纪，晶圆厂成本狂飙，2002 年，英特尔 CEO 保罗估测，建设一个晶圆厂要花费 20 亿美元。到了 2006 年，三星公司的晶圆厂成本为 40 亿美元，2014 年，三星建设存储器晶圆厂的费用达到了 140 亿美元。 
在成本压力的德摩克里斯之剑下，半导体大公司不堪重负，陆续退出芯片制造领域，转成无厂模式。就连曾经放言 “好汉都有晶圆厂” 的超威半导体总裁桑德斯，也不得不关停了晶圆厂业务。 
张忠谋这个第一个吃螃蟹的芯片佛陀，起步早，登峰快，踩中了猪也能飞的时代最大风口，腾云驾雾，高飞远翔，会当凌绝顶，一览众山小。 
在芯片的 130 纳米时代，全球尚有 22 家半导体公司有最先进的晶圆厂；到了 22 纳米时代，全球只剩英特尔、三星、IBM、台积电、格罗方德、联华电子、中芯国际 7 家半导体公司有最先进的晶圆厂；到了 7 纳米时代，只剩下台积电、三星和英特尔。当 2020 年 5 纳米芯片量产时，赛道上只剩下了台积电和三星。 
台积电创立的十年前，中国大陆响起一声春雷。 
1978 年，中国全国科学大会的现场，33 岁的任正非作为军人代表参会。 
 
科学大会快结束时，罗 瑞 卿大将把任正非等代表召集到一处，给他们做了发言，核心意思是，邓 小 平同志判断，国际形势会有一段较长的和平时期，以后中国要以经济建设为中心，中国人必须把握时代的机遇。 
市场经济的巨大机遇，中国人抓住了。 
中国的崛起以及张忠谋对于芯片的全部思考都建立在同一个地基上 —— 市场经济，而这也是苏联芯片的 “复制模式” 所缺少的。 
第六回 芯片战争戛然而止，苏联元帅一声叹息 
1983 年，苏联军方击落一架进入苏联领空的韩国民用客机。 
苏联元帅奥加科夫称，这架韩国客机的飞行员正在执行情报任务，侵入苏联领空，实属咎由自取。 
 
奥加科夫元帅的人生目标是与美国开战。 
尽管奥加科夫志向远大，但他也明白，在芯片驱动的军事力量方面，苏联已经无可挽回地落后了。 
20 世纪 80 年代，有个流行的笑话，一位克里姆林宫官员自豪地宣称，同志们，我们已经制造出了世界上最大的微处理器。 
苏联芯片的落后，迫使苏联军方的武器设计师尽可能地限制复杂电子产品在军事上的应用，使得苏联的武器系统继续保持 “愚笨”，而美国的武器则正在学习 “思考”。 
20 世纪 60 年代初，美国便在 “民兵 11 号” 导弹上安装了一台由德州仪器芯片驱动的制导计算机，而苏联的第一台使用芯片的导弹制导计算机直到 1971 年才通过测试。 
20 世纪 80 年代中期，美国的 MX 导弹被测评估计有 50% 的概率会落在目标 364 英尺以内，而一枚与之相当的苏联导弹 SS-25 只能大致落在距离目标的 1200 英尺以内。 
在战争中，几百英尺的差距足以决定成败。 
美苏两个超级大国都渴望有能力摧毁对方的核武库，即使是核弹头，也必须具有相当精准度才能摧毁加固的发射井，如果一方的精准度远高于另一方，那么更为精确的一方就有机会在第一次打击中使对手的核力量瘫痪。 
苏联最悲观的预测，如果美国在 20 世纪 80 年代发动第一次核打击，可能会致使苏联 98% 的洲际弹道导弹被摧毁。 
对战争起到关键作用的核潜艇方面，苏联与美国也不可同日而语，20 世纪 80 年代，美国核潜艇已经引入了伊利亚克四号超级计算机，这是第一台使用半导体存储芯片的计算机，该芯片由仙童公司制造。 
伊利亚克四号通过卫星与舰艇、飞机上的传感器相连，用于追踪苏联核潜艇，令后者极易被美国发现。 
奥加科夫元帅对这些数据分析之后，得出结论，美国在导弹精度、反潜艇战等方面的芯片优势可能会威胁到苏联核武库的生存能力。 
 
20 世纪 80 年代末，曾经的热血青年尤里被半导体厂解雇，原因是尤里拒绝了克格勃的指令，拒绝惩罚那些犯了 “ 政 治 错误” 的员工。 
失去了尤里等专家的忠心支持后，苏联的芯片产业更加每况愈下。 
苏联的芯片业过度依赖军事客户，几乎没有民用消费市场，而美国、欧洲和日本的芯片消费市场蓬勃发展，为半导体供应链的专业化提供了资金，诞生了从使用超纯硅片到光刻设备的各种专业公司。 
换言之，苏联的芯片产业只生产了西方芯片市场的一小部分，一位苏联从业者估计，仅日本在半导体领域的资本投资就是苏联的八倍。 
此外，苏联还缺乏国际供应链。 
美国与其盟友合作，形成了一种高效的全球化分工，日本主导了存储芯片的生产，美国生产了大部分微处理器，荷兰在光刻设备市场举足轻重，东南亚的廉价人工则完成了大部分半导体的最终装配。 
 
美国模式和苏联模式在海湾战争中受到了实践检验。 
某种程度上，海湾战争是苏式装备与美式装备的对决，伊拉克的惨败反映出苏式装备的落伍，而苏式装备的落伍实质上是苏联芯片技术的落伍。以海湾战争中备受关注的精确制导武器为例，苏联导弹的命中率有 60 米的偏差，美军的导弹偏差仅有 15 米。 
海湾战争暴露了苏联的外强中干。 
1990，苏联领导人戈尔巴乔夫意识到 “复制” 战略的局限性，主动前往美国硅谷进行正式访问，硅谷的科技大亨们为戈尔巴乔夫举办了一场沙皇式的盛宴，在这场暗流涌动的宴会上，仙童创始人和苹果 CEO 坐在戈尔巴乔夫两侧，推杯换盏，频频敬酒。 
席上，戈尔巴乔夫承诺从东欧撤出苏联军队结束冷 战，作为交换，他希望获得美国的技术，并鼓励美国的科技大亨们投资苏联。 
在之后的参观斯坦福大学行程中，戈尔巴乔夫在美国校园里走来走去，向观众击掌致意，对他们说道，冷 战已经过去了，我们不要为谁赢了而争吵。 
 
谁输谁赢，已无需赘言。 
早在 1983 年，苏联元帅奥加科夫就曾在私底下对美国记者莱斯利说，冷 战结束了，你们赢了。 
1991 年，海湾战争结束五个月后，华沙组织解散，海湾战争结束十个月后，苏联宣布解体。 
继承了苏联主要遗产的俄罗斯，在芯片领域仍然犹如困兽。 
尾声 
2000 年，因芯片的发明人罗伯特・诺伊斯已经去世，这一年的诺贝尔物理学奖颁给了芯片的共同发明人杰克・基尔比。 
 
杰克・基尔比 
与基尔比一起分享这个奖项的还有俄罗斯科学家佐瑞斯・阿尔费罗夫，他在 20 世纪 60 年代，作为苏联专家对半导体器件产生光的机理进行了基础研究。 
在那个激情人心的年份，苏联发射成功了人类的第一颗人造卫星，苏联宇航员加加林成为第一个进入太空的人类，尤里在 1962 年制造的芯片更是令苏联一度摘下科学超级大国的桂冠。 
白云苍狗，世事无常。 
苏联解体后的日子里，俄罗斯一家在苏联时期挑大梁的半导体工厂被降级，开始为麦当劳欢乐餐玩具生产小芯片。 
空空说 
对于苏联芯片的 “复制” 战略，《芯片战争》一书作者有个一针见血的观点： 
“复制” 策略意味着苏联半导体的创新道路是由美国设定的，作为苏联最敏感、最秘密的行业之一，它却仿佛是硅谷一个运营不善的前哨。 
从苏联芯片的消亡史来看，其教训主要是没有主动拥抱市场经济，沉迷在计划经济的窠臼里，而在计划经济体制下，无法充分运用市场这只手将芯片产业 “搞活”，更无法开发出芯片广阔的民用市场，幻想通过机械复制而不是自主研发与美国竞争，乃至终于功亏一篑。 
 
via: 
 苏联芯片消亡真相 原创 哲空空 蓝钻读书 2024 年 06 月 10 日 09:53 河北 https://mp.weixin.qq.com/s/1FLu6l2PDiSNgTSP_1mWxQ"
揭秘AI芯片巨头：CPU、GPU、FPGA、ASIC特性与优劣对比,2024-05-06,"1、前言 
目前，智能驾驶领域在处理深度学习AI算法方面，主要采用GPU、FPGA 等适合并行计算的通用芯片来实现加速。同时有部分芯片企业开始设计专门用于AI算法的ASIC专用芯片，比如谷歌TPU、地平线BPU等。在智能驾驶产业应用没有大规模兴起和批量投放之前，使用GPU、FPGA等已有的通用芯片可以避免专门研发定制芯片（ASIC）的高投入和高风险，但是，由于这类通用芯片设计初衷并非专门针对深度学习，因而存在性能不足、功耗过高等方面的问题。这些问题随着自动驾驶行业应用规模的扩大将会日益突出。 
本文从芯片种类、性能、应用和供应商等多角度介绍AI芯片，用于给行业内入门新人扫盲。 
2、什么是人工智能（AI）芯片？ 
从广义上讲，能运行AI算法的芯片都叫AI芯片。 
目前通用的CPU、GPU、FPGA等都能执行AI算法，只是执行效率差异较大。 
但狭义上讲一般将AI芯片定义为“专门针对AI算法做了特殊加速设计的芯片”。 
 
 
目前AI芯片的主要用于语音识别、自然语言处理、图像处理等大量使用AI算法的领域，通过芯片加速提高算法效率。AI芯片的主要任务是矩阵或向量的乘法、加法，然后配合一些除法、指数等算法。AI算法在图像识别等领域，常用的是CNN卷积网络，一个成熟的AI算法，就是大量的卷积、残差网络、全连接等类型的计算，本质是乘法和加法。 
对汽车行业而言，AI芯片的主要用于就是处理智能驾驶中环境感知、传感器融合和路径规划等算法带来的大量并行计算需求。 
AI芯片可以理解为一个快速计算乘法和加法的计算器，而CPU要处理和运行非常复杂的指令集，难度比AI芯片大很多。GPU虽然为图形处理而设计，但是CPU与GPU并不是专用AI芯片，其内部有大量其他逻辑来实现其他功能，这些逻辑对于目前的AI算法来说完全无用。目前经过专门针对AI算法做过开发的GPU应用较多，也有部分企业用FPGA做开发，但是行业内对于AI算法必然出现专用AI芯片。 
 
3、为什么要用AI芯片？ 
人工智能从功能上来看包括推理和训练两个环节，智能驾驶行业亦然。在训练环节，通过大数据训练出一个复杂的神经网络模型，目前大部分企业在训练环节主要使用英伟达的GPU集群完成。推理环节是指利用训练好的模型，使用大量数据推理出各种结论。因此，训练环节对芯片的算力性能要求比较高，推理环节对简单指定的重复计算和低延迟的要求很高。 
从应用场景来看，人工智能芯片应用于云端和设备端，在智能驾驶领域同样具备云服务器和车载的各种计算平台或域控制器，在智能驾驶深度学习的训练阶段需要极大的数据量和大量运算，单一处理器无法独立完成，因此训练环节只能在云服务器实现。相对的在设备端即车上，各种ECU、DCU等终端数量庞大，而且需求差异较大。因此，推理环节无法在云端完成，这就要求车上的各种电子单元、硬件计算平台或域控制器有独立的推理计算能力，因此必须要有专用的AI芯片来应对这些推理计算需求。 
传统的CPU、GPU都可以拿来执行AI算法，但是速度慢，性能低，尤其是CPU，在智能驾驶领域无法实际投入商用。 
 
 
比如，自动驾驶需要识别道路、行人、红绿灯等路况和交通状况，这在自动驾驶算法里面都是属于并行计算，如果是CPU去执行计算，那么估计车撞到人了也没算出来个结果，CPU并行计算速度慢属于先天不足。如果用GPU速度要快得多，毕竟GPU专为图像处理并行计算设计，但是GPU功耗过大，汽车的电池无法长时间支撑正常使用，而且GPU价格相对较高，用于自动驾驶量产的话普通消费者也用不起。另外，GPU因为不是专门针对AI算法开发的ASIC，执行AI计算的速度优势还没到极限，还有提升空间。 
在智能驾驶这样的领域，环境感知、物体识别等深度学习应用要求计算响应方面必须快！时间就是生命，慢一步就有可能造成无法挽回的情况，但是保证性能快效率高的同时，功耗不能过高，不能对智能汽车的续航里程造成较大影响，也就是AI芯片必须功耗低，所以GPU不是适合智能驾驶的最佳AI芯片选择。因此开发ASIC就成了必然。 
 
4、AI芯片的种类 
当前主流的AI芯片主要分为三类，GPU、FPGA、ASIC。GPU、FPGA均是前期较为成熟的芯片架构，属于通用型芯片。ASIC属于为AI特定场景定制的芯片。行业内已经确认CPU不适用于AI计算，但是在AI应用领域也是必不可少，另外一种说法是还有一种类脑芯片，算是ASIC的一种。 
 
 
FPGA（Field Programmable Gate Array，现场可编程门阵列）具有足够的计算能力和足够的灵活性。FPGA的计算速度快是源于它本质上是无指令、无需共享内存的体系结构。对于保存状态的需求，FPGA中的寄存器和片上内存（BRAM）是属于各自的控制逻辑的，无需不必要的仲裁和缓存，因此FPGA在运算速度足够快，优于GPU。同时FPGA也是一种半定制的硬件，通过编程可定义其中的单元配置和链接架构进行计算，因此具有较强的灵活性。相对于GPU，FPGA能管理能运算，但是相对开发周期长，复杂算法开发难度大。 
ASIC（Application Specific Integrated Circuit特定用途集成电路）根据产品的需求进行特定设计和制造的集成电路，能够在特定功能上进行强化，具有更高的处理速度和更低的能耗。缺点是研发成本高，前期研发投入周期长，且由于是定制化，可复制性一般，因此只有用量足够大时才能够分摊前期投入，降低成本。 
4.1 CPU (CentralProcessing Unit) 
中央处理器作为计算机系统的运算和控制核心，是信息处理、程序运行的最终执行单元，CPU 是对计算机的所有硬件资源（如存储器、输入输出单元） 进行控制调配、执行通用运算的核心硬件单元。 
优点：CPU有大量的缓存和复杂的逻辑控制单元，非常擅长逻辑控制、串行的运算 
缺点：不擅长复杂算法运算和处理并行重复的操作。 
对于AI芯片来说，算力最弱的是cpu。虽然cpu主频最高，但是单颗也就8核,16核的样子，一个核3.5g,16核也就56g，再考虑指令周期，每秒最多也就30g次乘法。还是定点的。 
生产厂商:intel、AMD 
4.2 GPU (GraphicsProcessing Unit) 
图形处理器，又称显示核心、视觉处理器、显示芯片，是一种专门在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上做图像和图形相关运算工作的微处理器。 
优点：提供了多核并行计算的基础结构，且核心数非常多，可以支撑大量数据的并行计算，拥有更高的浮点运算能力。 
缺点：管理控制能力（最弱），功耗（最高）。 
生产厂商：AMD、NVIDIA 
 
 
4.3 FPGA（Field Programmable Gate Array） 
FPGA是在PAL、GAL等可编程器件的基础上进一步发展的产物。它是作为专用集成电路（ASIC）领域中的一种半定制电路而出现的，既解决了定制电路的不足，又克服了原有可编程器件门电路数有限的缺点。  
优点：可以无限次编程，延时性比较低，同时拥有流水线并行和数据并行（GPU只有数据并行）、实时性最强、灵活性最高 
缺点：开发难度大、只适合定点运算、价格比较昂贵 
生产厂商：Altera（Intel收购）、Xilinx 
 
 
4.4 ASIC（Application Specific IntegratedCircuit） 
ASIC,即专用集成电路，指应特定用户要求和特定电子系统的需要而设计、制造的集成电路。目前用CPLD（复杂可编程逻辑器件）和 FPGA（现场可编程逻辑阵列）来进行ASIC设计是最为流行的方式之一。 
优点:它作为集成电路技术与特定用户的整机或系统技术紧密结合的产物，与通用集成电路相比具有体积更小、重量更轻、 功耗更低、可靠性提高、性能提高、保密性增强、成本降低等优点。 
缺点：灵活性不够，成本比FPGA贵 
主要性能指标：功耗、速度、成本 
生产厂商：谷歌、地平线、寒武纪等 
 
 
4.5 四种芯片的特性总结 
CPU是一个有多种功能的优秀领导者。它的优点在于调度、管理、协调能力强，计算能力则位于其次。而GPU相当于一个接受CPU调度的“拥有大量计算能力”的员工。 
GPU 作为图像处理器，设计初衷是为了应对图像处理中需要大规模并行计算。因此，其在应用于深度学习算法时，有三个方面的局限性： 
第一，应用过程中无法充分发挥并行计算优势。深度学习包含训练和应用两个计算环节，GPU 在深度学习算法训练上非常高效，但在应用时一次性只能对于一张输入图像进行处理，并行度的优势不能完全发挥。 
第二，硬件结构固定不具备可编程性。深度学习算法还未完全稳定，若深度学习算法发生大的变化，GPU 无法像FPGA 一样可以灵活的配置硬件结构。 
第三，运行深度学习算法能效远低于FPGA。学术界和产业界研究已经证明，运行深度学习算法中实现同样的性能，GPU 所需功耗远大于FPGA，例如国内初创企业深鉴科技基于FPGA 平台的人工智能芯片在同样开发周期内相对GPU 能效有一个数量级的提升。 
FPGA，其设计初衷是为了实现半定制芯片的功能，即硬件结构可根据需要实时配置灵活改变。 
研究报告显示，目前的FPGA市场由Xilinx 和Altera 主导，两者共同占有85%的市场份额，其中Altera 在2015 年被intel以167 亿美元收购， Xilinx则选择与IBM进行深度合作，背后都体现了 FPGA 在人工智能时代的重要地位。 
尽管 FPGA 倍受看好，甚至百度大脑、地平线AI芯片也是基于FPGA 平台研发，但其毕竟不是专门为了适用深度学习算法而研发，实际仍然存在不少局限： 
第一，基本单元的计算能力有限。为了实现可重构特性，FPGA 内部有大量极细粒度的基本单元，但是每个单元的计算能力（主要依靠LUT 查找表）都远远低于CPU 和GPU中的ALU模块。 
第二，速度和功耗相对专用定制芯片（ASIC）仍然存在不小差距。 
第三，FPGA 价格较为昂贵，在规模放量的情况下单块FPGA 的成本要远高于专用定制芯片。 
人工智能定制芯片是大趋势，从发展趋势上看，人工智能定制芯片将是计算芯片发展的大方向。 
5、AI芯片算力对比 
5.1 通用芯片—GPU 
GPU（Graphics Processing Unit）即为图形处理器。NVIDIA公司在1999年发布GeForce256图形处理芯片时首先提出GPU的概念。从此NVIDIA显卡的芯就用这个新名字GPU来称呼。GPU使显卡削减了对CPU的依赖，部分替代原本CPU的工作，特别是在3D图形处理方面。由于在浮点运算、并行计算等方面，GPU可以提供数十倍乃至于上百倍于CPU的性能。 
GPU相比CPU更适合人工智能计算。GPU和CPU分别针对的是两种不同的应用场景，他们的设计目标不同，CPU需要很强的通用性来处理各种不同的数据类型，同时逻辑判断又会引入大量的分支跳转和中断的处理。这些都使得CPU的内部结构异常复杂。而GPU擅长的则是在不需要被打断的纯净的计算环境中进行类型高度统一的、相互无依赖的大规模数据处理，人工智能的计算恰巧主要是后者，这使得原本为图像处理而生的GPU在人工智能时代焕发第二春。 
CPU的逻辑运算单元（ALU）较少，控制器（control）占比较大；GPU的逻辑运算单元（ALU）小而多，控制器功能简单，缓存（cache）也较少。架构的不同使得CPU擅长进行逻辑控制、串行计算，而GPU擅长高强度的并行计算。GPU单个运算单元处理能力弱于CPU的ALU，但是数量众多的运算单元可以同时工作，当面对高强度并行计算时，其性能要优于CPU。现如今GPU除了图像处理外，也越来越多的运用到别的计算中。 
CPU根据功能划分，将需要大量并行计算的任务分配给GPU。GPU从CPU获得指令后，把大规模、无结构化的数据分解成许多独立部分，分配给各个流处理集群（SMM）。每个流处理集群再次把数据分解，分配给调度器，调度器将任务放入自身所控制的计算核心core中完成最终的数据处理任务。 
GPU性能较强但功耗较高。以NVIDIA开发的GPU为例，Xavier最高算力为30Tops，功耗为30W，NVIDIA最新发布的GPUA100相比Volta架构的640个Tensor Core，A100核心的TensorCore减少到了432个，但是性能大幅增强，支持全新的TF32运算，浮点性能156TFLOPS，同时INT8浮点性能624TOPS，FP16性能312TFLOPS，同时功耗也达到了400W。 
5.2 半定制化芯片—FPGA 
FPGA（Field－ProgrammableGate Array），即现场可编程门阵列。它是在PAL、GAL、CPLD等可编程器件的基础上进一步发展的产物。它是作为专用集成电路（ASIC）领域中的一种半定制电路而出现的，既解决了定制电路的不足，又克服了原有可编程器件门电路数有限的缺点。 
FPGA内部有很多可配置逻辑模块（CLB），这些模块是现实逻辑功能的基本单元，FPGA可通过灵活地配置CLB来令其实现工程师想要实现的逻辑功能。FPGA的并行处理能力也很强大，其可编程性也适用于不断优化的深度学习算法的运算。目前很多公司基于FPGA开发人工智能处理器。于2016年成立的深鉴科技，就在研发深度学习通用解决方案。2016年初，深鉴科技就设计了基于FPGA、针对深度学习的DPU硬件架构。该产品实现了高性能功耗比，并且成本也比GPU产品低很多。今年8月加州的Hot Chips大会上，百度也发布了其基于FPGA芯片的A.I加速芯片—XPU。该芯片有256核，旨在寻求性能和效率的平衡，处理多样化计算任务。 
基于FPGA开发的人工智能处理器具有高性能、低能耗、可硬件编程的特点。 
1）高性能 
除了GPU，FPGA也擅长并行计算，基于FPGA开发的处理器可以实现更高的并行计算。而且FPGA带有丰富的片上存储资源，可以大大减少访问片外存储的延迟，提高计算性能，访问DRAM储存大约是访问寄存器存储延迟的几百倍以上。 
2）低能耗 
相比于CPU和GPU，FPGA的能耗优势主要有两个原因：1）相比于CPU、GPU，FPGA架构有一定的优化，CPU、GPU需要频繁的访问DRAM，而这个能量消耗较大，FPGA可以减少这方面的能耗。2）FPGA的主频低，CPU和GPU的主频一般在1-3GHz之间，而FPGA的主频一般在500MHz一下。因此，FPGA的能耗要低于CPU、GPU。 
3）可硬件编程 
FPGA可硬件编程，并且可以进行静态重复编程和动态系统重配置。用户可像编程修改软件一样修改系统的硬件功能，大大增强了系统设计的灵活性和通用性。使得FPGA可以灵活地部署在需要修改硬件设置场景中。 
FPGA+CPU异构架构被越来越多地研究和认可。相比于CPU+GPU，因为FPGA的高性能低功耗等优势使FPGA+CPU可以提供更好的单位功耗性能，且更易于修改和编程。因此FPGA适合做可并行计算的任务，如矩阵运算。如果是一些判断类的问题，FPGA算得并没有CPU快。所以已经有研究人员探讨FPGA+CPU的架构模式。 
5.3 全定制芯片—ASIC 
ASIC（Application Specific IntegratedCircuit）在集成电路界被认为是一种为专门目的而设计的集成电路。是指应特定用户要求和特定电子系统的需要而设计、制造的集成电路。ASIC的特点是面向特定用户的需求，ASIC在批量生产时与通用集成电路相比具有体积更小、功耗更低、可靠性提高、性能提高、保密性增强、成本降低等优点。简单地讲，ASIC芯片就是通过台积电等代工厂流片的芯片。目前，基于ASIC的人工智能芯片有地平线BPU、谷歌的TPU。 
基于ASIC开发人工智能芯片开发周期较长。基于ASIC开发人工智能芯片更像是电路设计，需要反复优化，需要经历较长的流片周期，故开发周期较长。 
量产后ASIC人工智能芯片成本及价格较低。虽然相较于FPGA， ASIC人工智能芯片需要经历较长的开发周期，并且需要价格昂贵的流片投入，但是这些前期开发投入在量产后会被摊薄，所以量产后，ASIC人工智能芯片的成本和价格会低于FPGA人工智能芯片。 
ASIC芯片性能功耗比较高。从性能功耗比来看，ASIC作为定制芯片，其性能要比基于通用芯片FPGA开发出的各种半定制人工智能芯片更具有优势。而且ASIC也并不是完全不具备可配置能力，只是没有FPGA那么灵活，只要在设计的时候把电路做成某些参数可调即可。 
ASIC人工智能芯片主要面向消费电子市场。ASIC更高的性能，更低的量产成本以及有限可配置特性，使其主要面向消费电子市场，如寒武纪等公司。 
5.4 类脑芯片 
类人脑芯片架构是一款基于神经形态的工程，旨在打破“冯·诺依曼”架构的束缚，模拟人脑处理过程，感知世界、处理问题。这种芯片的功能类似于大脑的神经突触，处理器类似于神经元，而其通讯系统类似于神经纤维，可以允许开发者为类人脑芯片设计应用程序。通过这种神经元网络系统，计算机可以感知、记忆和处理大量不同的信息。类脑芯片的两大突破：1、有望形成自主认知的新形式；2、突破传统计算机体系结构的限制，实现数据并行传送、分布式处理，能以极低功耗实时处理大量数据。 
 
6、总结 
 
 
CPU 有强大的调度、管理、协调能力。应用范围广。开发方便且灵活。但其在大量数据处理上没有 GPU 专业，相对运算量低，但功耗不低。 
GPU：是单指令、多数据处理，采用数量众多的计算单元和超长的流水线，如名字一样，图形处理器，GPU善于处理图像领域的运算加速。但GPU无法单独工作，必须由CPU进行控制调用才能工作。CPU可单独作用，处理复杂的逻辑运算和不同的数据类型，但当需要大量的处理类型统一的数据时，则可调用GPU进行并行计算。 
FPGA：和GPU相反，FPGA适用于多指令，单数据流的分析，因此常用于预测阶段，如云端。FPGA是用硬件实现软件算法，因此在实现复杂算法方面有一定的难度，缺点是价格比较高。将FPGA和GPU对比发现，一是缺少内存和控制所带来的存储和读取部分，速度更快。二是因为缺少读取的作用，所以功耗低，劣势是运算量并不是很大。结合CPU和GPU各自的优势，有一种解决方案就是异构。 
ASIC芯片：是专用定制芯片，为实现特定要求而定制的芯片。除了不能扩展以外，在功耗、可靠性、体积方面都有优势，尤其在高性能、低功耗的移动端。谷歌的TPU、寒武纪的MLU，地平线的BPU都属于ASIC芯片。谷歌的TPU比CPU和GPU的方案快30-80倍，与CPU和GPU相比，TPU把控制缩小了，因此减少了芯片的面积，降低了功耗。 
四种架构将走向哪里？ 
众所周知，通用处理器（CPU）的摩尔定律已入暮年，而机器学习和Web 服务的规模却在指数级增长。 
人们使用定制硬件来加速常见的计算任务，然而日新月异的行业又要求这些定制的硬件可被重新编程来执行新类型的计算任务。 
将以上四种架构对比，GPU未来的主攻方向是高级复杂算法和通用性人工智能平台，其发展路线分两条走：一是主攻高端算法的实现，对于指令的逻辑性控制要更复杂一些，在面向需求通用的AI计算方面具有优势；二是主攻通用性人工智能平台，GPU的通用性强，所以应用于大型人工智能平台可高效完成不同的需求。FPGA更适用于各种细分的行业，人工智能会应用到各个细分领域。 
ASIC芯片是全定制芯片，长远看适用于人工智能。现在很多做AI算法的企业也是从这个点切入。因为算法复杂度越强，越需要一套专用的芯片架构与其进行对应，ASIC基于人工智能算法进行定制，其发展前景看好。类脑芯片是人工智能最终的发展模式，但是离产业化还很遥远。 
几个品牌的SOC及域控制器做的还是不错的，尤其是基于NVIDIA Xavier以及前期PX2等芯片的开发。国内大部分企业的应用比较集中在Xavier平台和Linux系统，尤其是新势力造车企业，而传统车企更青睐TI、瑞萨等半导体公司的智能AI芯片以及QNX系统。国内基于Xavier做开发的企业很多，天津优控智行目前的域控制器产品在行业内属于中等偏上水平，但是其软件工具和服务做得相对有些优势，后期有时间也扒一扒地平线、智行者等企业的域控制器学习学习。"
类脑智能：基于脑启发的智能研究新范式,2023-10-13,"来源：Bacomics 
 作者：孙宏泽 郭大庆 
 当前，以大模型为代表的人工智能呈现出高耗能、训练困难、不可解释的典型特点，并在安全性和伦理对齐方面暴露出明显的短板。为了缓解和规避这些问题，引导人工智能向通用人工智能发展，研究人员开始重新审视脑科学研究对于智能模型的积极意义。由此，以神经元和神经环路为基本要素，受大脑结构和功能启发的类脑智能成为智能概念研究的新风口，有望真正实现低功耗、可解释的通用人工智能。 

 ▲图1：类脑智能研究现状及脉冲神经网络 
 截至目前，类脑智能研究可以从四个水平进行概括：神经元水平、模型水平、算法水平和硬件水平（如图1）。其中，神经元水平的研究注重对生物神经元复杂机制的挖掘，主要包括动力学复杂的神经形态计算、树突计算等等。模型水平上，研究人员通过从神经环路、脑模拟等不同维度进行智能模型的模拟，或者使用和传统ANN相同的前馈层级结构进行普适性的神经网络构建。而类脑智能模型的算法设计具有相对较为宽泛的理论约束，神经环路的结构、功能，人类智能的行为模式都可以作为启发类脑智能算法的理论依据，并在很多具体智能任务上取得了较好的成果。为了适应类脑智能在具体应用上的部署，面向类脑的芯片设计也成为很多研究人员关注的重点，这其中比较有代表性的包括神经形态芯片和基于第四类电子元器件的忆阻器芯片。通过定制化的芯片设计，类脑智能算法在边缘端和云计算等领域都展现出了优异的性能。为了兼顾神经元动力学复杂性和模型计算的简洁性，现阶段类脑智能的研究通常以LIF（Leaky Integrate and Fire）神经元搭建的脉冲神经网络为基本工具。通过基于代理梯度的时空反向传播等算法，脉冲神经网络可以像传统人工神经网络一样进行参数优化，并实现应用层面的快速部署。凭借脉冲神经网络，大量类脑智能相关研究实现了快速发展。 

 ▲图2：脑启发算法的一些代表性工作 
 受大脑认知机制原理启发，当前脑启发算法从功能、结构、行为等不同水平对人脑智能模式进行了深入研究。其中，受神经可塑性机制启发，研究人员通过给基础LIF神经元引入可训练阈值、参数化膜时间常数等神经元内源可塑性机制，从而构建了神经元协同学习、神经元动态自适应等脉冲神经网络训练方法，在图像分类、时间信度分配等智能任务上取得了性能的提升[1][2]。同传统的人工智能研究相同，注意力机制在类脑智能当中同样是一个重要的课题。并且因为类脑智能强调更严格的生物约束，因此，脑启发的注意力算法在结构设计上具有更强的解释性，可以为以后的注意力相关研究提供指导[3]。此外，受人类智能行为启发，脑启发算法在缓解灾难性遗忘问题上也展现了较大的潜力。这其中，通过模仿人类睡眠、复习、知识梳理等行为，研究人员使类脑神经网络具备了连续学习的能力[4]。这类研究为GPT等预训练模型在类脑网络上的展开提供了基础，有望成为连接类脑智能和LLAMA模型的桥梁。 

 ▲图3：类脑智能应用示例 
 考虑到类脑智能模型在低功耗、稀疏表征、可解释性上的优势，研究人员开始尝试在经典的智能场景中，部署类脑智能应用。其中，在多模态目标跟踪、机器人空间定位、多尺度脑模拟、机器人群体控制等任务上，类脑智能模型取得了具有竞争力的结果[5][6]。相较于传统人工智能模型，类脑智能模型在硬件平台上的功耗更低；且受益于LIF模型具有更高的时空动力学复杂性，类脑智能模型的设计通常更简洁，呈现出更好的生物合理性。除了上述已经初步形成规模的类脑智能应用实例外，基于类脑智能的机器人精细控制、人脸检测识别、运动估计等更具有难度的智能场景应用研究也已经逐渐展开，并产生了初步的成效。 

 ▲图4：面向工程的类脑智能优化问题 
 随着类脑智能模型的成熟和应用场景的复杂化，面向工程的类脑智能优化问题日趋重要。而LLAMA的成功，使预训练大模型逐渐成为新的研究热点，在类脑智能领域，基于脉冲神经网络的类GPT模型也逐渐被提出[7]。因此，为了压缩模型的参数、降低模型的功耗，面向类脑智能的网络轻量化方案变得越来越重要。其中，基于跨模型知识蒸馏的参数压缩方法，可以保证压缩后的学生模型与压缩前的教师模型具有相同的表征空间，并且无需迭代训练[8]。在保证模型性能损失可控的情况下，跨模型知识蒸馏方法可以大幅度压缩模型参数至原始模型的10%以内，展现出优异的效果。而面向边缘端计算的类脑芯片，是类脑智能定制化计算加速的重要手段。除了以IBM TrueNorth和浙大达尔文为代表的神经形态芯片外，第四代元器件-忆阻器因其独有的阈值转换特性，也催生出了一系列高效、低功耗的忆阻器芯片。当前，类脑芯片仍然具有很大的发展空间。但研究人员相信，随着技术的革新和软硬件兼容性的提升，未来类脑芯片将会成为当前芯片解决方案的重要补充力量。 
 当前，世界主要国家和组织都将脑科学当做本世界最为重要的研究课题之一，为了对人脑智能涌现机制进行深入挖掘，不同规模的“脑计划”正如火如荼的展开。而作为一种基于脑启发的智能研究新范式，类脑智能也有望乘脑科学研究的东风，为面向更复杂场景的智能研究提供一种全新的可能。 
 参考文献： 
 [1] Zhang, Anguo, et al. ""Event-driven intrinsic plasticity for spiking convolutional neural networks."" IEEE Transactions on Neural Networks and Learning Systems 33.5 (2021): 1986-1995. 
 [2] Sun, Hongze, et al. ""A synapse-threshold synergistic learning approach for spiking neural networks."" IEEE Transactions on Cognitive and Developmental Systems (2023). 
 [3] Cai, Wuque, et al. ""A Spatial–Channel–Temporal-Fused Attention for Spiking Neural Networks."" IEEE Transactions on Neural Networks and Learning Systems (2023). 
 [4] Tadros, Timothy, et al. ""Sleep-like unsupervised replay reduces catastrophic forgetting in artificial neural networks."" Nature Communications 13.1 (2022): 7742. 
 [5] Yu, Fangwen, et al. ""Brain-inspired multimodal hybrid neural network for robot place recognition."" Science Robotics 8.78 (2023): eabm6996. 
 [6] Shen, Guobin, et al. ""Brain-inspired neural circuit evolution for spiking neural networks."" Proceedings of the National Academy of Sciences 120.39 (2023): e2218173120. 
 [7] Zhu, Rui-Jie, Qihang Zhao, and Jason K. Eshraghian. ""Spikegpt: Generative pre-trained language model with spiking neural networks."" arXiv preprint arXiv:2302.13939 (2023). 
 [8] Xu, Qi, et al. ""Constructing deep spiking neural networks from artificial neural networks with knowledge distillation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. 
 未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）大脑研究计划，构建互联网（城市）大脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。每日推荐范围未来科技发展趋势的学习型文章。目前线上平台已收藏上千篇精华前沿科技文章和报告。 
   如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”"
类脑计算讲解,2023-06-07,"承接 3D模型制作 
当前，人工智能的发展有两个主要路径，一个是沿计算机科学发展而来的深度学习途径，另一个是沿着模仿人脑发展而来的类脑计算途径。 
类脑计算途径 
这个方向是以模拟人脑神经网络计算为基础而发展出的一种新型芯片，通过模拟神经元和突触的典型特征，如存算一体、脉冲编码、异步计算、动力学模型等特点，希望通过这些模拟从而达到更高的智能水平。典型代表包括IBM TrueNorth、Intel Loihi 和基于ARM架构发展而来的SpiNNaker。 
定义 
总体来说，类脑计算可分为狭义和广义两类： 
狭义的类脑计算是模仿大脑神经结构和工作原理而创新出来的神经形态计算（如SNN芯片设计）、SNN类脑模型、脑仿真、STDP学习方法和类脑智能应用，学术界称其为 Brain-like Computing（也称做Neuromorphic Computing）。 
广义的类脑计算是借鉴脑的结构和工作原理，但又不局限对脑的模拟（SNN），还包括融合传统的人工神经网络(ANN)等具有更多类脑特征的异构神经网络，是一种融合当前计算机科学和神经科学的计算发展途径，学术界称其为Brain-inspired Computing。 
人脑特性 
结构：人脑由大量神经元通过突触连接在一起，每个神经元通过接受发送脉冲、建立新的突触，从而构建起一个庞大的计算网络。 
工作机制：生物神经元主要由树突、轴突和胞体组成。树突负责收集其他神经元传来的信息并通过电流的形式将其传给胞体, 胞体相当于一个中央处理器, 树突传来的电流引起胞体膜电位变化，当膜电位超过一定阈值时，神经元将发出一个脉冲信号，通过轴突传给其他神经元。 
只有当膜电位达到放电阈值才会激发神经脉冲传递信息，大部分的神经元在活动周期中都处于静息状态，从而实现大脑的超低功耗高效运转。
类脑计算如何模仿大脑的计算机制 
与传统冯诺依曼架构下的计算机相比，人脑无法高效地进行精确的数值计算，但人脑的优势在于智能和高效。例如，自动忽略非必要信息，从而减少能耗；在处理信息的同时能够对其进行加工、理解、抽象和归纳。此类功能在识别物体，对视频、音频数据的理解等场景中均有体现。类脑计算就是以人脑为蓝本，实现新一代计算体系结构，主要思路是以事件触发（Event-driven）为运算机制，基于脉冲神经网络来仿真人脑，从而构造新型计算系统。 
脉冲神经网络(SNN) 
脉冲神经网络是受生物启发的新一代人工神经网络，以脑科学为导向，沿着脑模拟方向发展，以0/1脉冲序列表达信息流，编码里包含了时间信息；另外，神经元内部具有动力学特征，具有事件驱动、稀疏发放等特点。 
主要特征 
• 时序性（与ANN最大的区别） 通过脉冲编码将输入信息转换成脉冲序列信号，并在信息传递过程中维持脉冲之间的时间关系。因此，神经元具有微观的记忆特性。 
• 事件驱动（事件驱动是实现稀疏计算的根本基础） SNN中的神经元不会在每个信息传递的周期都被激活，只有当神经元膜电压积累到特定的电压阈值时才会放电（发放脉冲）。神经元激活后，会发放一个脉冲信号传递给其他神经元，其他神经元会根据这个脉冲信号增加或减少膜电压。 
关键要素 
任何脉冲神经元的设计都要考虑三个问题：神经元模型，更新连接权值的机制和神经元互联结构。 
• 大脑中神经元模型与网络结构在人出生之时已基本形成，且在人的成长过程中几乎不会发生大的变化。变化主要体现在成长过程中与外界环境不断交互，获取新知识并与已有知识建立联系，从而提升问题解决和环境自适应能力。 
• 基于脉冲神经网络的仿脑特性，其学习规则的构建处于次要位置，而对学习机制的模仿是脉冲神经网络发展和应用的关键。 
• 如何找到一个兼具生物合理性和计算效率的脉冲神经元模型，以及如何建立脉冲神经元模型与AI任务之间的关系，是类脑计算领域的核心问题。目前学术界和产业界对于类脑计算的探索及前沿应用主要围绕SNN及其扩展的神经网络展开，因此我们将关注聚焦在SNN上。 
新型存储器件-忆阻器 
忆阻器结构简单、集成密度高，理论上一个忆阻器就可以实现一个神经突触的功能，可极大的提升突触密度，是从硬件层面实现类脑神经网络的高效方式之一。 
对于忆阻材料，由于其可以仿真神经突触之间的突触强度，可一定程度简化神经元网络设计，但在算法层面上要研发更匹配的神经网络模型。 
常见的忆阻器可以分为阻变忆阻材料(RRAM)、磁性忆阻材料(MRAM)、相变忆阻材料(PCRM)和其他新型忆阻材料等。 
目前，热门的研究方向集中在RRAM和MRAM，其中已有公司在做RRAM的商业落地。 
基于此，分析师认为RRAM有望率先走向技术成熟，并实现商业化转变。"
窥探当今AI芯片中的类脑模型（转）,2022-04-06,"来源：脑人言 
 撰文丨邓 磊（加州大学-圣塔芭芭拉分校UCSB 博士后） 
 责编丨高茂森 Soma 
 排版丨夏獭 





 科学研究的动机，并非仅仅是出于能看到多酷炫的应用，而是因为我们尚有诸多未知世界需要探寻，这是一切应用的前提。 








 AI芯片如火如荼 

 AI芯片是当今人工智能热潮中不可或缺的一个名词，且受国际局势影响愈加受到重视。摩尔定律难以为继，通用处理器的性能提升速度大为减缓，在晶体管物理微缩和计算体系架构改进带来的收益之争中，后者渐处上风。 
 在此背景下，学术界和工业界纷纷将视野转向针对特定领域设计高性能的专用芯片，拟将架构设计发挥到极致。 
 AI芯片作为最为典型的领域专用芯片代表，受到国内外研发单位的高度关注。国外有谷歌、苹果、英特尔、IBM、英伟达等巨头领衔，国内也有寒武纪、地平线、深鉴、华为、阿里等公司响应，最近清华大学的天机芯更是登上了Nature杂志的封面[1]，其形势可谓如日中天。 
   
 然而，AI芯片中所谓的“智能”究竟源自于哪些模型，又和我们的大脑有何关系，他们的过去和未来将会怎样，本文带你一探究竟。 








 人工神经网络 






 基本模型 
 深度学习又是当今机器学习的宠儿，其以人工神经网络（Artificial Neural Network, ANN）为主要模型。 
 ANN由大量神经元（Neuron）通过突触（synapse）连接而成，从输入到输出呈现层级结构，当层数较多时则被称为深度神经网络（Deep Neural Networks）。 
 图1 人工神经元示意图 
 每个神经元的基本结构如图1所示，而基本的计算原理为，其中x和y分别为输入和输出激活值、w为突触连接的权重值、b为偏置值、φ为非线性激活函数。 
  (a) 全连接神经网络；(b) 反馈神经网络；(c)卷积神经网络。 
 根据神经元的连接拓扑可以分为全连接神经网络、卷积神经网络、反馈神经网络等，如图2所示。 
 相比于全连接神经网络，卷积神经网络通过引入二维特征图与突触核的卷积操作获得了强大的局部特征提取能力，被广泛用于图像处理领域。 
 而反馈神经网络通过引入反馈连接，建立时序动力学模型，被广泛用于处理语音文本等时序数据。 
 其实，ANNs模型的建立也是在不同时期中分别借鉴了神经科学的知识： 






 人工神经网络的概念以及基本神经元模型于1943年就已提出[2]，这正是试图模拟脑皮层以神经元网络形式进行信息处理的体现。 
 卷积神经网络的局部感受野是受到大脑视觉系统的启发。 
 深度神经网络的层级构建是源于脑皮层的分层通路。 

 只不过在深度学习的后续发展中，研究者更加偏重把神经网络视为一个黑匣，用于拟合从输入到输出的复杂映射关系： 





 只需要给网络的输出定义一个收敛目标（目标函数，比如每张图像的输出对应到正确的类别）并描述为一个优化问题，然后用梯度下降的方式去更新系统参数主要是突触权重,使得输出逐渐逼近想要的结果。 
 原则上网络越大，特征提取的能力就会越强，也就需要越多的数据来训练网络更新参数使网络收敛，因此计算量也大幅增加。 
 故而，深度学习也被称为数据和算力驱动的智能。虽然深度学习以解决实际应用为目标而与神经科学渐行渐远，但近两年也有科学家试图在大脑中找到梯度下降的证据和吸收新的脑科学成果[3-8]。 



 深度学习的腾飞之路 
 20世纪末到21世纪初很长一段时间内神经网络方法不被多数人看好，其表现也不如传统机器学习模型，多亏图灵奖得主Hinton、LeCun和Bengio三位前辈的坚持，才有了今天的成就。 
 深度学习的飞速发展主要归功于大型数据库的诞生（如ImageNet）、高性能计算平台（如GPUs）的出现、友好开发软件的耕耘（如Tensorflow/Pytorch等）和神经网络模型的改进（如卷积神经网中的VGG/Inception/ResNet/DenseNet），缺一不可。 
 由于成熟的算法、工具和应用场景，深度学习受到学术界和工业界的一致认可。深度学习加速器也现身各大学术顶会，并受到巨头公司和创业公司的青睐，这其中就包括研发TPU的谷歌、手机内置神经引擎的苹果以及国内的寒武纪、地平线、深鉴科技、华为、阿里等，当前主要热门方向为研制运行ANNs模型的高性能云计算平台和低功耗终端器件。 







 脉冲神经网络 





 基本模型 

 与深度学习不同，神经形态计算（neuromorphic computing）是目前智能模型中的又一大主要分支，它更加注重模拟大脑回路的行为，这里给大家介绍两类神经形态模型：脉冲神经网络（Spiking Neural Network, SNN）和神经动力学网络（Neural Dynamic Network）。 
 SNN与ANN主要有两大不同，其一是采用脉冲编码—spike code(0/1)，其二是具有丰富的时间动力学。SNN神经元如图3所示，其模型可简单描述为： 

 其中，(t)为时间步，t是时间常数，u为神经元膜电位，s为神经元的脉冲输出，和分别为神经元静息电位和重置电位，为脉冲发放阈值，表示每个神经元会累加对应突触输入脉冲序列中一定时间窗内的脉冲信号，用于刻画时间衰减效应(距离当前时刻越近的脉冲输入对膜电位影响越大)。 

 图3 脉冲神经元示意图 




 135编辑器 
 虽然上述以微分方程为基础的SNN神经元模型看起来比ANN模型更为复杂，但它已经是最为简化的SNN模型了，被称为泄漏积分发放模型(Leaky Integrate and Fire, LIF)[9]，其它更为复杂的模型如Izhikevich[10]和Hodgkin & Huxley[11]仅凭借现有计算机还难以仿真大规模网络。 
 原理上，SNN的时间积分效应可以使得其能够处理时序问题，尤其是稀疏数据（事实上大脑神经元发放频率也是非常稀疏的）；其膜电位泄漏和阈值发放效应，进一步使得其能够具有一定的去噪功能（事实上大脑脉冲信号也具有很多噪音，但大脑仍能正常工作）。 
 这两个特性，目前的ANN模型都不具备(反馈神经网络尽管具备时域处理能力，但与SNN很不相同)。虽然理论上有较大潜力，但由于SNN神经元的多变量时空动力学比较复杂，而脉冲发放活动又不可导，导致其无法使用具有全局性的梯度下降算法进行学习。 




 135编辑器 
 在相当长一段时间里，其主要的学习规则都是非监督的脑启发STDP(spike timing dependent plasticity)规则[12]，仅利用突触两端神经元的局部脉冲响应来更新突触权重以满足学习目标。  
 虽然STDP具有一定的生物基础，但它太过于局部化，难以获得类似梯度下降算法的全局优化能力，导致其在诸多应用中表现差于ANN模型。 
 直到最近三年，才有研究者通过将预先采用梯度下降算法训练好的ANN模型转换为其SNN版本[13,14]，或者通过对脉冲发放函数进行导数逼近，进而直接计算每个时刻膜电位和脉冲活动的梯度以实现基于梯度下降的有监督算法学习[15,16]，这些努力逐渐使得SNN的应用性能接近目前的ANN模型。 



 争议丛生但未来可期 





 支持SNN的器件通常称为神经形态器件（如欧洲的SpiNNaker[17]和DYNAP[18]，IBM的TrueNorth[19]，Intel的Loihi[20]），其研究的动机主要是因为SNN的类脑特性使其更有希望通过构建大规模系统获得类脑智能。 

 不过从应用角度出发，目前的神经形态器件大多只能体现低功耗的特性，这是由于SNN神经元的输入输出信号为二值脉冲的缘故，可以去除输入和突触权重运算中的笨重乘法，再加之脉冲活动非常稀疏且允许事件驱动的异步电路设计。 
 而在功能方面，SNN所表现出的性能尚不能与深度学习匹敌，这也是工业界对神经形态方案的热情有所消退甚至持怀疑和观望态度的原因。 




 135编辑器 
 但笔者认为，目前的神经形态计算还有很大的提升空间，未来仍有可期： 
 一方面，深度学习的进步是数据、算力、软件和模型共同进步的结果，非一日之功，而神经形态计算在上述优化上还处于起步阶段。期待研究者们继续从各个子领域努力前行，逐步迭代发展。 







 神经动力学网络 





 基本模型 

 这类模型通常用于研究脑回路的响应特性，成果相对分散。这里给大家介绍比较典型的连续吸引子网络（Continuous Attractor Neural Network, CANN）[21,22]，其神经元模型可以简单总结为如下： 
   

  其中大多数变量含义与SNN类似。此外，x为神经元在二维空间中的坐标位置（如图4所示，以二维CANN为例），为两个对应位置神经元之间的突触连接权重，是神经元发放频率，为外部刺激，β和k为常数。 
 可以看到，CANN把SNN中“从膜电位到脉冲信号”的转换调整为了“从膜电位到发放频率”的转换，公式中的除法归一化体现了大脑中普遍存在的神经元抑制作用，使得发放频率不会无限增长，整个系统趋于稳定。 




 135编辑器 
 在CANN中，突触权重通常配置为高斯峰形状，即，相距越近的神经元连接越强，反之越弱，而α控制着高斯峰的半径。如此，在没有外部刺激的时候，CANN模型的神经元发放频率响应也呈现稳定的高斯函数形状，称为响应波包（Bump），表示网络的一个吸引子状态。 
 CANN不仅有简洁的形式和特殊的动力学特性，在大脑中也具备生物学基础： 






 大脑中头朝向神经元在空间上沿着一个环分布构成一维CANN，神经元群活动可以预测头转动的趋势。 
 海马中的空间神经元（Place Cell）互相连接构成一个二维CANN，神经元活动的波包可以编码动物的空间位置，支持大脑的空间导航。 
 少数动物如蝙蝠的空间定位系统甚至会用到三维CANN。 

 关于CANN的计算原理、生物基础和应用场景的更多信息，感兴趣的读者可以参考北京大学吴思教授课题组微信公众号“吴思Lab计算神经科学及类脑计算”最近发布的CANN专帖【学术思想】连续吸引子神经网络：神经信息表达的正则化网络模型。 

 图4 二维CANN模型示意图 




 135编辑器 
 CANN的应用场景有很多，这里主要给大家详细描述目标追踪的实现[23]。当图4中二维神经元阵列对应到视频中的像素阵列、每个神经元的接收对应位置像素强度的前后帧差分信号刺激时，网络便可实现目标追踪。  
 详细而言，网络初始时对目标所在的方框区域初始化为高响应值，其他区域为0；开始运行后，网络所有神经元按照CANN规则进行计算，而目标所在区域的强差分输入会牵引着神经元响应波包平滑移动，波包位置对目标进行实时追踪。突触权重高斯峰的半径决定了追踪的性能：太宽则容易受邻近目标干扰，太窄则容易跟丢目标。 
 这种吸引子跟随外部输入的牵引而移动是CANN的特有动力学特性，对目标的追踪也非常平滑自然。除目标追踪的应用外，波包编码本身也是一种神经元群编码的体现，相比于单神经元编码，更具有鲁棒性。 
 另一方面，一簇CANN神经元可以处理某种模态信息的不同任务，也可以处理不同模态的信息，多簇CANN可以用长程连接进行信息交互，因此CANN也有潜力为大脑提供多模态信息处理的统一框架。 



 实际应用任重道远 





 前面讲述的深度学习和SNN都有对应的专用处理芯片，而神经动力学网络由于本身研究比较分散，应用也不成体系，在硬件中的实现仍较为少见。  
 MIT曾实现了十多个神经元的简单CANN模型[24]，但并未引起重视。最近，清华大学的类脑芯片—天机芯通过对CANN模型突触连接的局部约束、数据的低精度化等硬件友好改造，实现了约800帧每秒的超快目标追踪[1,23]。 
 目前多数神经动力学网络还是以复现神经回路响应模式居多，距离实际应用还任重道远，需要持续不断地进行挖掘。 







 天机芯 





 跨域融合思想 

 上述模型各有特点和优缺点，孰优孰劣尚无定论。与现有深度学习加速器和神经形态器件分别支持ANN和SNN模型不同，天机芯[1]的目标是促进通用人工智能的发展，所以在保持专用芯片高效能的同时需要尽可能提高对上述模型支持的通用性。 
 由于不同模型的迥异计算原理、信息编码方式与应用场景，导致其所需的计算与存储架构以及优化目标相差较大，这点从现有深度学习加速器和神经形态器件的独立设计和应用体系可以看出来。这里需要注意的是，分别设计不同模型的专用模块再简单放置到一起是行不通的，原因如下。第一，很难确定各自的配置比例，因为现实应用中的工作负载往往是多变的；第二，面积和功耗都不高效，处理单一同构模型时，会导致总体利用率很低；第三，处理混合异构模型时，需要专门的信号转换单元，增加额外成本降低效率。 




 135编辑器 
 在天机芯的设计中，实现异构融合有两个关键点。首先，深入研究大多数目前的主流神经网络模型，包括人工神经网络、脉冲神经网络与神经动力学网络等，建立一个通用的模型描述框架，并把这些模型进行合理拆解后映射至包括轴突、树突、突触、胞体以及互连网络等基本模块上，归纳每个模块应具备的算子功能。每个模块都最大程度复用不同模型工作模式的存储和计算资源，所以最终面积只比单一模式高3%。  
 然后，基于现有神经形态芯片的众核可扩展架构（每个芯片中有许多个互相连接的基本功能核），并仔细设计上述几个基本模块，包括模式可独立配置的轴突和胞体（输入/输出模块），模式共享的树突和突触（运算模块），以及统一的路由协议和路由网络（连接模块）。当轴突和胞体工作在相同模式下，整个网络可支持典型单一同构模型；当轴突和胞体工作在不同模式下，整个网络可支持目前尚缺乏研究的混合异构模型，有望推动神经网络新模型的探索。  



 无人自行车应用演示 

 选择什么样的平台来演示天机芯的基本功能并非易事。首先，这应该是一个类似大脑的多模态系统，覆盖感知、决策和执行的完整链路，并能够为异构融合的多种模型提供任务支撑，这与目前很多AI系统演示的单一任务不同。其次，这应该是一个能够与现实环境交互的真实系统，而不是停留在机房实验。再者，这个系统最好对处理芯片有功耗和实时性要求，以体现专用芯片的优势。最后，这个系统必须是安全可控的，能够方便实验。综上所述，无人智能自行车平台应运而生，其具有语音识别、目标探测和追踪、运动控制、障碍躲避以及自主决策等功能，是一个运行在户外场景同时对功耗和实时性具有需求的一个嵌入式机器人，算得上是一个五脏俱全的小型类脑平台。  

 其中，目标探测采用的是ANN中的卷积神经网络，在相机采集的图像中探测目标位置；车身平衡控制采用的是ANN中的全连接神经网络，根据陀螺仪采集的车身姿态实现对转向电机的PID控制，保持自行车平衡；语音命令识别采用的是SNN模型，将语音信号转换为spike脉冲信号特征后进行命令分类；目标追踪便是前面介绍过的CANN模型；自主决策是一个有限状态机，实现上述不同模型的融合通信，因此是一个异构的混合模型。 




 135编辑器 





 总结 



 纵观当今的AI芯片，从计算机科学和神经科学角度寻求动机，采用了不同的神经网络模型，呈现了不同的硬件架构设计，应用和性能也各有所长。  
 笔者认为，在现阶段没有必要急着对各类途径分出胜负。 
 一方面应该继续保持研究的多样性，使各个领域迭代发展，毕竟每一条路线目前看来都不能保证成为终极方案。  
 另一方面，面对目前已知的各类神经网络模型，从数学根本上去探寻他们的表达能力差异从而寻求可控的大一统模型，也是很好的着力点。 
 科学研究的动机，并非仅仅是出于能看到多酷炫的应用，而是因为我们尚有诸多未知世界需要探寻，这是一切应用的前提。 



 参考文献 
 [1] Pei, Jing, Lei Deng, Sen Song, Mingguo Zhao, Youhui Zhang, Shuang Wu, Guanrui Wang et al. ""Towards artificial general intelligence with hybrid Tianjic chip architecture."" Nature 572, no. 7767 (2019): 106. 
 [2] McCulloch W S, Pitts W. A logical calculus of the ideas immanent in nervous activity. Bull Math Biophys, 1943, 5(4): 115–133. 
 [3] Sacramento, Joao, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. ""Dendritic error backpropagation in deep cortical microcircuits."" arXiv preprint arXiv:1801.00062 (2017). 
 [4] Guerguiev, Jordan, Timothy P. Lillicrap, and Blake A. Richards. ""Towards deep learning with segregated dendrites."" ELife 6 (2017): e22901. 
 [5] Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. ""Dynamic routing between capsules."" In Advances in neural information processing systems, pp. 3856-3866. 2017. 
 [6] Roelfsema, Pieter R., and Anthony Holtmaat. ""Control of synaptic plasticity in deep cortical networks."" Nature Reviews Neuroscience 19, no. 3 (2018): 166. 
 [7] Marblestone, Adam H., Greg Wayne, and Konrad P. Kording. ""Toward an integration of deep learning and neuroscience."" Frontiers in computational neuroscience 10 (2016): 94. 
 [8] Ullman, Shimon. ""Using neuroscience to develop artificial intelligence."" Science 363, no. 6428 (2019): 692-693. 
 [9] Gerstner, Wulfram, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. 
 [10] Izhikevich, Eugene M. ""Simple model of spiking neurons."" IEEE Transactions on neural networks 14, no. 6 (2003): 1569-1572. 
 [11] Hodgkin, Alan L., and Andrew F. Huxley. ""A quantitative description of membrane current and its application to conduction and excitation in nerve."" The Journal of physiology117, no. 4 (1952): 500-544. 
 [12] Song, Sen, Kenneth D. Miller, and Larry F. Abbott. ""Competitive Hebbian learning through spike-timing-dependent synaptic plasticity."" Nature neuroscience 3, no. 9 (2000): 919. 
 [13] Diehl, Peter U., Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer. ""Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing."" In 2015 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2015. 
 [14] Sengupta, Abhronil, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. ""Going deeper in spiking neural networks: VGG and residual architectures."" Frontiers in neuroscience 13 (2019). 
 [15] Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. ""Spatio-temporal backpropagation for training high-performance spiking neural networks."" Frontiers in neuroscience 12 (2018). 
 [16] Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. ""Direct training for spiking neural networks: Faster, larger, better."" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 1311-1318. 2019. 
 [17] Furber, Steve B., Francesco Galluppi, Steve Temple, and Luis A. Plana. ""The spinnaker project."" Proceedings of the IEEE102, no. 5 (2014): 652-665. 
 [18] Moradi, Saber, Ning Qiao, Fabio Stefanini, and Giacomo Indiveri. ""A scalable multicore architecture with heterogeneous memory structures for dynamic neuromorphic asynchronous processors (dynaps)."" IEEE transactions on biomedical circuits and systems 12, no. 1 (2017): 106-122. 
 [19] Merolla, Paul A., John V. Arthur, Rodrigo Alvarez-Icaza, Andrew S. Cassidy, Jun Sawada, Filipp Akopyan, Bryan L. Jackson et al. ""A million spiking-neuron integrated circuit with a scalable communication network and interface."" Science345, no. 6197 (2014): 668-673. 
 [20] Davies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou et al. ""Loihi: A neuromorphic manycore processor with on-chip learning."" IEEE Micro 38, no. 1 (2018): 82-99. 
 [21] Wu, Si, Kosuke Hamaguchi, and Shun-ichi Amari. ""Dynamics and computation of continuous attractors."" Neural computation20, no. 4 (2008): 994-1025. 
 [22] Fung, CC Alan, KY Michael Wong, and Si Wu. ""A moving bump in a continuous manifold: a comprehensive study of the tracking dynamics of continuous attractor neural networks."" Neural Computation 22, no. 3 (2010): 752-792. 
 [23] Deng, Lei, Zhe Zou, Xin Ma, Ling Liang, Guanrui Wang, Xing Hu, Liu Liu, Jing Pei, Guoqi Li, and Yuan Xie. ""Fast Object Tracking on a Many-Core Neural Network Chip."" Frontiers in neuroscience 12 (2018). 







 脑人言，与大脑对话 











 张亚勤、刘慈欣、周鸿祎、王飞跃、约翰.翰兹联合推荐 

 这是一部力图破解21世纪前沿科技大爆发背后的规律与秘密，深度解读数十亿群体智能与数百亿机器智能如何经过50年形成互联网大脑模型，详细阐述互联网大脑为代表的超级智能如何深刻影响人类社会、产业与科技未来的最新著作。 

 《崛起的超级智能;互联网大脑如何影响科技未来》2019年7月中信出版社出版。刘锋著。了解详情请点击：【新书】崛起的超级智能：互联网大脑如何影响科技未来 





 未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。 

 未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。 

   如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文” 

 


 

 
更多参考 
https://cloud.tencent.com/developer/article/1069150 https://blog.csdn.net/AS7062031/article/details/108870125"
"【博士每天一篇论文-综述】An overview of brain-like computing Architecture, applications, and future trends",2024-01-09,"阅读时间：2023-11-16 
1 介绍 
年份：2022 作者：欧嵬，肖世涛，海南大学 期刊： Frontiers in Neurorobotics 引用量：3 这篇论文是关于脑科学计算的概述，介绍了常用的神经元模型包括Hodgkin-Huxley模型、积分-火灾模型、漏电积分-火灾模型、Izhikevich模型和AdEx IF模型，它们模拟和建模了人脑神经系统的基本结构和功能单元。编码神经系统信息的两种常用方法是速率编码和时间编码。脑科学计算在机器学习、模式识别等领域具有重要应用。 
2 相关研究 
 
2.1 类脑计算 
大脑类计算机使用脉冲神经网络（SNN）代替经典计算机的冯·诺伊曼架构，并使用微观和纳米光电子器件模拟生物神经元和突触的信息处理特性。 脑样计算模型可以分为三个主要方面：神经元的模拟、神经系统的信息编码和神经网络的学习算法。 1）Hodgkin-Huxley (HH)模型是最接近生物实际的神经元模型，被广泛应用于计算神经科学领域。它可以模拟多种神经元功能，例如激活、失活、动作电位和离子通道。HH模型以离子活动来描述神经元的电活动。细胞膜包含钠、钾和漏电通道，每种离子通道都有不同的门控蛋白质，可以限制离子的通过，因此膜内外每种离子的渗透性不同，导致神经元具有丰富的电活动。在数学层面上，门控蛋白质的结合效应等效于离子通道的电导。离子通道的电导作为一个依赖变量，随着离子通道的激活和失活变量而变化。离子通道的电流由离子通道的电导、离子通道的反向电位和膜电位决定。总电流由漏电流、钠电流、钾电流和由于膜电位变化而产生的电流组成。因此，HH模型也将细胞膜等效于电路图。 2）Integrate-and-fire (IF)和leaky integrate-and-fire (LIF)模型描述了神经元膜电位随时间变化的模型。1907年，Lapicque提出了积分-火模型。其中，IF模型根据模型中神经元膜电位随输入电流的变化来描述神经元的膜电位。LIF模型在IF模型的基础上增加了神经元电压泄漏的模拟。当一段时间内没有电流输入时，膜电压会逐渐泄漏至静息电位。与IF模型相比，LIF模型更接近生物实际，模拟了神经元的膜电位泄漏特性。 3）Izhikevich模型是在非线性动力系统的角度上提出的，能够展示多种生物神经元的放电行为。Izhikevich模型的算法复杂度接近LIF模型，它通过调整参数a、b、c和d来调节辅助变量U，从而展示类似HH模型的放电行为。不同于HH模型，Izhikevich模型的参数没有明确的生理意义。 4）AdExIF模型是对Izhikevich模型的改进。它减慢了膜电压的响应速度，导致神经元在恒定电压刺激条件下脉冲传递的频率逐渐降低。可以把这看作是神经元的响应逐渐减慢，在发送脉冲后逐渐“累积疲劳”的过程。AdExIF模型的这一特征更接近HH模型的放电行为结果。 五种模型对比如下： 
神经元模型电路形式优点缺陷HH电容电阻电路接近生物神经元，高准确性表达复杂，操作复杂LF电容操作简单简单模型带有记忆效应LIF电容电阻电路模拟静息状态，操作简单模型简单，忽略许多神经动力学性质Izhikevich无模拟多种放电模式计算效率低AdEx lF无模拟多种放电模式在恒定电压刺激下降低脉冲射频率 
2.2 神经信息编码 
（1）频率编码（Rate coding） 频率编码主要利用随机过程方法生成脉冲序列。适用于泊松编码的神经元响应函数由一系列脉冲函数组成。 （2）时序编码（Temporal coding） 时序编码通常使用首次脉冲发射的时刻作为时序编码的机制。脉冲发射时间与图像像素的强度值相关。时序编码是一种将刺激表示为多个神经元联合活动的方法。常用的模型是高斯群编码模型。 （3）种群编码（population code） 种群编码是使用多个神经元的联合活动来表示刺激的方法。高斯种群编码是最常用的群集编码模型。 
2.3 类脑芯片 
（1）Darwin芯片 Darwin芯片是基于脉冲神经网络的神经形态计算处理器，采用事件驱动的工作方式。它使用Addressevent表示法(AER)来编码输入和输出的脉冲信息。AER包含产生脉冲的神经元ID和脉冲生成的时间戳。Darwin芯片根据输入的AER包进行事件驱动的处理。脉冲路由器将脉冲转换为带权重的延迟信息，通过访问存储器和SDRAM进行存储和传输。 （2）Tianjic芯片 Tianjic芯片是一种用于Spiking Neural Networks (SNNs)和Artificial Neural Networks (ANNs)的神经形态计算芯片。它可以在SNN模式和ANN模式之间切换，并使用相同的计算器进行信息处理。该芯片具有多核架构，支持大规模网络或多个网络的并行处理。它具有可重构的路由表，可以实现任意的连接拓扑结构。 （3）TrueNorth芯片 TrueNorth芯片是IBM开发的一种仿脑结构的计算芯片，它通过模拟脑结构和进行神经仿真来构建脑样计算芯片。TrueNorth芯片以神经元作为主要计算单元，通过脉冲信号的传导过程进行神经仿真。它采用基本的神经形态计算单元——神经元核作为整个网络的基本构建单元，通过突触连接实现神经元之间的信息传递。TrueNorth芯片具有多核架构，使用核块之间的水平和竖直线路传输数据。 （4）Neurogrid芯片 Neurogrid芯片是一种神经元芯片，它由轴突、突触、树突和细胞质部分组成。Neurogrid芯片有四种结构可供选择：完全专用(FD)、共享轴突(SA)、共享突触(SS)和共享树突(SD)。该芯片可根据其架构和实现方式对轴突、突触、树突和细胞进行分类。它可以通过模拟实现或完全数字实现。Neurogrid芯片可用于神经元网络的模拟和处理。 （5）BrainScaleS-2芯片 BrainScaleS-2芯片是另一种神经元芯片，其架构集成了模拟和数字电路块。该芯片的数字处理器核心被称为可塑性处理单元(PPU)，模拟核心则包括突触和神经元电路、PPU接口、模拟参数存储器和所有与事件相关的接口组件。BrainScaleS-2芯片还具有数字可塑性处理器，该处理器专用于高度并行的单指令多数据(SIMD)操作，并具备建模能力。该芯片的架构经过优化，使得神经元电路位于模拟核心的中心，以实现最高效的排列。 
项目和机构制造工艺神经元数量突触数量神经元模型学习算法优势缺陷Darwin，浙江大学180纳米CMOS2,0484,194,304LIF\高度可配置单芯片，规模较小Tianjic，清华大学28纳米CMOS40,000100,000,000LIFSTDP异构融合\TrueNorth28纳米CMOS1,000,000256,000,000LIF\高度可配置仅支持离片学习Neurogrid180纳米CMOS1,048,576数亿QIF\高吞吐量无可塑性BrainScaleS-265纳米CMOS196,60850,331,648AdEx IFSTDP混合可塑性规则未展示处理实际任务的能力 
2.4 应用 
（1）医疗健康 脑类计算的主要应用是基于脑-计算机接口技术。这一技术可以在以下四个方面发挥作用：监测、改善、替代和增强。 （2）智能教育 脑类计算研究希望通过直接观察相应的脑电波和脑活动解码来研究儿童的发育和心理问题。 （3）智能交通 类脑研究可以用于自动驾驶汽车的感知和决策能力提升。比如，采用类脑芯片的视觉传感器可以实现自动驾驶汽车对图像中亮度变化的快速检测。此外，类脑芯片也能用于自动驾驶车辆的声音识别和定位，使自动驾驶更加安全可靠。 （4）军事应用 类脑研究具有超低功耗、大规模并行计算和实时信息处理的技术优势。类脑芯片可用于军事目标的实时高速动态视觉识别和追踪。它还可以用于构建认知超级计算机，实现对大规模数据的快速处理。特别是在高速动态识别领域，超低延迟实时高速动态视觉识别技术非常重要。 
2.5 面临的挑战 
（1）大脑与机器之间的差异 由于大脑的复杂性以及与机器的巨大差异，导致了大脑信号获取的困难、脑机互动的低效、缺乏大脑智能干预手段、对大脑区域干预目标的高要求以及融合系统构建的难度。 （2）传统计算机架构的限制 传统计算机结构在发展中遇到了两个主要瓶颈，即冯·诺依曼结构引起的存储墙效应和摩尔定律的失效。传统处理器架构在处理实时智能问题时效率低下、能耗高，无法构建适合处理非结构化信息的算法，并且程序或数据传输的速率与中央处理器处理信息的速率不匹配，造成了存储墙效应。此外，随着芯片尺寸逐渐逼近单个原子的尺寸，设备逐渐接近各自物理微型化的极限，性能提升的成本将变得更高，技术实现将变得更加困难。因此，研究人员寄希望于脑型计算以突破当前的技术瓶颈。 （3）硬件模拟大脑计算过程的挑战 使用硬件模拟大脑计算过程仍然面临着架构、器件和芯片方面的重要挑战。一方面，传统CMOS等传统技术在芯片存储密度和功耗方面已经遇到了瓶颈，而新型纳米器件仍存在着工艺稳定性差、难以扩展等问题。脑型材料和器件需要新的技术突破当前瓶颈。另一方面，脑型系统需要数百亿个神经元共同工作，然而现有的脑型芯片在有限的硬件资源和能耗的约束下，难以实现大规模互连的神经元和高效的神经脉冲信息实时传输。 （3）大脑思考效率的提升 目前人脑思考的效率需要紧急提高。大脑芯片的超低功耗、超低延迟、实时高速动态视觉识别、跟踪技术以及传感器信息处理技术是国家国防科技战略水平的关键技术。特别是超低延迟实时高速动态视觉识别技术在高速动态识别领域具有极其重要的作用。"
【芯片架构】深入理解嵌入式领域芯片架构：探讨不同类型（ARM Cortex、RISC-V、MIPS、PowerCP）的嵌入式处理器及其在实际应用中的优缺点,2024-06-16,"嵌入式系统是现代科技的基石之一，从智能手机、智能家居到工业自动化，无处不在。嵌入式处理器作为这些系统的“大脑”，其架构和性能直接影响着整个系统的效率和功能。本文将深入探讨几种主流的嵌入式处理器架构，包括ARM Cortex系列、RISC-V、MIPS、PowerPC等，分析它们的特点、优势与局限，以及在不同应用场景下的表现。 
 
 
 
 🧑 博主简介：现任阿里巴巴嵌入式技术专家，15年工作经验，深耕嵌入式+人工智能领域，精通嵌入式领域开发、技术管理、简历招聘面试。CSDN优质创作者，提供产品测评、学习辅导、简历面试辅导、毕设辅导、项目开发、C/C++/Java/Python/Linux/AI等方面的服务，如有需要请站内私信或者联系任意文章底部的的VX名片（ID：gylzbk） 
 
 
 💬 博主粉丝群介绍：① 群内初中生、高中生、本科生、研究生、博士生遍布，可互相学习，交流困惑。② 热榜top10的常客也在群里，也有数不清的万粉大佬，可以交流写作技巧，上榜经验，涨粉秘籍。③ 群内也有职场精英，大厂大佬，可交流技术、面试、找工作的经验。④ 进群免费赠送写作秘籍一份，助你由写作小白晋升为创作大佬。⑤ 进群赠送CSDN评论防封脚本，送真活跃粉丝，助你提升文章热度。有兴趣的加文末联系方式，备注自己的CSDN昵称，拉你进群，互相学习共同进步。 
 
 
 
 
 【芯片架构】深入理解嵌入式领域芯片架构：探讨不同类型（ARM Cortex、RISC-V、MIPS、PowerCP）的嵌入式处理器及其在实际应用中的优缺点 
 引言ARM Cortex 系列：嵌入式市场的主导者ARM Cortex-A 系列ARM Cortex-M 系列ARM Cortex-R 系列 
 RISC-V：开放标准的新星概述优点缺点应用场景 
 MIPS：曾经的巨头，现今的转型概述优点缺点应用场景 
 PowerPC：工业与专业应用的坚守者概述优点缺点应用场景 
 结论与展望 

 
 
引言 
嵌入式系统是现代科技的基石之一，从智能手机、智能家居到工业自动化，无处不在。嵌入式处理器作为这些系统的“大脑”，其架构和性能直接影响着整个系统的效率和功能。本文将深入探讨几种主流的嵌入式处理器架构，包括ARM Cortex系列、RISC-V、MIPS、PowerPC等，分析它们的特点、优势与局限，以及在不同应用场景下的表现。 
ARM Cortex 系列：嵌入式市场的主导者 
ARM Cortex-A 系列 
 
概述：ARM Cortex-A系列处理器主要针对高性能计算，如移动设备、服务器和高端嵌入式应用。优点：高性能、丰富的生态系统、广泛的应用场景。缺点：相对较高的功耗和成本。应用场景：智能手机、平板电脑、智能电视、服务器等。代表作： 
Cortex-A53: 这是ARM的一个非常成功的处理器核心，被广泛用于许多移动设备和嵌入式系统中。它是一个高效的低功耗核心，常见于八核或多核配置。Cortex-A72: 提供比A53更高的性能，但功耗也相应增加。它常用于高性能的移动设备和小型服务器。Cortex-A73: 继续提升性能，提供更好的能效比，适合高性能应用和高端移动设备。Cortex-A75: 进一步改进性能和效率，虽然不是最新型号，但仍然在一些高性能系统中找到应用。Cortex-A76: 针对旗舰级移动设备和高性能计算设计，提供卓越的单核性能。
ARM Cortex-M 系列 
 
概述：Cortex-M系列适用于微控制器(MCU)，以其低功耗和成本效益著称。优点：低功耗、易于使用、广泛应用于各种嵌入式设备。缺点：性能有限，不适合复杂计算密集型任务。应用场景：物联网(IoT)设备、汽车电子、消费电子产品等。代表作： 
Cortex-M0: 最简单的Cortex-M核心，提供精简的性能，适合成本敏感型应用。Cortex-M3: 提供更强大的性能，常见的微控制器核心，例如STM32F1系列。Cortex-M4: 引入DSP指令和FPU，适合音频和信号处理应用，例如STM32F4系列。Cortex-M7: 在M4的基础上提供更高的性能，适用于复杂的嵌入式应用，例如NXP的LPC55S69。
ARM Cortex-R 系列 
概述：专注于实时应用，如汽车引擎控制和通信基础设施。优点：实时响应能力、高可靠性。缺点：市场定位较窄，不如Cortex-A和Cortex-M系列流行。应用场景：汽车电子、工业自动化、通信系统等。代表作： 
Cortex-R4: 专为实时应用设计，常见于汽车和通信系统中。Cortex-R5: 提供增强的安全特性和实时性能，适用于汽车电子和工业控制系统。Cortex-R8: 为汽车应用优化，提供更强的性能和安全性，满足汽车行业的严格要求。
RISC-V：开放标准的新星 
 
概述 
RISC-V是一种开放标准的指令集架构(ISA)，旨在提供免费的硬件设计规范，促进处理器创新。 
优点 
开放性：任何人都可以设计、制造和销售RISC-V兼容的芯片，无需支付授权费用。简洁性：RISC-V架构设计简洁，易于理解和实现。可扩展性：通过模块化设计，允许用户根据需要添加自定义指令。 
缺点 
生态系统：相比ARM和MIPS，RISC-V的软件生态和开发工具链尚在发展中。认知度：尽管RISC-V在学术界和开源社区受到欢迎，但在商业市场上的认知度和市场份额仍需提高。 
应用场景 
教育与研究：RISC-V的开放性使其成为教学和研究的理想选择。嵌入式系统：特别是在需要定制化和低成本的嵌入式应用中。物联网(IoT)：RISC-V的小尺寸和低功耗特性非常适合IoT设备。 
MIPS：曾经的巨头，现今的转型 
 
概述 
MIPS曾是嵌入式处理器市场的重要参与者，以其高性能和低功耗闻名，但近年来市场份额被ARM大幅侵蚀。 
优点 
高性能：MIPS架构在某些领域仍表现出色。成熟的技术：MIPS拥有成熟的工具链和丰富的开发资源。 
缺点 
市场地位：在ARM和RISC-V的双重压力下，MIPS的市场地位逐渐边缘化。授权费用：与RISC-V相比，MIPS的授权模式可能限制了其在开源社区的普及。 
应用场景 
高性能计算：在某些需要高性能计算的嵌入式应用中，MIPS仍有一席之地。网络设备：MIPS在路由器、交换机等网络设备中有一定的应用基础。 
PowerPC：工业与专业应用的坚守者 
 
概述 
PowerPC架构由IBM、Apple和Motorola联合开发，主要用于高端工作站、服务器和一些专业的嵌入式应用。 
优点 
高性能：PowerPC在多线程处理和浮点运算方面表现出色。稳定性：在工业控制和航空航天领域有良好的声誉。 
缺点 
成本：PowerPC的高成本限制了其在大众市场的应用。生态系统：与ARM和x86相比，PowerPC的软件生态系统较小。 
应用场景 
航空航天：PowerPC在需要高稳定性和可靠性的领域如航空航天中应用广泛。高端服务器：在某些高性能计算和企业级服务器中，PowerPC仍有一席之地。 
结论与展望 
不同的嵌入式处理器架构各有千秋，选择哪种架构应基于具体的应用需求、性能目标和成本考虑。ARM Cortex系列凭借其广泛的生态系统和优秀的性能，在消费电子和移动设备领域占据主导地位；RISC-V作为一种新兴的开放标准，正逐渐在教育、科研和嵌入式领域获得关注；而MIPS和PowerPC虽市场份额受限，但在特定的专业应用中仍保有竞争力。 
随着技术的不断进步和市场需求的变化，嵌入式处理器的未来充满了无限可能。RISC-V的开放性或许会吸引更多创新者的加入，推动处理器设计的多元化；而ARM和MIPS等传统架构也可能通过持续的技术革新，保持其在市场上的领导地位。无论未来如何变化，了解和掌握不同处理器架构的特点和适用场景，对于从事嵌入式系统设计和开发的工程师来说至关重要。"
写给Android开发者的芯片知识,2024-06-07,"写给Android开发者的芯片知识 
作为 Android 开发者，免不了要处理 arm、armv7a、armv8a、x86 等架构适配，加上今年各个行业都遇到了“缺芯”的困扰，以及美国对华为的芯片制裁，制裁的是什么技术？本文尝试讲清楚下面几个问题： 
Android动态库适配到底在适配什么？CPU、架构、指令集、芯片之间的关系和联系；armv7a中的a指什么？芯片的核心技术和门槛是什么？ 
CPU 
接触过计算机的朋友们都知道CPU的含义，就是中央处理器，是负责计算机主要运算任务的组件。一般我们把CPU比喻为人的大脑。而了解略深的用户会听说CPU有x86、ARM等分类，前者主要用于PC而后者主要用于手机平板等设备。 
那么这里的x86、ARM指的是什么呢？ 
指令集 
CPU执行计算任务时都需要遵从一定的规范，程序在被执行前都需要先翻译为CPU可以理解的语言。这种规范或语言就是指令集（ISA，Instruction Set Architecture）。程序被按照某种指令集的规范翻译为CPU可识别的底层代码的过程叫做编译（compile）。x86、ARM v8、MIPS都是指令集的代号。指令集可以被扩展，如x86增加64位支持就有了x86-64。厂商开发兼容某种指令集的CPU需要指令集专利持有者授权，典型例子如Intel授权AMD，使后者可以开发兼容x86指令集的CPU。 
指令集最直接的解释就是CPU理解的语言，对于人类语言之用来交流的，是在一定的环境中孕育出来的。不同的语言之间可以互相”翻译“，特定的一圈人之间还可以发明创造新的语言。世界上这么多语言，为什么国际间沟通主要是用英语？为什么汉语的使用人数最多？ 
类似的原因，指令集主流的只有这么几个，不在于创造它的复杂度，而在于它的基础和生态环境。 
下面介绍ARM、x86、MIPS、PowerPC、RISC-V等主流的几个指令集。 
ARM系列 
ARM架构，称为进阶精简指令集机器（Advanced RISC Machine，更早称作：Acorn RISC Machine），是一个32位精简指令集（RISC）处理器架构，其广泛地使用在许多嵌入式系统设计。由于节能的特点，ARM处理器非常适用于行动通讯领域，符合其主要设计目标为低耗电的特性。 
在今日，ARM家族占了所有32位嵌入式处理器75%的比例，使它成为占全世界最多数的32位架构之一。ARM处理器可以在很多消费性电子产品上看到，从可携式装置（PDA、移动电话、多媒体播放器、掌上型电子游戏，和计算机）到电脑外设（硬盘、桌上型路由器）甚至在导弹的弹载计算机等军用设施中都有他的存在。在此还有一些基于ARM设计的派生产品，重要产品还包括Marvell的XScale架构和德州仪器的OMAP系列。 
优势：价格低；能耗低； 
ARM 授权方式：ARM 公司本身并不靠自有的设计来制造或出售 CPU ，而是将处理器架构授权给有兴趣的厂家。ARM 提供了多样的授权条款，包括售价与散播性等项目。对于授权方来说，ARM 提供了 ARM 内核的整合硬件叙述，包含完整的软件开发工具（编译器、debugger、SDK），以及针对内含 ARM CPU 硅芯片的销售权。对于无晶圆厂的授权方来说，其希望能将 ARM 内核整合到他们自行研发的芯片设计中，通常就仅针对取得一份生产就绪的智财核心技术（IP Core）认证。对这些客户来说，ARM 会释出所选的 ARM 核心的闸极电路图，连同抽象模拟模型和测试程式，以协助设计整合和验证。需求更多的客户，包括整合元件制造商（IDM）和晶圆厂家，就选择可合成的RTL（暂存器转移层级，如 Verilog）形式来取得处理器的智财权（IP）。借着可整合的 RTL，客户就有能力能进行架构上的最佳化与加强。这个方式能让设计者完成额外的设计目标（如高震荡频率、低能量耗损、指令集延伸等）而不会受限于无法更动的电路图。虽然 ARM 并不授予授权方再次出售 ARM 架构本身，但授权方可以任意地出售制品（如芯片元件、评估板、完整系统等）。商用晶圆厂是特殊例子，因为他们不仅授予能出售包含 ARM 内核的硅晶成品，对其它客户来讲，他们通常也保留重制 ARM 内核的权利。 
生产厂商：TI （德州仪器）/Samsung（三星）/Freescale（飞思卡尔）/Marvell（马维尔）/Nvidia（英伟达） 
x86系列/Atom处理器 
xx86或80x86是英代尔Intel首先开发制造的一种微处理器体系结构的泛称。 
x86架构是重要地可变指令长度的CISC（复杂指令集电脑，Complex Instruction Set Computer）。 
Intel Atom（中文：凌动，开发代号：Silverthorne）是Intel的一个超低电压处理器系列。处理器采用45纳米工艺制造，集成4700万个晶体管。L2缓存为512KB，支持SSE3指令集，和VT虚拟化技术（部份型号）。 
现时，Atom处理器系列有6个型号，全部都是属于Z500系列。它们分别是Z500、Z510、Z520、Z530、Z540和Z550。最低端的Z500内核频率是800MHz，FSB则是400MHz。而最高速的Z550，内核频率则有2.0GHz，FSB则是533MHz。从Z520开始，所有的处理器都支持超线程技术，但只增加了不到10%的耗电。双内核版本为N系列，依然采用945GC芯片组。双内核版本仍会支持超线程技术，所以系统会显示出有4个逻辑处理器。这个版本的两个内核并非采用本地设计，只是简单的将两个单内核封装起来。 
MIPS系列 
MIPS是世界上很流行的一种RISC处理器。MIPS的意思是“无内部互锁流水级的微处理器”(Microprocessor without interlocked piped stages)，其机制是尽量利用软件办法避免流水线中的数据相关问题。它最早是在80年代初期由斯坦福(Stanford)大学Hennessy教授领导的研究小组研制出来的。MIPS公司的R系列就是在此基础上开发的RISC工业产品的微处理器。这些系列产品为很多计算机公司采用构成各种工作站和计算机系统。 
MIPS技术公司是美国著名的芯片设计公司，它采用精简指令系统计算结构(RISC)来设计芯片。和英特尔采用的复杂指令系统计算结构(CISC)相比，RISC具有设计更简单、设计周期更短等优点，并可以应用更多先进的技术，开发更快的下一代处理器。MIPS是出现最早的商业RISC架构芯片之一，新的架构集成了所有原来MIPS指令集，并增加了许多更强大的功能。MIPS自己只进行CPU的设计，之后把设计方案授权给客户，使得客户能够制造出高性能的CPU。 
PowerPC系列 
PowerPC 是一种精简指令集（RISC）架构的中央处理器（CPU），其基本的设计源自IBM（国际商用机器公司）的IBM PowerPC 601 微处理器POWER（Performance Optimized With Enhanced RISC；《IBM Connect 电子报》2007年8月号译为“增强RISC性能优化”）架构。二十世纪九十年代，IBM(国际商用机器公司)、Apple（苹果公司）和Motorola（摩托罗拉）公司开发PowerPC芯片成功，并制造出基于PowerPC的多处理器计算机。PowerPC架构的特点是可伸缩性好、方便灵活。 
PowerPC 处理器有广泛的实现范围，包括从诸如 Power4 那样的高端服务器 CPU 到嵌入式 CPU 市场（任天堂 Gamecube 使用了 PowerPC）。PowerPC 处理器有非常强的嵌入式表现，因为它具有优异的性能、较低的能量损耗以及较低的散热量。除了象串行和以太网控制器那样的集成 I/O，该嵌入式处理器与“台式机”CPU 存在非常显著的区别。 
RISC-V 
RISC-V是一个基于精简指令集（RISC）原则的开源指令集架构（ISA）。V表示为第五代RISC.与大多数指令集相比，RISC-V指令集可以自由地用于任何目的，允许任何人设计、制造和销售RISC-V芯片和软件。RISC-V被认为是继x86、ARM之后的第三大CPU架构。 
RISC-V包括三大优势， 
 模块化：RISC-V将不同的部分以模块化的方式组织在一起，这种模块化是x86与ARM架构所不具备的。指令数目少：受益于短小精悍的架构以及模块化的特性，RISC-V架构的指令数目非常的简洁。RISC-V全面开源，且具有全套开源免费的编译器、开发工具和软件开发环境，大大降低指令集修改的门槛。
RISC-V作为新兴架构，以其精简的体量，未来的IoT（物联网）领域中能取得绝对的优势。IoT的“碎片化”需求，IoT芯片设计速度要快、成本要低、能量身定制。同时嵌入式市场具备少量多样的特点，在各细分应用场景并未形成真正壁垒，架构的选择五花八门。以上原因成就了RISC-V绝佳的突破口。RISC-V的开源能降低成本，也能让用户自由修改，可定制化，RISC-V生态与敏捷设计同源。 
目前，国内外已有多家芯片企业投入大量资金研发RISC-V在IoT领域的应用。2015年，RISC-V基金会正式成立，吸引了包括英伟达、NXP、三星、Microsemi在内等企业的加入。迄今为止，该基金会已吸引了全球28个国家327多家会员加入。目前，阿里、亚马逊等都有相关芯片研制出，甚至就连苹果A4/A5、AMD速龙/Zen架构之父JimKeller也创业投身RISC-V了。 
架构 
我们常说的单核，双核，四核，以及主频等概念。 
CPU的基本组成单元即为核心（core）。多个核心可以同时执行多件计算任务，前提是这些任务没有先后顺序。 
核心的实现方式被称为微架构（microarchitecture）。微架构的设计影响核心可以达到的最高频率、核心在一定频率下能执行的运算量、一定工艺水平下核心的能耗水平等等。此外，不同微架构执行各类程序的偏向也不同，例如90年代末期Intel的P6微架构就在浮点类程序上表现优异，但在整数类应用中不如同频下的对手。 
常见的代号如Haswell、Cortex-A15等都是微架构的称号。注意微架构与指令集是两个概念：指令集是CPU选择的语言，而微架构是具体的实现。i7-4770的核心是Haswell微架构，这种微架构兼容x86指令集。 
对于兼容ARM指令集的芯片来说这两个概念尤其容易混淆：ARM公司将自己研发的指令集叫做ARM指令集，同时它还研发具体的微架构如Cortex系列并对外授权。但是，一款CPU使用了ARM指令集不等于它就使用了ARM研发的微架构。Intel、高通、苹果、Nvidia等厂商都自行开发了兼容ARM指令集的微架构，同时还有许多厂商使用ARM开发的微架构来制造CPU。通常，业界认为只有具备独立的微架构研发能力的企业才算具备了CPU研发能力，而是否使用自行研发的指令集无关紧要。微架构的研发也是IT产业技术含量最高的领域之一。 
下面先介绍ARM CPU架构 
ARM架构 
CPU家族 
 
ARM CPU家族主要包括如下系列，其中需要记住的有三个，分别是Cortex-A系列、Cortex-R系列和Cortex-M系列。 
Cortex-A系列：主要是面向应用的处理器。Cortex-A系列含有整数运算的指令集架构和浮点数运算的指令集架构，并且支持单指令多数据流高性能计算指令，Cortex-A系列是ARM家族中最丰富的指令集。Cortex-A系列在手机端开发领域被广泛使用。ARMv7-A是指令集为32位的Cortex-A架构，ARMv8-A是对ARMv7-A的扩充，现在使用ARM架构的手机大多使用的是64位的ARMv8-A架构，如Cortex-A57、Cortex-A53以及新发布的Cortex-A76架构。同时，ARM公司也在不断地扩展ARMv8-A指令架构，先后发布了ARMv8.1、ARMv8.2、ARMv8.4和ARMv8.5。Android手机场景多应用了Cortex-A系列，我们Android开发里面的armv7a的a就是代表了Cortex的a系列。Cortex-R系列，是ARM家族中体积最小的处理器。Cortex-R系列处理器主要用于对实时性要求较高的硬件平台，比如硬盘、各类控制器等。Cortex-R系列处理器支持ARM、Thumb和Thumb-2指令集。Cortex-M系列，主要是针对超低功耗和核心最小面积进行设计的，所以目前Cortex-M系列的实时操作系统RTOS仅支持32位Thumb的指令集。ARM Cortex-M系列使用Thumb-2指令集，这样可以减少一定的指令代码量，从而减少内存需求，进而就可以更加高效地利用缓存。Thumb-2指令集兼容16位的Thumb指令。早期处理器SecurCore系列，它们是提供安全解决方案的架构。SecurCore架构是一个针对安全的解决方案，早期处理器SecurCore被用在少量单片机中。早期的ARM芯片，版本号比较简单，架构复杂度也低。 
ARM商业模式介绍 
微架构研发完成，或者说核心研发完成，接下来就是将其组装为芯片了。过去的芯片仅仅包括CPU部分，如今大量的芯片集成了CPU、GPU、IO等多种不同的功能组件，此时这种芯片就不是传统意义上的“CPU”了。将各种功能组件组装为芯片的技术含量相比微架构研发来说是较低的，因而业界能做此类工作的企业也数量较多。不同的微架构研发企业使用不同的商业模式。在PC时代，几大主要的CPU研发厂商都只是自己研制微架构自己用。而ARM公司的微架构是授权模式。ARM自己开发微架构后将它们上架出售，其他厂商可以拿这些核心组装为芯片来使用或销售。由于这种模式对第三方的技术能力要求很低，加上ARM的微架构在低功耗领域表现优异，这种模式获得了广泛成功。 
如果你发现某款芯片标明使用了Cortex系列核心，则一定是这种模式的产物。如前所述，仅仅从ARM购买微架构来组装芯片的厂商是不能被称作CPU研发企业的，这些芯片也不能被称为“xx厂商研发的CPU”。典型如华为的海思920、三星Exynos 5430，只能说是“使用ARM Cortex-A15核心的芯片”。 
但是如果一款基于ARM指令集的芯片使用了厂商自主研发的微架构情况就不同了。高通骁龙800、苹果A7就是这样的例子–它们分别使用了高通、苹果自主研发的CPU，比如高通骁龙800，使用了自研的Krait微架构。 
下面我们看几款主流手机的CPU。 
主流CPU供应商及应用手机 
在手机端我们重点关注高通、苹果、联发科、华为、三星几家公司设计的芯片，因为它们设计的ARM芯片架构已经可以覆盖主流机型。 
高通 
高通主要的骁龙系列，目前8XX系列属于高端的骁龙系列，6XX系列定位于中端市场。不过高中端市场也会有交叉，并不是绝对的。骁龙芯片被大量流行手机搭载，包括vivo、OPPO、小米等品牌手机。 
12 月 1 日消息 高通在 2020 骁龙技术峰会上正式发布了骁龙 888 旗舰平台处理器，将支持下一代旗舰智能手机 
骁龙 888基于三星5nm 工艺制成 ， CPU 采用 1 x 2.84GHz (ARM最新 Cortex X1 核心)+3 x 2.4GHz (Cortex A78)+4 x 1.8GHz (Cortex A55)，GPU 为 Adreno 660，采用 X60 5G modem 基带，支持 WiFi 6E、Bluetooth 5.2。下面是骁龙888的架构图。 
 
市面上我们看到的小米11 Pro，魅族18 Pro/魅族18等就是用了骁龙888， 而魅族17 Pro使用的是骁龙865。 
 
联发科（MTK） 
中国的芯片研发公司，以中低端芯片为主。联发科的芯片设计业务已经深度布局家庭娱乐、手机平板和IoT、便携式电子产品等多个领域。我们熟知的索尼，TCL、海尔等电视都有使用联发科芯片；同时它的芯片方案被亚马逊、Google、阿里等国内外一线品牌采用，这三家在智能音箱业务上占据了全球销量前三的位置，市场占有率高达70%以上，天猫精灵同样搭载的是联发科的芯片； 
三星 
旗下业务众多，芯片只是其中一项，手机也是其重要业务之一，从零部件如芯片、屏幕、存储、电池等，到手机整机都可以制作。Exynos8895是三星发布的一款偏高端的芯片，它的中端芯片有Exynos7870等。三星芯片在中国移动互联网市场的占比较少。国内市场三星手机大部分并不是使用的三星芯片，而且高通，这里肯定有一些其他方面的考量。 
苹果 
苹果手机中的芯片也都是ARM架构的，目前苹果公司已经有了很强的芯片设计能力。苹果A系列的CPU频率并不是很高。苹果的芯片是自产自销的，芯片架构会直接应用在自家的苹果手机上，所以更加注重性能的提升，而对芯片成本的考量相对少一些，使用了大的芯片面积来换取高性能和低功耗，这一点和大部分同行业公司有显著差别。在苹果A10处理器上，流水线技术使用了六发射，可以同时对六条流水线进行处理，同时A10片上缓存达到了2MB。通过A10可以看到，苹果公司的芯片架构成本比其他公司的更高，综合性能也非常好。从Geekbench跑分也可以看到苹果芯片的性能优势，A10的性能比ARM 73标准架构的性能高75%以上。 
今年苹果新推出的M1 Pro与M1 Max，M1Pro款芯片内存带宽最高可以达到200 GB每秒，支持32 GB内存。材料是五纳米工艺具有337根晶体管，具有10个核心的中央处理器，处理速度与上一个芯片相比的话，速度提高了70%左右。 
 
 
华为海思 
1991年，华为成立了自己的ASIC设计中心，专门负责设计「专用集成电路」（Application-specific integrated circuit，ASIC）。当时的华为，创立仅仅四年，员工只有几十人，资金非常紧张，一度濒临倒闭的边缘。奠定基业的C&C08数字程控交换机，还是三年后的事情。 
这个ASIC设计中心的成立，意味着华为开始了IC设计的漫漫征途。 
1993年，ASIC设计中心成功研发出华为第一块数字ASIC。 
随后，分别在1996年、2000年、2003年，研发成功十万门级、百万门级、千万门级ASIC。总的来说，每一步都算是沉稳有力。 
时间到了2004年10月，这时的华为，实力已今非昔比，销售额达到462亿人民币，员工人数也达到数万人。有了一定底气的华为，在ASIC设计中心的基础上，成立了深圳市海思半导体有限公司，也就是我们现在经常说的——「华为海思」。 
华为手机端的麒麟芯片是不对外销售的，所以截至2019年5月，市面上可体验到华为芯片性能的设备只有华为手机。华为的芯片设计能力的提升速度可谓有目共睹。从麒麟970开始，华为将高性能的专用神经网络芯片NPU带进了SoC。 
市面上我们看到的华为 Mate 40 Pro 4G全网通和华为P50 Pro手机使用的是8核的麒麟9000（1Cortex-A77@3.13GHz + 3Cortex-A77@2.54GHz + 4Cortex-A55@2.05GHz）；华为P40使用的是八核HUAWEI Kirin 990 5G(麒麟990 5G)（2Cortex-A76 Based 2.86GHz + 2Cortex-A76 Based 2.36GHz + 4Cortex-A55 1.95GHz）；而华为P50使用了八核的骁龙888 4G版（1Cortex-X1@2.84GHz + 3Cortex-A78@2.42GHz + 4*Cortex-A55@1.8GHz）。 
芯片 
说到芯片我们不得不面对一个灵魂拷问：”麒麟芯片是国产吗？“ 
要回到这个问题，我们必须先搞清楚芯片半导体生产过程。 
 
半导体芯片生产主要分为 IC（Integrated Circuit 集成电路） 设计、 IC 制造、 IC 封测三大环节以及设计、晶圆制造、EDA工具、芯片原材料、封装、测试、设备七大板块。 
IC 设计主要根据芯片的设计目的进行逻辑设计和规则制定，并根据设计图制作掩模以供后续光刻步骤使用。IC 制造实现芯片电路图从掩模上转移至硅片上，并实现预定的芯片功能，包括光刻、刻蚀、离子注入、薄膜沉积、化学机械研磨等步骤。IC 封测完成对芯片的封装和性能、功能测试，是产品交付前的最后工序。 
 
芯片制作是一个复杂过程，如果单从真个链路上讲，麒麟只是其中的一环，芯片自然谈不上纯国产，如果从某一个环节的完整性来讲，整个设计是有自己东西的。 
麒麟主要是基于ARM的RISC架构进行SOC的开发，所谓SOC就如联想设计电脑，就是把一些功能芯片整合在一起，包括CPU、GPU、闪存、基带等等。而要将这些核芯整合在一起，本身就需要非常强大的设计能力。 
 
就上图华为麒麟960来说，黄色部分是华为不能设计的核芯。对于麒麟985华为已经能自研GPU了。在SOC大构架下，这种设计能力，很多公司能在短时间内得以提升，真正考验芯片设计能力的是核芯层次的微架构设计。 
有这方面能力的国内芯片设计公司屈指可数，华为和龙芯是其中的佼佼者。 
什么是芯片？ 
芯片，主要原料就是储量最丰富成本最廉价的二氧化硅。要弄清楚为什么原材料是二氧化硅我们先来复习两个概念： 
导体半导体 
很明显，像导体这种直男没啥可折腾的，所以导线到了今天仍然是铜线，绝缘体的命运也差不多。 
半导体这种暧暧昧昧的性格最容易搞事情，所以与电子设备相关的产业基本都属于半导体产业，如芯片、雷达。芯片是计算机的核心，它的最基本单元是非黑即白的0和1。 
经过计算筛选，科学家用硅作为半导体的基础材料。硅的外层有4个电子，假设某个固体由100个硅原子组成，那么它的满轨道就挤满了400个电子。这时，用10个硼原子取代其中10个硅原子，硼这类三价元素外层只有3个电子，所以这块固体的满轨道就有了10个空位。这就相当于在挤满人的公交车上腾出了几个空位子，为电子的移动提供了条件。这叫P型半导体。 
同理，如果用10个磷原子取代10个硅原子，磷这类五价元素外层有5个电子，因此满轨道上反而又多出了10个电子。相当于挤满人的公交车外面又挂了10个人，这些人非常容易脱离公交车，这叫N型半导体。 
现在把PN这两种半导体面对面放一起会咋样？N型那些额外的电子必然是跑到P型那些空位上去了，一直到电场平衡为止，这就是大名鼎鼎的“PN结”。(动图来自《科学网》张云的博文) 
这时候再加个正向的电压，N型半导体那些额外的电子就会源源不断跑到P型半导体的空位上，电子的移动就是电流，这时的PN结就是导电的;加个反向的电压，P型半导体那里再抽电子到N型半导体，而N型早已挂满了额外的电子，多出来的电子不断增强电场，直至抵消外加的电压，电子就不再继续移动，此时PN结就是不导电的。 
 
芯片的制作 
1. 芯片设计 
芯片行业的设计领域，指的是规格制定、架构设计到tape－out的所有流程。芯片在晶圆厂生产之前的所有流程都属于设计领域： 
仅从事芯片设计，没有其他生产、封装、测试业务的公司称之为fabless或者design house（国内的华为海思、紫光展锐、中兴微电子、比特大陆、寒武纪、汇顶科技、全志，美国的高通、博通等）。既有芯片业务，又有芯片晶圆制造业务的公司，称为IDM（Integrated Device Manufacture），国内的士兰微，美国的英特尔，韩国的三星、海力士，意大利的意法半导体等。 
2. 晶圆制造 
在芯片行业，把仅从事晶圆制造的企业称之为foundry。 
把单晶硅圆柱切片，就得到了圆形的硅片，因此就叫“晶圆”。 
晶圆加工的大概流程： 
首先在晶圆上涂一层感光材料，这材料见光就融化，这里就用到了我们常听到的光刻机，它可以用非常精细的光线，在感光材料上刻出图案，让底下的晶圆裸露出来。然后，用等离子体这类东西冲刷，裸露的晶圆就会被刻出很多沟槽，这套设备就叫刻蚀机。再用离子注入机在沟槽里掺入磷元素，加热退火处理，就得到了一堆N型半导体；用离子注入机撒上硼，就有了P型半导体。 
 
上图晶圆上的小方块就是芯片，一块晶圆可以做多个芯片。芯片放大了看就是成堆成堆的电路。 
为啥不把芯片做的更大一点呢？这样不就可以安装更多电路了吗？ 
一是成本考虑：一块300mm直径的晶圆，16nm工艺可以做出100块芯片，10nm工艺可以做出210块芯片，于是价格就便宜了一半。二是性能问题：大芯片的布线比小芯片更长，所以延时也更明显，驱动电流也大很多，由此导致整体设计更臃肿，性能上还是会吃亏。 
3. EDA工具 
EDA是Electronic design automation（电子设计自动化）的英文简称。EDA工具是指利用计算机辅助设计软件，来完成超大规模集成电路芯片的功能设计、综合、验证、物理设计等流程的设计方式。在芯片行业，把提供EDA工具的企业称之为EDA设计服务供应商。 
目前EDA工具厂商的三巨头——cadence、synopsys、mentor公司垄断了绝大多数市场份额，其他EDA厂商很多都是在三巨头的阴影之下，夹缝之中求生存。 
4. 芯片原材料 
芯片生产需要大量的原材料，比如硅晶圆、光刻胶光刻胶等，其中需求量最大的当属硅晶圆，硅晶圆在芯片制造材料中占比最高，达到37%。而硅晶圆的主要原材料是硅，硅的主要评判指标是纯度，如果硅原子之间有一堆杂质，那电子就很难在满轨道和空轨道之间跑顺畅。而纯度越高制造难度越大。用于太阳能发电的高纯硅要求99.9999%，这玩意儿全世界超过一半是中国产的，早被玩成了白菜价。芯片用的电子级高纯硅要求99.999999999%(别数了，11个9)，几乎全赖进口，直到2018年江苏的鑫华公司才实现量产。 
5. 封装 
芯片封装，简单点来讲就是把Foundry生产出来的集成电路裸片放到一块起承载作用的基板上，再把管脚引出来，然后固定包装成为一个整体。 
6. 测试 
测试的话包括CP测试、FT测试等等，包括了芯片的功能测试、可靠性测试、老化测试等等。 
感觉芯片行业技术难度最低的就是测试和封装，这部分国内和国外已经没有太大差距。 
7. 设备 
芯片行业的设备主要有晶圆制造过程中需要使用的光刻机、蚀刻机，测试过程中需要使用的ATE测试基台。 
阿斯麦是全球唯一的高端光刻机生产商，每台售价至少1亿美金，而且产量还不高，无论是台积电、三星，还是英特尔，谁先买到阿斯麦的光刻机，谁就能率先具备7nm工艺。 
2017年只生产了12台，2018年24台，这些都已经被台积电三星英特尔抢完了，2019年预测有40台，这些这个东西是很容易被人卡脖子的： 
第一：英特尔有阿斯麦15%的股份，台积电有5%，三星有3%，有些时候吧，钱不是万能的。第二，美国整了个《瓦森纳协定》，敏感技术不能卖，中国、朝鲜、伊朗、利比亚均是被限制国家。
中国的刻蚀机比光刻机要好很多，16nm刻蚀机已经量产运行，7-10nm刻蚀机也在路上了，所以美国在刻蚀机上没有做太多限制。 
离子注入机又比较差了，70%的市场份额是美国应用材料公司的，2017年8月终于有了第一台国产商用机，水平不做评价。 
涂感光材料得用“涂胶显影机”，日本东京电子公司拿走了90%的市场份额。即便是光刻胶这些辅助材料，也几乎被日本信越、美国陶氏等垄断。 
为什么”光刻机“这么难攻破？说到底设备最后的竞争还是材料。比如发动机，工业皇冠上的明珠，其核心技术说白了就是涡轮叶片不够结实，油门踩狠了就得散架，无论是航天发动机、航空发动机、燃气轮机，只要带个“机”字，我们腰杆都有点软。引用别人对材料的分析： 
 
 而材料技术除了烧钱、烧时间，有时还要点运气。还是以发动机为例：金属铼，这玩意儿和镍混一混，做出的涡轮叶片吊炸天，铼的全球探明储量大约2500吨，主要分布在欧美，70%用来做发动机涡轮叶片，这种战略物资，妥妥被美帝禁运。前几年在陕西发现一个储量176吨的铼矿，可把土工乐的，马上拼了老命烧钱，这几年苦逼生活才有了起色。稀土永磁体，就是用稀土做的磁铁，能一直保持磁性，用处大大的。高品位稀土矿大多分布在中国，所以和“磁”相关的技术，土工比美帝还能嘚瑟，比如核聚变、太空暗物质探测等。据说，土工前几年也对美帝禁运，逼得美帝拿铼交换，外加陕西安徽刨出来的那点铼，J20的发动机才算有些眉目。关键核心材料，全球总共约130种，也就是说，只要你有了这130种材料，就可以组装出世界上已有的任何设备，进而生产出已有的任何东西。 
 人类的核心科技，某种程度上说，指的就是这130种材料，其中32%国内完全空白，52%依赖进口。在高端机床、火箭、大飞机、发动机等尖端领域比例更悬殊，零件虽然实现了国产，但生产零件的设备95%依赖进口。这些可不是陈芝麻烂谷子的事情，而是工信部2018年7月发布的数据，还新鲜着呢。 
 
看看国内半导体产业整个链路的情况： 
 
总结 
本文总结了CPU、指令集、架构、芯片的概念。并对他们的概念做了详细介绍和区分：指令集相当于函数头，微架构相当于函数体，微架构说白点就是CPU硬件图纸，设计这里有几个晶体管，那里怎么连接晶体管，而芯片是设置设计的物理实现。并详细介绍了ARM架构以及芯片设计的流程及关键技术。 
参考 
一文看懂光刻机 | 半导体行业观察史上最全半导体产业链全景图点沙成金：半导体芯片"
存内计算：释放潜能的黑科技,2024-01-31,"什么是存内计算？ 
存内计算技术是一种新型的计算架构，它将存储器和计算单元融合在一起，以实现高效的数据处理。存内计算技术的优势在于能够消除数据搬运的延迟和功耗，从而提高计算效率和能效比。目前，存内计算技术正处于从学术到工业产品落地的关键时期，随着技术的不断进步和应用场景的不断催生，预计存内计算技术将成为AI计算领域的主要架构。 
 
陈巍博士是存算一体芯片技术的专家之一，他指出存算一体技术比冯诺依曼架构更适合AI计算的架构，是比冯氏架构更适合AI计算的架构。存算技术也被AspenCore预测为2022年的全球半导体行业十大技术趋势。目前存算技术正处在从学术到工业产品的跃迁的关键时期。包括阿里达摩院最近刚发布的基于SeDRAM的近存计算芯片，就充分展示了存算技术在数据中心场景的算力和能效实力。 
 
为何存内计算如此重要？ 
存内计算的重要性在于它解决了传统计算方式中存在的瓶颈问题。在传统计算中，数据通常需要从存储器传输到处理器进行计算，然后再将结果传回存储器。这一过程中的数据传输耗时耗能，限制了计算速度的提升。 
存内计算通过在存储器中进行计算，消除了数据传输的瓶颈。这意味着计算过程更加高效，能够更快地处理大规模数据集，同时减少了能耗，使得计算过程更为环保。 
 
存内计算的应用领域 
存算一体技术在不同应用领域具有广泛的适用性，主要包括以下几个方面： 
 人工智能和大数据计算： 存算一体技术被确定为未来的科技趋势，特别适用于人工智能和大数据计算。通过将AI计算中的大量乘加计算的权重部分存储在存储单元中，并在存储单元的核心电路上进行修改，可以在读取的同时进行数据输入和计算处理，从而提高设备性能并降低成本。存内计算和存内逻辑特别适用于深度神经网络应用和基于AI的大数据技术。感存算一体： 集成传感、储存和运算的感存算一体架构能够解决冯诺依曼架构的数据搬运功耗瓶颈。该技术与传感器结合，实现零延时和超低功耗的智能视觉处理能力。这种架构对于视觉应用存内计算神经拟态芯片具有重要意义，有效降低能耗。类脑计算： 存算一体技术天然适合应用在类脑计算领域，成为类脑计算的关键技术基石。类脑计算借鉴生物神经系统的信息处理模式和结构，旨在使计算机像人脑一样将存储和计算合二为一，高速处理信息。存算一体芯片在类脑计算的大算力高能效领域有着重要的应用前景。人工智能场景和元宇宙计算： 存算一体芯片适用于各类人工智能场景和元宇宙计算，包括可穿戴设备、移动终端、智能驾驶、数据中心等。在这些领域，存算一体技术能够提供高算力、低功耗、高性价比的优势。端侧和云计算设备： 存算一体技术在端侧设备中能够满足对成本、功耗、时延、开发难度的敏感要求，竞争力影响约占30%。在云计算和边缘计算的大算力设备中，存算一体芯片具有更大的优势，竞争力影响约占90%。在边缘端，存算一体芯片具有高算力、低功耗、高性价比的优势。云计算市场： 针对当前云计算算力市场，存算一体技术有望抢占市场份额。由于GPU的单一架构不能适应不同AI计算场景的算法离散化特点，存算一体芯片以其高能效大算力的优势有望另辟蹊径，满足不同领域的需求，尤其在图像、推荐、NLP等领域。
 
正在研究存内计算的公司 
存内计算技术是一种新型的计算架构，它将存储器和计算单元融合在一起，以实现高效的数据处理。存内计算技术的应用场景包括物联网、深度学习、大数据分析等领域. 目前，有许多公司在研究和开发存内计算技术，以下是一些主要的公司： 
Diablo Technologies: 提供Memory1，这是一种容量为传统DRAM的4倍的内存技术。GigaSpaces: 提供用于实时洞察和极端事务处理的最快存内计算平台。Hazelcast: 提供快速的存内计算服务。IBM: 提供用于大数据分析和机器学习的存内计算技术。Intel: 提供用于计算系统制造商的处理器，以及制造主板芯片组、网络接口控制器和集成电路、闪存、图形芯片、嵌入式处理器等。Microsoft: 提供用于深度学习和大数据分析的存内计算技术。Oracle: 提供用于大数据分析和机器学习的存内计算技术。SAP: 提供用于大数据分析和机器学习的存内计算技术。其它正在研究存内计算的公司可以点击该链接查看：https://www.ventureradar.com/keyword/In-Memory%20Computing 
这些公司都在不断推进存内计算技术的发展，以满足不同领域的需求。存内计算技术正处于从学术到工业产品落地的关键时期，随着技术的不断进步和应用场景的不断催生，预计存内计算技术将成为AI计算领域的主要架构。 
存内计算的未来 
存内计算作为一种颠覆性的计算方式，将在未来发挥越来越重要的作用。随着存内计算技术的不断发展，未来存内计算技术将会在以下方面得到进一步的应用和发展: 
 存算一体芯片：存算一体芯片是存内计算技术的一种重要实现方式，它将存储单元和计算单元集成在一起，以减少数据传输和能耗，提高计算效率。存算一体芯片的应用前景广泛，特别适用于人工智能、大数据处理、图像处理、自然语言处理等领域。存内计算算法：存内计算算法是存内计算技术的另一重要组成部分，它是实现存内计算的关键。存内计算算法的发展将会对存内计算技术的应用和发展产生重要影响。存内计算应用：存内计算技术的应用领域非常广泛，包括人工智能、大数据处理、图像处理、自然语言处理等领域. 随着存内计算技术的不断发展，存内计算技术将会在这些领域得到更广泛的应用和发展。
存内计算技术的未来发展充满了机遇和挑战，但是相信随着技术的不断进步和应用场景的不断拓展，存内计算技术将会在计算机技术的未来发挥越来越重要的作用。 
资料来源 
[1] Top In-Memory Computing Companies Top ranked companies for keyword search: In-Memory AND Computing [2] 存内计算最强科普 [3] 存内计算正式进入主流 [4] 存内计算技术发展趋势分析 
 作者信息 作者 ： 繁依Fanyi CSDN： https://techfanyi.blog.csdn.net 掘金：https://juejin.cn/user/4154386571867191"
Quantinsti 博客中文翻译（十九）,2024-11-11,"原文：Quantinsti Blog 
 协议：CC BY-NC-SA 4.0 
 
隐马尔可夫链简介 
 
 原文：https://blog.quantinsti.com/intro-hidden-markov-chains/ 
 
这篇文章最初发表在 Quants 门户上。 
作者博诺洛 
在你希望确定投资回报的情况下，你可能有所有的专业知识来做这件事，但如果没有某些信息(缺失的部分)，就不可能得出一个结论性的数字。实际上“假设你有你投资组合中所有资产的所有回报的价值；如果没有每项资产产生回报的比率，我们将无法真实反映特定时间点的投资组合回报，因此我们可能无法提供准确的回报估计。”一个称为隐马尔可夫模型的过程可以用来解释这个问题。 
这个过程分为两个部分:一个可观察的部分和一个不可观察的或“隐藏的”部分(van Handel，2008)。然而，从可观察的过程中，我们可以提取关于“隐藏”过程的信息。因此，我们的任务是从观察到的过程中确定未观察到的过程。 
隐马尔可夫模型(HMM)有两个定义性质。(I)它假设当时的观察是由某个过程产生的，该过程的状态对观察者是隐藏的，并且(ii)它假设该隐藏过程的状态满足马尔可夫性质。对某些人来说，HMM 可能看起来很复杂，但是一旦理解了什么是马尔可夫模型，它就很容易理解了。我们将研究这两个模型组件，然后考虑有助于构建这些 hmm 的高级技术。 
构建隐马尔可夫模型 
【隐藏过程】 
如果满足以下条件，则称一个过程具有马尔可夫特性: 
对于任何一个⊆s，任何一个值 n 和任何一个时间值t1tt2<…<tnt17】tn+1t13】确实如此** 
 
这意味着，要确定流程的下一个状态，可以只考虑流程当前所处的状态，而忽略之前发生的所有事情，因为该信息已经包含在当前状态中。 
我们需要一些属性和定义来帮助我们最终理解 HMM 的概念 
时间同质性:这发生在从 a 到 b 的概率与时间无关的时候，也就是说，你在这个过程中走了多远并不重要；只要这些过程在一个步骤中从 a 移动到 b，整个过程的概率是相同的。当一个过程具有这种性质时，我们说这个过程是时间同质的，如果不是；时间非同质虽然可以处理无限个状态，但在我们的财务环境中，处理有限个不可约的状态就足够了。**不可约状态:**经过一定数量的步骤，从任何一个状态转移到另一个状态都是可能的。 
 
这个概率矩阵是这样的: 
 
注意:这些排放概率是这个过程下一步走向的主要驱动力。根据我们的时间同质性假设，我们可以计算出该过程在 t 步后处于状态 j 的概率，假定它开始于 i 我们将矩阵 P 乘以自身 t 次，然后读出PnT17】的ijthT13】元素** 
示例: 
让我们考虑两个概率转移矩阵，每个矩阵有两个转移状态，一个是时间均匀的，另一个不是。 
非时齐情况 
 
然后和 
 
这里改变状态的概率取决于你在时间中的位置。与此过程相反，时间齐次矩阵给出了与时间无关的常数概率。 
在这种情况下 
快速任务 
 
 
然后 
 
马尔可夫转移的可视化表示 
保持我们的分析简单，让我们研究一个三态过程 S = {1，2，3} 
从任何一个状态移动到任何其他状态的概率由概率矩阵 P 给出，概率矩阵 P 由下式给出: 
 
下图给出了这种现象的另一种观点: 
 
根据该图，我们可以在一个转变(时间步长)内移动到任何状态或保持在当前状态。这对于 A 和 C 来说都是正确的，但是一旦进程到达 B，它必须在下一个转换中移动，这是因为停留在 B 的概率是 0。 
有了马尔可夫链，我们可以看到，它们会保留未观察到的信息，并最终产生一个更现实的模型，这就是我们首先关注 hmm 的原因。 
可观察的过程 
隐藏状态是由一些性质决定的，我们可以通过推导这些性质来更好地理解这些隐藏状态的行为。 
为了得到这个过程的密度估计值，我们需要解许多方程组。像 Baum-Welch 算法和 Viterbi 算法这样的算法给出了极其精确的估计，但是由于它们的复杂性，我们将暂时避开它们，但是稍后再回到它们。相反，我们将研究卡尔曼滤波器，因为它遵循的过程类似于 HMM 推导中使用的过程，因此它将使我们直观地理解 HMM 是如何产生的。卡尔曼滤波器是一种广泛应用于控制系统和航空电子设备的数学技术，用于从一系列不完整和有噪声的测量中提取信号。 
从不同的角度来看，我将首先列出卡尔曼滤波器和 HMM 方法之间的区别。 
 
从上面，我们得到一种感觉，通过卡尔曼滤波器找到估计更简单，但同时，我们观察 HMM 如何将估计提高到一个全新的水平。我画了这个表来演示，如果不是暗示，在尝试寻找 HMM 的估计值之前，首先熟悉卡尔曼滤波器是多么的必要。 
卡尔曼滤波 
卡尔曼滤波器的关键点 
这是一种找到过程估计值的方法。过滤来自于减少或“过滤掉”不想要的变量的原始用法。在我们的例子中是估计误差 
滤波器估计值 
假设我们有两个过程，一个状态过程和一个观察过程，由下面的线性方程组给出: 
 
 
这里的 
 

 
 
 A 

 
 ∗ 

 
 k 

 

{A} *{k} 
 

A∗k和 
 

 
 
 C 

 
 ∗ 

 
 k 

 

{C}* {k} 
 

C∗k可以是矩阵或变量，甚至可以是简单的常量值。 
通常假设 
 

 
 
 v 

 
 ∗ 



k 
 

+ 
 

1 
 

 

{v} *{k+1} 
 

v∗k+1和 
 

 
 
 w 

 
 ∗ 



k 
 

+ 
 

1 
 

 

{w}* {k+1} 
 

w∗k+1是独立且同分布的高斯或正态分布，具有均值 0 和一些协方差矩阵(理想情况下是对角线，以反映观测值之间的独立性)。 
卡尔曼滤波方程的估计值是使用相当先进的方法得出的，这些方法需要对多变量分析有足够的了解。因此，我将在这里给出隐藏过程的期望值和方差的矩方程。 
时间更新方程式为 
 
 
并且测量值更新等式: 
 
 
 
第二组方程(*测量值更新)*确定过程的均值和协方差将在哪里给出第一组方程(*时间更新)*的结果。 
卡尔曼滤波器增益 ( K k+1 ) 用于反映我们合成的模型相对于一些观察到的(通常是历史的)模型之间的误差项的显著性。如果在 0 和 1 之间的概率的增益很小，这将意味着估计的模型是相对接近现实的，即现实的良好度量。如果这个概率非常大，那么它可能表明我们模型是无效的，需要用统计中可用的许多误差最小化方法进行重新评估。事实是，确定卡尔曼增益与获得期望过程的估计值一样重要。该滤波器增益的推导将在附录中提供。 
随机动态规划 
埃里克·b·拉伯和他的同事周华以一种简单而令人满意的、有意识的方式剖析了动态编程(DP)是如何运作的。首先，他们将问题(过程)分成子区间。观察到这些“子问题”是相关的，他们建议单独解决子过程，并将答案存储在表中，然后使用记录的答案来回答初始问题。 
在任何包含隐藏变量(如 HMMs)的模型中，确定变量序列的任务被称为解码(Read，2011)，其目标是从观察到的序列中确定最可能的隐藏状态序列(Blunsom，2004)。维特比算法已经被用来为一个被观察的序列寻找单个最佳状态序列，并且这样做的方式让人不得不佩服。 
维特比算法 
该算法大致遵循一个四步过程，在该过程的最后，将导出最可能的转换过程。 
初始化 
 
 
递归 
 
 
结束 
 
 
一旦训练集中的所有转换都被考虑，代码将提取给定所有先前事件的最可能(最大概率)事件。 
最优状态序列回溯 
 
回溯允许从递归步骤的结果中找到最佳状态序列(Blunsom，2004)。 
有趣的是:给定算法获得最可能序列的能力，没有简单的方法来获得第二好的序列。 
工作示例 
Bhar 和 Hamori 使用 HMM 分析 G7 国家的股票市场回报，使用月度回报(2004，第 43 页)。当两人意识到美国对其他 G7 国家施加强大的影响，但有趣的是，其他国家对美国的影响并不小或没有时，这种情况就出现了。 
假设整个 G7 月度回报的模型由以下形式的双态马尔可夫模型给出 
 
回报被分成波动性高和低的时段。一个国家保持当前状态的估计概率由低波动性时和高波动性时给出。 
下表显示了 Bhar 和 Hamori 对七个国家的实证研究结果。给定来自 Baum-Welch 算法和 Viterbi 算法的平滑概率和参数估计，我们有: 
 
这些统计数据不仅仅是为了假设模型；他们的主要目的是提供更多的数据信息，从而帮助决策者有效地制定战略。从该表中获得的一些信息: 
日本最有可能长时间处于不稳定状态，而美国可能会更快离开不稳定时期。在一个更加不稳定的体制下，意大利可能会获得更多的平均回报 
作为练习，读者可以更仔细地观察从数据中可以得出什么样的推论。 
参考书目 
巴尔和哈莫里(2004 年)。*隐马尔可夫模型。*伦敦:Kluwer 学术出版社。 
布伦松，P. (2004 年)。隐马尔可夫模型。 
里德，J. (2011 年)。隐马尔可夫模型和动态规划。 
范·汉德尔(2008 年)。隐马尔可夫模型:课堂讲稿。 
免责声明:本客座博文中提供的观点、意见和信息仅属于作者个人，不代表 QuantInsti 的观点、意见和信息。本文中所做的任何陈述或共享的链接的准确性、完整性和有效性都不能得到保证。我们对任何错误、遗漏或陈述不承担任何责任。与侵犯知识产权相关的任何责任由他们承担。T3】 
深度学习和神经网络简介 
 
 原文：https://blog.quantinsti.com/introduction-deep-learning-neural-network/ 
 
作者:沙古塔·塔西尔达 
“人工智能将是谷歌的终极版本。理解网络上一切的终极搜索引擎。Alphabet 首席执行官拉里·佩奇(Larry Page)在 2000 年表示:“它会准确理解你想要什么，并给你正确的东西。”当时我们大多数人都认为谷歌只是一个简单的搜索引擎。今天，人工智能和深度神经网络学习正在使几年前甚至难以想象的事情成为可能！ 
感谢像机器学习和深度学习这样的领域，它们是人工智能的一部分，我们已经能够在机器内自动化和集成“学习”。 
在这篇介绍深度学习的文章中，我们将学习什么是深度学习，以及它如何被用于人工智能领域。 
本文关于“深度学习与神经网络导论”的内容如下: 
什么是深度学习？深度学习的历史深度学习和机器学习的区别深度神经网络的工作深度学习的应用 
什么是深度学习？ 
深度学习是一种涉及使用人工深度神经网络的机器学习方法。就像人类的大脑由通过发送和接收信号来处理信息的神经细胞或神经元组成一样，深度神经网络学习由多层相互通信和处理信息的‘神经元’组成。 
深度学习中的‘深’指的是网络内部的层数；层数越多，网络越深。 
深度神经网络可以处理标记的数据以及未标记的数据，这允许监督和非监督学习。然而，需要大量数据来训练深度神经网络，这可能有助于产生更准确的结果。 
这些网络还能够识别输出中的错误或损失，并在不需要人工干预的情况下纠正它们。 
深度学习历史 
除了介绍深度学习和理解它是什么，我们还将了解它是如何开始的。正如我们现在所知，深度学习网络基本上是具有许多层的深度神经网络。 
1943 年，沃尔特·皮茨和沃伦·麦卡洛克创建了第一个神经网络数学模型，展示了人类大脑的思维过程。从这里开始了深度神经网络和深度学习的旅程。 
这个旅程可以表示如下: 
 
1957——弗兰克·罗森布拉特(Frank Rosenblatt)提交了一篇题为《感知器:一种感知和识别自动机》的论文，该论文由一种使用双层神经网络进行模式识别的算法或方法组成。1965 -阿列克谢·伊瓦赫年科和 V.G .帕拉开发了第一个工作神经网络，阿列克谢·伊瓦赫年科于 1971 年创建了一个 8 层深度神经网络，并在计算机识别系统 Alpha 中进行了演示。这是深度学习的实际介绍。1980 - Kunihiko Fukushima 开发了“ Neocognitron ”，这是一个具有多个卷积层的人工深度神经网络，用于识别视觉模式。1985——特里·塞伊诺夫斯基创建了 NETtalk ，这是一个学习英语单词发音的程序。1989——Yann le Cun，利用卷积深度神经网络，开发了一个可以阅读手写数字的系统。2009 -由于深度学习模型需要大量的标记数据来在监督学习中训练自己，费-李非推出了 ImageNet ，这是一个大型的标记图像数据库。2012——谷歌大脑进行的“猫实验结果出炉。这项实验基于无监督学习，其中深度神经网络与未标记的数据一起工作，以识别猫图像中的模式和特征。然而，它只能正确识别 15%的图像。2014——脸书开发了 DeepFace ，这是一个深度学习系统，可以识别并标记照片中用户的面部。 
机器学习和深度学习的区别 
深度学习实际上是机器学习的一个子集，因此这两个术语经常被人们混淆。但是，它们的能力不同。这也是为什么在阅读《深度学习导论》的同时，有必要了解机器学习的原因。 
机器学习模型缺乏识别错误的机制，在这种情况下，程序员需要介入调整模型，以做出更准确的决策，而深度学习模型可以识别不准确的决策，并在没有人工干预的情况下自行纠正模型。 
但要做到这一点，深度学习模型需要大量的数据和信息，不像机器学习模型。 
深度神经网络的工作 
到目前为止，我们已经了解了深度学习的基础和介绍，现在是时候了解深度神经网络如何工作了。 
如前所述，深度神经网络得名于网络中的高层数。现在让我们通过参考下面给出的图表来理解这些层是什么，以及它们如何在深度神经网络中使用，以给出最终输出: 
 
深层神经网络的层数 
通过查看该图，我们看到在这个深度神经网络中存在 4 层，即第 1 层、第 2 层、第 3 层和第 4 层。每个深度神经网络由三种类型的层组成，它们是: 
**输入层(第 1 层)😗*该层是深度神经网络中的第一层，它提供处理信息所需的输入参数。它只是将这些参数传递给下一层，而不在这一层进行任何计算。 
**隐藏层(第 2 层和第 3 层)😗*深度神经网络中的这些层对从前几层接收的输入执行必要的计算，并将结果传递给下一层。为了提高深度神经网络的效率，决定层的数目和每层中神经元的数目是至关重要的。隐藏层数越多，网络越深。 
**输出层(第 4 层)😗*深度神经网络中的这一层，在接收到前几层的结果后，给我们最终的输出。 
现在我们已经了解了网络中存在的层的类型，让我们了解这些层实际上是如何工作的，并给出输出数据。 
每个神经元都连接到下一层中的所有神经元，并且所有这些连接都具有与其相关联的一些权重。但是这些砝码是什么，为什么要用？ 
深度神经网络中的权重 
权重顾名思义，就是用来给某个特征附加一些权重。为了获得所需的输出，某些功能可能比其他功能更重要。 
例如，在预测第二天的股票价格时，前几天的收盘价和 SMAs 将被视为比高或低价格更重要的特征，这将影响这些参数的权重。 
这些权重用于计算每个神经元的加权和。x1、x2、x3、x4 表示与深度神经网络中的相应连接相关联的权重。 
除了权重，每个隐藏层都有一个与之关联的激活函数。 
深度神经网络中的激活函数 
激活函数基于它们的加权和来决定神经元是否应该被激活。这些也用于通过使用像 sigmoid 和 tanh 这样的函数来引入非线性，从而允许对更复杂的任务进行计算。如果没有激活函数，深度神经网络将充当简单的[线性回归模型](https://quantra.quantinsti.com/course/trading-with-machine-learning-regression target=)。 
以下是一些使用的激活功能的示例: 
Tanh: 避免梯度偏差整流线性单元(ReLU): 用于图像处理Softmax: 保留离群值的相关性 
除此之外，我们还为每一层添加了一个**“偏置”神经元**，以使激活函数能够沿 x 轴向左或向右移动，从而使我们能够更好地拟合激活函数。当输入为绝对零时，作为常数项的偏置项也作为输出。 
深度神经网络的处理 
该处理开始于使用从输入层接收的输入来计算第一隐藏层中每个神经元的加权和。加权和是输入与每个连接的相应权重的乘积之和。 
对应于每一层的激活函数然后作用于这些加权和，以给出最终输出。这个过程也可以称为正向传播。 
处理完成后，将预测输出与实际输出进行比较，以确定误差或损失。为了使深度神经网络准确工作，必须最小化该损失函数，以使预测输出尽可能接近实际输出。当我们最初为深度神经网络中的连接选择随机权重时，它们可能不是最佳选择。 
因此，为了最小化损失函数，我们需要调整权重和偏差以获得准确的结果。反向传播是用于调整权重和偏差的过程，以便我们获得权重和偏差的最佳值，从而使我们的结果更准确。 
深度学习应用 
 
这篇关于“深度学习简介”的博客涵盖了深度学习的定义、差异、历史和应用，并帮助我们了解人工深度神经网络是如何工作的。 
现在，我们已经熟悉了深度学习的介绍，我们可以继续前进，获取更多关于它的知识。在下面的课程中，你还可以学习如何使用深度学习和神经网络进行 Python 交易。 
你可以在 Quantra 上注册神经网络教程，在这里你可以使用先进的神经网络技术和最新的研究模型，如 LSTM & RNN，来预测市场和寻找交易机会。Keras，使用相关的 python 库。 
推荐阅读 
用于股票价格预测的神经网络的工作训练用于股票价格预测的神经网络Python 中的人工神经网络使用 Keras 预测股价走势深度学习——使用 Python 中 TensorFlow 的人工神经网络RNN、LSTM 和 GRU 进行交易 
免责声明:本文中提供的所有数据和信息仅供参考。QuantInsti 对本文中任何信息的准确性、完整性、现时性、适用性或有效性不做任何陈述，也不对这些信息中的任何错误、遗漏或延迟或因其显示或使用而导致的任何损失、伤害或损害承担任何责任。所有信息均按原样提供。 
蒙特卡洛分析简介 
 
 原文：https://blog.quantinsti.com/introduction-monte-carlo-analysis/ 
 
本文最初发布在 Quants 门户网站上。 
由博诺洛·莫洛皮扬 
安德森等人(1999)将充满神秘色彩的蒙特卡洛定义为通过模拟变量函数的样本均值来逼近期望值的艺术。在斯坦·乌兰姆和约翰·冯·诺依曼(Anderson，1999 年)之间，蒙特卡罗(Monte Carlo)一词曾被用作建造更好的原子弹的随机模拟的一个代号，后来演变成一种用于多种学科的方法，包括物理、金融、力学，甚至用于城镇规划和人口研究等领域。 
蒙特卡罗方法与确定性方法非常不同(麦克雷什，2004)。在确定性模型的情况下，在给定解释变量的情况下，因变量的值只能是由数学公式给出的唯一值。这种模型不包含随机成分(Rotelli，2015)。相比之下，蒙特卡罗并不求解显式方程，而是通过模拟单个粒子并记录其平均行为的某些方面(记录)来获得答案(Briesmeister，2000 年)。考虑到蒙特卡罗方法的广泛应用和涉及的问题，我们将这篇文章分成三个部分，以便有一个清晰的理解。 
首先，我简要介绍了蒙特卡罗方法的历史，然后以物理学中的一个例子为例强调了它的一些用途，并展示了它在金融中的必要性，最后介绍了统计驱动的重要性抽样。 
在以后的著作中，我们将更深入地研究技术方面的东西，给出重要概念的严格定义，然后继续讨论马尔可夫链蒙特卡罗(MCMC ),因为它们在最近的蒙特卡罗计算中起着重要作用。在我的最后一篇文章中，我将结合所有这些方面，形成对称为蒙特卡罗方法的“黑箱”的坚实而全面的理解，重点关注 Metropolis-Hasting 算法，并探索乌兰和冯诺依曼创造的未来可能性。 
蒙特卡洛的历史 
正如一个苹果落在牛顿的头上点燃了一股新的科学潮流，斯坦·乌兰不得不生病才能发现后来成为大量科学问题答案的东西。尽管乌兰和冯·诺依曼形式化并创造了术语蒙特卡罗，但近似方法的早期证据仍然存在。最引人注目的是布丰的针实验。这个实验如下: 
一根长度为 L 的针以随机的方式被扔在一个光滑的桌子上，桌子上划有平行线，平行线之间的距离为 2L。观察者记录指针是否与一条直线相交。从实验中推导出，随着样本的增加，针头越线的概率趋于 1/pi。”(舒斯特，1974)。 
1964 年，乌兰躺在病床上，不知道用 52 张牌拼成的坎菲尔德纸牌游戏能否被成功观察到。经过一些思考并使用纯数学方法，他玩了 100 次游戏，记录了所有成功的玩法，赢的比例反映了拿到一手好牌的几率。他获得了他的结果，很快他就提出了大多数数学物理和微分方程的问题，这些问题将从这种实用的计算方法中受益。那年晚些时候，他把这个想法传授给了约翰·冯·诺依曼(埃克哈特，1987)，他们开始研究更复杂的计算问题，后来又研究核武器。 
蒙特卡洛这个名字的灵感也来自于摩纳哥的轮盘赌游戏，这是一种简单的产生随机数的游戏。另一个有点滑稽的动机是因为乌兰的叔叔喜欢去蒙特卡洛玩轮盘赌，因为这是以他的名义。 
蒙特卡罗的应用 
如上所述，蒙特卡罗方法可以应用于许多研究领域。最早的应用是由发明人完成的，其中他们使用 MC 方法来解决使用电子数字积分器和计算器(ENIAC)的裂变装置中的中子扩散和倍增问题。这后来发展成为 MCNP。 
 
在物理科学中的应用 
蒙特卡罗 N 粒子(MCNP)是一个通用的、连续能量的、广义几何的、依赖于时间的、耦合的中子/光子/电子蒙特卡罗输运程序。它可以用于几种传输模式:仅中子、仅光子、仅电子、中子/光子组合传输，其中光子由中子相互作用产生、中子/光子/电子、光子/电子或电子/光子。 
虽然术语可能会令人困惑，但假设对蒙特卡罗方法知之甚少，也没有 MCNP 的经验，朱迪思·f·布里斯米斯特认为，当然，通过实践，即使是新手也可以掌握概念(布里斯米斯特，2000)。 
在金融领域的应用 
MC 方法已被证明是现代金融中一种有价值且灵活的计算工具。重点关注资产定价:基本证券及其基础状态变量通常被建模为连续时间随机过程。蒙特卡洛方法被用于评估以预期表示的证券价格只是一个时间问题(Boyle，Broadie，& Glasserman，1997)。由于 MC 方法在处理日益复杂的金融工具方面的灵活性，我们可以清楚地看到 MC 方法在衍生产品和其他金融产品定价中的必要性。本文的第三部分提供了一个在金融中使用蒙特卡罗方法的详细例子。 
重要性抽样 
重要抽样是选择一个好的分布来模拟随机变量(Anderson E. C .，1999)。从直觉上讲，我们必须从重要区域获取样本，以便获得准确的结果。这是通过对样品中的重要区域进行加权来实现的；因此得名重要性抽样。与其名称的重要性相反，抽样本身并不是抽样，而是一种近似方法。现在让我们以直观的方式探索重要性抽样: 
假设我们希望对某一因素进行分析，但没有相关数据可供我们进行分析，或者我们现有的数据不能提供足够的结果。然后，我们生成一个符合以下特性的随机样本: 
设 g(x)为原始样本分布(如果存在)，h(x)为建议样本分布。 
h(x)应该接近与|g(x)| 成正比从 h(x) 中模拟数值应该很容易对于可能实现的任何值 x，计算密度 h(x)应该是容易的。 
遵守这些要求可能很难需要足够的时间投入，但在处理上述两个问题时证明是有效的。我选择在这个阶段包括重要性抽样，因为它对以后的讨论更有启发性，尤其是关于蒙特卡罗改进技术。我们会发现重要抽样和马尔可夫链之间有着重要的关系。 
我们已经将蒙特卡洛定义为一种广泛使用的近似技术，并了解了它有趣而又有些悸动的历史(悸动是因为这个名字的本质是与炸弹构造联系在一起的)。从冯·诺依曼使用 ENIAC 到现代计算机使用，MC 在数据/过程的模拟和操作中发挥了重要作用，这对我们的生活方式以及如何与世界互动产生了巨大影响。 
在本文的下一部分，我们将更深入地探讨 MC 中固有的其他组件，并讨论 Metropolis-Hasting 算法作为蒙特卡罗马尔可夫链过程的一个特例。 
马氏链、中心极限定理和 Metropolis-Hastings 
在文章的前一部分，我给出了蒙特卡罗的一般概述，并介绍了重要性抽样。现在，我们通过对一些被广泛使用但却被误解或由于其重要性而被普遍忽视的概念给出严格的定义来进行更深入的探讨。此后，我们探讨了 Metropolis-Hastings 算法，它构成了许多马尔可夫链蒙特卡罗方法的基础。 
马尔可夫链蒙特卡罗 
在普通蒙特卡罗之后不久，洛斯·阿拉莫斯发明了马尔可夫链，他使用计算来获得这种模拟。这些链通常是从各种算法中获得的，其中 Metropolis 算法是最早期的，也是最重要的，因为它为 Metropolis-Hasting 算法等更复杂的算法铺平了道路。由于蒙特卡洛积分从要求的分布中抽取样本，并形成样本平均值以逼近期望值，因此马尔可夫链蒙特卡洛通过长时间运行巧妙构建的马尔可夫链来抽取这些样本(吉尔克斯、理查森、&【施皮格尔哈特】 
让我们在此给出一些重要术语和概念的严格定义(一些定义摘自 Geyer C . J:Markov Chain Monte Carlo 简介): 
定义 
马尔可夫链: 一个序列 X 1 ， X 2 ， X 3 ，…某集合的随机元素的条件分布若为*Xn+1*给定 X 从这个定义中，我们看到，为了计算下一个随机变量，我们只需要当前的信息，而不需要我们当前所处的位置，从而大大减少了花费在寻找和计算时间上的时间。 
可逆性: 一个转移概率分布相对于初始分布是可逆的，如果，马尔可夫链的 X 1 ， X 2 ， X 3 ，… 对的分布是可交换的。可逆性简化了中心极限定理(CLT)的应用，因为当这个性质成立时，CLT 的条件要简单得多。 
平稳性: 一个随机过程(x t : t 在参数空间的一个元素中)是平稳的如果所有的 X t 具有相同的均值、方差和自相关 
中心极限定理: 为平稳的、不可约的、可逆的马氏链和 和 以及方差σ 与σ 有限则 其中 为样本估计。我们可以进一步用一种通用的方式解释如下:对于任意随机变量序列 X1、 X2、 X3、… 用 X1、 X2、 X3 值得注意的是，各种形式的 CLT 的存在都有其特定的要求 
蒙特卡罗估计和中心极限定理 
为了使用 CLT 来估计蒙特卡罗误差(这里没有讨论，但同样重要)，我们需要方差的一致估计，或者至少是渐近分布已知的方差估计(Geyer，1992)。已经提出了许多方差估计方法，大多数来自时间序列文献，并且适用于任意平稳随机过程，而不仅仅适用于马尔可夫链(Geyer，1992)。我们现在来看一个标准时间序列的方法。 
非重叠批量是指 
在非重叠批次均值下(批次:为 n 项马尔可夫链保守迭代的子序列 X 1 ， X 2 ，…。 X T 同样，中心极限定理将适用于每一批。 
批次平均值由给出 
 
这个方差将用于计算我们的中心极限定理。 
Metropolis-Hasting 算法 
Metropolis-Hastings 算法由以下步骤给出 x【t】 
1 生成 
2 乘 
我们称 p(x，y) 为接受概率。通过执行这个过程，我们得到所谓的大都市加速马尔可夫链 X 1 ， X 2 ，…。*XT*同 X T 大致按(x) 分布。(f(x) 是我们可能认为我们理想的样本分布)对于大 T 。(Kroese，Taimre，& Botev，2011 年)。 
Metropolis-Hasting 背后的主要目的是模拟一个马尔可夫链，使得这个链的平稳分布与目标分布一致。 
尽管 MC 方法通常比传统数值方法更有效，但其实施可能非常困难，且在时间和分析上计算昂贵。黑斯廷斯提出了一个源于 Metropolis 等人(1953)的广义方法。这种采样方法的主要特点是 
计算依赖于 p(x) 仅通过 p(x’)/p(x)形式的比值，其中 x’和 x 是样本点，不需要因式分解 p(x) 也不需要确定归一化常数 
关于 Metropolis-Hastings 有更多的文献，但我们强调的在这一点上已经足够了。理想情况下，我们试图用 Metropolis 这样的算法做的事情是拿出一个尽可能接近现实的样本，从而确保在尽可能现实的设置下测试任何提议的模型。 
到目前为止，我们已经收集了足够的理论知识来构建一个可用于投资的策略。在文章的最后一部分，我们将利用 uptil 现在开发的所有概念来评估它们的重要性。 
金融应用 
在蒙特卡洛领域，存在大量的应用。在这最后一部分，我把所有以前的工作结合在一起，并把我们到目前为止收集的理论付诸实践。 
应用 Metropolis-Hastings 算法 
从上一节中，我们推出了一种方法，通过这种方法，我们可以使用任意因子生成一个样本，这些因子组合起来可以相当准确地反映现实 
为了绝对简单起见，我们将只考虑我们希望建模的一个变量，但该流程适用于资产组合或这些资产的衍生品，只需进行最小的调整。在这里，我们利用动态资产定价来估计均衡，并利用均值回归理论作为我们策略的基础来开发套利机会。我将忽略实际的计算，以便直观地理解这里开发的理论的用法: 
假设资产 A 具有相当大的波动性，我们希望利用这一观察结果。我们拥有的算法使我们能够模拟我们自己的股票运动，并将其与 A 的运动相匹配。假设布朗运动为我们的 q(x) (建议分布)我们应用 Metropolis-Hastings 算法记录所有观察值。 
在这一点上，我们有两种可供选择的方法用于比较: 
备选方案 1:由于我们有一个明确的流程，我们可以根据观察到的趋势直接绘制出这个伪趋势，然后根据价格差异做出交易决策。然后，我们构建一个程序，根据两者之间的关系做出买卖决定。 
当生成的过程高于资产价格时，这将反映买入，因为我们假设价格将缩回到我们的模型中(记住，我们的模型给出了一个平稳分布，这意味着随着时间的推移，价格将收敛到这个稳定状态(Johannes & Polson，2002))对于低于资产价格的生成流程，将执行卖出触发器。 
备选方案 2:为了进一步加强我们的决策，我们可以使用基于生成的样本路径的回归模型。通过这样做，我们甚至能够假设资产 A 运动的未来趋势，同时了解和控制当前的价格波动。同样，在第一模型中应用的交易决策也可以使用回归模型作为基准来应用。 
这是一个过于简化的模型，只给出了一种可能的用途。布朗运动的使用作为我们的 q 被包括在内，这是基于它的特性来模拟金融运动，并在其计算中包括噪声(Morters & Peres，2008)。此外，虽然从理论上讲，我们没有对 q 进行限制，但重要的是要注意，建议密度的选择通常会影响算法的性能。(约翰内斯&波尔森，2002 年，第 26 页) 
障碍选项和重要性抽样 
在描述和使用选项时，我们几乎总是考虑普通选项，并假设这同样适用于外来类型。我会走一条稍微不同的路，考虑一个奇特的选择；准确地说是障碍选项。下面的例子是从 Kroese D，P Taime T 和 Botev Z，I 的《蒙特卡洛方法手册》中摘录的: 
考虑一个下跌买入期权，有一个受监控的障碍，到期时的收益由给出 
 
其中 
用和来表示 k = 1，2，3，…。期权中出现正收益的情况很少，因此期权价格的计算很大程度上取决于这种情况的出现。因此，估计稳健概率是必要的，重要抽样可能会发挥巨大作用。 
此外:为了获得一个好的阳萎抽样密度，我们使用了所谓的交叉熵方法。这其中包括获取一个格式的 pdf 
 
其中 标准正态随机向量ζ的 pdf。可以通过使用被称为“打了就跑”算法的 Metropolis-Hasting 算法的变体来获得所提出的 pdf 的进一步增强。 
结论说明和总结 
Metropolis-Hastings 方法有一个巨大的应用，我们可能不会在单个博客系列中讨论它，尽管它只是在 20 世纪 90 年代才被普遍接受(Hitchocock，2012)。Metropolis-Hastings 算法的各种修改，包括独立采样器和随机漫步采样器，进一步提供更相关的预测。黑斯廷斯看到了位于大都市中心的马尔可夫链的转移矩阵，将他的目标分布呈现为马尔可夫链的π(x)的不变分布(Hitchocock，2012，第 155 页)，如前所述，这一特征重塑了许多学科。 
在一天结束时，所有这些练习的全部目的是想出一个接近真实但在我们控制范围内的样本。考虑到这一点，存在许多旨在利用关于模型的已知信息来获得更准确估计值的方差缩减技术(Kroese & Rubinstein，2008)。我们在第二部分中简要地提到了一种这样的技术，即非重叠批处理方法。其他众所周知的可能提供适度方差减少的技术包括使用控制和/或算术随机变量、分层抽样(Kroese & Rubinstein，2008 年)和最受欢迎的重要性抽样。(克罗泽，塔米姆雷，&博特夫，2011 年) 
Kroese 等人认为重要抽样是最重要的方差缩减技术之一(2011 年，第 362 页),更重要的是它能够找到罕见事件概率的估计值。与本系列中讨论的大多数主题一样，这里只给出了介绍性材料，在学术文献中可以找到大量采样方法的推导。 
对于稳健的结果，包含交叉熵是有益的。交叉熵方法的目的是获得密度，使得该密度和最佳密度(实际)之间的距离尽可能小。加权重要性抽样是与金融市场最相关的一种变化，因为它将更多的权重分配给对过程有重要输入的因素。 
综合运用所有这些方法将会大大提高回报，大幅降低风险，并有助于形成稳健的投资策略。 
参考书目 
安德森(1999)。 蒙特卡罗方法和重要性抽样。博伊尔，p .，布罗迪，m .，&格拉斯曼，P. (1997)。证券定价的蒙特卡罗方法。经济与控制学报，1267-1321。Briesmeister，J. F. (2000 年)。【MCNP 】-一个通用的蒙特卡罗 N 粒子输运代码。埃克哈特(1987 年)。 斯坦乌兰、约翰·冯·诺依曼和蒙特卡洛法。 从 ScienceMadness.org 获得的。盖耶，C. J. (1992)。实用马尔可夫链蒙特卡罗。统计科学第 7 卷第 4 期，473-483 页。吉尔克斯，W. R .，理查森，s .，&施皮格尔哈特，D. J .(未注明)。介绍马尔可夫链蒙特卡罗。希区柯克博士(2012 年)。大都市加速算法的历史。，254-257 页。约翰尼斯，m .，&波尔森，N. (2002)。 MCMC 金融 e .T5】Kroese，D. P .，Tamimre，t .，& Botev，Z. I. (2011 年)。 蒙特卡洛方法手册。 新泽西霍博肯:约翰·威利&父子公司Kroese，D. P .，& Rubinstein，R. Y. (2008 年)。 模拟和蒙特卡洛法。 新泽西:约翰·威利&父子公司d . kro ese，t . taim re，& Botev，Z. I. (2011 年)。蒙特卡罗方法手册。新泽西:约翰·威利&的儿子们。莫特斯，p .，&佩雷斯，Y. (2008 年)。 布朗运动。麦克雷什博士(2004 年)。 蒙特卡洛模拟和金融。罗泰利，F. (2015 年)。 随机过程。 比勒陀利亚。舒斯特，E. F. (1974)。 布冯的针实验。 美国数学协会。许，，张，张，(2010)。均值风险优化和投资组合选择的蒙特卡罗方法。南汉普顿大学。 
免责声明:这篇客座博文中提供的观点、意见和信息仅属于作者个人，并不代表 QuantInsti 的观点、意见和信息。本文中所做的任何陈述或共享的链接的准确性、完整性和有效性都不能得到保证。我们对任何错误、遗漏或陈述不承担任何责任。与侵犯知识产权相关的任何责任由他们承担。 
投资组合管理简介 
 
 原文：https://blog.quantinsti.com/introduction-portfolio-management/ 
 
马里奥·皮萨·培尼亚 
对投资组合管理的权威解读，这篇博客是一篇介绍性文章，也是你的指南。它解释了你需要了解的关于投资组合管理的所有知识，比如技术、类型、衍生品等等。 
以下是我们在这篇文章中所涵盖的内容: 
什么是投资组合管理？投资组合管理的定义什么是投资组合管理技术？投资组合管理的类型投资组合管理中的资产类别衍生品和投资组合管理投资组合管理的传统方法衡量投资组合回报的工具投资组合的回报率差异结论 
 
什么是投资组合管理？ 
投资组合管理是一个非常通用的术语，用来指经理管理资产组合的风格。 
在金融市场上，有许多资产可供选择，如股票和公司债券、国库券、商品、货币、指数、期权、房地产投资信托基金等等。因此，有必要应用管理技术来管理可以界定一些关键方面(如风险和预期回报)的资产组合。 
不幸的是，我们不可能获得零风险的无限回报，因为一个依赖于另一个，如果我们想增加投资组合的回报，通常会以承担更多风险为代价。 
 
项目组合管理的定义 
根据定义， 
 
 有效的投资组合被定义为在给定回报下风险最小的投资组合，或者说，在给定风险水平下回报最高的投资组合。 
 
仅在纽约证券交易所，就有超过 2800 家上市公司，而在美国的衍生品市场，芝加哥商业交易所，也有数千种合约可供选择。如果我们也考虑国际市场，选择的数量实在太多了。 
因此，无论我们管理的是 10 万美元的资本还是数十亿美元的资本，拥有有助于选择适当工具的工具都是很方便的，或者说，建立一个允许包含指定风险并使回报最大化的投资组合也是一样。也就是说，使用投资组合管理技术。 
 
什么是投资组合管理技术？ 
投资组合管理技术允许我们，至少不承担不必要的风险，来限制每一项投资和整个投资组合的风险。它有助于尽可能提高绩效，并能够分析和比较使用相同管理风格的其他投资组合。它们也允许我们对未来做一个假设，并回答“如果”这个问题。 
虽然投资组合管理不是一门精确的科学，因为它处理的是未来的不确定性，但它允许我们有一个适当的框架来做出投资决策。 
 
投资组合管理的类型 
因此，我们认为有必要对管理风格进行分类，例如，可以分为以下几组: 
 被动管理投资组合:通常，被动投资组合的经理认为，实际上不可能获得比市场本身更高的业绩，因此他们只限于跟踪市场。**主动管理投资组合:**这些投资组合经理认为，如果有可能获得比市场更高的回报，因此通过更积极的投资组合管理，理论上他们能够获得 Alpha (Alpha 是相对于市场的超额回报)。出于比较的目的，了解投资组合的贝塔系数也是很有意思的，因为这表明了它与市场的偏差。激进管理型投资组合:他们是有能力发现和利用市场无效性的投资组合经理，他们带着市场偏见进行投机。
还有必要将不同类型的资产分为可管理的类别，为此我们已经提到了一些可能的资产类别，但它们也可以按风险、资本化、国家、部门、价值或增长等进行分类。 
面对如此多样的风格和资产，投资组合管理技术使我们能够标准化管理技术，并对业绩和风险进行充分的衡量，从而为每种类型的投资者创建不同的投资组合。 
既然任何投资组合都是由资产组成的，那么让我们试着定义一些我们能在投资组合中找到的最常见的资产。 
 
投资组合管理中的资产类别 
投资组合的典型组成部分包括: 
股票或权益结合现金 
股票或权益:这些资产被认为是高风险的，因此提供了获得高回报的机会。它们还可以按国家、部门和/或资产价值或增长细分，前者往往提供股息，而后者往往不提供股息，但回报通常高于市场。此外，他们在股东大会上提供权利，因为事实上，股东拥有公司。(有时 A 类和 B 类股票也有例外，后者不提供投票权。) 
**债券:**这些资产被认为是低风险的，因此提供适中的回报，好处是从一开始，投资回报就是已知的。这些资产是借给政府和公司的贷款，到期日和固定回报或一定频率的息票支付是预先确定的。它们还可以细分为公司或政府债券、到期日、评级等。 
**现金:**我们必须预见到我们投资组合的资金，因为它不会总是 100%投资，有时现金可以用于在货币市场上进行操作，风险可以从很低到很高，这取决于所选择的货币。 
 
衍生品和投资组合管理 
另一方面，有些衍生产品，顾名思义，其价值(以及回报)来自基础产品。 
一些最著名的衍生品有: 
期货选择 
期货 
期货是双方之间的合约，一方在未来某个交割日期买入/卖出资产，另一方卖出/买入该资产。该合同由清算所担保。 
到期时，双方都有义务，一方交付货物，不管当前的市场价格如何，另一方有义务接收货物，不管当前的市场价格如何，因为价格是在交易时确定的。 
尽管需要明确的是，交易商通常不会在到期日之前保持合约未平仓，因为他们通常持有投机头寸。 
期货价格来源于一种基础资产，如股票指数、商品等。 
在这里我们可以发现: 
指数(道琼斯、标准普尔 500、纳斯达克、罗素 2000 等。)贵金属(金、铂等。)工业金属(铜、铅等。)能源(石油、天然气、乙醇等。)农业(大豆、豆类、玉米、小麦等。)软(肉、活牛、橙汁、糖、棉花等。) 
请注意，你也可以通过 ETF 投资这些产品，就像投资股票一样，尽管 ETF 值得单独研究。 
选项 
期权是一种工具，其中一方购买在某一日期以固定价格购买/出售资产的权利，另一方出售该权利，这种操作由清算所保证。期权价格来源于基础资产，如股票、股票指数、商品等。 最后，还有其他不太常见的投资工具，但根据所设计的投资组合，这些工具可能对多样化有吸引力 
房地产投资信托基金，OTC(场外)操作，如 
认股权证，风险投资公司，以及可以为特定客户创建的特定产品等。
如你所想，这个列表既不完整也不详尽，但是它包含了投资组合管理中最流行的工具。 
我们还看到，为了帮助您选择最适合您的管理策略的工具，可以用不同的方式对工具进行分类。 
除了种类繁多的可用金融工具外，在决定采用哪种工具时，还必须考虑其他因素，如波动性、流动性、信息可用性和交易成本。 
有了所有这些要素，我们显然需要一个框架来促进投资组合管理，帮助我们选择特定时刻的最佳资产，界定要支持的风险水平，以及建立回报目标。 
从行业的角度来看，同样重要的是建立标准指标，允许将投资组合的表现与行业的其他表现进行比较。 
 
投资组合管理的传统方法 
投资组合由资产组成，投资组合经理负责决定将哪些资产纳入投资组合。为此，有两种传统的方法 
自下而上的方法自顶向下分析法 
自下而上的方法 
这种方法根据分析师定义的标准进行资产选择，如市盈率、相对优势、行业等等。这种技术通常被称为选股。 
自上而下的方法 
在自上而下的方法中，分析师将选择行动的任务划分到过滤器中，从数量上减少了可包含在投资组合中的选项的数量。例如，可以通过分析来决定我们要投资的市场或国家，然后决定投资的行业，最后，根据某种标准(例如相对实力)来挑选股票。 
 
衡量投资组合回报的工具 
其他简单而强大的工具用于衡量回报，在这些工具中，我们可以找到: 
 **算术回报:**资产的确切回报可以通过“t”时间的价格减去“t-1”时间的价格加上股息(如果有的话)除以“t-1”时间的价格来衡量。**对数回报:**资产的回报可以用“ln”来衡量，即时间“t”的价格加上股息(如果有)除以“时间-1”的价格。这个度量是聚合的，允许我们简单地通过添加简单的句点来计算任何返回长度。**几何收益率:**或复合几何收益率，计算投资在整个期间的实际增长率。**投资组合回报:**这里有几种方法，因为投资组合可以有简单或复杂的管理，组成投资组合的资产可以提供股息、股份支付、分割、资本流动、杠杆等。尽管我们可以用算术回报来计算给定时期的投资组合表现。我们将取 t 时刻投资组合的初始值，减去 t-1 时刻投资组合的价值，加上股息(如果有)，再除以 t-1 时刻投资组合的价值。**相对回报:**了解一个投资组合的表现本身就是一个指标，它只提供了一个关于经理创造回报能力的数据，但如果我们把它与同类的其他回报放在一起看，这个数据就变得更加相关了。例如，如果我们的投资组合由纽约证券交易所的股票组成，我们可以将它的表现与标准普尔 500 指数进行比较，或者我们可以将它与拥有类似特征的投资组合和风格的其他经理的表现进行比较。
 
投资组合的回报率差异 
最后，衡量资产风险的最简单的工具是回报率的方差，它让我们了解相对于平均值的离差，但当然，投资组合中的资产会受到其他需要量化的风险的影响。Markowitz 是最早对风险及其特征进行研究的人之一，他发展了现代投资组合理论，并证明了多样化的好处，尽管他假设收益遵循正态分布，正如我们将看到的，这不是市场的真实行为。 
这项研究是我们将在这一系列文章中看到的其他模型的基础，如资本资产定价管理(CAPM) 。 
 
结论 
在关于投资组合管理的文章系列中，我们将探讨一些构建投资组合的理论和策略，如资本资产定价管理(CAPM)、 Fama-French 三因素模型或多策略投资组合。 
如果我们没有控制投资组合的风险/收益，就不会有管理，我们将在本系列中看到关于风险/收益分析的帖子。 
当我们建立投资组合并进行风险/收益分析时，我们可以使用优化技术来帮助我们最大限度地提高性能，如蒙特卡洛分析或 ML 技术。最后，我们将看看 ML 和日内风险投资组合管理技术。 
在这门关于量化投资组合管理的课程中，你可以学习如何使用各种量化技术构建投资组合和管理风险。 
参考文献 
投资组合理论和业绩分析拉尔夫·文斯的投资组合数学手册使用资本资产定价和 Fama-French 三因素模型的现代投资组合管理- EPAT 项目计算协方差矩阵和投资组合方差投资组合分析:计算风险和回报使用蒙特卡洛模拟的投资组合优化投资组合分析:绩效测量和评估多策略投资组合:有效结合量化策略 
*免责声明:股票市场的所有投资和交易都涉及风险。在金融市场进行交易的任何决定，包括股票或期权或其他金融工具的交易，都是个人决定，只能在彻底研究后做出，包括个人风险和财务评估以及在您认为必要的范围内寻求专业帮助。本文提到的交易策略或相关信息仅供参考。*T3】 
价格行动交易简介[网上研讨会] 
 
 原文：https://blog.quantinsti.com/introduction-price-action-trading-webinar-18-october-2022/ 
 
https://www.youtube.com/embed/j0xjch0L9aw?rel=0 
 
演示幻灯片 
//www.slideshare.net/slideshow/embed_code/key/yk1KuulbLcorgQ 
Price Action Trading - An Introduction from QuantInsti 
 
关于会议 
这节课通过理解价格行为，向你介绍不使用技术指标交易的技巧。它涵盖了几个重要的价格行为交易工具，如供求分析，模式，支点等。 
价格行为交易的需要价格行为交易的基础价格行为交易工具回溯测试和评估价格行为交易策略价格行为交易自动化互动问答 
 
扬声器 
Varun Kumar pot hula(QuantInsti 的量化分析师) 
 
Varun 拥有金融工程硕士学位。他拥有交易员、全球宏观分析师和 algo 交易策略师的工作经验。目前，作为一名量化分析师，他在 QuantInsti 的内容和研究团队工作，他的贡献有助于为算法和量化交易领域的学习者提供服务。 
 
本次活动于: 2022 年 10 月 18 日星期二 东部时间上午 9:30 | IST 时间晚上 7:00 |新加坡时间晚上 9:30 
金融学中贝叶斯统计的简明介绍 
 
 原文：https://blog.quantinsti.com/introduction-to-bayesian-statistics-in-finance/ 
 
由 Vivek Krishnamoorthy 
This post is intended to be the first of a multi-part series on Bayesian statistics and methods in quantitative finance. My write up here is a short introductory post that went rogue. 
动机 
I must confess that when I first encountered the Bayesian approach to inferential statistics in my formal education, it was tough going. The material I read and the teachers who taught me were excellent but it took months before I could fully appreciate them. I spent countless hours trawling through the internet (blogs, websites, online forums, etc.) and other traditional sources (books & academic journals) trying to build a fundamental understanding of this subject. I managed to get there eventually. My aim here is primarily, to help you, the reader, learn from my missteps. This entire series attempts to provide an intuitive feel for Bayesian statistics and its applications without getting too caught up in the scary math. We are dealing with an exciting and challenging subject so I must warn you that it will demand of you both effortful thinking and hard work. My secondary aim is to stave off (at least for a little while) the question asked of me by colleagues, friends and family - “When will you be done writing?” If these posts pique your curiosity to explore further, a lot of good stuff is available both in print and on the web with varying levels of complexity. I will point you to some of these resources in the coming posts. 
谁应该读这篇文章？ 
The main prerequisite for understanding these notes is some exposure to probability theory and statistics. I seek to introduce Bayesian statistics in an accessible way to readers who have some conversance with classical statistical analysis (which is mostly any kind of inferential analysis you may have studied so far unless stated otherwise). I have tried to find a middle ground between scientific rigour where theorems are proved and a purely empirical approach dictated by observations and analysis minus any theorems. I hope that readers are comfortable with some of the concepts I have listed below. If a number of them are unfamiliar to you, I would recommend you acquaint yourself with some of them to profit from the time and effort you invest here. This glossary can give you a once-over if you are looking for something quick and dirty. An overview of these ideas should suffice; you do not need exceptional mathematical sophistication. The ideas will get clearer as you work with them here and elsewhere. 
 
 连续和离散随机变量；分配；期望值；方差:概率分布函数(PDF)；概率质量函数(PMF)；最大似然估计；贝叶斯定理；条件概率；边际概率；伯努利、二项式、正态、高斯、伽玛、贝塔、学生 t 分布；人口；样本；样本均值；样本方差；人口平均数；人口方差；协方差和相关性。 
 
I would urge you to pick up a pencil and paper to work out the derivations or exercises that crop up through this series. I sometimes don’t spell out every step of the way. So it will help you both, fill in any missing details, and sharpen your own understanding of the topic. 
本帖意图 
In this post, I set the stage for our grand endeavour by providing a gentle introduction to Bayesian statistics, a branch of statistical analysis founded on Bayes’ Theorem. I contextualize it by first covering some ground on the two main schools of thought in statistical analysis viz. the frequentist and the Bayesian. I then proceed to establish how the differences between them impact their respective philosophical styles. Once we do this, I pick an example to get us comfortable with the Bayesian approach to probabilistic problems. I conclude my article with stating Bayes’ Theorem and display its formal use with another example. My presentation of example, theorem, example is by design. This should hopefully help us make the connection between the illustrations and the underlying principles they embody. Let’s get started. 
布置背景 
Imagine a scenario where a data scientist or an economic researcher has collected data about a phenomenon that she is studying. This data may be collected by observing a number of subjects at a certain point in time (cross-sectional data) or by observing a subject over a number of time periods (time-series data). It could also be a combination of cross-sectional and time-series observations i.e. observing a number of subjects tracked over multiple time periods (panel data). In econometric studies, these subjects are usually individuals, firms, regions, or countries and the time-periods are yearly, quarterly, daily or higher frequencies. In quantitative finance, we typically track the movements of different asset classes through time. A short excursion: The data that econometricians (I use this as a motley term to include quantitative analysts, financial economists, empirical economic/financial researchers and even some data scientists) work with are almost always observational. This is fundamentally different from the data generated via controlled experiments in many of the pure and applied sciences (like medicine, physics, engineering, etc.). Coming back to the main story, a key choice that our researcher would need to make is the approach to statistical inference i.e. using frequentist statistics or Bayesian statistics. This is an important choice point and a good place to tee off on our journey. I now proceed to compare and contrast classical (frequentist) statistics and Bayesian statistics in drawing inferences. 
哲学背景 
Statistical analysis and the subsequent inferences we draw from it are based on probability theory. The way in which probability is defined and interpreted has created two schools of statistical thought, viz. frequentist statistics and Bayesian statistics. The frequentist worldview (also called the classical or traditional approach) refers to the philosophical approach of Ronald Fisher. It views the probability of an event as the long run frequency of occurrence of that event (hence the name). We would, therefore, measure the probability of that event as the frequency at which it occurs after repeating the experiment ad infinitum. However, this is not always possible in practice. For instance, if we want to compute what the probability of a global recession occurring in the coming year is, we do not have a large sample of data available since we only have reliable economic data for about a century or so. In cases like these, frequentists use theoretical results and techniques to arrive at the probability of occurrence. I’ll elaborate a little more on this as we move along. So in summary, for frequentists, the probability is inextricably tied to the long run frequency of occurrence of events. The Bayesian (named after its discoverer Thomas Bayes) worldview of probability is more visceral. It interprets probability as a subjective opinion i.e. it is a measure of belief or plausibility that we have of an event occurring. We update our opinions (as measured by probability) as and when we receive more information. Simply put, for Bayesian decision makers, the probability is a statement of an individual subjective opinion. While I do highlight the subjective nature quality of our definition here, I must hasten to add that the axioms of probability still need to be satisfied. This philosophy of quantifying our beliefs or opinions as a probability comes quite naturally to us. One of the basic principles of learning is to assimilate the information that arrives from the external environment and update our extant knowledge (what we casually refer to as common sense) with this newly acquired information. This is the kernel of the Bayesian worldview which animates the Bayesian statistics enterprise. This is also how we operate in the real world where we form beliefs (and by extension, assign probabilities) based on what we know. Let’s say we assign a certain probability to a particular candidate winning the elections six months from now. As time passes by, we would continually update our beliefs or opinions (as measured by the probability of the candidate winning) based on news reports, opinion polls, etc. so as to reflect the changing realities. The divide between frequentists and Bayesians is fundamentally one of philosophy which I show has wider ramifications in their differing approaches to statistical analysis. 
频率主义者 v/s 的主观概率 
One of the enduring controversies in probability theory is about the type of events where probabilities (in the frequentist sense) can be defined. I had briefly alluded to it earlier in a case where the experiment could not be performed repeatedly. As per the frequentist definition, the only situations where probabilities hold any meaning are those where we examine the relative frequency of occurrences of an event as the number of observations tends to infinity i.e.  where k is the number of occurrences of the event and n is the number of repetitions of the experiment. This method of assigning probabilities creates two issues. First, even in cases where the experiment is recurrent, it requires us to conduct the experiment an infinite number of times which is impossible. The second is a more serious issue. We are unable to assign probabilities to events which are not the outcomes of repeated experiments. This would not please the Bayesians among us who take a more subjective view of probability. They view probability as a reflection of their uncertainty about the state of the world. The way they see it, probability and uncertainty are tautological. A Bayesian decision maker would assign probabilities to the outcomes of repeated experiments and also to statements about the winner of the next national election (i.e. the outcome of a non-recurrent experiment). I linger on these differences and flesh it out a little more in the following section. 
细节决定成败 
The frequentist approach has a different take on uncertainty. In this world, uncertainty stems only from the randomness that is implicit in the realizations of any experiment or phenomenon. In other words, the data generated would be random or uncertain, however, the underlying phenomenon studied is fixed but unknown. In contrast, the Bayesian researcher notes from her first principles, an inherent uncertainty in the phenomenon being studied. She expresses this doubt before commencing her study based on her existing knowledge and calls it the prior probability. Once she completes her study of the phenomenon, she incorporates this knowledge (this is data in statistics-speak) to update her own subjective beliefs and calls it the posterior probability. I now depict an illustration to get a flavour of the Bayesian way of thinking. Once we are familiar with this, I conclude our learning for this post with a postulation of Bayes’ Theorem alongside an application in inferential statistics. 
例 1 
Consider a scenario as shown below: 
一家手机制造商有三家不同的工厂生产手机。50%的手机来自工厂 1，30%来自工厂 2，其余来自工厂 3。工厂 1 的手机有 10%有缺陷，同样，工厂 2 的手机有 20%有缺陷，工厂 3 的手机有 25%有缺陷。 
Suppose we find a defective phone and want to know the likelihood that it came from plant 1. One way to go about solving this is to initially assume there are a total of 100 phones. We can then make the following statements based on what we know. 
50 部手机将来自工厂 1；其中 10%是次品，所以我们算出有 5 个是次品；∴剩下的 45 个将是无缺陷的。同样，30 部手机来自工厂 2；6 将是有缺陷的，24 将是无缺陷的。并且，20 部手机来自工厂 3；5 会有缺陷， 15 会无缺陷。 
We can see that a total of 16 phones are defective out of which 5 are from plant 1. Let us tabulate our results for clarity. 
植物生产的手机数量缺陷手机数量无缺陷手机的数量oneFiftyfiveForty-fiveTwoThirtysixTwenty-fourthreeTwentyfiveFifteen 
∴∴ the likelihood of a defective phone being manufactured in plant 1 is 5/16. 
定理 
I now use the following notations to frame our problem statement using Bayes’ Theorem. Let AA be the event that the phone is defective, Bi be the event that the phone was manufactured in plant i, and P(⋅) denote the probability. I also use the conventional notation for conditional probability, P(⋅|⋅). Without loss of generality, Bayes’ Theorem for our example can be algebraically expressed as: Our instinctive approach earlier is consistent with the application of Bayes’ theorem as seen above. What makes this theorem so handy is it allows us to invert a conditional probability. I now employ a simple illustration to demonstrate how Bayes’ theorem can be used in an inferential setting. 
例 2 
Let us "
PaperSpace 博客中文翻译（二十二）,2024-11-07,"原文：PaperSpace Blog

协议：CC BY-NC-SA 4.0

如何训练缩放的 YOLOv4 对象检测模型

原文：https://blog.paperspace.com/how-to-train-scaled-yolov4-object-detection/

随着 Scaled-YOLOv4 的发布，物体检测技术向前迈进了一步——这是一种用于物体检测的最新机器学习模型。

在这篇博文中，我们将看看在创建缩放的 YOLOv4 模型中涉及的突破，然后我们将通过一个示例来研究如何在自定义数据集上概括和训练该模型，以检测自定义对象。

Detecting lifts and jet skis from above via drone using Scaled-YOLOv4. Training data: public Aerial Maritime dataset.

我们在本教程中包含了以下资源:

缩放后的 YOLOv4 分解
缩放后的 YOLOv4 回购
公共航空海事数据集
缩放的 YOLOv4 自定义培训代码(如下)

在本教程中，我们将利用 Roboflow 进行计算机视觉数据管理，利用 Paperspace 进行 GPU 计算资源。

为什么选择 Scaled-YOLOv4？

基于微软 COCO 基准测试的，缩放后的 YOLOv4 现在是对象检测的最佳模型。

Scaled-YOLOv4 achieves record breaking performance on the COCO benchmark. Not pictured YOLOv4-tiny running at 1774 FPS on the RTX 2080ti (source)

在 Roboflow，我们发现，在推理速度和网络准确性的权衡连续体中，缩放后的 YOLOv4 系列模型在效率和所有其他现有的对象检测网络(由 T2 平均精度衡量)中名列前茅。

如果你想了解为什么 Scaled-YOLOv4 这么好，请查看我们在 Roboflow 博客上写的 Scaled-YOLOv4 分解。我们还建议查看一下缩放的 YOLOv4 论文，以探索原作者提出的基准。

一旦您阅读了支持技术，让我们开始培训吧！

组装自定义对象检测数据

为了监督我们定制的缩放 YOLOv4 对象检测器，我们需要收集对象检测训练数据。如果您想直接学习本教程，您可以使用数据集页面右上角的Fork按钮来分叉公共航空海事数据集:

To fork the public aerial maritime dataset, use the Fork Dataset feature in Roboflow

收集您自己的图像

或者，如果您想使用自己的图像，我们建议您收集能够代表您的模型在部署时将面临的条件的图像。你可以从一小批图像开始，以评估可行性，然后扩大规模——但一般来说，图像越多样化，最终结果越好。

标注您的数据

在本教程中，我们将使用公共航空海事数据集中的影像。一旦你有了你想用来训练你的模型的图像，就该给它们贴标签了。

您现在可以直接在 Roboflow 中标记您的数据，如下所示:

Uploading data Roboflow after creating a new dataset

Labeling data in Roboflow

要给图像加标签，需要在要检测的对象周围绘制边界框。这里有一些关于为这种计算机视觉应用标记图像的提示:

标记每幅图像中的每个感兴趣的对象
标记一个物体的整体
标记遮挡的对象
创建紧密的边界框
创建特定的标签名称
保持清晰的标签说明
使用标签工具，如 CVAT 、标签、矩形标签和机器人流程

要在 Roboflow 中手动标记数据，首先需要上传原始图像并创建一个新的数据集。在新数据集页面中，开始标注所需的全部操作就是单击图像并绘制边界框。关于注释的更多细节，请查看标签文档。

将数据导出到图纸空间

一旦您对标注的数据集感到满意，您就可以继续在 Roboflow 中生成数据集版本。为此，从 Roboflow 的数据集视图中选择Download。您还可以选择任何您喜欢的预处理和增强选项。

Select Download to generate a dataset

You should end up with a curl link to the dataset

选择Download后，选择TXT > Scaled-YOLOv4作为输出格式，然后选择Get Link获得一个curl链接到您的数据。

请保留此链接，因为您将使用它在一分钟内将您的数据集导入到您的 Paperspace 笔记本中。

Choose the Scaled-YOLOv4 dataset format

Visualizing our training data within Scaled YOLOv4

在 Paperspace 上设置我们的培训环境

为了训练我们的缩放 YOLOv4 模型，我们首先需要提供 GPU 资源来运行我们的训练作业。

由于在家庭中使用更大的网络时，Scaled-YOLOv4 培训需求会大幅增加，因此 Paperspace 是一个自然的起点，因为有各种各样的按需 GPU 支持的实例可用。

当然，您可以使用任何可用的 GPU 资源，并且仍然遵循本教程。

创建托管集群

首先，您需要创建一个 Paperspace 帐户。

登录到 Paperspace 后，导航到Gradient和Clusters，然后选择Create a Managed Cluster。

Navigate to the Clusters tab in Paperspace Gradient and select Create a Managed Cluster

您将看到新的私有集群处于Provisioning状态，集群将需要几分钟的时间进行配置，因此请耐心等待。

It will take a few minutes for the new cluster to provision

创建笔记本

配置好集群后，您可以在Notebooks选项卡中启动笔记本进行培训。

Select Create Notebook to initialize a new notebook in Paperspace

Name your notebook and select an auto-shutdown interval

Select an instance from your new private cluster

在图纸空间的笔记本设置提示中:

在步骤 1 中，给你的笔记本起一个名字，例如 YOLOv4 教程。您可能还想将auto-shutdown设置为 1 小时，以防万一您稍后忘记关机时过度充电。
在步骤 2 中，我们可以将容器选择留空。我们将在步骤 4 中指定一个自定义容器。
在第 3 步中，选择My Private Clusters，然后选择带有您想要训练的 GPU 的私有集群。更大的 GPU 将更快地训练你的模型——我们在本教程中使用了 P4000。
在步骤 4 中，打开Advanced Options并将下面的 NVIDIA PyTorch 容器复制到Container Name字段中:
nvcr.io/nvidia/pytorch:20.06-py3

1

Paste this image location in Container Name field under Advanced Options

高级选项应该是这样的:

Enter the image location into the Container Name field

点击Create Notebook，Paperspace 将在您的新私有集群上启动您的培训环境。这个过程也可能需要几分钟。

安装缩放的 YOLOv4 依赖项

现在是时候打开朱庇特笔记本了。

Installing a few dependencies in the new notebook

一旦我们进入笔记本电脑，我们需要在准备培训之前进行一些安装。

安装依赖项

我们克隆了缩放后的 YOLOv4 回购，并切换到yolov4-large分支。

#clone Scaled_YOLOv4
!git clone https://github.com/WongKinYiu/ScaledYOLOv4.git# clone repo
%cd ./ScaledYOLOv4/
#checkout the yolov4-large branch
!git checkout yolov4-large
%cd ..

1
2
3
4
5
6

接下来，我们将为我们的 GPU 安装mish-cuda，这样我们就可以在笔记本的 GPU 上快速运行 mish 激活功能。

!git clone https://github.com/JunnYu/mish-cuda
%cd mish-cuda
!python setup.py build install
%cd ..

1
2
3
4
下载数据

最后，从 Roboflow 导入您的curl链接，以正确的格式导入您的数据。这应该从ScaledYOLOv4 repo 文件夹向上一个目录执行。

!curl -L ""https://app.roboflow.com/ds/h91wwIw5An?key=[YOUR KEY HERE]"" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip

1

Downloading data into the notebook

开始规模化 YOLOv4 培训

现在我们已经设置好了一切，我们只需要调用一个命令来开始对我们的定制数据进行训练。

cd ./ScaledYOLOv4/
!python train.py --img 416 --batch 16 --epochs 50 --data '../data.yaml' --cfg ./models/yolov4-csp.yaml --weights '' --name yolov4-csp-results--cache

1
2

以下选项是可能的:

- img: define input image size
- batch: determine batch size
- epochs: define the number of training epochs. (Note: often, 3000+ are common here!)
- data: set the path to our yaml file
- cfg: specify our model configuration
- weights: specify a custom path to weights.
- name: result names
- nosave: only save the final checkpoint
- cache: cache images for faster training

1
2
3
4
5
6
7
8
9

一旦训练开始，你要观察 图(平均精度)度量上升，如果它稳定下来你就可以停止脚本。

训练结束后，你可以看看你的 Tensorboard 指标，再次关注地图:

# Start tensorboard
# Launch after you have started training
# logs save in the folder ""runs""
%load_ext tensorboard
%tensorboard --logdir runs

1
2
3
4
5

Scaled YOLOv4 Tensorboard

按比例放大

如果你想使用更大版本的网络，在训练中切换 cfg 参数。在models文件夹中，你会看到各种型号配置选项，包括yolov4-p5、yolov4-p6和著名的yolov4-p7。为了训练这些较大的模型，单个 GPU 可能不适合您，您可能需要启动一个多 GPU 服务器，并通过分布式启动在多 GPU 上进行训练:

python -m torch.distributed.launch --nproc_per_node 4 train.py --batch-size 64 --img 896 896 --data coco.yaml --cfg yolov4-p5.yaml --weights '' --sync-bn --device 0,1,2,3 --name yolov4-p5 

1
使用缩放的 YOLOv4 模型进行推理

现在您已经训练了缩放的 YOLOv4 模型，您可以利用您的模型对新图像进行推断。为此，我们将模型指向数据集的测试集，并将检测脚本指向我们的自定义权重(您也可以在此处指定视频):

!python detect.py --weights ./runs/exp0_yolov4-csp-results/weights/best.pt --img 416 --conf 0.4 --source ../test/images

1

并且推理发生得很快(特别是在 GPU 上)

/content/ScaledYOLOv4
Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.4, device='', img_size=416, iou_thres=0.5, output='inference/output', save_txt=False, source='../test/images', update=False, view_img=False, weights=['./runs/exp1_yolov4-csp-results/weights/best.pt'])
Using CUDA device0 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB', total_memory=16130MB)

Fusing layers... Model Summary: 235 layers, 5.24921e+07 parameters, 5.04494e+07 gradients
image 1/32 /content/teimg/DJI_0262_JPG.rf.3878c367b5f00b7ce0f5c9bdcb4d8486.jpg: 416x416 Done. (0.020s)
image 2/32 /content/teimg/DJI_0262_JPG.rf.47ce7cf6d8e3e310ab9b2b5c15ebba72.jpg: 416x416 Done. (0.020s)
image 3/32 /content/teimg/DJI_0262_JPG.rf.560b36a2e292c1b3dee7eae7e1f3fbf0.jpg: 416x416 1 docks, Done. (0.021s)
image 4/32 /content/teimg/DJI_0262_JPG.rf.5f24b2ccccf544d3bb0c3cb740be0f4b.jpg: 416x416 1 lifts, Done. (0.021s)
image 5/32 /content/teimg/DJI_0262_JPG.rf.66b031d30a28587d2c06f38af05cb4ec.jpg: 416x416 1 docks, Done. (0.021s)
image 6/32 /content/teimg/DJI_0262_JPG.rf.8c378a23b8822f63a44ad24c8787fab3.jpg: 416x416 1 lifts, Done. (0.025s)

1
2
3
4
5
6
7
8
9
10
11

Inference speed of YOLOv4-CSP on Colab V100, single batch, 50FPS

然后，我们可以可视化我们的网络测试推理。

Test inference on an image the model has never seen.

导出重量和部署

最后，在笔记本的最后，我们可以下载我们定制的模型重量。这些目前在 PyTorch 框架中，您可以使用我们用于培训的相同软件来调用它们。你也可以把这些权重转换成其他框架，比如Tensor RT、ONNX、TorchScript、TensorFlow Saved Graph、TFLite。

这些其他格式的实现将是具有新依赖性的新软件。旅程开始了！

后续步骤

一旦您对在笔记本中训练您的网络有了感觉，您可以考虑在存储库中正式化您的训练和推理流程，并利用 Paperspace 的自动缩放功能来部署您的推理网络。

结论

恭喜你！您已经学习了如何使用 Scaled-YOLOv4 在自定义对象上训练最先进的技术。

在 Paperspace 和 Roboflow，我们总是对您下一步可能要做的事情感到兴奋。

一如既往，快乐训练！

快速行动，深入思考:研究是如何进行的@ Paperspace

原文：https://blog.paperspace.com/how-we-do-research-at-paperspace/

[2021 年 12 月 2 日更新:本文包含关于梯度实验的信息。实验现已被弃用。有关当前梯度资源的更多信息，请参见梯度文档

先进技术组是 Paperspace 的一个以研发为中心的团队，由 ML 的工程师和研究人员组成。作为一个团队，我们有兴趣探索深度学习、数据工程、计算机系统和 UI/UX 方面的高级主题，并致力于构建智能应用程序。如果你对我们的工作感兴趣，考虑申请我们的研究奖学金！

在本帖中，我们将以高级研究工作流程的形式，对高级技术小组(ATG)用于探索各种研究的工具和实践进行概述。我们的许多研究课题都位于深度学习和计算机系统等领域的交叉点。我们倾向于快速行动，处理雄心勃勃的计算密集型实验，由于我们通过 Paperspace 的梯度平台提供了许多非常有用的工具和强大的计算能力，我们可以追求涉及学术界更传统的研究小组有时会回避的主题的研究问题。

在这里，我们概述了研究工作流程的一般进展，我们发现这在我们处理的项目类型中非常有用。我们将讨论我们通常如何从最初的探索阶段前进，在这个阶段我们确定问题的范围并得到一些初步的结果。然后，我们将介绍如何在纸张空间云上扩展我们的实验。最后，我们将介绍我们如何将实验版本化，并跟踪研究议程的内部进展。

跟上 ML 消防水管

在机器学习领域，分享的想法和发表的论文数量之多几乎令人难以理解。要跟上每天冒出来的每一个新想法是非常困难的，其中许多当然是在基础突破上的渐进改进。在 ATG，我们的研究人员带着他们打算追求的特定想法加入团队，通常还有一些如何实现的想法。过去的想法和项目包括 GPU 内核编程、对抗性学习方案和神经架构搜索。

我们一直致力于引入与 ATG 的深度跨区域协作文化，并且经常会有想法转变，以纳入团队中其他感兴趣的成员或 Paperspace 中任何其他人的专业知识。我们也对许多 ML 理论家偏离的主题开放，包括可解释性、设计和人在回路系统。通过午餐和学习讲座、阅读小组会议以及允许任何人与其他人就一个有趣的项目展开对话的普遍开放的文化，可以分享新的想法。我们已经有软件工程师、项目经理和深度学习研究人员兴奋地讨论深度神经网络中模块化和修剪的含义。这是一次很棒的经历。聪明的人、天生的好奇心和高度协作的文化导致了许多不可思议的想法和项目在 Paperspace 形成。

探索一个想法:研究的面包和黄油

对于那些不熟悉研究的人来说，深入研究似乎是一项非常模糊和令人畏惧的任务，尤其是如果你的唯一经历是阅读论文和看到最终结果。实验的现实，尤其是在 ATG，是我们从一些实验结果的小延伸或问题开始的。我们可能会试图复制一篇论文的结果，或者在一个新的领域测试一个想法。自然地，当我们更好地理解作品的含义和基础时，有趣的想法和扩展就会出现。

当一个新的想法作为这个过程的结果开始形成时，我们把它缩小到经验上或理论上可测试的东西。尽可能缩小范围并保持简单是至关重要的，这样就可以清楚、直接地看到所产生的机制或期望的结果。作为一个例子，考虑我们可能想要测试一个新的修剪机制。我们将首先训练一个简单的完全连接的前馈架构，并在那里测试修剪机制，而不是在 ResNet 这样的复杂架构上测试新的修剪方案。然后，我们可以将 CNN 添加到探索性代码中，并在新架构上测试剪枝机制。

无论是重新实现另一篇论文的结果还是尝试一个你自己的新想法，目标都是在过程中有一个高水平的粒度和控制。在我们的团队中，我们发现梯度笔记本在这个过程中是一个非常有价值的工具。渐变笔记本允许我们使用预装了库和软件的容器，并向我们展示了一个 Jupyter 笔记本界面，该界面可以访问共享工作区，从而实现快速迭代和探索。

由于快速行动和测试许多小范围的可能性以获得概念性和经验性的理解是关键，我们非常频繁地使用这个特性。我们最近还在探索在笔记本电脑中使用 Gradient SDK ，使我们能够快速启动实验和更大的工作负载。如果我们生成了一个有用的结果，我们可以将它存储到共享的工作空间存储中，如果我们愿意，可以在更严肃的后续实验中使用它。此外，如果研究的某些内容是计算密集型的，即使我们将其范围缩小到概念验证实验，Gradient 也允许我们指定我们希望为我们的笔记本提供什么样的 GPU，这是我们在 Google Colab 等其他服务或本地 Jupyter 笔记本安装上无法做到的。

巨大的初始成果带来了大规模的后续实验。

哇哦。对我们新想法的初步探索产生了一些非常有趣的结果。我们的假设可能是正确的，那么现在呢？嗯，在很多领域，包括深度学习，你的方法或结果真的应该在一些更大的基准任务上进行测试。有些可能很小，但有些可能计算量很大。

较大的实验倾向于更加结构化，并且涉及相当程度的软件工程。它们需要更长的时间来设置，也要进行更严格的测试，以确保培训确实在进行。这通常是当我们团队的成员开始转向更有组织的代码库而不是单一的文件时。我们将开始使用设计原则，并开始真正记录一些工程决策。作为一名研究人员，当涉及到这些更大规模的实验时，笔记本界面开始变得有点缺乏，因为我不再花大部分时间通过小调整重新运行细胞并快速重新设计代码库。

在 ATG，我们可以访问 Gradient 的实验接口，这允许我们基本上将特定代码库的计算密集型运行视为作业。这些实验将运行我们指定的代码，并访问我们之前指定的共享工作区。结果是能够并行进行多个实验，并快速获得结果。我们还利用多节点特性和适当的分布式培训。Gradient 还自动解析关于我们的模型流程的统计数据，因此我们可以获得一些关于性能和其他重要指标的有用分析。

关于工具的快速说明。我们倾向于使用 Tensorflow，因为它有广阔的生态系统和对大型系统级实验的支持。我们也用过 Pytorch，发现它非常有用。

使用渐变 CI 体验版本控制

ML 研究中的一个正在进行的问题，也许是 CS 研究中的一个普遍问题，是决定如何将你的研究模型和实验版本化。作为研究人员，我们有时会发现调整代码库中的小值，如超参数值，可能会对我们的结果产生巨大的影响。但是，将学习率从 0.001 改变到 0.005 是否构成了我们正在跟踪的一个全新的实验？在 ATG，我们从我们的软件工程根源中获得灵感，并决定任何有用的提交变更都应该构成一个实验版本。毕竟，一个失败实验的成本肯定比跟踪许多增量实验的成本要高。Paperspace 的 GradientCI 工具可以跟踪变化，并在我们需要时自动运行这些变化作为实验。它还会以类似于 Gradient 客户端的方式，自动生成我们想要的各种指标的有用报告。

做研究没有正确的方法！

真的没有。研究过程应该是对你正在做的工作有意义的东西和让你的研究小组感到舒适和兴奋的东西的结合。在 ATG，我们结合了工程和研究背景，并发现我们上面提到的方法对测试 DL 和系统领域的大量有趣的想法非常有用。

从笔记本等灵活的工具转移到实验等更强大的界面，似乎遵循了我们正在进行的研究工作的自然流程，并允许我们利用软件工程最佳实践来提高工作效率。随着我们团队的成长，我们与全球其他世界级研究人员合作并建立更紧密的联系，我们希望进一步改善我们开放、合作和好奇的文化。

有兴趣加入 Paperspace 吗？你可以点击这里查看我们的招聘信息！

通过 Paperspace 向您的客户分发您的产品

原文：https://blog.paperspace.com/how_to_setup_sales_trials_on_paperspace/

Paperspace 是一种强大的分发工具，以简单易管理的方式展示您产品的功能。您的潜在客户将能够在几分钟内用任何样本或真实世界的数据来测试您的软件。只需要一封电子邮件邀请。

它是如何工作的？

拥有需要基于桌面安装的产品的公司面临着特殊的挑战。

演示产品通常是必要的
安装过程中的摩擦搅动了销售漏斗中的潜在客户。这可能是由于许多不可控的因素造成的，包括不良的硬件要求、不良的操作系统要求等。
预装的云桌面，准备了产品和演示材料，提高了从潜在客户到合格销售线索的转化率。
带着昂贵(通常很重)的设备去贸易展会可能会很麻烦。借助 Paperspace 的云桌面，可以在轻量级 Chromebooks 上进行演示。

下面是通过 Paperspace 的云桌面向潜在客户部署您自己的软件的入门指南。

想要更多曝光？

如果您希望您的销售支持工作流以 Paperspace 为特色，请给我们发送电子邮件至 hello@paperspace.com，或发推特给我们，包括标签#PoweredByPaperspace

指南大纲

当计划使用 Paperspace 向潜在客户试用您的软件时，本实用指南将确保说明以下内容:

账户管理

机器建议

机器管理

创建机器
设置您的试用机器
测试您的产品
创建模板
创建新机器
考虑

用户管理

添加新用户
分配用户到机器
设置明确试用日期
考虑

部署和计费

部署注意事项

伙伴关系

联系我们

结论

**1。**账户管理
1\. Sign Up
2\. Provide Billing Info
3\. Request Team Access from Support@paperspace.com 

1
2
3
**2。**机器管理

Paperspace 提供了一个简单的用户界面来创建、删除和管理机器。在我们的在向潜在客户分发产品试用版时，您应该考虑以下问题。

考虑:
你需要多少台机器？

Windows/Linux？

由于试用中利用率的不确定性，我们建议按月计费。

创建一台机器

选择最近的区域

选择 Windows 或 Linux 模板

根据您的需要选择每月或每小时。

选择你的计划:

高性能如果您的产品利用了我们的 GPU，请务必选择 GPU+ 、 P5000 或 P6000 。

选择您的存储空间 —您可以在未来随时增加存储空间，但我们强烈建议您的产品演示至少要有 100GB 或以上的存储空间。

点击创建

设置您的试用机器

安装您的软件及其所有依赖项。如果您的软件尚未包含样本数据或文件，我们建议您包含样本数据或文件。

保存在桌面上的简单文本文件(即常见问题解答、自述文件等)。)为您的潜在客户的试用体验提供指导，也会证明是有用的。

测试您的产品

在创建模板以方便分发之前，请确保您的产品按预期工作。

创建模板

在控制台中，从该机器创建一个模板
注意:
模板当前仅限于最初创建它们的数据中心。例如，从 NY1 中的机器生成的模板将不能用于在西海岸或欧洲创建机器。

创建新机器

根据需要从模板创建新机器

**3。**用户管理

试用期需要围绕访问进行仔细规划。在开始分发试用产品之前，您应该能够回答以下问题。很重要的一点是，要想清楚产品试用将如何成为有凝聚力的销售流程和剧本的一部分。虽然您可以让多个团队成员成为团队管理员，但请考虑确保最佳销售体验所需的工作流程。

考虑:

这些用户可以访问多长时间？几天？几个月？

我们将如何管理这些试验的部署？

添加新用户

通过“用户”选项卡，您可以查看和管理所有现有用户，并创建新用户。

添加用户后，他们会收到一封电子邮件，通知他们可以访问。创建新用户时，您可以选择将他们标记为团队管理员。

将用户分配到特定的机器。

我们不允许多人同时使用同一台机器。因此，请确保将额外的机器作为试验计算的一部分。如上所述，只有团队管理员可以将用户分配给机器。虽然您可以有几个团队管理员，但请仔细考虑这可能会对您的销售行动手册的设计产生什么影响。

设定明确的审判日期。

设置这些日期将有助于确保您可以在试用期到期后从用户的计算机上停用用户。我们目前正在构建一个允许编程控制的 API。在那之前，大部分都是手动的。

**4。**部署和计费

部署

对于您的潜在客户来说，登录您的销售演示机器将会非常简单。他们只需要自己的电子邮件和密码就可以通过浏览器访问。不需要安装。对于键盘快捷键，我们建议您的潜在客户将我们的专用应用下载到他们基于 Windows 或 MacOS 的笔记本电脑/台式机上。

计费

每月的纸张空间账单。我们建议您将 Paperspace 的成本纳入您的销售支持预算，以了解是免费试用还是付费试用最适合您的产品。

**5。**伙伴关系

我们一直有兴趣与那些希望通过 Paperspace 发布软件的公司建立合作伙伴关系。我们已经与 Fast.ai、H2O.ai 和 Parsec 等公司和组织合作，为用户提供定制模板，预配置所有必要的软件和依赖项，让他们的客户获得出色的体验。如果这是你有兴趣讨论的事情，请在hello@paperspace.com告诉我们！

**6。**结论

Paperspace 强大的云桌面正在推动营销和销售组织内部协作的未来。我们轻松的用户体验使销售团队能够通过向潜在客户轻松分发台式机产品的销售演示来提高销售速度，从而消除传统试用期的瓶颈。

对 Paperspace 如何为您的销售团队提供动力有什么想法？我们希望你能在hello@paperspace.com与我们分享你自己的经历。

尽情享受吧！

要让您的销售组织加入 Paperspace，请在此注册。

我们需要你的帮助！

我们正在寻找专注于销售和营销工作流的内容作家，以帮助建立我们的社区。给 hello@paperspace.com 发电子邮件，附上写作范例和教学想法

关于基于 ARM 的深度学习架构，超级计算的历史可以教给我们什么？

原文：https://blog.paperspace.com/hpc-supercomputing-deep-learning-history-arm/

NVIDIA 最近发布了Grace——一款新的基于 ARM 的 CPU 芯片，针对神经网络工作负载进行了优化。

这让我们不禁想知道 Grace 是否标志着 64 位 x86 体系结构在高性能计算(HPC)领域长达 15 年以上的主导地位开始终结？对于我们这些构建深度学习应用的人来说，这可能意味着什么？我们会很快将代码移植到 ARM 上吗？

我们希望通过了解 HPC 的一些历史来解决 NVIDIA 的声明引发的这些和其他问题。希望我们可以先了解一点我们是如何走到这一步的，然后这对深度学习的未来可能意味着什么。

超级计算早期

超级计算有着悠久而迷人的历史——从 20 世纪 60 年代最初的 IBM、CDC 和 Cray 机器，它们以每秒 10^6 浮点运算数来衡量性能，到今天的机器，它们计划在 2021 年底突破每秒 10^18 浮点运算数的障碍。

超级计算也是一种观赏性运动。每年有两次500 强榜单在两次和最大的超级计算会议期间发布——争夺榜单榜首的竞争非常激烈。自 1993 年以来，500 强榜单一直在持续发布，这对所有参与者来说都是一件大事——从托管这些集群的站点到组成这些集群的组件制造商，再到使用这些集群的科学界(和国家)。

同样值得注意的是，Top 500 仅限于发布 public 的集群。虽然不是这篇博客的主题，但超级计算也有很长的保密历史——从艾伦·图灵在 20 世纪 40 年代破解德国海军之谜到高级模拟和计算程序，该程序至今仍负责维护美国的核储备。

但那是以后的事了。我们今天想要了解的是，我们在超级计算架构方面已经取得了哪些进展，我们将走向何方——特别是当它与日益并行化的机器学习和深度学习应用相关时。

向量处理器的时代

The 80 MHz Cray-1 was so highly anticipated by the time it was released in 1975 that a bidding war broke out between two US national labs to secure Unit #001. (Source)

在 1966 ，控制数据公司 (CDC)和德州仪器 (TI)都向市场推出了利用向量处理的机器。从此，早期的超级计算被向量处理器或阵列处理器机器所统治。在 20 世纪 70 年代中期之前，大型机制造商 CDC 和国际商用机器公司(IBM)一直占据主导地位，直到克雷(CDC 的一个分支)接过了接力棒。

国家研究委员会的一份报告解释了向量架构的早期优势:

Cray-1 支持向量体系结构，在这种体系结构中，浮点数的向量可以从内存加载到向量寄存器中，并在算术单元中以流水线方式进行处理，其速度远远高于标量操作数。向量处理成为超级计算的基石。

1976 年，当著名的 Cray-1 在洛斯阿拉莫斯国家实验室发射时，Cray 已经在制造效率惊人的向量处理机了。CDC、Cray 和其他人开创的体系结构在 20 世纪 90 年代一直保持主导地位。

今天这个事实特别值得注意，因为在这些早期 Cray 机器的技术架构和现代基于 GPU 的机器的并行计算架构之间有一些现代的相似之处。

个人电脑改变游戏

当早期的超级计算先驱们忙于建造单处理器机器时，英特尔在 1971 年生产了 T2 第一个商用微处理器。到 20 世纪 80 年代初，这项技术已经足够成熟——也就是说可以大规模生产了——个人电脑革命开始成形。

大约在 1984 年，当麦金塔电脑第一次引起 T2 全国的关注时，个人电脑进入了主流。事后看来，我们可以看到苹果和其他早期的个人电脑组装商正乘着微处理器成本大幅下降的东风。

Microprocessors have been getting cheaper for decades. (Source)

到 20 世纪 90 年代，Cray 和其他继续生产单处理器机器的传统超级计算制造商将无法跟上商用 CPU 架构提供的性价比。

要理解其中的原因，看看微处理器市场增长的速度有多快是有帮助的:

Semiconductor sales with few exceptions have been increasing for decades. (Source)

在此期间，微处理器的成本直线下降。到 1995 年，克雷破产了。约翰·马科夫在当时为《纽约时报》写道:

但在 20 世纪 90 年代，廉价而强大的微处理器芯片的出现给计算世界带来了变革，极大地削弱了价值数百万美元的“大铁”系统，而这正是克雷的标志。此外，冷战的结束意味着政府采购机器的预算下降，这些机器曾经是美国武器实验室的支柱，也是“星球大战”战略防御计划的核心。

旧的“大铁”系统被商品风格的微处理器架构取代——这一事实自 Cray(第一个)消亡以来塑造了过去 25 年的计算。

力量

下图显示了自 20 世纪 90 年代初该榜单发布以来，Top500 超级计算机所使用的处理器/架构。如果我们一直向左看，我们可以看到克雷最近几年的统治地位。

Top500 supercomputers have had a large number of dominant architectures through the years. (Source)

不久之后，我们看到太阳微系统公司的 SPARC 和 DEC 公司的阿尔法的出现。就像 IBM 的 POWER 一样，这些 RISC 架构在 20 世纪 80 年代早期受到了 Berkeley RISC (由 ARPA 资助)工作的严重影响。

RISC 项目(以及同时期斯坦福大学的 MIPS 项目)着手了解如何创造一个更简单的 CPU。其概念是以这样一种方式限制处理器的指令集，即它仍然能够执行大多数频繁执行的计算，同时降低复杂性。

20 世纪 90 年代初，我们也开始看到大规模并行系统。到 1993 年，英特尔 Paragon 可以支持 1000 多个英特尔处理器。富士通的数字风洞于 1994 年与 QTY 166 矢量处理器一起发货。到 1995 年，Cray 也开始用其 T3E 系统运送大规模并行系统。

大约在同一时间，我们看到 IBM 在 1990 年左右推向市场的 POWER(后来演变成 PowerPC)的出现。

POWER 最终会将 HPC 的市场权力让给 x86，但这要等到 x86 64 位真正起飞之后。在那之前，权力是一种过渡手段。在有限的时间内，性能优于 16 位和 32 位 x86，但在 2004-2005 年左右，64 位 x86(首先是 AMD，然后是 Intel)开始占据前 500 位。

随着并行性的出现(以及很快 64 位架构的出现)，新架构将很快取代旧架构。

x86 取而代之，行业永不回头

尚不清楚行业决定转向 x86 架构的确切时间，但几年前发生的一个重要事件似乎已经推动了这一进程 IBM 在 20 世纪 80 年代初为其个人电脑选择了英特尔 8088 。

英特尔的 x86 架构并不一定比我们刚刚了解到的基于 RISC 的 PowerPC 架构等替代产品更好，但也许英特尔只是在满足蓬勃发展的市场需求方面做得更好。

英特尔接着在 1985 年发布了第一款 32 位 x86 芯片，到 21 世纪初，英特尔和 AMD 都成功地批量生产了低成本的 64 位 x86 处理器。

到 2005 年中期，80%以上的 500 强超级计算机都采用 x86-64。

然后，在没有太多警告的情况下，到 2005 年免费午餐结束了——CPU 时钟速度不再像几十年前那样增长。我们实际上可以看到这种现象，在 2005 年之前的几年中，峰值速度一直处于平稳状态。

Top supercomputer speeds occasionally plateau for periods of time over 60 years. (Source)

现在，我们突然看到多核矢量指令集芯片需要回到菜单上。旨在渲染图形的大规模并行芯片的设计者——比如英伟达的人——肯定会注意到这一点。

加速器的兴起

早在 2003 年，与 CPU 上的顺序处理相比，甚至与多核 CPU 相比，GPU 上的多核并行处理就开始显示出巨大的潜力。

With CPUs plateauing leading into the mid-2000s, NVIDIA GPU processors began to show promise. (Source)

NVIDIA 认识到了 CPU 的瓶颈，并希望为 GPU 带来除渲染图形之外的另一个用例，在 2007 年发布了 CUDA ，以帮助开发人员在 GPU 上运行通用计算任务。

值得注意的是，加速器的兴起是超级计算的一种回归。就架构而言，大规模并行 GPU 加速系统的设计更接近 Cray，而不是 x86。

GPU 加速的回报比任何人合理预测的都要高。近二十年来，高性能计算领域一直由 x86 CPU 架构主导，现在它的重心已经大规模转移到 GPU 加速器上。

Machines in the Top500 using accelerators since 2006. (Source)

截至 2020 年，500 强中有 146 家正在使用英伟达 GPU 加速卡——包括最近前 10 名的个系统。

NVIDIA 在加速卡领域并非没有挑战。看到 CPU 到矢量加速器过渡的机会，英特尔在 2011 年发布了骑士渡口/骑士角架构(后来被称为至强融核)，这是一款基于 x86 的众核处理器，但具有不同的线程模型和额外的 SIMD 通道(AVX 系列指令)。

尽管承诺简单(无需重写代码即可在 x86 上运行)，但不幸的是，英特尔似乎没有准备好与 NVIDIA 的 CUDA 部署和爆炸式社区竞争，或者在竞争中失败。

武器经济学

从今天的 500 强来看，并不明显的是，尽管 x86 几乎和 Linux 一样在列表中占据主导地位(这是一个完美的 500/500)，但 x86 体系结构本身对于现代 HPC 来说并不完全是必要的。

在 GPU 加速器时代，CPU 在 HPC 环境中完成的大部分工作只是简单地向 GPU 馈送数据。没有真正的高度架构依赖性。今天，CPU 方面的最大限制是内存和 PCI-e 带宽。(这就是为什么像 NVLink 这样的解决方案似乎有助于 NVIDIA 大幅提高 CPU 性能。)

正如 POWER/PowerPC 曾经是 HPC 的默认选择，早期的多核 AMD 处理器曾经是新 HPC 系统的首选一样，今天看来 ARM 有可能取代基于英特尔的 x86，成为世界上最快的集群的主导架构。

去年被 NVIDIA 以 400 亿美元收购的 ARM 最初是受 20 世纪 80 年代 RISC 的启发。该公司成立于 1990 年，使用的架构[与英特尔的 x86 架构](https://www.androidauthority.com/arm-vs-x86-key-differences-explained-568718/#:~:text=Arm%20is%20RISC%20(Reduced%20Instruction,(Complex%20Instruction%20Set%20Computing).&text=This%20is%20a%20key%20difference,%2C%20instruction%20set%2C%20and%20hardware.)大不相同。他们的工作导致了Acorn Achimedes——第一台基于 RISC 的个人电脑。

ARM 芯片因其极低的功耗和高效率而备受青睐。它们也正处于生产热潮之中——很像上世纪 90 年代个人电脑革命之初的微处理器。正如一位作者所说:

基于 Arm 的产品显然无处不在。任何“智能”设备都可能包含 Arm 技术。作为最受欢迎的微处理器架构，Arm 报告称，其硅合作伙伴在 2020 年第四季度出货了创纪录的 67 亿片基于 Arm 的芯片，超过了其他流行的 CPU 指令集架构的总和…虽然 Arm 一直是几乎所有智能手机的首选 CPU，但 Nvidia-Arm 的收购将对更广泛的计算行业产生惊人的影响。

ARM 的成本下降如此之快也就不足为奇了——当个人电脑首次压低 x86 价格时，我们看到了同样的推动作用。ARM 支持的不仅仅是智能手机。也是像物联网传感器，MacBooks，数据中心服务器之类的东西！

因此，得知 500 强中排名第一的计算机是位于日本神户的富士通公司生产的基于 ARM 的 Fugaku 就不足为奇了。

Fugaku 拥有 158，976 个 A64FX 处理器(顺便说一下，这些处理器取代了旧的 SPARC 处理器)，并将继续成为地球上最快的公共计算机，直到 2021 年底。

这对深度学习意味着什么

由于架构、成本和效率的原因，我们可能会看到高性能计算从 x86 到 ARM 的全行业过渡，这种过渡可能会渗透到深度学习计算中。

尽管作为计算的一个子集，HPC 历史悠久，但它是深度学习的优秀领导者。从张量核也有利于传统 HPC 应用的意义上来说，这些应用非常相似。

训练深度学习模型的计算需求每年都在以天文数字的速度增长:

Deep learning compute time over the years (Source)

同时，计算成本也在降低。

How much does it cost to train ResNet-50? (Source)

这是自 CDC 和 IBM 大型机时代以来计算的相同自然循环:计算需求增加，而单位计算成本降低。

深度学习成本正在快速降低。一位分析师写道:

根据其成本下降的速度，人工智能处于非常早期的阶段。在摩尔定律的第一个十年，晶体管数量每年翻一番——或者说是其后几十年变化速度的两倍。我们在人工智能训练和人工智能推理方面看到的 10-100 倍的成本下降表明，人工智能在其发展中处于萌芽状态，或许未来几十年将会缓慢但持续地增长。

ARM 代表了一条令人兴奋的道路，可以满足深度学习对计算的贪得无厌的胃口。产量已经到位——每季度大约 70 亿个 ARM 单元。许可模式也已到位，允许供应商扩展，以提供与加速卡的深度集成。世界顶级超级计算机已经展示了基于 ARM 架构的可能性。

那么，期待很快看到深度学习库移植到 ARM 上合理吗？

可能性看起来很大。如果有一件事是确定的——如果它真的发生了，它会比我们预期的要早得多。

Paperspace 赞助了一个以 GANs 为主题的拥抱脸社区冲刺

原文：https://blog.paperspace.com/hugging-face-community-sprint-gan/

我们最喜欢的话题之一是 GANs 或生成性对抗网络。近年来，GAN 的实现已经激增，其结果是用简单的指令集来生成像素艺术、印象派绘画以及介于两者之间的一切。最近几个月，我们发表了关于 JoJoGAN 、 GFP-GAN 、 VQGAN-CLIP 、 Wasserstein GANs 、超分辨率 GANs 和 DCGANs 的文章，仅举几例。

这就是为什么我们如此兴奋地宣布，我们将与我们的朋友合作，在本月以 GAN 为重点的社区冲刺拥抱脸提供免费的计算资源。

请继续阅读，了解如何加入这一有趣的活动。

介绍

Paperspace 和 Hugging Face 合作为即将到来的 HugGAN 社区冲刺的参与者提供免费计算资源。

参与者将有两周的时间使用 Paperspace compute 和拥抱面部工具来训练和展示 GANs。

日期

开球:2022 年 4 月 4 日
投稿:2022 年 4 月 4 日-4 月 15 日
奖品公布:2022 年 4 月 22 日

如何参与
点击查看活动信息页面
填写这张表格
创建拥抱面部中枢账户并加入拥抱组织
加入不和谐频道
建议

需要一些 GANs 合作的想法吗？我们掩护你！这里有五个尚未在拥抱脸空间上出现的 GAN 库:

Pixray - Source

Pixray :一种图像生成方法，用于创建高度精细的像素和绘画图像和艺术品。Pixray 使用 CLIP 将文本提示转化为独特的生成作品。
Transeditor :这款基于变形金刚的双空间 GAN 允许高度可控的面部调制，如变换姿势或改变发型。
:这款场景生成 GAN 可以生成无限数量的复杂而独特的场景，如风景或城市景观。

Anycost-GAN - Source

Anycost-GAN :一种用于面部图像编辑的 GAN 方法，可以通过修改通道和再分辨率配置设置在各种计算成本上运行。
StyleMapGAN: 这种 GAN 利用潜在的空间维度对面部图像进行精细的细节编辑。
使用 Keras 调谐器自动优化超参数

原文：https://blog.paperspace.com/hyperparameter-optimization-with-keras-tuner/

超参数是决定机器学习模型结构并控制其学习过程的配置。它们不应该与模型参数(如偏差)混淆，后者的最佳值是在训练期间确定的。

超参数是可调整的配置，可手动设置和调整以优化模型性能。它们是顶级参数，其值有助于确定模型参数的权重。两种主要类型的超参数是确定模型结构的模型超参数(如层数和层单元)和影响和控制学习过程的算法超参数(如优化算法和学习速率)。

用于训练神经网络的一些标准超参数包括:

1.隐藏层数

2.隐藏层的单位数

3.退出率——通过在训练期间随机退出节点，可以使用单一模型来模拟大量不同的网络架构

4.激活函数(Relu，Sigmoid，Tanh) -在给定一个或一组输入的情况下，定义该节点的输出

5.优化算法(随机梯度下降、Adam Optimizer、RMSprop、e.t.c) -用于更新模型参数和最小化损失函数值的工具，根据训练集进行评估。

6.损失函数——衡量你的模型在预测预期结果方面有多好

7.学习率——控制每次更新模型权重时，响应估计误差而改变模型的程度

8.训练迭代次数(epochs) -学习算法在整个训练数据集中工作的次数。

9.批量大小——这个梯度下降的超参数控制着在模型的内部参数更新之前训练样本的数量。

在建立机器学习模型时，会设置超参数来指导训练过程。根据初始训练后模型的性能，这些值被反复调整以改进模型，直到选择出产生最佳结果的值的组合。调整超参数以获得优化机器学习模型性能的正确值集的过程被称为超参数调整。

在深度学习中，调整超参数可能具有挑战性。这主要是由于需要正确设置的不同配置、重新调整这些值以提高性能的几次尝试以及为超参数设置次优值所产生的不良结果。在实践中，这些值通常是基于某些推理来设置和微调的，例如特定问题的一般原则(例如，使用 softmax 激活函数进行多类分类)、构建模型的先前经验(例如，将隐藏层的单元逐渐减少到原来的 2 倍)、领域知识和输入数据的大小(为较小的数据集构建更简单的网络)。

即使有了这种认识，仍然很难为这些超参数得出完美的值。从业者通常使用试错法来确定最佳超参数。这是通过基于他们对问题的理解来初始化值，然后在为模型选择具有最佳性能的最终值之前，根据模型的性能在几次训练试验中本能地调整值来完成的。

以这种方式手动微调超参数对于管理计算资源来说通常是费力、耗时、次优和低效的。另一种方法是利用可扩展的超参数搜索算法，如贝叶斯优化、随机搜索和超波段。Keras Tuner 是一个可扩展的 Keras 框架，它提供了这些内置的算法，用于深度学习模型的超参数优化。它还提供了一种优化 Scikit-Learn 模型的算法。

在本文中，我们将学习如何使用 Keras Tuner 的各种功能来自动搜索最佳超参数。任务是使用 Keras 调谐器获得最佳超参数，以构建一个模型，对 CIFAR-10 数据集的图像进行准确分类。

1.设置。

使用 Keras Tuner 需要安装 Tensorflow 和 Keras Tuner 包，并导入构建模型所需的库。
KerasTuner 需要 Python 3.6+和 TensorFlow 2.0+。这些预装在梯度机器上。

# install required packages
pip install tensorflow
pip install keras_tuner

1
2
3
# import required packages
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Flatten, Convolution2D, BatchNormalization
from tensorflow.keras.layers import ReLU, MaxPool2D, AvgPool2D, GlobalAvgPool2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
import keras_tuner as kt
from sklearn.model_selection import train_test_split

1
2
3
4
5
6
7
8
9
10
2.加载并准备数据集。

我们将加载包含 10 个对象类的 50，000 个训练和 10，000 个测试图像的 CIFAR-10 数据集。你可以在这里阅读更多关于数据集的内容。我们还归一化图像像素值以具有相似的数据分布并简化训练。

预处理数据集版本被预加载到 Keras 数据集模块中，以便于访问和使用。

2.1 加载数据集并归一化图像像素值。
# load the CIFAR-10 dataset from keras
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Normalize the image pixel values
img_train = x_train.astype('float32') / 255.0
img_test = x_test.astype('float32') / 255.0

# split the train data into train and validation sets
x_train, y_train, x_val, y_val = train_test_split(x_train, y_train, test_size=0.25)

1
2
3
4
5
6
7
8
9
3.打造超模。

现在我们已经完成了设置并准备好了输入数据，我们可以为超调构建模型了。这是通过使用 Keras Tuner 定义一个搜索模型(称为超级模型)来完成的，然后将该模型传递给一个调谐器进行超调。

超模型要么通过创建一个定制的模型构建器函数，利用内置模型来定义，要么为高级用例子类化 Tuner 类。
我们将使用前两种方法来创建搜索模型，以自动调整我们的超参数。

3.a .使用定制模型。

为了使用定制模型，我们将通过定义我们需要的层来定义模型构建功能，定制用于找到最佳参数的搜索空间，并在我们不调整超参数时为它们定义默认值。

3.a.1 定义建模功能。

该函数采用一个参数(hp ),该参数实例化 Keras Tuner 的超参数对象，并用于定义超参数值的搜索空间。我们还将编译并返回超模型以供使用。我们将使用 Keras 功能模型模式来构建我们的模型。

# function to build an hypermodel
# takes an argument from which to sample hyperparameters
def build_model(hp):
​
inputs = Input(shape = (32, 32, 3)) #input layer
x = inputs
​
# iterate a number of conv blocks from min_value to max_value
# tune the number of filters
# choose an optimal value from min_value to max_value
for i in range(hp.Int('conv_blocks',min_value = 3, max_value = 5, default=3)): # Int specifies the dtype of the values
filters = hp.Int('filters_' + str(i),min_value = 32,max_value = 256, step=32) 
​
for _ in range(2):
# define the conv, BatchNorm and activation layers for each block
x = Convolution2D(filters, kernel_size=(3, 3), padding= 'same')(x)
x = BatchNormalization()(x)
x = ReLU()(x)
​
# choose an optimal pooling type
if hp.Choice('pooling_' + str(i), ['avg', 'max']) == 'max': # hp.Choice chooses from a list of values
x = MaxPool2D()(x)
else:
x = AvgPool2D()(x)
​
x = GlobalAvgPool2D()(x) # apply GlobalAvG Pooling
​
# Tune the number of units in theDense layer
# Choose an optimal value between min_value to max_value
x = Dense(hp.Int('Dense units',min_value = 30, max_value = 100, step=10, default=50), activation='relu')(x)
outputs = Dense(10, activation= 'softmax')(x) # output layer 

# define the model
model = Model(inputs, outputs)
​
# Tune the learning rate for the optimizer
# Choose an optimal value frommin_value to max_value
model.compile(optimizer= Adam(hp.Float('learning_rate', min_value = 1e-4, max_value =1e-2, sampling='log')), 
loss= 'sparse_categorical_crossentropy', metrics = ['accuracy'])
return model

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
理解代码。

**第 3 行:**我们定义了一个模型构建函数( build_model )并传递了一个参数 (hp )，该参数实例化了 Keras Tuner 包的超参数对象，用于定义超参数值的搜索空间。

第 5-6 行:我们定义我们的输入层，并将其传递给一个变量(x)

**第 11 行:我们为我们的模型定义卷积块数量的搜索空间。我们使用 hp.Int 函数来创建一个整数超参数搜索空间。这创建了从最小值 + 1 到最大值的搜索空间。这将在 4 和 5 个卷积块的空间中搜索使精度最大化的最佳值。

**第 12 行:**我们为块中每个卷积层的滤波器数量定义一个搜索空间。32 的步骤将连续卷积层的滤波器单元增加 32。

第 14-24 行:我们为每个块定义一组三层。每个子层对输入应用卷积、批量归一化和 ReLU 激活。惠普。池层的 Choice 函数随机选择一个提供的池应用于输入。然后，我们将预定义的滤波器搜索空间传递给卷积层。

**第 26 行:**我们应用全局平均池和密集层，搜索空间从最小值到最大值，步长为 10。我们还通过激活 softmax 来定义输出层。

第 34-40 行:最后，我们使用输入和输出层定义模型，编译模型并返回构建好的超模型。

为了编译模型，我们用 hp 定义了一个学习率搜索空间。Float 函数创建一个从 0.0001 到 0.002 的搜索空间，用于选择最佳学习率。

3.a.2 初始化搜索算法(调谐器)。

在构建了超模型之后，我们现在可以初始化我们的搜索算法了。我们将不得不从内置的搜索算法中进行选择，如*贝叶斯优化、超波段和随机搜索、*经典机器学习模型。

在我们的例子中，我们将使用超波段搜索算法。tuner 函数接受参数，如超级模型、用于评估模型的目标、用于训练的 max_epochs 、每个模型的 hyperband_iterations 的数量、用于保存训练日志(可以使用 Tensorboard 可视化)的目录和 project_name 。

# initialize tuner to run the model.
# using the Hyperband search algorithm
tuner = kt.Hyperband(
hypermodel = build_model,
objective='val_accuracy',
max_epochs=30,
hyperband_iterations=2,
directory=""Keras_tuner_dir"",
project_name=""Keras_tuner_Demo"")

1
2
3
4
5
6
7
8
9
3.b .使用内置模型。

Keras Tuner 目前提供了两个可调的内置模型，HyperResnet 和 HyperXception 模型，它们分别搜索 Resnet 和 Xception 架构的不同组合。使用内置模型定义调谐器类似于使用模型构建功能。

# Initialize a random search tuner
# using the Resnet architecture
# and the Random Search algorithm
tuner = kt.tuners.RandomSearch(
kt.applications.HyperResNet(input_shape=(32, 32, 3), classes=10),
objective='val_accuracy',
max_trials=30)

1
2
3
4
5
6
7
4.运行最佳超参数搜索。

然后，我们可以使用我们的调谐器在定义的搜索空间内搜索模型的最佳超参数。该方法类似于使用 Keras 拟合模型。

# Run the search
tuner.search(x_train, y_train,
 validation_data= (x_test,y_test), 
 epochs=30,
 callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])

1
2
3
4
5
5.获得并显示最佳超参数和模型。

可以使用调谐器实例的 get_best_hyperparameters 方法获得定义的搜索空间内的模型的最佳超参数，并使用 get_best_models 方法获得最佳模型。

# Get the optimal hyperparameters
best_hps= tuner.get_best_hyperparameters(1)[0]

# get the best model
best_model = tuner.get_best_models(1)[0]

1
2
3
4
5

我们还可以查看最佳超参数。在我们的例子中，我们可以这样实现:

nblocks = best_hps.get('conv_blocks')
print(f'Number of conv blocks: {nblocks}')
for hyparam in [f'filters_{i}' for i in range(nblocks)] + [f'pooling_{i}' for i in range(nblocks)] + ['Dense units'] + ['learning_rate']:
print(f'{hyparam}: {best_hps.get(hyparam)}')

1
2
3
4

这将显示卷积块的数量、卷积和密集层的过滤器和单位的最佳值、池层的选择以及学习率。

我们还可以使用适当的 Keras 函数查看优化模型的概要和结构。

# display model structure
plot_model(best_model, 'best_model.png', show_shapes=True)

# show model summary
best_model.summary()

1
2
3
4
5
6.训练模型。

最后，在调用 fit 函数来训练模型之前，我们将使用最佳超参数建立模型。

# Build the model with the optimal hyperparameters
# train the model.
model = tuner.hypermodel.build(best_hps)
model.fit(x_train, y_train, 
validation_data= (x_val,y_val), 
epochs= 25,
 callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)])

1
2
3
4
5
6
7

在这里，我对模型进行了 50 个时期的训练，并添加了一个 EarlyStopping 回调，以便在模型不再改进时停止训练。

6.评估模型。

我们可以在测试集上评估模型。我们将使用模型的损失和准确度得分来评估模型。您可以尝试其他适用的指标。

# evaluate the result
eval_result = model.evaluate(x_test, y_test)
print(f""test loss: {eval_result[0]}, test accuracy: {eval_result[1]}"")

1
2
3
总结。

超参数是机器学习模型性能的关键决定因素，用试错法来调整它们是低效的。Keras Tuner 应用搜索算法在定义的搜索空间中自动找到最佳超参数。

在本文中，我们利用 Keras 调谐器来确定多类分类任务的最佳超参数。我们能够使用自定义模型和内置模型在超模中定义搜索空间，然后利用提供的搜索算法自动搜索几个值和组合，为我们的模型找到超参数的最佳组合。

您可以查看 Keras Tuner guide ,了解如何在 Tensorboard 上可视化调优过程、分发超调过程、定制搜索空间以及为高级用例子类化 Tuner 类。

在 CIFAR10 数据集上应用模型可解释性算法

原文：https://blog.paperspace.com/illustration-of-integrated-gradients-on-cifar10-dataset/

介绍

像 Torchvision 和 Torchtext 这样的 PyTorch 库支持专门的数据，如计算机视觉和自然语言数据。torch vision . datasets模块演示了如何使用内置类加载数据。使用 torchvision.datasets 模块时，您可以通过各种子类从众所周知的数据集中加载图像数据。本教程演示了如何将 Captum 库中的模型可解释性算法应用于一个简单的模型和来自 CIFAR 数据集的测试样本。

在本教程中，我们将建立一个基本模型，类似于这里的所示的。然后，我们将使用 IntegratedGradients、显著性、DeepLift 和 NoiseTunnel 等归因算法将图像的标签归因于输入像素，并将其可视化。在学习本教程之前，需要安装 torchvision 和 captum。

导入库:
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline

import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F

from torchvision import models

from captum.attr import IntegratedGradients
from captum.attr import Saliency
from captum.attr import DeepLift
from captum.attr import NoiseTunnel
from captum.attr import visualization as v

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
使用数据加载器为培训准备数据

我们通过下面的代码加载测试和训练数据集，定义图像转换器，并支持分类标签类。在使用数据对 NN 模型进行训练和测试之前，可能需要对数据进行一些调整。数据的值可以被标准化以促进训练过程，被补充以产生更广泛的数据集，或者从一种类型的对象改变为张量。

通过使用**transforms . compose**，我们定义了一个转换集合。这个类将接受一个转换列表，然后按照给定的顺序应用它们。在这一步中，我们首先将图像转换为张量，然后根据预设的平均值和标准偏差对张量的值进行归一化。torch . utils . data . data loader类使批处理变得简单。

一种常见的做法是以“小批量”的方式通过模型输入样本，在每次迭代中重新排列数据以最小化过度拟合，并利用 Python 的多重处理来加速数据检索。一个名为DataLoader的 iterable 用一个简单的 API 为我们包装了这种复杂性。我们的模型使用两个卷积层、两个最大池层和三个全连接或线性层。

 class Net(nn.Module): ## create layers as class attributes
def __init__(self): ## Parameters initialization with __init__() function
super(Net, self).__init__() ## call the parent constuctor
self.conv1 = nn.Conv2d(3, 6, 5) ## Appy our first set of conv layers
self.pool1 = nn.MaxPool2d(2, 2) ## Apply our first set of max pooling layers
self.pool2 = nn.MaxPool2d(2, 2) ## Apply our second set of maxpooling layers
self.conv2 = nn.Conv2d(6, 16, 5) ## second set of conv layers
self.fc1 = nn.Linear(16 * 5 * 5, 120) ##first set of fully conneted layers
self.fc2 = nn.Linear(120, 84) ## second set of fullly conneted layers
self.fc3 = nn.Linear(84, 10) ## third set of fully connected layer
self.relu1 = nn.ReLU() ## Apply RELU activation function
self.relu2 = nn.ReLU()
self.relu3 = nn.ReLU()
self.relu4 = nn.ReLU()

def forward(self, x): ## specify how the model handles the data. 
x = self.pool1(self.relu1(self.conv1(x)))
x = self.pool2(self.relu2(self.conv2(x)))
x = x.view(-1, 16 * 5 * 5)
x = self.relu3(self.fc1(x))
x = self.relu4(self.fc2(x))
x = self.fc3(x)
return x

## Model initialization
net = Net()

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
定义损失函数和优化器

接下来，我们需要指定损失函数(也称为标准)，以及优化它的技术。损失函数决定了我们的模型表现如何，它用于计算实际结果和预测之间的损失。在训练过程中，我们将调整模型参数，以尽量减少损失。确保在代码中包含您的型号的model.parameters() 。本教程将 CrossEntropyLoss() 函数与随机梯度下降(SGD)优化器结合使用。以下代码显示了如何使用 torch.optim 和 torch.nn 包来创建损失函数和优化器。

# Initialize criterion and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

1
2
3
训练模型

我们在“模型”文件夹中保存了一个预训练模型的版本，以便我们可以加载它，而不必每次都从头开始训练过程。您可以从这里下载:

https://github . com/py torch/captum/blob/master/tutorials/models/cifar _ torch vision . pt

USE_PRETRAINED_MODEL = True
## If using the pretrained model, load it through the function load_state_dict
if USE_PRETRAINED_MODEL:
print(""Using existing trained model"")
net.load_state_dict(torch.load('models/cifar_torchvision.pt'))
else:
for epoch in range(5):# loop over the dataset multiple times

running_loss = 0.0 ## Resetting running_loss to zero 
for i, data in enumerate(trainloader, 0): ## restarts the trainloader iterator on each epoch.
# get the inputs
inputs, labels = data
# If you don't reset the gradients to zero before each ##backpropagation run, you'll end up with an accumulation of them. 
optimizer.zero_grad()

outputs = net(inputs) ## Carry out the forward pass. 
loss = criterion(outputs, labels)## loss computation
loss.backward() ## Carry out backpropagation, and estimate ##gradients. 
optimizer."
PythonLibrary 博客中文翻译（十七）,2024-11-10,"原文：PythonLibrary Blog 
 协议：CC BY-NC-SA 4.0 
 
本周 PyDev:Christopher Truncer 
 
 原文：https://www.blog.pythonlibrary.org/2018/01/22/pydev-of-the-week-christopher-truncer/ 
 
本周，我们欢迎克里斯托弗·特兰瑟(@克里斯特兰瑟)成为我们的本周 PyDev！他是面纱框架的联合创始人和当前开发者。Christopher 主要用 Python 开发笔测试工具。你可以在 Github 或他的网站上看到他在忙些什么。让我们花一些时间来更好地了解他！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我从小就喜欢电子游戏和玩电脑。出于某种原因，我从未想过我会有机会将这种热情转化为职业，所以我开始去学校学习刑事司法。不久之后，当我意识到我可以通过玩电脑来谋生时，我转到了信息技术专业。我一直想从事计算机安全方面的工作，当我的室友第一次让我黑他的电脑时，我就被迷住了。 
毕业后，我开始担任系统管理员，后来进入计算机安全领域，进行渗透测试和红队评估。不幸的是，我从未接受过任何正式的发展培训。从发展的角度来看，我所学到的一切都是自学或通过愿意帮助我的人来指导的。 
我也喜欢训练巴西柔术，我喜欢视频游戏，并且非常喜欢开发工具来帮助我做计算机安全/黑客工作。 
你为什么开始使用 Python？ 
我想要一种我认为比较容易掌握的语言，而且我日常使用的许多其他工具都是用 Python 开发的，所以我认为这是一种不错的语言。我也真的很想钻研 Python 或任何语言，因为我做的很多任务都可以自动化，我想知道如何自动化它们。 
在我开始最初的几个项目后，它们变得相当大，迫使我真正开始学习 Python 的来龙去脉。维护各种项目有助于确保我与时俱进。 
本周派德夫:克里斯蒂·希顿 
 
 原文：https://www.blog.pythonlibrary.org/2018/01/15/pydev-of-the-week-christy-heaton/ 
 
本周，我们欢迎克里斯蒂·希顿( @christytoes )成为我们本周的 PyDev！Christy 是 Python 软件基金会的博客作者。你可以通过她的 Github 页面或者查看她的网站来了解她在做什么。让我们花些时间去更好地了解她吧！ 
 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我学的是人类学，后来又学了地理信息系统(GIS)。GIS 对我来说是一个完美的领域，因为它将我对人、技术和制图的兴趣结合在一起。我现在是一名 GIS 项目经理，也是华盛顿大学的 GIS 和 Python 讲师。在爱好方面，我喜欢把有共同兴趣的人聚集在一起，这就是为什么我帮助组织 PyLadies Seattle 和 Maptime Seattle。我也是 Python 软件基金会的博客作者。 
你为什么开始使用 Python？ 
在开始我的 GIS 分析师职业生涯时，我对使我的工作更容易、更快、更准确以及扩展我的制图软件的功能感兴趣。我的 GIS 应用程序内置了一个 Python API，所以就编程语言而言，它是唯一的选择。随着我开始将脚本整合到我的工作流中，并给我的老板留下深刻印象，我开始意识到 Python 的全部潜力，现在我将它用于各种事情，从 GIS 工作流到测试 web 服务，再到构建 web 应用程序。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我经常使用 SQL，也涉猎过 R 和 JavaScript。但是 Python 是我最喜欢的。 
你现在在做什么项目？ 
我在工作中使用 Python 进行测试和自动化。我还在为华盛顿大学的 Python 编程证书进行课程开发。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
我真的很喜欢请求。它让我想做的与检查服务或网页抓取相关的事情变得如此简单和干净。Jupyter 笔记本是我的另一个最爱。我发现它们对于测试开源地图库非常有用，因为它们允许你只用几行代码就可以在笔记本上创建地图！GeoPandas 是我的首选开源 GIS 库。 
你还有什么想说的吗？ 
谢谢！ 
感谢您接受采访！ 
本周 PyDev:Claudia Regio 
 
 原文：https://www.blog.pythonlibrary.org/2021/01/18/pydev-of-the-week-claudia-regio/ 
 
本周，我们欢迎 Claudia Regio(@ Claudia Regio)成为我们本周的 PyDev！Claudia 是 Python 数据科学的项目经理，主要负责微软 Visual Studio 代码中的 Python 笔记本。她还在微软的开发博客上写博客。 
让我们花些时间去更好地了解克劳迪娅！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我来自意大利，4 岁的时候搬到了大西雅图地区。从小到大，我生活和呼吸壁球，如果不是因为 COVID，我仍然会！我一直是并将永远是一个超级数学书呆子，现在已经辅导数学超过 10 年了。我就读于华盛顿大学，主修应用物理学，辅修综合数学和应用数学，同时还是德尔塔·泽塔女生联谊会和 UW 男子壁球队的成员。 
毕业后，我在 T-Mobile 担任系统网络架构工程师时，从华盛顿大学获得了数据科学证书，以提高我的数据分析和数据科学技能。 
在担任该角色两年后，我过渡到微软的项目经理，负责 VS 代码中的 Python 扩展，专注于数据科学和人工智能组件和功能的开发。 
你为什么开始使用 Python？ 
我的数据科学证书课程让我在 2017 年开始学习 Python。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我在大学期间学习了 Java，虽然我喜欢 Java 是一种强类型语言，但在数据科学方面，没有哪种语言能胜过 Python。 
你现在在做什么项目？ 
我目前正在管理 VS 代码中的 Python + Jupyter 扩展伙伴关系。虽然我们最近发布的 Jupyter 扩展在 VS Code Insiders 中为其他语言提供了 Jupyter 笔记本支持，但我重点关注这两个扩展的协作，以便为使用 Python 的数据科学家创建最佳的笔记本体验。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
Scikit-learn 将永远拥有我的心灵<3 你认为 Python 笔记本最好的特性是什么？ 
我们目前在 Python 笔记本中最好的特性包括变量浏览器、数据查看器和我个人最喜欢的 Gather(一个免费的 VS 代码扩展)。 
当在笔记本上进行实验和原型制作时，随着用户探索不同的方法，它通常会变得很忙。在最终达到期望的结果(例如数据的特定可视化)之后，用户将需要手动管理该特定流所涉及的单元。这项任务既费力又容易出错，让用户没有一个强大的方法来聚集相关的代码。在笔记本用户中常见的第二种情况是，软件工程师的任务是将现有的笔记本转换成生产就绪的脚本。提取不需要的导入、图表和输出的过程通常非常耗时，并且还会导致错误。Gather 是一个免费的 VS 代码扩展，它获取创建选定单元格的内容所需的所有相关和依赖代码，并将这些代码依赖项提取到一个新的笔记本或 Python 脚本中。这有助于节省数据科学家和开发人员大量的笔记本清理时间！ 
为什么 Python 开发人员和数据科学家应该使用 Visual Studio 代码而不是另一个编辑器？ 
VS Code 是一个免费的开源编辑器，拥有一系列扩展(来自微软和开源社区)、产品和功能，旨在为开发人员和数据科学家提供无缝体验。一些例子包括: 
Python (附带 Jupyter 扩展):包括智能感知、林挺、调试、代码导航、代码格式化、Jupyter 笔记本支持、重构、变量资源管理器、测试资源管理器、代码片段等功能！Pylance :语言服务器，用丰富的类型信息提升您的 Python 智能感知体验，帮助您更快地编写更好的代码。实时共享:让您能够与其他人实时协作编辑和调试，无论您使用的是什么编程语言或正在构建的应用程序类型。它允许你立即(并且安全地)共享你当前的项目，然后根据需要，共享调试会话、终端实例、本地主机 webapps、语音呼叫等等！Gather :一个代码清理工具，它使用静态分析技术来发现并复制所有用于生成单元格结果的相关代码到一个新的笔记本或脚本中。Python 安装程序的编码包:一个安装程序包，帮助学生和新程序员快速入门，安装 VS 代码，上面所有的扩展，以及 Python 和 numpy、pandas 等常用包。Azure Machine Learning :从 Visual Studio 代码接口使用Azure Machine Learning service轻松构建、训练和部署机器学习模型到云或边缘。超过 350 个社区在 VS 代码市场上贡献了 Python 相关的扩展！ 
正是这些扩展和开源社区之间建立的伙伴关系，以及开发部门始终为客户构建的思维模式，为开发人员和数据科学家在 VS 代码方面创造了无与伦比的体验。 
你还有什么想说的吗？ 
我要感谢与我一起工作的令人难以置信的团队(David Kutugata、Don Jayamanne、Ian Huff、Jim Griesmer、Joyce Er、Rich Chiodo、Rong Lu ),是他们让这款工具变得栩栩如生，我还要感谢所有与我们合作并帮助我们为数据科学家打造最佳工具的客户！ 
如果有人想提供任何额外的反馈、功能请求或对产品有所帮助，您可以在此提出！ 
克劳迪娅，谢谢你接受采访！ 
本周 PyDev:康纳·福斯特 
 
 原文：https://www.blog.pythonlibrary.org/2020/08/31/pydev-of-the-week-connor-ferster/ 
 
本周我们欢迎康纳·福斯特成为本周的 PyDev！Connor 是最近出现在 PythonBytes 播客上的handcals的作者。你可以在 Github 上看到更多康纳的作品。 
现在就让我们花点时间去了解一下康纳吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我今年 38 岁，31 岁回学校读土木技术，然后是土木工程。土木工程是建造任何与“文明”有关的东西的学科:水资源和管道、道路和交通、建筑和桥梁。我目前在加拿大做建筑方面的结构设计培训工程师。我的教育包括在学院和大学之间的一个特殊的转校年，在这一年里，除了其他事情，我还学习了 Matlab 编程。正是在那一年，我真正进入了编程，主要是作为一种创造性的发泄。我不得不说，用 Python 编程可能是我现在最大的爱好，但我也喜欢烹饪、做面包和阅读。最近一直在读乌苏拉·k·勒古因的《万物》。 
在回到大学之前，我也做过猎头、企业主、销售人员和音乐学校的辍学生(按时间倒序排列)。 
你为什么开始使用 Python？ 
我第一次听说 Python 是在大约 2009 年的一次家庭聚会上。那时，我有我的在线钱包业务，我已经雇佣了我朋友的公司来建立网站。他是 Ruby on Rails 的忠实粉丝，他让我对学习 Ruby 产生了兴趣。当我参加这个家庭聚会时，我遇到了一些其他的 web 开发人员，我告诉他们我对学习 Ruby on Rails 的兴趣。他们说，“噗噗！忘了露比吧。学 Python 吧！”他们向我展示了他们的一些 Python 项目，并谈论 Python 有多棒(虽然我不记得他们说了什么！).从那以后，我就知道了“Python”，只是作为一个名字，其他的就不多了。 
2018 年，我看到大学开设了一门叫做“系统化程序设计导论”的课程，这是非计算机专业的一种计算机科学课程。这个课程是教其他学科的人如何为他们自己的学科编写程序。当我看到这门课程将用 Python 来教授时，我就报名了，尽管它没有给我的学位学分。那门课改变了我的生活。我相信我在期末考试中得了 A+,那次考试全是手写的 Python 代码。 
从此，我爱上了 Python。我太爱 Python 了。 
你现在在做什么项目？ 
我有两个开源项目，我正在建设，称为“handcalcs”和“forallpeople”。Handcalcs 是一个为简单、单一目的而开发的库:用 Python 代码编写的计算可以显示出来，就像您手写的一样。Forallpeople 是 Python 的另一个单元库，但它是为遵循“约定优于配置”方法的快速和随意计算而设计的。 
除了这些项目，我还在公司内部从事一些专有项目，这些项目与工作流自动化以及它与结构设计过程的关系有关。我很感激我的公司允许我在空闲时间从事 handcalcs 和 forallpeople 开源项目。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
像许多皮托尼斯塔一样，我热爱帕特里布。它只是完美地处理了与路径相关的一切。 
在标准图书馆，我真的很重视收藏。不仅因为它提供的容器，还因为它编写代码的方式。有一天我打开了源代码，在仅仅 15 分钟的阅读时间里，我学到了很多关于 Python 的知识。我认为 shapely 很棒。能够执行 2D 几何查询和比较是非常强大的。 Handcalcs 建立在 pyparsing 之上，这是一个非常漂亮和有用的库。 对于绘图，我真的很欣赏 plotly，但我正在关注 ipyvolume，并希望在未来更多地使用它(当悬停在字形上时，我正在等待自定义弹出窗口)。 
我刚刚开始使用一个名为 sxl 的新库，用于在非常大的 Excel 文件中延迟加载单个表和表范围。这是一个简单的库，但它运行得非常好！您不必将整个工作簿加载到内存中来迭代文件中的某个区域或工作表。 
几个月来，我一直在赶时髦。我认为对于一些 Python 用户来说，它可能是一个革命性的库:为自己的 Python 脚本创建简单而快速的 web 界面的能力就像魔术一样。我不知道我是否可以在这里这么说，但是我真的很喜欢我自己的模块:handcalcs 和 forallpeople。我每天都依靠它们来提高工作效率，我无法想象没有它们的工作生活！它们对我太重要了。 
你的手提袋包的起源是什么？ 
我花了一个夏天作为学生为我现在的公司编程设计计算电子表格。这是一次很棒的经历，但是，知道如何使用 Python 之后，我开始对使用 Excel 感到非常沮丧。在工程领域，手写计算仍然是设计笔记的一种“黄金标准”，通常会成为项目法律历史的一部分。Excel 通常用于提高处理效率，但它并不透明:您在单元格中键入公式，1)公式仅包含单元格引用(除非您定义了单元格名称), 2)公式是隐藏的。为了便于说明，您必须在相邻的单元格中单独键入公式，这样您就可以将工作表作为报表或计算表打印出来。但是，您键入的公式只是文本:结果值与您键入的格式化公式没有关系，您必须更改两者才能更新。这使得维护很麻烦。 
我希望我输入的公式与我输入的公式的格式化表示相同——改变表示，结果就会改变。像 SMathStudio 和 MathCAD 这样的软件包可以做到这一点，但你必须手动在工作表中布置公式，最重要的是，这些软件包都不显示数字替换。这就是我们在学校被教导的:首先你写出公式，然后你写出数值的替代，然后你写出答案。这允许快速验证计算。此外，我想让计算和笔记自己格式化:我没有时间在格式化上做文章。 
然而，没有软件(我见过的)可以真正做到这一切，这就是我写 handcalcs 的原因。 
Handcalcs 是一个旨在与 Jupyter Lab/Notebooks 一起使用的库，前提很简单:在 Python 单元格中键入一个计算，并让它在 Latex 中自动呈现出来，就像您手写一样。Jupyter 会为您处理格式。使用您的浏览器(通过 HTML 导出)或通过 Latex(如果您已安装)打印到 PDF。此外，将 handcalcs 与任何其他库一起使用，以增强 Jupyter 的工作流程，例如 papermill，它允许您运行参数化笔记本。 
主要的 handcalcs 接口是通过“% % render”Jupyter cell magic 实现的，但我也编写了一个装饰器接口，您可以将它放在任何计算函数上，以便将函数源呈现给 Jupyter 或 Streamlit。这个 decorator 接口非常强大，因为它允许您编写小的常用计算“库模块”,您可以将其导入到 Juptyer 笔记本中，而无需重新键入它们。我仍然在想办法使用装饰界面！ 
为什么人们要用你的包而不用乳胶之类的东西？ 
啊！当你使用手钙时，你实际上是在使用乳胶！Handcalcs 将为您呈现您的计算结果，因此您不需要了解 LaTex。然而，使用%%tex cell magic，您还可以使用 handcalcs 来生成原始的 LaTex 代码，您可以在自己的代码中使用这些代码。tex 文档。LaTex，更确切地说是 Mathjax 和 Katex，使手动计算成为可能。这样，乳胶就像是通量电容器。 
但是如果我们将 handcalcs 与 Excel、SMathStudio(这是非常好的)和 MathCAD 进行比较，那么除了我之前提供的所有理由之外，我想说拥有和使用自由软件真的非常重要。Python 是自由软件，Jupyter 是自由软件；LaTex、Mathjax、Katex 是自由软件；而 handcalcs 是自由软件。我认为自由软件领域代表了一个更大的世界的缩影，我希望我们可以创造一个更大的世界，在这个世界里，人们的创造性天赋被给予，以提高所有地方的所有人。当未来的人们使用你的作品并以此为基础时，这可能是一种永生的方式。 
康纳，谢谢你接受采访！ 
本周 PyDev:库珀·利斯 
 
 原文：https://www.blog.pythonlibrary.org/2021/05/24/pydev-of-the-week-cooper-lees/ 
 
本周我们欢迎库珀·里斯(@库珀·里斯)成为我们本周的 PyDev！库珀对 Python 编程语言做出了贡献。他也是 bandersnatch 和 black 等的维护者。 
你可以在库珀的网站或 T2 的 GitHub 网站上看到他还在做什么。让我们花些时间更好地了解库珀！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我是一个重新定居的澳大利亚人，住在美国加利福尼亚州的南太浩湖。我现在受雇于脸书公司，已经在那里工作了 8 年多。总体来说，我喜欢运动，板球+澳大利亚规则足球是我最喜欢的运动，既可以观看也可以玩。我也滑雪和滑雪板(住在山里有帮助)。夏天，我喜欢公路自行车赛。 
我在澳大利亚新南威尔士州伍伦贡大学学习“互联网科学”。它原本是一个新的基于互联网的计算机科学/电子工程学位，我相信它已经退休了。从大学开始，我参加了许多 Cisco + Juniper 网络课程和考试+大量的自学。 
你为什么开始使用 Python？ 
我在澳大利亚核科技组织的第一份工作开始了我有意义的 python 生活。一位同事告诉我用 Python 编写更多“文明”的代码，而不是我为了完成系统管理任务而拼凑的 perl。我的第一个有意义的 Python 代码是 Python 2.4 中用于 Solaris 10 邮件中继主机的 LDAP to Sendmail 别名文件生成器。我从未体验过< 2.4。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我懂一点 C，Perl，还有围棋。但是我真的很努力地学习了 Python，而且学得相当好。到目前为止，Python 是我最喜欢的语言，因为我的大多数工作负载都需要为 asyncio 编写代码，所以线程池和进程池允许我完成工作。 
你现在在做什么项目？ 
在 OSS 世界中，我目前帮助维护以下软件包: 
banders natch-Python PyPI PEP X 镜像软件黑色 -固执己见的 AST 安全型 Python 代码格式flake 8-bugbear-AST flake 8 插件发现不良代码气味 
在我的日常工作中，我是脸书的一名生产工程师，负责我们内部的路由协议守护进程，这些守护进程运行我们自己的 FBOSS 交换机。我们开发和维护的主要软件是: 
开/关内部 BGP 守护进程服务器 VIP 注入(通过 thrift + BGP) 
是我的主要项目。我还在帮助消除我们数据中心网络中剩余的传统 IPv4 的长尾效应。我们正在慢慢地把它推到我们的边缘。99.我们 99%的内部流量都是 IPv6。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
核心库:偏向于我所贡献的那些库:pathlib + venv 
第三方: 
点击-使用子命令+颜色输出等轻松剪辑。aioprometheus:我喜欢度量，所以我喜欢把普罗米修斯的出口放在我写的所有东西上aiohttp:运行服务器或客户端到基于 http 的服务的最佳异步 http 库uvloop:加速 asyncio 程序的最好方法是使用基于 C libuv 的快速事件循环 
你还有什么想说的吗？ 
贡献开源。从分类，写代码和 CI，都有帮助！概念证明公关是值得的，让你的观点。自动消除你的痛苦。总是添加单元测试！ 
库珀，谢谢你接受采访！ 
本周 PyDev:科里·本菲尔德 
 
 原文：https://www.blog.pythonlibrary.org/2016/08/01/pydev-of-the-week-cory-benfield/ 
 
本周，我们欢迎科里·本菲尔德( @lukasaoz )成为我们本周的 PyDev！Cory 是 Python 语言的核心开发者，尤其是 urllib3，同时也是请求包的核心开发者。他还是 Hyper 项目的主要维护者，该项目是一组为 Python 项目提供 HTTP/2 功能的相关项目。让我们花些时间来更好地了解科里吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我在苏格兰的大学里学习物理，最初是想在专业物理的更实际的一端结束自己的职业生涯:也许做一些像医学物理这样的事情。在我获得学位之前，我从来没有写过程序。然而，很长一段时间以来，我一直是一名电脑“超级用户”:我拥有个人电脑和苹果电脑，熟悉 Linux、Windows 和 OS X，从 15 岁起我就开始组装个人电脑。因此，当我发现自己在一个计算物理模块中盯着 Mathematica 屏幕时，我的大脑中突然灵光一闪，我意识到这是一件我可以真正快乐地做的事情。因此，我获得了物理学硕士学位，并获得了一份为电信行业编写软件的工作。 
当我不写软件的时候，我喜欢做很多不同的事情来放松。我玩电子游戏(像现在世界上大多数人一样)，但我喜欢看伟大的电影和电视节目。我也非常喜欢烹饪、阅读和与人相处。有时我甚至会写:我不擅长定期写作，但我认为我的写作还可以！ 
你为什么开始使用 Python？ 
我刚开始在大学里用 Mathematica 做物理工作，虽然 Mathematica 很棒，但它并不是构建实用程序的有用工具。所以我四处寻找其他语言，在我看来，使用 C 和 Perl 没多久，Python 就成了世界上最显而易见的东西。所以我用 Python 写了一些网络抓取工具，这将我带入了 Python 开源社区。我从未回头！ 
你还知道哪些编程语言，哪种是你最喜欢的？ 
我是那种喜欢拿起新语言来玩的人，所以“知道”在这里有点棘手。我会说我最喜欢 Python，然后按照我对这种语言的熟悉程度大致降序排列:C、Go、Rust、Javascript、Swift、C#、Java、Objective C。我对 Haskell 等强函数性语言略有涉猎，但不会说我“知道”Haskell，因为我还没有在愤怒中使用过它。我也喜欢 lisp，尽管我也没有写过多少 Lisp 代码:也就是说，Hy 显然是我最喜欢的 Lisp，因为它让我可以使用我已经习惯的所有 Python 库！ 
你现在在做什么项目？ 现在我很分散，但是我主要关注 Python 中的 HTTP 和 HTTP/2。这意味着我的时间大致分布在维护几个流行的 Python 库上:Requests 和 urllib3 是最值得注意的，也是消耗我时间最多的。除此之外，我还维护了 Python 中唯一的 HTTP/2 库，包括 hyper-h2，它是 Python 中许多 HTTP/2 实现的基础。最后，我顺便参与了许多其他 OSS Python 项目，如 Python Cryptographic Authority 和 Twisted。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
当然不包括我自己！ 
在 Python 标准库中，我非常喜欢 itertools:生成器对我有很大的吸引力，itertools 使得以一种非常灵活的方式使用它们成为可能。在标准库之外，我认为 CFFI 很棒:不管你的 VM 是如何实现的，都能调用 C 语言是非常强大的，它消除了使用 PyPy 的最大障碍之一，我认为这是 Python 社区最大的资产之一。 
作为一门编程语言，你认为 Python 将何去何从？ 
在 Python 社区中，担心像 Go 这样的语言是否会吃掉我们的午餐是很常见的。我不相信那种说辞。Python 是一门伟大的语言:它简洁、易读，当与 PyPy 等工具结合使用时，对于大多数用例来说，它的速度已经足够快了。 
就目前而言，我看到 Python 正在整合，并构建出允许 Python 与其他语言更紧密集成的工具。转向 CFFI 而不是编写显式 C 扩展是其中的一个重要部分，因为它允许 Python 程序员将 Python(例如 PyPy)的精华与其他语言的精华结合起来(通过调用编译后的 dll，而不用担心所使用的具体 Python 实现)。未来的应用程序将越来越多地涉及较小的组件之间的调用，要么通过 HTTP APIs 之类的 RPC 机制，要么通过使用 CFFI 之类的东西的直接函数调用。Python 非常适合成为那个环境的一部分。 
你对当前的 Python 程序员市场有什么看法？ 
据我所知，Python 程序员非常容易被雇佣。Python 是一种成熟的语言，许多公司都愿意将他们的业务押在这种语言上，这给了 Python 程序员在这个行业中很大的权力和灵活性。对于任何求职者来说，Python 技能都是一大优势。 
你还有什么想说的吗？ 
只有当人们对参与开源软件感兴趣时，Python 几乎肯定是一种可以参与的语言:有许多伟大的项目正在进行，他们希望社区做出更多的贡献，参与进来是一种回报的好方式。 
感谢您接受采访！ 
本周 PyDev:克雷格·布鲁斯 
 
 原文：https://www.blog.pythonlibrary.org/2015/11/02/pydev-of-the-week-craig-bruce/ 
 
本周，我们欢迎克雷格·布鲁斯(@克雷格·布鲁斯)成为我们本周的 PyDev。让我们看看他说了什么！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我的背景是计算化学和化学信息学。真正的意思是，我被训练在早期药物发现的小领域工作，而且基本上没有听说过，在那里计算机被用来帮助指导药物设计。在药物上市之前，这项工作通常需要十年时间。 
近年来，从脚本到 web 堆栈，Python 已经成为我的编程语言选择。近年来，科学 Python 堆栈有了巨大的发展，使得专注于特定的研究变得更加容易。我几乎没有接受过正式的 IT/CS 教育，但通过我的教育和之前在编程和系统管理方面的角色，我学到了很多。 
我的爱好包括徒步旅行、滑雪和遛狗，我住在新墨西哥州，在那里我们享受令人惊叹的户外运动。 
你为什么开始使用 Python？ 
这是一份新工作的要求，主要是运行和开发一个 Django 支持的网站。除了网站之外，我需要执行的科学任务使用了一个 C++工具包，该工具包具有 Python 绑定，因此 Python 对于这个角色的每个编程方面都非常有用。 
你还知道哪些编程语言，你最喜欢哪一种？ 
Perl、Java 和 JavaScript，但我更愿意用 Python 和 Bash 编写，因为这两者的结合允许我部署和运行我需要的大多数东西。 
你现在在做什么项目？ 
我正在开发一个基于云的产品，这是一个药物研发平台，供制药公司使用。它是用 Python 写的，有一个 Django web app 和 API。我的工作主要是在后端和 DevOps 方面。 
工作之余，我是 Django Events Foundation North America(DEFNA)的三位联合创始人之一，该基金会负责 DjangoCon US 的运营，所以我作为财务主管的角色让我很忙。DjangoCon US 是本月早些时候举办的，我很高兴我们收到了积极的反馈。2016 年的准备工作已经开始，请务必关注我们的@djangocon，了解最新动态！ 
哪些 Python 库是你最喜欢的(核心或第三方)？ Django 继续成为人们的最爱，因为它是如此的多才多艺和成熟。如果我需要其他东西，我几乎可以保证有第三方应用程序来帮助。我目前的项目大量使用了 AWS，所以对流层(https://github.com/cloudtools/troposphere)在使云形成模板变得容易方面是无价的(例如，不写任何 JSON，这是我最讨厌的)。 
你还有什么想说的吗？ 
感谢邀请参加本系列！ 
谢谢！ 
一周的最后 10 个 PyDevs 
海梅·费尔南德斯·德尔罗瑞安·米切尔卡罗尔心甘情愿迈克尔·福格曼特雷西·奥斯本特里匈奴克里斯托弗·克拉克马修·努祖姆肯尼斯·洛夫梅拉妮·克拉奇菲尔德 
本周 PyDev:Cris Medina 
 
 原文：https://www.blog.pythonlibrary.org/2019/07/22/pydev-of-the-week-cris-medina/ 
 
本周我们欢迎 Cris Medina(@ tryexceppass)成为我们的本周 PyDev！克里斯是广受欢迎的trypexceptspass博客的作者。他也是 sofi 和 korv 的维护者。你可以在 Github 上赶上克里斯的其他项目。让我们花些时间去更好地了解他吧！ 
 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我出生在多米尼加共和国。我在那里读完高中，去了波多黎各学习计算机工程，专攻硬件。但从我记事起，我就一直在以某种形式编写软件。我爸爸把 IBM System 360 Basic 作为我的第一语言介绍给我。真不敢相信 
我职业生涯的大部分时间(现在已经持续了 17 年)都花在了测试工程上，以及开发执行这些测试和维护它们的基础设施所需的所有硬件和软件工具上。其余的时间，我担任正式的软件工程角色。 
我喜欢花一些空闲时间听音乐。我妈妈是一名音乐老师，她很早就让我开始学习钢琴。虽然随着年龄的增长，我开始接触弦乐器。今天，我主要弹古典吉他，但我也拥有几种类型的吉他，并涉足其他弦乐器。 
我也喜欢烹饪。我的家人来自地中海的各个地方，所以我的大部分食物都有那种闪光。烹饪让我想起了一点开发过程:你对自己想要什么有一些想法，你按照一套基本的说明去做，但通常你会加入一些额外的味道，让它成为你的，这可能需要几次迭代才能做到恰到好处。 
你为什么开始使用 Python？ 
大约 10 年前，当我编写一些后端代码来支持我自己的 Java 应用程序时，我尝试了一下。当时办公室里有很多关于 Python 和 Perl 的争论，但是我已经知道我不喜欢 Perl 的语法，因为我试图阅读我同事写的 ode。所以我决定给 Python 一个机会，从此再也没有回头。 
真正的问题是:为什么我不早点开始！ 
你还知道哪些编程语言，你最喜欢哪一种？ 
除了 Python，我还使用 C++ / C 编写驱动程序，用于嵌入式系统开发，在一些嵌入式系统中汇编，Visual Basic 和 Java 用于商业应用程序，加上通常的 Bash 脚本，JavaScript 用于 web 开发，以及一些其他定制的东西。 
我也尝试过 Erlang、Go 和 Rust，但是除了训练示例之外，还没有一个用例支持我。 
我肯定最喜欢 Python。是最实用的。语法接近我的想法，不需要疯狂的样板代码。我很少需要花时间去弄清楚如何使用一个给定的结构。您还可以在几分钟内建立新的软件，PyPI 中大量可用的模块，加上内置的库，几乎可以处理任何用例。它可以很容易地与 C 集成，因此也可以解决人们对动态语言的任何性能问题。 
你现在在做什么项目？ 
在工作中，我只是花了几年时间作为 HPE CloudVolumes 的首席开发人员，但我现在正转变回测试工具和自动化基础设施的角色。 
在家里，我大部分时间都花在我的 tryexceptpass.org 博客上，给 sofi 和 korv 添加新功能。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
我用 docopt 为 CLI 做了很多工作。我喜欢你定义界面的方式，因为它是自文档化和功能化的。 
我喜欢 sanic for REST API dev，因为它易于使用，包含电池，并且是异步的。 
我将 asyncssh 用于很多事情，包括使用 ssh 密钥进行身份验证的加密套接字，以及需要远程 ssh 的自动化。因为它支持 asyncio，所以它确实有助于并行化。 
还有 websockets，我用它实现了几个资源管理系统、GUI 框架(比如 sofi)和通知系统。 
你创建 tryexceptpass 网站的动机是什么？ 
当我意识到我是一个暗物质开发者时，我知道我需要做些什么。有很多很酷的东西与我以前构建的系统非常相似，但我没有办法分享它们，也不被允许。在某些情况下，我已经解决了一些公共图书馆甚至没有意识到存在的问题。 
直到我与管理层讨论了每个开源提交所需的法律监督，我才意识到我需要改变策略。即使我在加入公司之前就拥有代码。所以我说:如果我不能贡献代码，也许我可以分享一些关于如何使用已经存在的代码的小要点。尽管我的第一篇文章是关于开源贡献应该如何被认为是职业发展的一部分并被所有公司所鼓励。😉 
你对未来想创建一个成功网站的内容作者有什么建议吗？ 
开始写吧。如果你还在想，那你就做错了！对我最有效的是一个老招数:作为意识流来写，不编辑，不改错，只前进。当你对一个主题倾吐完你的想法后，才回去编辑或整理。 
你是如何成为 sofi 和 korv 的维护者的？ 
我构建 sofi 是因为我厌倦了编写相同的 HTML 和 JavaScript 代码来构建基于 Bootstrap 的网站。在这样做的过程中，我意识到我可以超越我的目标，尝试将它变成一个可重用的框架，这样我就可以从 Python 中驱动所有的网站行为，而永远不必用 JavaScript 思考。在这样做的过程中，我还注意到我可以使用这个框架的后端来与任何其他图形用户界面接口，比如游戏引擎，所以我也花了时间在 Unity3D 客户端上。最终的目标是成为 3D 图形用户界面的引导程序，在这里你需要知道一些关于引擎的知识来为你的应用程序建立一个 3D 界面。 
我之所以把它们放在一起，是因为我认为在 SSH 套接字上构建一个带有基于密钥的认证的异步接口对于后端或特权连接有一定的价值。到目前为止，它对我很有效。 
你还有什么想说的吗？ 
我真的很喜欢 Python 社区。我遇到了一群来自世界各地的了不起的人，每个人都非常乐于助人、细心、真诚和有趣。所以我真的很喜欢鼓励人们继续参加活动，贡献代码、博客、开放空间或在会议、文档、问题上的演讲，无论你能做什么。 
事实上，如果你喜欢写作，想谈谈你最近面临的 python 挑战，或者你是如何解决一个特定问题的，请给我的 Twitter 帐户@ tryexceptpass 发送一条 DM。我正在接受网站上帖子的投稿。 
克里斯，谢谢你接受采访！ 
本周 PyDev:克里斯蒂·弗拉德 
 
 原文：https://www.blog.pythonlibrary.org/2020/05/25/pydev-of-the-week-cristi-vlad/ 
 
本周我们欢迎克里斯蒂·弗拉德(@克里斯蒂瓦拉 25 )成为我们本周的 PyDev！克里斯蒂在他的 Youtube 频道上用 Python 教授网络安全。他还写了一些书，并在他的博客上撰文。你也可以在那里看到他的书。 
让我们花些时间来更好地了解 Cristi！ 
 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我一直喜欢数字。拥有土木工程硕士学位，我决定在完成学业后放弃这个领域的一个很好的工作机会，去尝试计算机方面的东西。 
企业家精神和改善我的生理状况的结合对我有很大的吸引力。所以我开始研究如何提高我的身体和精神能力，我钻研了生物化学、人体解剖学和各种科学文献，最终我写了 7 本关于身体改善的书。 
带着与生俱来的好奇心，我总是试图自学计算机编程，但有几次都惨败。我尝试学习 JAVA，因为我也想戴上 Android 开发者的帽子。这是在 2011 年到 2015 年之间。 
我想给编程最后一次机会，如果我没有任何进展，我会完全放弃努力。 
你为什么开始使用 Python？ 
JAVA 绝对不是我要走的路。它又丑又没吸引力。所以我的下一个最佳选择必须是干净、直观、非常直截了当的。那就是 Python。那是 2015 年，Python 有一个由热情的开发人员组成的成熟社区。 
所以我用大量的资源轰炸自己来吸收 Python，非常强调实践。我使用 biopython 和其他库对基因组学和 Python 的结合进行了大量练习。那救了我。当谈到计算机编程时，我开始觉得我并不完全迟钝。 
你还知道哪些编程语言，你最喜欢哪一种？ 
Python 是迄今为止我最喜欢的语言。然而，随着我开始更好地掌握它，我也探索了其他概念，比如 Javascript、C++、C、Bash 和汇编。 
你现在在做什么项目？ 
我目前没有积极地、有目的地从事大型编程项目，但是我经常使用 Python 和 Bash 来编写自己的工具，用于渗透测试和 bug 搜索。 
我最近的两个项目是两个课程，帮助有抱负的和已建立的网络安全专业人员在日常工作中学习和使用 Python。 
用于渗透测试人员的 python:http://bit.ly/2I0sRkm 
我的 Youtube 频道是另一个正在进行的项目，我经常在那里教授 Python 在笔测试和 bug 赏金猎人中的应用。 
https://www.youtube.com/channel/UCXXXoi68Hv6caNLWfw7j8MQ 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
我会说 requests、beautifulsoup 和 socket 是我的前三名，主要是因为我在教学和实践中经常使用它们。当然，作为一名网络安全专业人员，我也喜欢与 scrapy 一起工作。 
图书馆太棒了！不要误解我。但是作为一个有抱负的极简主义者，我尽力做到最好。我不想从头开始重新发明轮子。这将是徒劳的。然而，只要有可能，我就尽量限制自己使用准系统。 
你是怎么用 Python 进入网络安全的？ 
这是一个自然的过程。在学习 Python 的同时，我也了解了 Kali Linux 和伦理黑客。那是一见钟情。 
您在网络安全方面使用的顶级工具有哪些？ 
打嗝套件、grep、cat 和 google。 
你还有什么想说的吗？ 
是的，Python 给了你超能力。学习如何用它来自动化你无聊的事情。从今天开始。 
克里斯蒂，非常感谢你接受采访！ 
本周 PyDev:克里斯蒂安·毛雷拉-弗雷德斯 
 
 原文：https://www.blog.pythonlibrary.org/2022/07/25/pydev-of-the-week-cristian-maureira-fredes/ 
 
本周，我们欢迎克里斯蒂安·毛雷拉-弗雷德斯( @cmaureir )成为我们本周的 PyDev！Cristián 是 Python 项目(又名 PySide6)的 Qt 的核心开发人员。你可以在 GitHub 或者 Cristián 的网站上追上 Cristián。 
让我们花些时间更好地了解克里斯蒂安！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我的名字是 Cristián Maureira-Fredes ，我来自智利圣安东尼奥，但自 2013 年以来我一直住在德国柏林。 
在学习计算机科学之后，我来到德国完成了我的硕士学位，并开始攻读天体物理学博士学位。完成学业后，我决定进入这个行业，并从 2018 年开始在 Qt 公司工作。 
在工作中，我目前的头衔是“R&D 高级经理”，这意味着我是几个团队的团队领导，一个专注于 Qt 框架的核心方面，另一个是 Qt 框架的官方 Python 绑定集，Python 项目的 Qt。 
我的大部分业余时间都花在了 Python 社区上，我帮助组织了在 Python 智利、 Python 西班牙以及一个名为Python en espa ol’的新全球社区的活动，该社区包括了所有说西班牙语的国家。 
你为什么开始使用 Python？ 
我很幸运地在我就读的大学的计算机实验室(LabComp)找到了一份学生工作，在那里我接触到了许多与系统管理员相关的任务，其中包括任务自动化。我开始用 Bash 编码，然后转向 Perl，在某个时候，有人提到了 Python，当时是 Python 2.5(我想)，但我一开始并不喜欢它，我对 Bash、Perl、Awk 和其他命令行语言和工具很熟悉。 
在 2009 年(Python 2.6…也许吧？)，我给了它第二次尝试，并开始更经常地使用它，从那时起，我一直是一个快乐的 Pythonista。 
你还知道哪些编程语言，你最喜欢哪一种？ 
在我的日常工作和学习中，我一直都在使用 C++，这是我非常喜欢的。在学会了用 C 编码之后，C++真的是一个飞跃，让我爱上了编程。在我学习期间，我同时使用了 C++和 Python，从那以后，我的心在它们之间分裂，所以我不能决定。然而，如果你问我关于“编程语言社区”的问题，我会毫不犹豫地选择 Python。 
你现在在做什么项目？ 
我是 Qt for Python 项目的维护者之一，所以这是我的主要职责，这很酷，因为这是一个开源项目，但我是有偿开发和领导的。 
除此之外，我真的很幸运成为了将官方 Python 文档翻译成西班牙语的团队的一员，现在我们正在等待 3.11 的发布，继续新的 Python 小版本。 
我做的其他事情，但不确定是否应该称之为项目，是我与社区互动的许多平台的机器人，如与 Github、通知、discord 任务自动化等的集成，会议或倡议的小静态网站，数据仪表板等。 
在社区方面，我目前正在与前面提到的社区一起组织活动、会议等等，老实说，这有时比编写软件项目的代码要多得多！但是我真的很喜欢这样做。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
PySide6 因为我一直在非常努力地研究它，并不时用它来创建讲座、研讨会、网络研讨会等等🙂 
除此之外，自从我第一次发现 NumPy 以来，它一直是一个令人惊叹的模块，它解决了我在学习期间遇到的一个真正的问题，并且大多数从 NumPy“继承”来的模块也很酷，比如熊猫。 
从核心…不确定我是否有喜欢的模块，但我确实喜欢不同内置对象的实现。在过去的几年里，我一直在谈论 CPython 的实现，但是很幽默，所以人们不会感到太害怕而不敢进入语言的核心。例如我的 2021 年欧洲 Python 演讲或者我在 PyConUS 2022 做的闪电演讲，让说英语的人感受一下非本地人学习 Python 的感受 
你是如何加入 Python 团队的？ 
前面说过，我刚读完博士就加入了 Qt 公司，甚至能找到一个需要 Python 和 C++的招聘广告都是一个惊喜，因为我知道 Qt 是纯 C++的。 
从大学时代就认识 Qt，这真是梦想成真。一年后，我甚至写了关于我的经历，因为这是我从来没有预料到的。 
你认为 Qt 对 Python 最好的 3 个特性是什么？ 
我在这里尽量不要有偏见，但大多数人都提到 PySide(Python 的 Qt)相当于 PyQt，但我们决定做的一个转变是，不要为 Python 用户“只做 Qt 绑定”,所以我们决定为 Python 生态系统添加更多东西，比如: 
1.启用 snake_case Qt API 而不是 camelCase 的选项，删除 setters 和 getters 并直接访问 Qt 属性。更多信息 
2.启用像 PyPy 这样的其他解释器。更多信息 
3.把 Qt-API 分解好，为 NumPy 数组、不透明容器提供 API，直接访问 C++对象，无需复制 
我认为 PySide 的这些方面确实有所不同，我们可以为 Python 用户提供更多的东西来开始使用 Qt。 
你还有什么想说的吗？ 
我要感谢你给我这个机会，我真的希望在 Python 社区分享我的一些生活经验可以激励更多的人加入他们的当地社区，为他们喜欢的项目做出贡献，甚至申请他们一直梦想工作的公司。 
克里斯汀，谢谢你接受采访！ 
本周 PyDev:西里尔·罗桑特 
 
 原文：https://www.blog.pythonlibrary.org/2016/02/22/pydev-of-the-week-cyrille-rossant/ 
 
本周，我们欢迎西里尔·罗桑(@西里尔·罗桑)成为我们的本周 PyDev！西里尔是 T2 VisPy T3 项目的核心开发人员之一。他还是几本 Python 书籍的作者，如学习 IPython 交互式计算和数据可视化和 IPython 交互式计算和可视化食谱。他的博客也很值得一试。让我们花些时间来更好地了解我们的同胞皮达尼斯塔！
能简单介绍一下自己吗(爱好、学历等): 
本周 PyDev:达明·阿维拉 
 
 原文：https://www.blog.pythonlibrary.org/2017/12/18/pydev-of-the-week-damian-avila/ 
 
本周我们欢迎达明·阿维拉成为我们的本周 PyDev！Damian 为 Anaconda 工作，这是一个 Python 和 R 的开源发行版，主要关注数据科学。他也是 Jupyter/IPython 幻灯片扩展《T2 崛起》的作者。你可以在 Github 上或者通过查看他的网站来感受一下达明在做什么。让我们花些时间来更好地了解我们的同胞 Pythonista！ 
 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
当然可以！ 
如今，我是一名软件开发人员，也是 Anaconda 公司的团队领导。在我之前的生活中，我作为生物化学家从阿根廷布宜诺斯艾利斯的联合国大学毕业。 
事实上，我目前与我的伴侣丹妮拉和我的猫 Bituinas 住在阿根廷的科尔多瓦，但每隔一周我都会去布宜诺斯艾利斯(700 公里外)，我 5 岁的儿子 Facundo 和他的母亲住在一起，我会和他呆几天。 
我的主要兴趣是数据科学、金融、数据可视化和 Jupyter/IPython 生态系统。在开源领域，我为几个项目做出了有意义的贡献，现在是热门项目的核心开发者，比如 Jupyter 、 Nikola 和 Bokeh 。 
我也开始了自己的项目，成为最受欢迎的项目。 
我在几个国家和国际会议上发表过演讲、教程和海报，我还撰写和教授过关于科学 Python 生态系统的教程。我是 Python Argentina、Scientific Python Argentina 和 PyData Argentina 社区的成员，也是 Jupyter 指导委员会的成员。 
有趣的事实是，我练习合气道已经好几年了，现在我正试图回到常规练习中来。 
你为什么开始使用 Python？ 
这个回答起来很有趣。几年前，大约 2011 年，我把玩金融模型作为一种爱好(奇怪的爱好，不是吗？).我发现自己需要创建比我使用的终端用户软件提供的模型更复杂的模型(IIRC，我使用的是一个叫 EasyReg 的软件)。然后，我告诉自己，你应该开始学习如何制作自己的模型了！！ 
我发现了一门关于“科学家的数值方法”的有趣课程，我们有 3 节 4 小时的入门课，内容是关于 3 种编程语言:Fortran、C++和…Python。我发现 Python 很容易上手(别忘了我是一名生物化学家，研究的是免疫系统的细胞，没有计算机，全是实验性的东西，没有编程的东西)。因此，我从零开始尝试用 Python 重新创建我的金融模型，并在 2011 年底参加了我的第一次阿根廷 PyCon，发表了一篇关于我的模型的演讲。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我一直在用其他语言编程，比如 JavaScript，CoffeeScript，还有一点 TypeScript。我经常使用 bash。我玩了很多 HTML 和 CSS。我学过一点 Fortran 和 C++。 
我最喜欢的还是 Python，我开始觉得 TypeScript 很有趣。 
你现在在做什么项目？ 
在我的工作中，我领导着一个团队在 Anaconda Enterprise Notebooks(Anaconda 企业笔记本)中工作，这是 Anaconda 平台 v4 的一个组件。在我的工作之外，在我的空闲时间，我通常为 Jupyter 生态系统做贡献。我也在 RISE 中工作过，这是我几年前写的一个流行的笔记本扩展，被很多人使用(可能比我知道的多，这很好！！). 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
我在这里有点偏见，但我相信 Jupyter 生态系统库是我最喜欢的，Jupyter 笔记本应用程序是首选。它完全是破坏性的，在多个领域都是有用的。我相信它演变成木星实验室会是一种“革命性的”。 
你创造 RISE 套餐的动机是什么？ 
我认为“展示”你的想法是很重要的，即使笔记本本身是一个可以实现这一目标的文件，它也不太适合在讲座或课堂上分享信息。幻灯片演示的概念在这种情况下有很多优势，我想把这些优势带到 Jupyter 的体验中。我的第一个想法是使用 nbconvert 从笔记本文档生成一个静态演示，这是几年前在那个包中实现的。 
但是，我也看到了拥有笔记本体验带给您的交互性的需求，这就是我开始 RISE 的原因，它本质上是笔记本的一个“slidy”视图，您可以在幻灯片中立即执行代码和实验内容，我认为这在当时非常强大，可以分享知识和见解。 
在维护这个项目的过程中，你学到的最重要的三件事是什么？ 
1)人是伟大的，我总是从我的工作中得到赞赏，看到许多人认为它有用，我感到非常有价值。 
2)你是免费给你时间，你应该关心你的用户，但是你的用户也应该关心你。不要让项目维护耗尽你的精力。如果有必要的话，抽出一些时间。 
3)当你和人们互动时，试着给他们提供食物。如果你给他们背景、反馈并讨论他们的想法，你会有一个新的贡献者。 
你还有什么想说的吗？ 
作为 Jupyter 社区的一员，我想鼓励你帮助我们！有多种方法可以帮助这个项目(不仅仅是编码！)，所以 ping 我们如果你有一些兴趣和时间。 
最后，正如你从我之前的回答中看到的，我经历过，你永远不知道生活会把你引向何方。谦虚学习…一切都会水到渠成。 
感谢您接受采访！ 
本周 PyDev:达米恩·乔治 
 
 原文：https://www.blog.pythonlibrary.org/2016/08/29/pydev-of-the-week-damien-george/ 
 
本周我们欢迎达米恩·乔治成为我们的本周派德夫！Damien 是 MicroPython 项目背后的人，这个项目允许你在微控制器上运行 Python 的一个版本。你可以在达米恩的网站或者访问他的 Github 页面了解更多关于他的信息。让我们花些时间来更好地了解我们的同胞 Pythonista！ 
 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我在澳大利亚的墨尔本长大，很早就开始玩电脑和电子产品。我年轻的时候有一台 Commodore 64，还记得从当地图书馆借的关于如何用汇编语言编程的书。实际上，它只是给你一堆“数据”语句来输入，但最终你可以做出一些非常酷的东西。高中时，我喜欢物理和数学，后来我上了大学，获得了科学和工程学位。我主修物理和计算机工程，然后攻读了物理学博士学位，之后我去了荷兰，最终去了英国，从事理论高能物理学的研究(额外维度、超对称、宇宙学等等)。在我作为一名物理学家的职业生涯中，我对编程和机器人技术保持着浓厚的兴趣，同时也做了很多兼职项目，包括一台自制的数控机床(见http://dpgeorge.net/cnc/)。 
你为什么开始使用 Python？ 
因为我需要一种既强大又允许快速开发的语言。我想我用 Python 做的第一个大项目是我和一个同事开发的 Paperscape 网站，它可视化了物理学方面的科学论文(见http://paperscape.org)。Python 在后端被广泛用于解析 TeX、LaTeX 和 PDF 文件，提取引用信息，以及维护论文的 SQL 数据库。 
你还知道哪些编程语言，你最喜欢哪一种？ 
这些年我用过很多语言，包括:很多架构的汇编语言，BASIC，Fortran(大部分是 77)，C，C++，Java，Haskell，Go，JavaScript。我非常喜欢 C，可以说它是我的最爱，但 C++也很好，现在是现代标准下更好的语言。我喜欢 Haskell，但不要把它用于任何严肃的事情。 
你现在在做什么项目？ 
我几乎把所有的时间都花在了开发 MicroPython 上(见https://micropython.org和https://github.com/micropython/micropython)。我想添加许多功能，进行一些优化，并开发新的平台来运行它。现在，我正专注于让 MicroPython 在 ESP8266 Wi-Fi 芯片上运行良好，这是我们今年早些时候运行的 Kickstarter 的一部分。这涉及到编程以及文档，教程和论坛管理。我也在帮助开发 MicroPython 的 BBC micro:bit 端口(见https://github.com/bbcmicrobit/micropython)。很多人以不同的方式为 MicroPython 项目做贡献，这很棒，有助于项目的发展。我期待看到 MicroPython 变得更大，并以新的有趣的方式使用。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
因为它们在 MicroPython 中的特殊需求，我喜欢 sys、os 和 struct 这样无聊的库。但是更有趣的是我们的“机器”模块，它以 Pythonic 的方式抽象了底层硬件。例如 GPIO 引脚、I2C 总线、模数转换器等等。在 BBC micro:bit 上有一个“microbit”模块，里面有很多有趣的东西，让孩子们可以轻松地对设备进行编程。 
作为一门编程语言，你认为 Python 将何去何从？ 
Python 不断发展并跟上了程序员的需求以及当前的技术，这是一件非常重要的事情，我认为这将使它在未来很长一段时间内成为五大编程语言之一。对不同语言的需求总是存在的，每种语言都有特定的需求，Python 将仍然是通用快速开发编程的最佳选择。 
你还有什么想说的吗？ 
我认为 Python 社区很棒。它友好、开放，有很多聪明人致力于让 Python 不仅成为一门伟大的语言，而且成为一个伟大的编程生态系统。更广泛地说，我认为开源社区和开源软件是当今软件和计算的基础部分，我很高兴能够为之做出贡献。 
感谢您接受采访！ 
本周 PyDev:丹·巴德 
 
 原文：https://www.blog.pythonlibrary.org/2017/03/06/pydev-of-the-week-dan-bader/ 
 
本周，我们欢迎丹·巴德( @ dbader_org )成为我们本周的 PyDev。丹每周做一期《Python 窍门》时事通讯/博客这是一份有趣的读物。他还写了一本关于 Python 技巧的书。我总是喜欢查看我的受访者的 Github 页面，看看他们喜欢写些什么，所以我鼓励你也去查看一下。让我们花点时间来了解更多关于 Dan 的信息！ 
 
你能告诉我们一些关于你自己的情况吗(爱好、教育等): 
大家好我的名字是丹·巴德，我是一个彻头彻尾的 Python 迷。自从我成功说服父母从当地报纸的分类广告上给我买了一台满是灰尘的旧 Commodore 64，我就一直痴迷于编程。而且从那以后我对写代码的热爱一直没有减弱。 
最终我获得了计算机科学的学士和硕士学位。我在全职岗位上工作了几年，现在我是一名独立的软件开发人员和顾问。 
我在德国出生并长大，但后来搬到了加拿大。我喜欢这两个国家——幸运的是，这些天我大部分时间都在远程工作，所以我有机会访问每个国家。我很高兴我找到了一个愿意忍受这些古怪行为的伴侣 
就爱好而言，我喜欢去健身房锻炼，烹饪素食(在温哥华唐人街买了我的第一口真正的铸铁炒锅，改变了我的生活，哈哈)，还有去远足。我也读了很多。总的来说，我发现让我最开心的是建筑材料。所以我试着去优化它。我时不时弹点吉他放松一下。 
你为什么开始使用 Python？ 
我相信“代码就是交流”一个好的程序员可以表达他或她的想法，这样机器就可以执行它们。但是一个伟大的程序员可以如此整齐地组织他的思想，以至于机器和人类可以毫不费力地理解它们。 
当然，有时候写超级简洁的代码来打动你的朋友和同事也不错。但我发现构建软件最大的动力是简化复杂的过程，并将其分解，以便机器可以执行它，同时平衡这一点与拥有一个可维护的软件的需要，其他程序员可以在未来改进和修改它。 
我发现我的哲学与 Python 这种语言及其令人惊叹的社区非常契合。很少有“牛仔/女牛仔程序员”的态度，更多的是一种务实的愿望，想把事情做好，同时把它们做好。我喜欢这样。 
在我的整个计算机科学教育中，我都涉猎了 Python。我想我第一次了解它是在我的一个朋友带了一本“X 天学会 Python”(或类似的东西)的小册子去滑雪旅行的时候。我立刻爱上了这种清晰的语法，它看起来就像是可执行的伪代码。我对 Python 和它的创造者了解得越多，我就越欣赏它。我非常尊重它的实用和简单的设计。 
我用 Python 写的第一个“严肃”的项目是我的学士论文需要的数据处理管道。它运行得如此之好，是如此令人愉快的体验，以至于我也开始在其他“严肃”的项目中使用 Python。从那时起，我试图找到在任何可能的地方使用 Python 的方法。 
你还知道哪些编程语言，你最喜欢哪一种？ 
嗯，我在我信赖的 Commodore 64 上开始学习 BASIC，然后在我有了第一台个人电脑后涉猎了其他语言(C 和 Pascal)。 
但是我一开始真的不能理解 C，所以我花了很多时间在 QBasic for DOS 上。出于某种原因，我一直对汇编语言非常感兴趣，我真的很想看看幕后发生了什么(想象一下，当我后来了解到 CPU 微代码时，我感到多么兴奋…).学习 x86 汇编并在其中编写几个程序，使得所有 C 语言的东西，如指针和内存管理“咔嚓”一声 c 基本上是一个带有奇怪语法 的宏汇编程序 
最终我发现了德尔福，它曾经是(或者现在仍然是？)是 Pascal 的 OOP 版本与优秀的 GUI 框架和图形编辑器的组合。我用它写了很多软件，比如小游戏和实用程序。 
德尔福简直太棒了。我记得我必须攒够钱才能邮购 Delphi 编译器和 IDE 的“完整版” 
Delphi 的标准库和它的 GUI 框架教会了我很多如何设计好的 API。与微软当时推出的 Win32 和 MFC 相比，Delphi 的方法看起来漂亮多了。我喜欢和德尔福一起工作。 
从那以后，我有一段时间痴迷于编写游戏引擎。令人着迷的是，这些游戏引擎几乎是小型操作系统，拥有自己的内存管理、文件系统抽象，甚至小型多线程脚本运行时。 
大约在那个时候，我第一次接触到互联网，我发现 Quake 1 的一些源代码在当时已经是开源的了。 
你可以从 id Software 的 FTP 服务器下载 Quake 的脚本语言编译器和游戏逻辑的源代码。我把那东西吃光了。最初我不知道发生了什么，但我不能放下这段代码。我把它打印出来，随身携带，并草草记下笔记…我从阅读约翰·卡马克的代码中学到了很多。他是我的英雄之一，我希望有一天能和他握手，感谢他对我这个程序员的影响。 
总之，我正在编写这些小的 OpenGL 渲染器、脚本系统和网络代码，我只是在这些兔子洞里越陷越深，而没有真正制作出一个可玩的游戏。 
在某个时候，我已经用自己的基于虚拟机的脚本语言和针对代码更改的热重新加载以及图形地图编辑器构建了几乎这个“游戏 SDK ”,但我想要构建的游戏(以及我向我的朋友承诺的游戏)几乎没有任何进展。 
这次经历教会了我很多关于实用设计和程序架构的知识。我太纠结于这一团乱麻了(尽管这很有趣)，以至于我没有在我说过要构建的东西上取得实际进展。我相信大多数开发者都熟悉这种效果 
我不想说得太冗长…因此，长话短说，在某个时候，我用 Java、Objective-C、JavaScript、CoffeeScript、Go 和 Swift 编写软件获得了报酬。在大学期间，我还涉猎了 Haskell 和一些更深奥的语言，如 Prolog 和 OCaml。 
现在让我们回到 Python: 
当然 Python 是我最喜欢的编程语言！我仍然记得我第一次用这些术语思考 Python 的时候: 
我被要求重写一个非常复杂的 Java web 应用程序，它是一个移动应用程序的 API 后端。我用 Python 和 Flask 和 SQLAlchemy 重建了整个系统。我花了大约一个月的时间来做这件事，并且我对所有事情都使用了测试优先的开发风格。 
整个经历是如此令人愉快，感觉如此“正确”,起初我都不敢相信。从那时起，我对 Python 的热爱从未消退 
你现在在做什么项目？ 
总的来说，我喜欢 Python 和编程。我想激励尽可能多的开发人员成为更好的工程师，并提高他们的技能。 
对我来说，编程是一种激情。我知道找到有同样感受的人有多难，尤其是当你在偏远地区长大的时候。因此，我试图在这个领域成为一股“向善的力量”。 
我一生中做过的几乎每一份技术工作都有一些教学元素。例如，我在大学期间辅导过一些编程课程，甚至做过一系列的介绍性讲座。 
或者，我会在我工作过的公司里，和工程团队一起做报告和回答问题&。在我工作的上一家公司，我开始了名为“清洁代码星期五”的每周活动。最终，我也开始作为首席开发人员管理其他开发人员，每周与他们进行一对一的交流。 
我总是发现这份工作的一部分非常有价值。我确实喜欢它。 
当我开始做越来越多的远程工作时，教学方面突然消失了。感觉不对劲… 
所以为了找回它，我逐渐建立了这些渠道，比如我的网站、我的每周 Python 时事通讯、我的 YouTube 频道和我的 Twitter 账户，在那里我谈论如何成为更好的 Python 开发者。 
这真是太有趣了！ 
你能以这种方式扩大自己的影响力，这真是太棒了…如果你能帮助 10 个开发人员在他们的工作上提高 10%,这已经产生了巨大的影响。 
这是线性增长的:训练自己在开发方面好 100 倍要困难得多。但是帮助 10，000 名开发人员在他们所做的事情上提高 1%呢？借助互联网的力量以及你通过文章和视频所能达到的范围，这是完全可能的。 
对我来说，这太棒了…所以，如果我能进入一个可以通过互联网帮助开发者谋生的方向，那就是我的最终目标。 
现在我正在写一本名为 Python 技巧的新书:这是一个令人敬畏的 Python 特性的自助餐。如果你已经学习 Python 有一段时间了，但是你仍然发现自己在想，我甚至不觉得我已经掌握了 Python 的皮毛，那么你应该去看看。 
一个稍微“疯狂”的副业是我和妻子刚刚推出的 Python 开发者赠品的品牌:nerd lettering . com:Mugs&Python 开发者的配件
哪些 Python 库是你最喜欢的(核心或第三方)？ 
我喜欢 Python 标准库中的集合模块。我获得的编程经验越多，我就越意识到通常最重要的事情是为你的数据选择正确的结构。一旦你用正确的数据结构表达了一个问题，解决方案通常会自然而然地随之而来。做一个大的重构总是感觉很棒，它把一些随着时间的推移而变得复杂的乱七八糟的东西换成一个清晰的数据结构。 
我最喜欢的第三方库是 pdb++ (一个改进的 pdb)和 bpython (另一个 Python REPL)。我猜它们更多的是开发工具，而不是你在自己的程序中用来依赖的东西。但是因为它们是如此伟大的工具，我可以在我参与的几乎所有项目中使用它们，所以我从中获得了很多价值。 
我真的很喜欢肯尼斯·雷兹在他的项目中所做的事情，比如请求或者克林特。我知道有些人认为“人类的 XYZ”在这一点上有些过头了，但 Kenneth 的东西和他设计 API 的方式至今仍是我的巨大灵感。我喜欢以用户为中心，帮助他们实现目标的库。 
作为一门编程语言，你认为 Python 将何去何从？ 
我们都会写 Python 3
键入提示将发挥重要作用，并打开 Python 以前不是最佳选择的领域，例如通过实现性能优化和使大型程序更容易维护(例如工具辅助重构) 
我确信 Python 将会扩大其在科学界的立足点。我的意思是，谁不想用一堆 R 代码来换取漂亮干净的 Python 呢？ 
当我还是个孩子的时候，我在德国长大，那时有一股关于“能和你一起成长的桌子”的热潮比如，你可以给你的孩子买一张可调节的桌子，这样他们就可以在整个教育生涯中使用它。 
Python 是一种很棒的“可调书桌”语言。 
虽然有微小但重要的区别，Python 也是一张漂亮得多的桌子。一个是你不会因为使用过去的小学而感到尴尬的，一个是你可以向你的女朋友/男朋友展示的。 
(好吧，让我们就此打住桌子的比喻。) 
基本上，我非常喜欢 Michael Kennedy 的观点，即 Python 是一种“全谱”语言。开始使用它很容易，但这不会妨碍你作为一名开发人员的成长，并利用它完成一些令人印象深刻的实际工作。 
你对当前的 Python 程序员市场有什么看法？ 
我认为今天是成为 Python 开发者的绝佳时机。根据搜索引擎趋势 Python 是目前世界上第二流行的编程语言。那是强大的。 
Python 开发人员的工作前景非常好。我每周都有好几次面试机会。从与招聘经理的交谈中，我相信对有 2-5 年经验的“中级”Python 开发人员有巨大的需求。公司非常需要这些人。 
实际上，这有点无耻，不久前我和另一个博客作者 Elena Ruchko 合作写了一篇文章来回答这个问题“Python 值得学习吗？” 
开发人员问我一些问题，他们说“看，我真的很喜欢 Python，但是我能指望以它为职业吗？Python 现在有什么市场？”这篇文章可以让我们对此有所了解。 
因此，如果你想了解更多细节，这篇文章将深入薪资数据和就业趋势，以衡量 Python 开发人员的市场需求。可以在这里找到:为什么要学 Python？下面是 8 个数据驱动的原因 
我们还包括了对行业专家的采访，所以我认为它为那些有兴趣深入挖掘的人提供了一个相当全面的概述。 
你还有什么想说的吗？ 
首先，亲爱的读者，恭喜你，如果你一路走到这里 
另外，谢谢 Mike 邀请我参与其中！你为 Python 社区所做的事情非常棒。我非常喜欢你在书中使用的“对话式”教学风格。对我来说，这比交给别人一本枯燥的技术参考资料更有吸引力。这是我试图在自己的作品中模仿的东西。 
哦，还有一件随机的事情:我相信每个开发人员都应该在某个时候阅读鲍伯·马丁的书《干净的代码》。说真的，如果你还没读过，现在就去读吧。这是我多年来一直推荐的一本书。 
非常感谢您接受采访！ 
本周 PyDev:戴恩·希拉德 
 
 原文：https://www.blog.pythonlibrary.org/2019/04/22/pydev-of-the-week-dane-hillard/ 
 
本周，我们欢迎戴恩·希拉德( @easyaspython )成为我们本周的 PyDev！戴恩是曼宁即将出版的新书《Python Pro 的实践》的作者。他也是 T4 的博客写手和网站开发者。让我们花点时间去了解一下戴恩吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我是一个有创造力的人，所以我的很多兴趣都在艺术和音乐上。我是一名竞技交际舞者，我是一名出版过的音乐家和摄影师。我为这些成就感到骄傲，但我被驱使去做这些事情，是为了个人成就感，而不是别的什么！我也喜欢和别人分享和讨论我学到的东西。当我有时间的时候我的下一个项目是开始探索饮食方式，这是一种通过书面历史探索食物及其文化影响的想法。我热爱烹饪(和一般的食物)已经很久了，我想更好地了解它的起源，我认为这是这一代人对整个行业要求更多的东西。应该很好玩！ 
你为什么开始使用 Python？ 
我喜欢用我的计算机工程技能来构建东西，不仅是为了工作，也是为了自己。很久以前，我曾用 PHP 为我的摄影业务写了一个网站，但我没有使用任何类型的框架，应用程序代码与前端代码混合在一起，很难管理。我决定尝试一个框架，在使用(并且不喜欢)Java Spring 一段时间后，我尝试了 Django。剩下的就是历史了！当时我开始使用 Python 做一些与工作相关的事情，发现它很好地适应了许多不同类型的任务，所以我继续使用它。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我相当经常地使用 JavaScript，尽管直到 jQuery 让位于反应式范式，我才真正开始喜欢它。我们现在经常使用 React 和 Vue，我很喜欢它用于客户端开发。我过去也使用过 Ruby，我发现它在某些方面很像 Python。我觉得我还是最喜欢 Python，但是坚持自己知道的东西很容易，对吧？我不介意学点铁锈或者快点走！我最初的背景主要是 C 和 C++，但我几乎不能管理我自己头脑中的内存，所以当我可以避免的时候，我不喜欢告诉计算机如何管理它的内存，但所有这些语言都有它们的位置。 
你现在在做什么项目？ 
在 ITHAKA 我们已经管理了一段时间的开源 Python REST 客户端 apiron 。我们刚刚发布了一个特性，在这个特性中我可以探索一些元编程，这非常棒。它最终减少了人们必须写的样板文件，这也是一个亮点。我还建立了一个新网站，作为一个投资组合，集中我的一些在线存在。它是用 Vue 写的，但这是我第一次有机会探索 vue-router 和其他几个库，以及一个用于写博客的无头 CMS。 
我大部分的空闲时间都花在了思考和写作我正在写的书上，这本书向软件开发新手介绍了一些在协作软件中很重要的概念，以 Python 为背景。我希望它能帮助刚毕业的人，转换学科的人，或者想用软件增加工作的人！这本书已经提前出版了，我们说话的时候，我还在不停地读新的章节。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
requests 库是更普遍的库之一，我们就是在它的基础上构建了 apiron。我已经开始使用 pytest 来代替 Python 的内置 unittest，我喜欢它简化现有测试的方式，同时也为用 fixtures 做更复杂的事情提供了工具。有一个很棒的包 zappa，用于将 Django 应用程序(或者任何基于 WSGI 的应用程序，我相信)部署到 AWS Lambda。如果您在 EC2 实例上花费太多，请仔细考虑！对于图像处理，枕头是很好的。我想尽快尝试一个叫 surprise 的东西，它可以帮助你建立类似于网飞或 Hulu 推荐电影的推荐系统。太多其他的名字在这里！ 
你是如何开始写书的？ 
我不知道大多数作者是如何做到的，但在我的案例中，出版商曼宁联系了我——可能是在看到我在网上写的博文之后。面对这样的机会，我很难判断自己是否真的准备好或者有资格写一本书，老实说，我仍然经常问自己。我试图把它框定为一个帮助别人的机会，所以即使我没有做出完美的东西，我希望我仍然能够说我做了那么多！ 
写这本书时你遇到了哪些挑战，你是如何克服的？ 
对我来说，找到时间并平衡它与其他优先事项是主要的斗争，正如我想象的许多作者一样。我前面提到的不确定性是另一个。让我感到惊讶的是，在编程环境中使用重载术语是多么容易；许多概念有相似的名称，许多英语单词对于未经训练的读者来说可能是模糊的！幸运的是，我的编辑将这些拒之门外，但是我经常出错！教书很辛苦。我发现减轻此类问题的最好方法是尽可能自动化。 
你还有什么想说的吗？ 
如果你正在考虑进入编程或写一本书或任何真正的东西，并且你很幸运有这样做的方法，那就去做吧！我发现我不知道自己对某样东西有什么感觉，直到我真正检查它，打开几个开关，发现它在引擎盖下是如何工作的。有时你会发现你并不像你想象的那样喜欢某样东西，但也许它揭示了你想探索的一些无关紧要的东西。最重要的部分是入门！ 
谢谢你接受采访，戴恩！ 
本周 PyDev:丹尼尔·加斯帕 
 
 原文：https://www.blog.pythonlibrary.org/2022/03/28/pydev-of-the-week-daniel-gaspar/ 
 
本周，我们欢迎丹尼尔·加斯帕成为我们的本周 PyDev！丹尼尔是《掌握 Flask Web 开发》的合著者。如果你想知道丹尼尔还在做什么，你应该看看他的 GitHub 简介。 
让我们花些时间来更好地了解丹尼尔吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我来自葡萄牙，住在里斯本附近，是两个漂亮聪明孩子的丈夫和父亲。 
喜欢弹吉他和做音乐。就像用我的原声乐器演奏和唱歌，或者用我的旧挡泥板和破折号创作原创歌曲。 
我也喜欢园艺和潜水。 
我很小的时候(大约 10 岁)就出于好奇开始编程，用 ASM 或 BASIC 在 ZX 频谱上制作自己的游戏。 
我有计算机科学的大学学位。 Flask-AppBuilder 的创建者，Mastering Flask Web Development的合著者，Apache Superset PMC 成员。 
自 2013 年以来，我一直是开源爱好者和贡献者。 
你为什么开始使用 Python？ 
我在很多自动化过程中使用 Perl，老实说，我讨厌它，python 显然是一门需要学习的新语言。Python 的可读性、灵活性和反射能力给我留下了深刻的印象。 
显然，大量可用的软件包使它在很短的时间内变得非常有吸引力和高效。 
然后开始用它进行 web 开发，先是用 Django，后来用 Flask。我非常喜欢这种语言在 python 3 中的发展方式，主要是类型注释。 
What other programming languages do you know and which is your favorite? 
In my teens self taught BASIC, Assembler and Pascal. Learned lots of interesting languages in college like C, C++, Smalltalk, Caml, PROLOG. 
Professionally worked with VS BASIC, PL/SQL, Java, Perl, Javascript, TypeScript, PHP, Ruby, Python, and even PROLOG for writing alert rules for an IBM system. 
Most interesting PROLOG loved the automatic backtracking, but I really enjoy Java, Python and TypeScript. 
What projects are you working on now? 
Currently working at Preset, a cloud SaaS for Apache Superset with the creator itself Max Beauchemin as the CEO. 
Recently I’ve been involved in extending Superset to support a multi-tenant configuration, so we are supporting multiple deploys that will serve multiple client contexts in a single-tenant database architecture. 
I’m actively working on Apache Superset, maintaining Flask-AppBuilder and a python elasticsearch SQLAlchemy dialect for Elasticsearch named elasticsearch-dbapi. 
Which Python libraries are your favorite (core or 3rd party)? 
Core, collections, itertools and functools are awesome. 3rd Party obviously Flask (great API), pytest (great fixtures), requests (really useful) and S"
PythonLibrary 博客中文翻译（十八）,2024-11-10,"原文：PythonLibrary Blog 
 协议：CC BY-NC-SA 4.0 
 
本周 PyDev:埃里克·斯诺 
 
 原文：https://www.blog.pythonlibrary.org/2018/04/02/pydev-of-the-week-eric-snow/ 
 
本周我们欢迎埃里克·斯诺(@ ericsnowcurrently)成为我们本周的 PyDev！Eric 是 Python 语言的核心开发人员。你可以查看他的博客，在那里他偶尔会发布他的 Github 简介，在那里你可以看到他最近在做什么。让我们花些时间来更好地了解 Eric！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
美国西部几乎是我一生的家，除了我上大学前在阿根廷的两年。我拥有杨百翰大学的计算机科学学士学位。我住在山里，喜欢和家人一起在户外度过时光。在我家周围，我有 17 棵苹果树，我照顾它们(并享受它们)。 
你为什么开始使用 Python？ 
我大学毕业后的第一份编程工作是在一个几乎完全用 Python 编写的系统上工作。我责怪我们杰出的范·林德伯格，因为他在我之前就在这个团队里，这个项目是他的主意。🙂 
你还知道哪些编程语言，你最喜欢哪一种？ 
除了 Python，我还广泛使用过 Go。除此之外，我在 Javascript、Java、Lua、Bash、Perl 和 PHP 方面也有一定的经验。我还尝试了几十种其他语言。我最喜欢的还是 Python。🙂 
你现在在做什么项目？ 
在过去的几年里，我的主要关注点是一个改进 Python 多核故事的项目。它包括 3 个主要部分: 
1.在 stdlib 模块中公开对子解释器的现有支持(参见 PEP 554) 2。改善解释器隔离 3。使 GIL 成为每个解释器的锁，而不是在所有解释器之间共享它 
除此之外，我还帮助过导入系统和 CPython 的运行时。当我完成多核项目后，我还有一些其他项目暂时搁置。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
在很大程度上，我坚持使用 stdlib。我最喜欢的包括 importlib(惊奇，惊喜)，inspect，itertools，collections，contextlib 和 unittest。总的来说，我被精心制作并映射到我大脑中的库所吸引。 
你是如何成为 Python 编程语言的核心开发人员的？ 
在开始我的第一份编程工作后，我天生的好奇心引导我逐渐探索 Python 语言特性的广度及其内部工作方式。几年后，我潜伏在邮件列表上，到 2010 年，我开始参与其中，特别是在进口签名列表上。我在 2011 年参加了(实际上是)我的第一次 PyCon，在那里我可以和几个已经和我一起工作过的核心开发人员(特别是 Nick Coghlan 和 Brett Cannon)面对面交流。我也参加了所有的短跑比赛。那次面对面的经历产生了巨大的影响。我第一次真正的代码贡献 是在 2011 年晚些时候，我参加了 2012 年 PyCon 的语言峰会，不久之后我的第一个 PEP 被接受了(PEP 421)。2012 年 9 月，我被授予了提交权限，在我能够腾出的时间里，我适度地使用了这些权限。 
你负责哪些模块？ 
我不能说我“拥有”任何，但确实帮助了 importlib。除此之外，我最感兴趣的是语言(例如语法和语义，类似语言参考)和 CPython 的运行时，例如嵌入、子解释器、启动。也就是说，我已经接触了其他几个模块，包括添加类型。SimpleNamespace 和 OrderedDict 的 C 实现。 
你还有什么想说的吗？ 
Python 社区非常棒，CPython 拥有我认为是任何主要开源项目中最受欢迎的核心团队之一。也就是说，参与其中仍然会令人望而生畏。我鼓励每个有兴趣的人加入进来，然后留下来。从核心导师邮件列表开始，到 PyCon 的 sprints。正如我自己的经验所表明的，贡献往往是逐渐增加的，而不是突然出现的。耐心点。要善良。找到你喜欢的事情，并专注于此。甚至不一定是核心 Python。有许多伟大的项目。最重要的是，抓住机会建立联系，享受新的友谊。这可能是我参与 Python 以来最大的收获。 
感谢您接受采访！ 
本周 PyDev:Erika Fille le gara 
 
 原文：https://www.blog.pythonlibrary.org/2018/12/03/pydev-of-the-week-erika-fille-legara/ 
 
本周，我们欢迎 Erika Fille le gara(@ efle gara)成为我们本周的 PyDev。Erika 是亚洲管理学院的教授和项目主管。她在菲律宾的 PyCon 大会上发表过演讲。你可以查看她的网站，看看她还在忙些什么，或者看看她在下面的讲话: 
https://www.youtube.com/embed/m7RDD8vFU6U?feature=oembed 
让我们花一些时间来更好地了解她！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
嗨，我是艾丽卡。我是训练有素的物理学家。我是你典型的研究生院(assoc。)教授/行政人员，她总是尽力在教学、研究和履行某些行政职责之间取得适当的平衡。目前，我从事的研究项目大多是行业驱动的。 
工作之余，我喜欢旅游和旅行摄影。然而，随着最近的职业转型，休闲旅游已经被最小化了。如今，我大部分空闲时间都在看书、听音乐，当然，还有看电视。我也喜欢周末时不时在高速公路上开车；它有助于大脑放松。我喜欢这样一个事实，在长途驾驶中，我可以不受干扰地听很棒的公路旅行播放列表。 
你为什么开始使用 Python？ 
我开始用 C++为我的本科研究写脚本。我的论文是关于复杂系统的研究；尤其是网络科学。我当时在实验室的一位同事向我介绍了 Python，当时他看到我用 C++编写非常非常长的脚本来构建复杂的网络模型。他向我展示了如何将我用 C++编写的大约 50 行(或更长)代码减少到只有几行，实际上用 python 编写的不到 10 行，这要感谢所有 Python 科学库的开发者和贡献者。从那以后，我再也没有回头。 
你还知道哪些编程语言，你最喜欢哪一种？ 
其他语言有 C/C++、R 和 MATLAB。我也熟悉 PHP。 
当然，我唯一喜欢的是 Python。 
你现在在做什么项目？ 
我目前正在从事几个研究项目，涉及城市系统建模、交通系统建模、社会系统中(mis/dis)信息传播的分析和建模，以及为位置分析建立预测模型。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
NumPy 和 SciPy，Matplotlib，PANDAS，scikit-learn，NetworkX，graph-tool，Keras，TensorFlow，H2Oai 
你还有什么想说的吗？ 
永远不要停止学习！这个世界，尤其是有了今天的技术，有如此多的东西可以提供。 
谢谢你接受采访，Erika！ 
本周 PyDev:伊森·史密斯 
 
 原文：https://www.blog.pythonlibrary.org/2020/08/24/pydev-of-the-week-ethan-smith/ 
 
本周我们欢迎 Ethan Smith ( @ethanhs )成为我们的本周 PyDev！Ethan 是 Mypy 项目的核心开发人员，该项目是 Python 的静态类型检查器。你可以在 Github 上看到伊森还参与了其他什么项目。 
让我们花一点时间来更好地了解 Ethan！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我出生并生活在旧金山湾区。我目前是加州大学伯克利分校的本科生，攻读电子工程和计算机科学学位。我对 Linux 和自由软件充满热情，所以我是开放计算设施的一名工作人员，这是一个基于自由软件为学生提供计算服务的校园组织。编程语言设计和量子计算是我最感兴趣的两个领域。我已经在 mypy 上工作了大约 3 年，和 mypy 团队一起工作是一次很棒的经历。我也不时尝试玩具编程语言。 
我目前在劳伦斯伯克利国家实验室工作，作为高级量子试验台的一部分，从事量子计算研究。我开发工具使量子计算机编程变得更容易。 
你为什么开始使用 Python？ 
我在高中时开始使用 Python，当时我需要在当地博物馆劳伦斯科学馆的暑期实习中使用它。我和一个实习同事一起研究一个库存系统，我们决定用 Python 来写。库存系统从未问世，但足以让我爱上 Python！那个夏天剩下的时间我继续学习 Python，再也没有回头！在那之后的几年里，我用 Python 为自己做了一些小项目，并尝试自定义窗口。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我会说我熟悉 bog 标准 C/C++/Java/C#/Ruby 和其他一些标准，比如 Rust 和 Scheme。我最近真的爱上了 Rust，我知道它被炒作了很多，但一旦你知道如何使用它，它就是一种乐趣。我还没有见过一种语言以同样的方式混合了干净的高级语义和极快的速度。高中时，我用 MIT 的 Scratch 向孩子们教授基本的编程概念，我一直认为它设计得很好。 
你现在在做什么项目？ 
我在很大程度上一直致力于量子编译器(这实际上比听起来要简单得多！).编译器产生最小或接近最小尺寸的量子程序。量子计算机执行的每个操作都存在高噪声，因此减少要执行的操作数量至关重要！我一直在使用 Rust 来扩展编译器，以便在更大更难的程序上工作得更快，这非常有趣。 
我也一直在尝试将 Python 编译成 WebAssembly 来进行确定性计算。这有很多应用，如可复制的 Jupyter 笔记本或实现更简单的分布式计算。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
这里我稍微作弊一下，选 pyo3，其实是一个 Rust 库，用于将 Rust 代码绑定到 Python 上。我最喜欢 Python 的一点是，您可以编写到快速本机代码的绑定，并享受 Python 的高度简单性。这种方法在 numpy 和 scipy 中有优先权。因为我最近很喜欢写 Rust，pyo3 意味着我可以同时使用我最喜欢的两种语言！ 
你是如何参与 Mypy 项目的？ 
我在 2016 年开始使用 mypy，当时我正在处理一个个人项目。我的主要开发机器运行 Windows，当我运行 mypy 时，它坏了！我熟悉了 mypy 的工作方式，然后修复了这个 bug。然后我决定我想帮忙，因为这似乎是一个迷人的项目。所以我开始帮忙，做一个讨厌的人，在 bug 追踪器上帮助分类问题，最终修复更多的 bug 并实现更好的分析。 
你最喜欢 Mypy 的什么功能？ 
我真的很喜欢 mypy 守护进程。我记得当我第一次将它移植到 Windows 上运行时，我对它在检查大型项目时的速度感到震惊。移植它是一个有趣的挑战，因为 Windows 并不像 Unix 那样真正支持守护进程，但是我离题了。展望未来，我对 PEP 612 https://www.python.org/dev/peps/pep-0612/,的实现感到兴奋，它将使输入装饰符和更高阶的函数变得容易得多。 
伊森，谢谢你接受采访！ 
本周 PyDev:埃齐奥·梅洛蒂 
 
 原文：https://www.blog.pythonlibrary.org/2016/03/28/pydev-of-the-week-ezio-melotti/ 
 
本周，我们欢迎埃齐奥·梅洛蒂成为我们的本周 PyDev！埃齐奥是 Python 语言的核心开发者。你可以通过他的比特桶页面了解他在做什么。让我们花点时间来了解一下他吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
目前我住在芬兰，在那里我学习并毕业于信息技术专业。当我毕业的时候，我教了一门关于 Python 编程的大学课程，现在我作为一名开发人员/教育者远程工作。 
自 2009 年以来，我一直是 CPython 核心开发人员，并且是 bug 跟踪器和 html 包的维护者。我花了很多空闲时间以不同的方式改进 Python，包括在 stdlib、文档、测试、基础设施方面的工作，指导 Google Summer of Code，组织 sprints，以及在会议上做演讲。 
我的爱好包括旅游、学习新语言(汉语和西班牙语)和野生动物摄影。 
你为什么开始使用 Python？ 
当我还在高中学习 C++的时候，Python 开始变得越来越流行，所以有一天我决定学习它。它的强大、简单和多功能性给我留下了深刻的印象。从那以后，我已经能够在我的大多数项目中使用 Python 了。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我的第一门语言是 Pascal，之后是 C 和 x86 汇编。在学习函数式编程的时候，我学了 Scheme，然后是 Haskell。当我刚开始从事网络工作时，我还必须学习 PHP 和 JavaScript。现在我主要使用 Python，但偶尔我仍然使用 C(当我需要性能时)、JavaScript(在客户端)和其他语言。猜猜哪个是我的最爱应该不难🙂 
你现在在做什么项目？ 
对于 Python，我目前正准备为即将到来的向 Git/GitHub (PEP 512)的迁移更新跟踪器。我还与 OeWF(奥地利空间论坛)和其他组织合作了一个科学项目，该项目旨在提供一个身临其境的火星虚拟现实模拟。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
有这么多很棒的图书馆，很难挑出最喜欢的。标准库已经涵盖了广泛的用例，当它不够用时，您几乎总能在 PyPI 上找到一个有效的替代品。 
你还有什么想说的吗？ 
为 Python 做贡献比您想象的要容易。如果你想让你最喜欢的语言变得更好，请通过 http://pythonmentors.com/和 https://mail.python.org/mailman/listinfo/core-mentorship的核心导师邮件列表联系我们。 
感谢您接受采访！ 
本周 PyDev:法比奥·扎德罗兹尼 
 
 原文：https://www.blog.pythonlibrary.org/2015/12/28/pydev-of-the-week-fabio-zadrozny/ 
 
本周我们欢迎法比奥·扎德罗兹尼(@法比奥夫兹)成为我们本周的 PyDev。他是 PyDev 项目的幕后推手，这是一个用于 Eclipse 的 Python IDE。法比奥还参与了许多其他项目，如果你查看他的 github 简介，你会发现这一点。让我们花些时间更好地了解他。 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
当然可以…我是一名软件工程师，拥有计算机科学学士学位(2002 年毕业)。我住在巴西的弗洛里亚纳波利斯。除了编程，我真的很喜欢打乒乓球(我十几岁的时候几乎就开始打职业乒乓球了，现在这是一个很好的爱好)。尽管如此，我现在除了工作之外的主要职业是抚养一个 6 个月大和一个 6 岁大的女儿。 
你为什么开始使用 Python？ 
我在 2003 年开始使用 Python，在 http://www.esss.com.br ESSS()从事科学计算项目——当时，我对 Python 的主要不满是没有一个合适的 IDE，所以， 这些年来，我在 PyDev(http://pydev.org)上工作了很长时间，以获得一个我喜欢的工作环境——有点讽刺的是，为了获得一个很棒的 Python 工作环境，我在 Java 上工作了很长时间——尽管我也参与了许多 Python 项目，并且肯定在这里吃了自己的狗粮！ 
你还知道哪些编程语言，你最喜欢哪一种？ 
我研究的主要语言是 Python 和 Java——所以，在 Eclipse 中开发像 PyDev 这样的 Python IDE 对我来说很自然😉 
除此之外，我已经用 C/C++完成了我的一部分工作，最近我开始用 Javascript 做一些项目(我也尝试过一些其他语言，因为我真的很喜欢尝试新的语言，但这些是我目前使用最多的语言)。 
Python 成为了我最喜欢的语言，因为它非常有表现力，并且使程序非常简洁。除了 Python 之外，我要说 Java 获得了第二名，尽管它的优点与 Python 非常不同，并且 ide 等工具发挥了更大的作用，因此，您必须掌握 IDE 才能令人愉快(没有 IDE 编写 Java 代码肯定不会令人愉快，而在 Python 中这是可行的-尽管 IMHO，PyDev 等 IDE 即使在 Python 中也有很大帮助，并且当项目变大时是必不可少的)。 
你现在在做什么项目？ 
嗯，我总是有一大堆令人兴奋的事情要做——目前我正在做: 
PyDev(http://pydev.org)目前是 Python 的顶级 ide 之一——我每天都在使用它，我已经为它工作了 12 年，通过社区贡献/众筹活动让它继续发展。此外，我对 PyDev 现在的状态非常满意——有很多用户，近年来，随着市场变得更加拥挤，有了更多的选择，我只找到了真正喜欢使用 PyDev/Eclipse 的人(LiClipse 使用户更容易启动和运行)。PyDev。调试器(https://github.com/fabioz/PyDev.Debugger/)是最初只在 PyDev 中的调试器的一个分离，以便它可以在其他 ui 中使用(目前它由 PyDev 和 PyCharm 使用)；LiClipse(http://liclipse.com)，为 PyDev 提供了一个单机，在 Eclipse 中增加了对多种语言的轻量级支持，使其整体上成为一个好得多的包；PyVmMonitor(http://www.pyvmmonitor.com)，Python 可用的最好的剖析和 vm 监控体验；speed tin:https://www.speedtin.com，这是我开始参与的最新项目——它的目标是在性能退化上线之前更容易发现它——或者就 CPython 2.7 而言，希望一些核心提交者能够看看诸如https://www . speed tin . com/reports/1 _ CPython 27 x _ Performance _ Over _ Time之类的东西，并为以前版本上损失的性能提供一些修复😉mu-repo(https://fabioz.github.io/mu-repo/):这是用 Python 制作的一个实用程序，用于处理多个 git 库。洛基(http://rocky-dem.com)，这是一个顶尖的粒子模拟器(模拟器实际上是用 C/C++完成的，在 GPU 中用 CUDA 工作，或者在 CPU 中用 OpenMP 工作，但我主要研究它的后处理能力，这些都是用 Python 完成的)。 
如你所见，我从不感到无聊😉 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
如果我必须挑选一个最喜欢的库，我可能会选择 greenlets，这是异步编程的神奇解决方案(我个人认为用它编程比用 asyncio 类的解决方案或基于回调的编程容易得多)。 
尽管如此，我真的很喜欢 Python 对 yield/generators 的支持，并且，在核心库中，我认为 collections 和 itertools 非常好，提供了组织和迭代数据结构的简洁方法。 
此外，我总是惊讶于 numpy 如何以一种高性能的方式处理大型数组，而 py.test 使测试变得非常简单。 
作为一门编程语言，你认为 Python 将何去何从？ 
嗯，希望 Python 能克服来自 Python 2/3 的破损，重新拥有同页的社区。 
我一直对此持否定态度，但最近我认为有更好的迹象表明会选择 Python 3，尽管像 Pyston 这样的东西仍然让我有点担心——也就是说:这是一个全新的 Python 实现，目标是 Python 2——不幸的是，我认为它真正表明了社区仍然是多么的破碎(因为，你知道，移植大型代码库肯定不是有趣的，昂贵的，并且有添加可能难以发现的 bug 的额外风险)。 
时间会证明一切，但我真的希望有一天我不用在程序中添加 cruft 来支持 CPython 2 和 3😉 
非常感谢！ 
一周的最后 10 个 PyDevs 
马赫什·文基塔查拉姆弗洛里安·布鲁欣马克-安德烈·莱姆堡尼克·科格兰阿伦·拉文德兰什么场布莱恩·施拉德克雷格·布鲁斯海梅·费尔南德斯·德尔罗瑞安·米切尔 
本周 PyDev:Facundo Batista 
 
 原文：https://www.blog.pythonlibrary.org/2014/10/20/pydev-of-the-week-facundo-batista/ 
 
本周我们邀请到了 Facundo Batista(@ Facundo Batista)加入我们。 
 
他是来自阿根廷的 Python 核心开发人员。如果你碰巧会说西班牙语，那么你可能会喜欢他的博客。让我们花点时间了解一下 Facundo 吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我是 Python 编程语言的专家。有着超过 8 年的 it 经验，我是这种语言的核心开发者，也是 Python 软件基金会的 成员。此外，因组织 PyCon Argentina 和 阿根廷 Python 社区以及对标准 库的贡献和翻译 Python 文档的工作，获得了 2009 年社区服务奖。 
我在阿根廷和其他国家(美国和欧洲)的主要 Python 会议上发表过演讲。总的来说，我在分布式协作体验方面有很强的 经验，参与 FLOSS 开发，与全球各地的人一起工作，超过 10 年。 
在 Movistar 和 Ericsson 担任电信工程师，在 Cyclelogic(首席开发人员)和 Canonical (技术负责人，现任职务)担任 Python 专家。 
我也喜欢打网球，有一个一岁大的孩子，他是个可爱的小家伙，喜欢拍照。 
你为什么开始使用 Python？ 
14 年前，当我在 Movistar 工作时，我需要在服务器端处理一些日志。 
服务器运行的是 SunOS(！).我知道 C 和其他语言不太适合做这项工作。我学习和使用 Perl 有几个月了，直到我发现 Python 并爱上了它。 
你还知道哪些编程语言，你最喜欢哪一种？ 我有过使用 COBOL、Clipper、Basic、C、C++、Java 和 Perl 的经验和工作经验(尽管我现在不重新学习就无法使用它们 )。 
我最喜欢的当然是 Python😉 
你现在在做什么项目？ 
我正在积极从事三个项目: 
这是一种压缩和构建整个维基百科以供离线使用的方法。输出可以是 CD、DVD，或者只是一个 tarball，它可以在 Linux、Mac 或 Windows 上自动运行，不需要安装任何其他东西。它的目标是为那些还没有互联网接入的学校/人们提供信息来源。目前我们只打包了西班牙语维基百科，但我们几乎准备好开始其他语言了Encuentro :这是一个桌面程序，可以从阿根廷公共电视台(这几天真的很牛逼)选择和下载大量的教育纪录片。网站和节目本身都是西班牙语的，因为电视节目只有那种语言。Linkode : Linkode 是有用的 pastebin！这是一种短暂的生活协作空间，一个动态的剪贴板。一些很棒的细节:你可以在任何地方、任何时间、毫不费力地创建链接代码。 可编辑的文本，不是静态的！ 每个新版本创建一个孩子:你有一棵树 代码/文本类型自动检测(和着色！) 永久链接码(但所有者仍然可以删除) 绝对匿名(除非你登录，这非常简单) 私有 URL:因为你无法猜测 UUIDs 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
我真的很喜欢 itertools 核心库。当然还有我写的那个十进制的， ；). 
关于外部库，我是 Twisted 的粉丝，这些天我用了很多漂亮的库。 
你还有什么想说的吗？ 感谢采访！ 
 
前一周的 PyDevs 
本·劳什 
PyDev of the Week: Fanilo Andrianasolo 
 
 原文：https://www.blog.pythonlibrary.org/2021/12/06/pydev-of-the-week-fanilo-andrianasolo/ 
 
This week we welcome Fanilo Andrianasolo (@andfanilo) as our PyDev of the Week! Fanilo is a core developer for several Streamlit extensions and a Community Ambassador for Streamlit itself. He also has several tutorials on Streamlit that you can check out. 
 
Let’s spend some time getting to know Fanilo! 
Can you tell us a little about yourself (hobbies, education, etc) 
Hello everyone! My name is Fanilo, I’ve been doing data analysis for around 8 years now, and am currently working as a Data Science & Business Intelligence Advocate / Tech Lead at Worldline. 
I graduated from Ecole Centrale Lyon, one of the French “grandes ecoles” where we are taught a broad set of scientific domains, from acoustic engineering to fluid mechanics. I liked most of the tutorials around algorithms and numerical computing, so I decided to take a semester at the University of Queensland in Australia to study software engineering and machine learning. I loved this course abroad so much I decided to make a career out of data analytics (I like to think koalas and surfing also contributed to this enthusiasm). 
Aside from work, I play and coach in badminton on a regional competitive level, try to play jazz piano while sipping a cup of tea and am learning video & music production. 
Why did you start using Python? 
Years ago, our Data Mining stack was gravitating around SAS and R. One of my main activities was converting R code to Spark in Scala for production on a Hadoop cluster. At times it was challenging as you are juggling between two very different paradigms. 
I knew there was a Python binding to Spark and I wanted to find an easier bridge between Data Scientists and Software Engineers in the Hadoop ecosystem; so I started rebuilding some of our data mining projects in Python with a senior data scientist colleague. 
We both grew very fond of the language! The syntax felt simple and readable, yet we could build powerful and complex data processing pipelines. What struck me the most was the ecosystem: did you know you could “pip install wget” on Windows to have a pseudo-wget command? That day I jokingly messaged my colleague “Python has a library for everything!”, and still regularly browse Pypi for niche and useful packages. 
What other programming languages do you know and which is your favorite? 
I’ve done my share of Scala during my Apache Spark years, and know my way around Java as it is the predominant language in the company I currently work in. In the JVM space, I’d like to try Kotlin one day, it looks like their community and Data Science ecosystem are growing and the syntax looks nice. 
I’m also a fan of building web applications to showcase my works. I don’t pretend to be a Frontend engineer, but I can write small Typescript/Vue/React apps. I find the Javascript world has matured a lot those past years and the Typescript compiler ranting about my code has definitely helped. 
Favorite language? I’ve been using Python pretty much everywhere now, from “check the quality of merchant data in our master customer database” to “downloading attachments from your email” processes. I have to thank the book “Automate the Boring Stuff with Python” for opening my eyes to using Python for every daily task. Who knows if Go, Rust, or Julia challenges it someday, and I’d like to add in C++ to build fancy audio processing tools. 
What projects are you working on now? 
I’m mostly involved with prototyping data-driven features for projects, reviewing and deploying Python code on an online learning project, as well as promoting Business Intelligence and Machine Learning to internal product/engineering teams and external customers. 
Outside of work, I started editing and publishing tutorials as slide carousels, as well as short Data Science skits with the hopes to build educational yet entertaining longer videos about Data Analysis later on. I also contribute a lot to the Streamlit community, but we will talk about this in a few questions. 
Which Python libraries are your favorite (core or 3rd party)? 
I am a big fan of Streamlit (https://streamlit.io/) as it enables me to quickly showcase and share visual data analysis projects. For example, one of my Machine Learning demos involved using a Tensorflow model to recognize live drawings in the same vein as the “Quick, Draw” game (https://quickdraw.withgoogle.com/). I struggled for 5 days with ECharts, Fabric.js, and Tensorflow.js, having to convert Python models to their JS counterpart and agonizing on CSS. Today with Streamlit I think this would take me less than 5 hours to build. Now I pretty much build a CLI and a Streamlit app as interfaces for every data quality and processing app I create at work. 
I like using Plotly Express and Altair for interactive plots, and FastAPI/Pydantic are pretty high on my list too. The collections and itertools core libraries have a lot of hidden gems I rediscover now and then. 
What are your contributions to Streamlit? 
I had never really contributed to any open source project or online community before. A year and a half ago when I first toyed with Streamlit, the forum and core team were still small. I would regularly see the founders Adrien, Amanda, and Thiago, along with some colleagues advise to other users on the 2-month old forum. The tone was very open and friendly, so I decided to help users on the forum too. I became very active there (I almost got the “365 days in a row” award!), so much that I got contacted by the team, became a forum moderator, was later invited as a guest on their chatroom, and participated in multiple beta testings. I am now part of the “Streamlit Creators” program (https://streamlit.io/creators) which is like being a Community Ambassador for Streamlit, and it comes with nice goodies! 
Today I am still very involved in the community in different ways: 
I maintain several Streamlit extensions: streamlit-echarts (https://github.com/andfanilo/streamlit-echarts) to display ECharts plots in an app, streamit-drawable-canvas (https://github.com/andfanilo/streamlit-drawable-canvas) to integrate a live drawing component, and streamlit-lottie (https://github.com/andfanilo/streamlit-lottie) to display Lottie animationsI am still hanging around the Streamlit forum (https://discuss.streamlit.io/) and Discord (https://discord.gg/bTz5EDYh9Z) helping users with their issuesI write tutorials like the Streamlit Components Hands-On (https://streamlit-components-tutorial.netlify.app/), and started doing a weekly Streamlit tip series on Twitter (https://twitter.com/andfanilo) 
If you needed to create a full-fledged website with Python, which framework would you use and why? 
There are a lot of options nowadays to build a web application in Python, from the top of my head I can think of Streamlit, Dash, Panel, Gradio, Voilà, Django, FastAPI delivering static pages…they all serve different use cases and come with different constraints regarding the mapping between widget and state. 
Whenever I need to show off and interact with some data processing or analysis, I will use Streamlit. I love the simplicity of its design and the low barrier of entry, and I believe you can still do very complex tools with it. But I also understand developers who are put off by the “rerun the whole script on every user interaction, put into cache or session state any heavy computation” lifecycle and prefer Dash or Panel for callbacks to define the mapping between user interaction and backend computation, especially for bigger, multipage web apps. To choose between those libraries, I don’t usually give recommendations (and there are plenty of articles on the web on this), rather I ask users to test each library, get a feeling of their API, and ask the community if some more advanced tasks you would need to dig into are possible in this framework. 
I did not have the opportunity to use Django yet, as my usual ML demos are single-page static apps without authentication, so worst-case scenario I can get by with React and FastAPI. I’m pretty sure Django is here to stay as one of the preferred frameworks for building “full-fledged websites with administration tools” though, whereas Streamlit/Dash/Panel/Gradio/Voilà would tend towards “providing Python users a way to create a web UI for their works”. 
Is there anything else you’d like to say? 
Have fun in what you do, don’t be scared to contribute in online communities and build a lot of small and silly projects to improve at first, as consistency beats intensity! 
Thanks for doing the interview, Fanilo! 
本周 PyDev:费尔南多·多利奥 
 
 原文：https://www.blog.pythonlibrary.org/2016/01/11/pydev-of-the-week-fernando-doglio/ 
 
本周，我们欢迎费尔南多·多格里奥成为我们的本周 PyDev。虽然 Fernando 使用 Python 的时间不长，但他确实为 Packt Publishing 撰写了《掌握 Python 高性能》一书。如果你想知道 Fernando 最近在做什么，我推荐你看看他的 Github 简介。让我们花些时间去更好地了解他吧！ 
 
能简单介绍一下自己吗(爱好、学历等): 
我是一个 32 岁的书呆子，这意味着我喜欢将编程作为一种爱好(我的 Github 账户上有一大堆正在开发的项目，我的脑子里还有很多！)但是我也喜欢看电视(很多！)和阅读科幻小说和幻想小说(有助于不时脱离现实世界)，最后，我尽可能多地花时间陪我的妻子和两个孩子。 
至于教育，我高中毕业后就开始学习计算机工程，但很快就辍学了。几年后，我做了一个短暂的技术职业(只有 3 年)作为一名软件分析师，这是我所知道的关于我的正规教育的全部内容，其余的我都是从街上(当然是互联网！). 
你为什么开始使用 Python？ 
当我开始接触大数据时，我开始使用 Python。事实上，为了写一些 MapReduce 作业，我更喜欢用 Python 而不是 JAVA 来写(我们不都是这样吗？).从那以后，我从来没有真正使用过它，直到我开始写这本书，这是我学习这门语言的一个很大的借口。 
你还知道哪些编程语言，你最喜欢哪一种？ 地段！Ruby，PHP，JAVA，Node.js(还有前端 js)，C，前阵子有的 C++甚至有的 C#。我可能忘记了一些，但那些是主要的。至于我最喜欢的，目前是 Node.js 
你现在在做什么项目？ 
除了我在 Python 方面的日常工作(我真的不能谈论这个)，我最大的副业是www.lookingforpullrequests.com。一个致力于帮助那些开源世界的新来者找到他们的第一个项目的网站，也帮助那些寻求帮助的项目找到他们的第一个项目。现在它是用 PHP 编写的，使用的是我为自己创建的一个旧的 RoR 风格的框架，名为 Makiavelo，但我正试图用 Node.js 和一些花哨的前端技术重新编写它，如 Angular 或 Reach。 你最喜欢哪些 Python 库(核心或第三方)？ 
我现在真的没有最喜欢的，我还没有机会使用这种语言，也没有机会拥有一个最喜欢的库。 
你看 Python 作为编程语言何去何从？ 
以我的拙见，我认为 Python 在科学界已经很成熟了(例如，我看到我的数据科学家们经常使用它)，但我不认为这种语言会很快出现。当你将你的开发者经验(不仅包括语言本身，还包括围绕它的整个社区)与 Web 开发进行比较时，Python 实际上无法与 Node.js 或 Ruby 竞争。我作为一个语言新手的经历充其量是坎坷的，许多维护不善的包、过时的文档和相当糟糕的包管理器导致了糟糕的体验。再说一次，这是我的经验，我确信还有其他来自其他技术/社区的人正遭受着更糟糕的问题(比如 JAVA 世界),他们将 Python 视为一股新鲜空气。 
再一次，以我的拙见，在希望这种语言获得其他人目前的大量关注之前，Python 社区还有很多工作要做。 
是什么让你决定写一本关于 Python 的书？ 嗯，老实说，一本 Python 的书并不是我的第一选择，但是当 PacktPub 的人向我提出要做这本书时，我决定这将是一个摆弄这种语言的好借口，多了解它一点并学习它。 
你能告诉我们你在写这本书的时候学到了什么吗？ 
实际上很多，主要是因为当时我对 Python 的了解有限。但是我也学到了很多关于概要分析的知识，不仅仅是 Python 编程语言。我认为书中的大多数概念也可以很容易地推广到其他技术上。 
感谢您接受采访！ 
本周 PyDev:费尔南多·马萨诺里 
 
 原文：https://www.blog.pythonlibrary.org/2021/06/07/pydev-of-the-week-fernando-masanori/ 
 
本周我们欢迎 Fernando Masanori(@ fmasanori)成为我们本周的 PyDev！费尔南多是巴西 FATEC 圣若泽多斯坎波斯大学的教授。他是第一个教僵尸编程的巴西 MOOCPython的创造者。你可以在费尔南多的网站或者查看他的 GitHub 简介上看到他还在做什么。 
 
让我们花些时间来更好地了解费尔南多！ 
你能告诉我们一些关于你自己的事情吗(爱好、教育等)。): 我快乐吗？？？？分享知识？。我做饭？同跳舞。我喜欢洗碗。我把眼镜放在哪里了？ 
你为什么开始使用 Python？ 
为了好玩。 
你还知道哪些编程语言，你最喜欢哪一种？ 
Ruby，Java，C，C ++。Python。 
你现在在做什么项目？ 
分析公开数据，主要针对社会影响问题。 
哪些 Python 库是你的最爱(第一方或第三方)？ 
熊猫，请求。 “蟒蛇换僵尸”是怎么产生的？ 
我在一个名为《僵尸之轨》的课程中学到了 Ruby。我决定复制这个想法。 
作为一名教师，你学到的三课是什么？ 
非常有耐心。想要学生的好处。质量和纪律。 
你还想说点什么吗？ 
分享知识会带来很多快乐。没有什么比用你的一生为他人做好事更好的了。 
Thanks for doing the interview, Fernando! 
本周 PyDev:Florian Bruhin 
 
 原文：https://www.blog.pythonlibrary.org/2015/12/14/pydev-of-the-week-florian-bruhin/ 
 
本周，我们欢迎 Florian Bruhin 成为我们本周的 PyDev。让我们花些时间来了解一下我们的同胞皮托尼斯塔吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我叫 Florian Bruhin，今年 22.9 岁，住在瑞士。 
我是一名电子工程师(尽管讽刺的是，我大部分时间都在编写运行在 PC 上的 Python)，并计划明年开始学习计算机科学。 
我的主要爱好是 python 项目，它比我想象的要大一点😉 
除此之外，我喜欢游泳，也喜欢在火车上呆相当长的时间。我不确定这是一种爱好还是必需品😉 
你为什么开始使用 Python？ 
早在 2011 年，我就有了修改一台电子打字机的想法，以输出推特和其他与场景相关的俏皮话。当时我知道(嵌入式)C、QBasic 和 bash，并且已经在很多事情上滥用了 bash，而它肯定不是最好的工具(比如编写 IRC 机器人)。 
我意识到我真的不想浏览这些语言的网站，所以我必须学习一种更好的语言。我无法在 Ruby 和 Python 之间做出选择，所以我最终花了几周时间分别学习了两者的基础知识。 
尽管 Ruby 有这个很棒的漫画教程，里面有可爱的狐狸，但我最终还是决定用 Python。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我知道很多 C 和 Bash，一些 C++和 Ruby，可能还有一些基本的，尽管我已经很多年没用过它了(我不介意！). 
Python 和 C 是我的最爱，分别是高级和嵌入式的东西。 
我目前正在学习 Javascript(因为我需要它，而不是因为我喜欢)，我想探索 Haskell、Rust 和 Go，但到目前为止还没有时间这样做。 
你现在在做什么项目？ 
qutebrowser 是我近 2 年以来的主要项目。我启动它是因为我找不到一个好的类似 vim 的浏览器，而且我可能不会很快停止。 
我也不时为 pytest 和它的插件做贡献。 
我也想为 pylint 做更多贡献，但是我主要贡献的是问题而不是代码，因为不幸的是，我的时间是有限的。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 我非常喜欢上面提到的项目。我只能推荐任何项目使用 pytest 和 pylint。我想我可以添加一些测试相关的工具，比如 tox 、 coverage.py 和假设。 
我对 Qt 和 PyQt 又爱又恨。我希望在 Qt 绑定之上有一个更 pythonic 化的 API，不幸的是 PyQt 的开发过程并不是很开放。 
我喜欢像请求和烧瓶这样的项目的 API 和代码，尽管我自己并不经常使用它们。 
你还有什么想说的吗？ 
趁还来得及，多写些测试吧！😉 
谢谢！ 
一周的最后 10 个 PyDevs 
马克-安德烈·莱姆堡尼克·科格兰阿伦·拉文德兰什么场布莱恩·施拉德克雷格·布鲁斯海梅·费尔南德斯·德尔里奥瑞安·米切尔卡罗尔心甘情愿迈克尔·福格曼 
本周 PyDev:Florian Dahlitz 
 
 原文：https://www.blog.pythonlibrary.org/2020/06/29/pydev-of-the-week-florian-dahlitz/ 
 
本周，我们欢迎 Florian Dahlitz(@ DahlitzF)成为我们本周的 PyDev！Florian 是 CPython 编程语言和 PyTest 框架的贡献者。他也是真实巨蟒的贡献者。你可以查看弗洛里安的个人博客或者获取他的时事通讯来了解他的最新消息。 
让我们花些时间去了解弗洛里安吧！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我叫 Florian，我在德国学习应用计算机科学。我目前正在写我的学士论文，重点是自然语言处理。在我的空闲时间里，我尽可能多地编写代码，写关于我发现或学到的东西的博客帖子，并且我做了很多运动！ 
你为什么开始使用 Python？ 
我从 2015 年 1 月开始我的编程之旅，学习 PHP，HTML，CSS，Javascript。然而，我最终的愿望是实现 web 之外的东西。这就是为什么我向自己介绍了 C，但不知何故最终大部分代码都是用 Java 和 Python 编写的。 
Python 是一门非常漂亮的语言，为初学者提供了一个简单的起点。然而，你可以自由地增加复杂性，甚至在其上建立大型基础设施(看看网飞、YouTube 和 Instagram 就知道了)。 
Python 生态系统中的人们非常友好，和他们一起工作非常有趣。这就是为什么我最终把大部分时间花在了 Python 生态系统上。 
你还知道哪些编程语言，你最喜欢哪一种？ 正如我已经提到的，我从用 PHP、HTML、CSS 和 Javascript 编码开始，最后学习 C、Java 和 Python。虽然我在做一些需要编写 Java 代码的项目，但我主要是在使用 Python，这也是我最喜欢的编程语言——到目前为止！ 
你现在在做什么项目？ 
我花了大部分时间自学新概念，并就此写文章，以便在这个旅程中带着其他人。此外，我开始为开源项目做越来越多的贡献。目前，我正在深入研究 CPython 和 pytest。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
这个问题很难回答！事实上，Python 标准库提供了一堆有用的、棒极了的包和模块。没有明确的最爱。然而，我尽可能坚持使用标准库，不安装第三方包。 
你为什么决定向 CPython 和 Pytest 投稿？ 
CPython 是一个非常棒的开源项目，有非常善良和乐于助人的人。当我想学习更多关于 Python 的知识并为开源项目做出更多贡献时，有什么比贡献 Python 本身更好的起点呢？ 
虽然 Python 是一个很好的贡献点，但是许多开放的问题需要对之前应该学习的某些概念有深入的理解。这就是为什么我在寻找另一个我可以参与的项目。由于 Pytest 是 Python 测试的事实上的标准，测试总是一件好事，所以我看了一下他们的问题跟踪器和源代码。Pytest 的核心开发人员也是很好的人，所以我想给他们一些时间来创建和维护这样一个好的框架。 
你最喜欢 Pytest 的什么功能？ 
我最喜欢 Pytest 的一点是它的设置相当简单。在过去的两年里，我写了很多 Java 测试，其中包括一堆设置和样板代码。尽管 Python 的 unittest 模块比 Java 测试需要更少的代码，但在我看来，使用 Pytest 编写测试要容易得多，因为你只需要利用 Python 的内置 assert 语句。那是相当漂亮的！ 
你是怎么和真 Python 扯上关系的？ 
这个问题很有意思！当时，我不仅对 Python 非常感兴趣，而且对 Docker 也非常感兴趣。在阅读了大量关于 Docker 的内容，在工作中使用它，并教授了一堂关于 Docker 基础知识的课之后，我开始阅读 Real Python 的 Docker 相关文章。我注意到其中很多都已经过时了，所以我联系了丹，告诉他这件事，并让他更新一下。这导致了更多真正的 Python 贡献，为各种视频教程添加了描述，并更新了“Flask by Example”系列。 
你还有什么想说的吗？ 
如果人们想知道我在做什么，他们可以看看我的博客或者订阅我的时事通讯。我在推特上也很活跃。因此，如果有人有问题，有他们想了解更多的话题，或者只是想聊聊 Python 和编程，随时欢迎他们联系我！ 
弗洛里安，谢谢你接受采访！ 
本周 PyDev:弗兰克·瓦尔卡塞尔 
 
 原文：https://www.blog.pythonlibrary.org/2020/10/05/pydev-of-the-week-frank-valcarcel/ 
 
本周我们欢迎 Frank Valcarcel(@ fmdfrank)成为我们的本周 PyDev！他是 Cuttlesoft 的联合创始人。如果你想了解 Frank 正在做的项目，请直接访问 Github 。 
让我们花些时间来了解一下 Frank 吧！ 
 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我是佛罗里达州人，但我现在住在科罗拉多州丹佛市，大约 4 年前我公司的总部搬到了这里。我们来自佛罗里达州的塔拉哈西，我在那里的佛罗里达州立大学上学，并在名为 Domi Station 的社区加速器中创办了我们的公司。 
我有点像工作狂，所以我不太喜欢业余爱好，但是当我有时间的时候，我喜欢阅读、骑自行车、钓鱼、徒步旅行和摄影。 
我也喜欢旅行，但疫情让我无法确定是否会回到那里。 
你为什么开始使用 Python？ 
实际上，我是在 Udacity 的第一批在线课程中开始学习 Python 的。我刚刚完成了我在大学的第一门编程课程(一门关于用 C++进行游戏开发的大二选修课)，想为即将到来的课程做进一步的准备。 
Udacity 课程教授了一些 CS 基础知识，许多测验都是用 Python 编写的。该课程的顶点项目之一是实现谷歌初始 PageRank 算法的简化版本和网络爬虫。这可能是在 beautifulsoup 被创造出来之前，所以这个任务绝对是一个挑战，但也很有趣。 
你还知道哪些编程语言，你最喜欢哪一种？ 
我从互联网浏览器时代的 JavaScript 开始，从那以后一直为 web 开发。我在大学里学过 C/C++，但后来再也没碰过它们。我用 Go 和 Java 编写过一些琐碎的程序，但是我认为真正精通的语言只有 C#、Ruby 和 PHP。 
Ruby 可能是我第二喜欢的语言，仅次于 Python。我真的很喜欢 Ruby 的表现力。 
我希望有一天能学会长生不老药。 
你现在在做什么项目？ 
我刚刚完成了一个相当复杂的后端项目，涉及 Django、Celery 和 Google 的 Firestore。我不经常担任工程师，因为我主要负责公司的日常运营，所以这对我来说是一个有趣的变化。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
我非常喜欢 Flask，甚至为它编写了一些包，但是现在我已经被 Django 吸引了，因为它的业务涉及了更多的企业级项目。 
我爱朱庇特！这是我最近常用的原型制作工具。 
spaCy 和 Prodigy 是来自 Explosion 团队的两个令人印象深刻的 NLP 和 AI 开发工具。 
其他著名的最爱: 
黑色pytest熊猫诗意VCR . py(Ruby 的 VCR 库的一个端口)记录/回放 HTTP 请求进行测试箭头“通过模仿日期时间模块，允许您的 Python 测试穿越时间”vscode-python(从技术上来说不是 python，但是它应该在这个列表中) 
你是如何组织 Python 活动的，比如 PyColorado 或 Boulder Python？ 
我从佛罗里达开始。在我的第一次 PyCon(蒙特利尔 2015)后，我回到家，受到启发，在塔拉哈西成立了一个 Python 用户组。已经有的用户组。Net 和 Java(主要由公共部门的开发人员使用)。这很有趣，教会了我们很多关于如何培养社区的知识，但我们最终把它交给了新的领导，并继续作为赞助商。 
当我们搬到科罗拉多时，我们很幸运地加入了已建立的团体。这使得在当地观众面前建立关系网、招募和练习会议演讲变得容易。 
当伊恩·道格拉斯宣布他们正在寻找新的组织者时，我正在博尔德·皮森做一个演讲，所以我自告奋勇。我就是这样认识了斯科特·维塔莱和佐伊·法默，他们将继续和我一起计划和组织 PyColorado。 
PyColorado 只是我们都有的一个想法。我去了很多地区会议，不能理解为什么科罗拉多没有自己的！这是一个非常美丽的州，这里有很多 Python 用户。所以我们给了自己几年的时间来完成它，招募了一些我们知道的最好的组织者，然后开始工作。 
这对我来说仍然是最高的成就，我希望有一天我们能够再举办一次。 
你能分享一下你的公司 Cuttlesoft 成立的背景吗？ 
在 FSU 的第一年后，我从课程和实习中积累了足够的经验，为一家全国性咨询公司找到了第一份合同工程师的工作。 
我在那份工作上呆了大约一年，结束时我发现我喜欢咨询工作，但不喜欢我现在工作的公司。我认为 Gerber 对 E-Myth 的解释总结了发生在我身上的事情，因为我清楚地记得我告诉一位同事“我可以做得更好”的那一刻。 
接下来的一年，我为初创公司做自由职业者，甚至帮助建立了一个团队，在福利管理行业创办了一家新公司。在这段时间里，我一直梦想着经营一家软件咨询公司。在乔治亚理工学院的一次周末黑客马拉松后，我终于成功说服了艾米丽和我一起做生意，于是就这样 Cuttlesoft 成立了。 
你还有什么想说的吗？ 
是的，两件事。 
如果可以的话，请向 PSF 捐款。或者考虑成为赞助商。这些捐赠支持了世界各地的伟大计划，并有助于支持我们的全球 Python 社区。对维护者友好，支持，最重要的是耐心。这是一般的建议，但感觉很重要。每个人都在某种程度上受到疫情的影响，对大多数人来说，生活可能充满了新的和意想不到的挑战。 
我鼓励每个人都练习换位思考，如果你想了解更多关于开源维护者面临的困境，我有两个建议给你: 
阅读 Nadia Eghbal 的书在公众中工作:开源软件的制作和维护观看 Brett Cannon 关于设定开源参与期望的演讲 
这一点对我来说切中要害，感觉就像是我在互联网上不断与陌生人进行的对话。我们不应该需要在开源中倡导公民话语。 
弗兰克，谢谢你接受采访！ 
本周 PyDev:弗兰克·维埃拉 
 
 原文：https://www.blog.pythonlibrary.org/2018/11/12/pydev-of-the-week-frank-vieira/ 
 
本周，我们欢迎弗兰克·维埃拉成为本周的 PyDev。Frank 是 Vibora 包的创造者，这是一个“快速、异步和优雅的 Python web 框架”你可以在他的 Github 简介上看到 Frank 还在忙些什么。让我们花一些时间来更好地了解 Frank！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我叫弗兰克·维埃拉，今年 25 岁，是一名非常熟练的 Dota 玩家(lol)和充满激情的软件开发人员。在我的空闲时间，我喜欢玩一些游戏和做一些业余爱好的项目，比如使用 Unity 的小游戏或者像 Vibora 这样的开源项目。 
你为什么开始使用 Python？ 
我在一家安全公司找到了一份工作，该公司的所有工作都使用 Python，我几乎立刻就爱上了它。 
你还知道哪些编程语言，你最喜欢哪一种？ 
Javascript / Go / C#…我最喜欢的是 Python，尽管我不是动态类型的狂热爱好者，Python 的部署也很痛苦(docker 在这方面很棒) 
你现在在做什么项目？ 
我正在对 Vibora 和一款手机游戏进行彻底的重构。希望我能完成这两项🙂 
VI Bora 包的起源故事是什么？ 
我在工作中使用一些 Redis 支持的 API(使用 Flask/Gunicorn ),经过一些基准测试后，我发现 Redis 几乎处于休眠状态，而几台机器的 CPU 负载为 100%…经过一些研究，我发现 Sanic/Japronto 可以在服务器上带来一些原始的性能，但他们仍然是年轻的项目，我对他们的方向不满意…所以现在，Vibora 仍处于早期阶段，缺少很多东西，离生产就绪还很远，但我希望我能尽快赶上🙂 
为什么人们要用它而不是 Flask 或 Django？ 
首先，Vibora 是一个正在进行的工作。不要用它替换你的 Flask/Django 应用。在我有机会让它稳定下来之前，这个项目在 Reddit 上爆炸了…我正在努力尽快得到一个稳定的版本，敬请期待🙂 但是要回答你的问题:Flask/Django 是同步框架，在处理 IO 挑战时还不错但远非最优(以我的拙见)。Vibora 还关注性能，但这并不是这些框架的重点。 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
肯定的请求。尽管我对它有一些批评，但我认为它以一种好的方式影响了绝大多数的 HTTP 库(在许多不同的编程语言中)。 
弗兰克，谢谢你接受采访！ 
本周PyDev:弗兰克·怀尔斯 
 
 原文：https://www.blog.pythonlibrary.org/2019/08/26/pydev-of-the-week-frank-wiles/ 
 
本周，我们欢迎弗兰克·怀尔斯( @fwiles )成为我们本周的PyDev！Frank是Revolution Systems的总裁和创始人，也是Django软件基金会的主席。如果你想了解Frank，你应该花点时间看看他的网站或者他的 Github账户。现在，让我们花些时间去更好地了解他！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我在堪萨斯州的一个小镇长大，大约有一万人，所以计算机在我生命的早期就成为了我的爱好。除此之外，我真的很喜欢烹饪，当我有时间的时候，我会拍些照片，但是这些天我主要是给孩子们拍照。 
我在堪萨斯大学学习了一段时间，主修计算机科学，然后转行经商，最终在网络繁荣时期辍学。 
 
你为什么开始使用Python？ 
我在2008年开始使用Python，它很快成为我的主要语言。当时我和雅各布·卡普兰-莫斯共用一间办公室，我们的朋友中野律纪知道这一点，并希望我写一篇文章，部分是对他最近创建的Django软件基金会的采访，部分是对Django的快速介绍。 
我说当然，然后意识到，废话现在我必须学习Python和Django。 
我很快意识到Django比我现在使用的要好，我发现Python也非常棒。事后看来，我真的很高兴自己被轻轻地推到了这个方向。 
你实际上仍然可以在网上找到文章的一个稍有破损的版本。 
你还知道哪些编程语言，你最喜欢哪一种？ 
大约十年来，我主要是一个Perl人，甚至在2001年和一个朋友一起写了一本关于Perl的书，但是我至少有十年没有使用它了。 
我懂的C/C++够多了，去危险。我想在某个时候在Go中做一个更大的项目，但是还没有找到时间。我也在半心半意地自学Rust，但是还没有认真地做过什么。但据我所知，它在我未来的锦囊妙计中有一席之地。 
Python显然是我的最爱，也是我总是首先使用的工具。 
你现在在做什么项目？ 
几个月前，我换了一台新的笔记本电脑，并尝试在Docker容器和完全12因素中做几乎所有的事情，这有一个我通常不会公开发布的事情的副作用。所以我比以前更多地尝试“公开”编码。 
我目前正在改进REVSYS的一些开源项目的文档，比如django-test-plus。 
对于工作，我主要在一家大型金融服务公司的财务管理/改善应用程序的后端工作。这是一个微服务后端，当然使用了Django，并在AWS中为它设置了一个良好的Kubernetes环境。 
哪些Python库是你最喜欢的(核心或第三方)？ 
哦，哇，很难确定我最喜欢的，因为我在各种工作项目中使用过很多很棒的，但我认为读者可能会发现有用但可能不知道的是: 
姜戈健康检查structlogshortuuidpytest点击envparse 
你是如何成为Django的核心开发者的？ 
我其实不是Django的核心开发者，但这是一个常见的错误。在担任DSF总裁和REVSYS主要专注于Django Consulting之间，很容易犯错误。虽然这些年来我为Django贡献了一些补丁，但实际上我在Django上的工作还没有我在里面和周围的工作多。 
然而，这个问题确实让我提到，我们Django社区实际上正在解散核心团队，并转向一个更加开放和透明的治理流程。你可以在这里阅读更多此【DEP】(https://github . com/django/deps/blob/master/draft/0010-dissolve-core . rst)。 
Django让你兴奋的是什么？ 
虽然我发现Django的最新版本是日常工作的乐趣，但我对Andrew Godwin最近提出的让Django完全异步的提议感到兴奋。 
REVSYS越来越多的应用程序需要实时组件，channels project和async在帮助实现这一点方面大有作为。 
Django软件基金会正在进行哪些项目？ 
我们目前正在改造和自动化我们的会员流程，使其更容易成为会员，也更容易让董事会管理和跟踪流程。目前的流程有点奇怪，我们遇到过一些情况，因为大部分是通过电子邮件完成的，所以正在进行的会员申请被遗漏了。 
除此之外，我们总是有我们通常的筹资需求，我认为明年的重点将是筹集足够的资金来帮助转移到async。 
弗兰克，谢谢你接受采访！ 
本周 PyDev:froilán Irizarry Rivera 
 
 原文：https://www.blog.pythonlibrary.org/2021/01/04/pydev-of-the-week-froilan-irizarry-rivera/ 
 
本周，我们欢迎 froilán Irizarry Rivera(@ froidotdev)成为我们本周的 PyDev！弗罗兰在 Twitch 上做代码直播。弗罗兰帮助建立了全栈骑士，这是一个每月聚会和更多。你也可以在 Github 上看到他在做什么。 
让我们花些时间来更好地了解 Froilán！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等)？ 
我来自波多黎各，我在那里上学并开始了我的职业生涯。我现在住在 DC 的华盛顿，在 GitHub 工作。我搬到了华盛顿的 DC，加入了美国数字服务公司 T1，然后我去了 Code.gov 的 T2 T3。所以开始的“我们只会在这里呆两年”变成了四年。 
我也在市政技术和社区建设领域工作了几年，负责波多黎各的代码(目前由我负责)、DC 的代码，以及家乡的通宵达旦。 
除了工作之外，我真的喜欢阅读和写作。我真的很喜欢在城市中漫步，尤其是如果我可以买一杯咖啡，坐在水边思考。视频游戏和几乎所有的动画对我来说也是很重要的。 
我为什么开始用 Python？ 
我的妻子是 Python 开发者，她向我介绍了这种语言。有一次，我决定单干，我厌倦了 Java，所以我在 PR 建立了一个小的 Python 开发商店，叫做黑仔兔子实验室(是的，杀手兔子？？？？).一旦我开始使用 Python，就很难停下来！ 
你还知道哪些编程语言，你最喜欢哪一种？ 
Javascript/Typescript 和 Go 是我的其他语言。 
我仍在学习围棋，虽然用得不多，但我喜欢使用它。 
Javascript 最初是一种必需品，但随着时间的推移，我真的开始喜欢它了。尤其是从 ES6 开始增加了语言。最近，我喜欢在代码中使用类型，所以我开始大量使用 Typescript，而不是普通的 JS。 
你现在在做什么项目？ 
我的大部分时间都被波多黎各的代码占用了，所以我没有在兼职项目上花太多时间。也就是说，我目前正在直播一个名为 github-project-migrator 的项目的一些开发。我已经休息了一段时间，并且已经开始用姜戈和 DRF 做一些我个人项目的 TODOs。和我的一个朋友一起，我们将开始着手制作 tesoro.pr ，这是一个针对波多黎各西班牙人的字典和语言资源的集合。在 Code for Puerto Rico，我与 contratospr-api 有一些 TODOs，这是一个索引政府合同数据并试图帮助人们调查和搜索它们的项目。 
你知道，写完这些之后，我想我真的不会花太多时间休息了 
哪些 Python 库是你最喜欢的(核心或第三方)？ 
从核心 Python 来说，我真的很喜欢 CSV 库。我不得不处理这种格式的数据，所以我觉得这个库很棒。 
至于第三方库，我非常喜欢 Django Rest 框架。 
为什么你要直播而不是写关于代码的博客？这种方法有什么利弊吗？ 
我发现直播比博客更个人化。我在那里向任何想看的人展示我是如何思考和工作的。 
我开始用西班牙语直播的一个重要原因是用这种语言创作科技内容。latinx 开发人员“制造”技术并展示如何在项目中工作的需求很大，但例子很少。有大量的“入门”和类似教程的内容，但是在我看来，在开发过程中，如何从事一个项目以及如何做决策是缺失的。 
我过去常常写博客，但是我发现写博客很难，而且内容可能会很快变得陈旧。在这一点上，我也刚刚开始使用直播来完成项目 
播客 Fullstack Knights 是如何产生的？ 
有一天，我和一些朋友坐下来，开始头脑风暴，讨论我们认为我们的技术社区需要什么，我们进入了更高层次的会谈。我们创建了 Fullstack Nights meetup 来实现这一目标。几年后，创始人们离开了，但我们仍然想谈论技术，谈论一些无聊的事情，所以我们创建了 Fullstack Nights 播客，作为仍在运行的 meetup 的伴侣。 
你还有什么想说的吗？ 
我想说谢谢你给我这个机会和这些书，它们很棒。我想我会对每个人说，保重，注意安全，过一个愉快的远离社交的假期。 
感谢你接受采访，弗罗兰！ 
本周 PyDev:Gabriel Pettier 
 
 原文：https://www.blog.pythonlibrary.org/2015/05/25/pydev-of-the-week-gabriel-pettier/ 
 
本周，我们欢迎 Gabriel Pettier(@ t hirtman)成为我们本周的 PyDev。他是 Kivy 项目的核心开发者，他写了一篇非常有趣的博客。让我们花些时间来更好地了解我们的同胞 Pythonista！ 
你能告诉我们一些关于你自己的情况吗(爱好、教育等) 
我是一名居住在法国巴黎的开发人员，我在网上被称为 tshirtman，我在法国 Valence 的一所技术学校学习了两年的计算机科学，然后在这个行业工作了两年后，我决定回到学校继续学习剩下的三年，以获得计算机科学工程学位。回到学校是一次相当冒险的经历，但由于很多原因，它是值得的。我有各种各样的爱好，画画，弹吉他，阅读，很多，这些天主要是电脑，当我设法把头伸出屏幕时就看书(然后这些天我仍然在屏幕上阅读，电子书很棒)。如你所见，漫谈也是我的爱好之一。 
你为什么开始使用 Python？ 
当我第一年学习计算机科学的时候，我通过一本介绍 python 基础知识的小书发现了 python，当时我们学习 Ada、C 和其他东西。我也做了很多年的 BASIC 语言，那时也用 OpenGL 做了一些 C 语言，但我从未见过像 Python 这样的东西。我很快就爱上了它非常简洁的语法,“缩进就是语法”的事情让我觉得这是一个天才的举动，我仍然对很少有语言遵循这一点感到敬畏。此外，简单和不引人注目是我从 Basic 开始就没有见过的，这里没有随之而来的缺乏可维护性和缺乏功能的高昂代价。 
你还知道哪些编程语言，你最喜欢哪一种？ 
和大多数计算机科学家一样，我很早就遇到了 C，并做了相当多的工作，但我主要了解到，在它简单的外表背后，它是一只复杂的野兽，随时准备在你移开目光的那一刻攻击你。我使用了一些 Java，一些 C++，当然还有 web 开发的常用工具，html/CSS/js——如果你问我的话，这是一组非常奇怪的技术——我看了 Haskell，我发现它非常有趣，但是我没有时间真正深入研究它，并对它做任"
PythonLibrary 博客中文翻译（十五）,2024-11-10,"原文：PythonLibrary Blog 
 协议：CC BY-NC-SA 4.0 
 
PyCon 2009 -与 Jeff Rush 讨论名称空间 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/27/pycon-2009-namespaces-talk-with-jeff-rush/ 
 
我参加的第一个 PyCon 演讲题为 关于 Python 名称空间(和代码对象) ，由杰夫·拉什主讲。这个演讲对我来说有点难懂。如果 PyCon 组织者发布了幻灯片和视频，一定要抓住它们并观看。快速浏览一下实例化如何创建各种对象，这些对象下面可能有代码对象。他还谈到了如何将变量和函数编译成代码对象。 
Rush 还经历了代码对象/名称空间的执行前和执行后阶段。一个有趣的事实是，生成器保持名称空间的时间比普通的本地名称空间长。我不太清楚这在日常编码中意味着什么…但这可能很重要。 
他的下一个话题是闭包。下面是他的简单例子: 

def salary_above(amt):
def wages_test(person):
return person.salary
return wages_test
12345 
然后他继续讨论早期的绑定和名称空间。这是他的一个例子: 

# Early binding
names = []
rollcall = names.append
rollcall(""John"")
12345 
他还谈到了授权和早期绑定，但正如我上面提到的，你最好只看视频和他的幻灯片。我不能像他那样解释清楚。 
 
PyCon 2009 -开放空间和其他亮点 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/31/pycon-2009-open-space-and-other-highlights/ 
 
既然我已经介绍了我在 2009 年 PyCon 上参加的主要内容，我想我应该写一篇关于我看到和做的其他内容的简短笔记。我已经提到了为作者和编辑以及 wxPython BoF 开放的空间，所以我在这里不再赘述。相反，让我们谈谈我参加的其他露天活动。我查看了当地用户组的网站，并获得了更多关于如何改进爱荷华网站的想法。在那次会议上，我还看到了更多的面孔，感觉还不错。凯瑟琳·戴弗林在那里，还有杰夫·拉什和其他一些我想不起名字的人。无论如何，我希望能够有效地利用他们的想法。 
我也去了别的地方，但不管是什么，我现在都想不起来了。哦好吧。周六晚上，我和 Christopher Barker 共进晚餐，他是 wxPython 浮动画布部件的作者，也是 wxPython 邮件列表的定期撰稿人。能和一个我几乎每天都在使用的项目中如此博学的人聊天真是太好了。我觉得我是一个糟糕的谈话伙伴；他们决定在我们吃饭的时候“庆祝”地球一小时，这让我们很难看到。 
我想现在就这样吧。如果你对 PyCon 有任何疑问，请给我写信。 
 
PyCon 2009 - Reddit 主题演讲和早间会谈(周日) 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/31/pycon-2009-reddit-keynote-and-morning-talks-sunday/ 
 
星期天，我去参加上午的闪电会谈有点晚了。没有一个真正引起我的注意，因为我被更新这个博客和试图更新自己的其他事情分散了注意力。不过，我肯定我错过了一些好东西。我没有错过的是 Reddit 的主题演讲。我仍然不知道 Reddit 到底是什么，但我现在知道它是用 Python 编写的，至少部分是。演示的人相当有趣，但他们诅咒了很多，似乎不知道如何对着麦克风说话。问答时间很长，但他们回答问题的方式让我觉得他们会成为优秀的政治家，因为他们一直在回避问题。偶尔他们会直接回答一个问题，这很酷，但大多数时候…没有那么多。 
主题演讲后，我去了古斯塔沃·巴比耶里的 Python 启用移动媒体中心 。唉，这不是很有趣…更多的是因为我对这个问题缺乏理解。我知道他谈到了 EFL 和 openbossa ，但是不要问我那些是什么。 
今天上午的第二个话题 Christopher Perkins 的 使用 Sphinx 和 DoctestsT3 创建健壮的文档。他做得很好，只有一个技术失误。他使用了一些他用 Sphinx 创建的截屏，并重组了他讨论过的文本(REST ),这是对技术的非常有效的利用。Chris 也是 TurboGears 项目的一部分，该项目在其文档和项目中使用了 Sphinx 和 REST。 
奇怪的是，接下来的谈话也是在休息。它叫做 在你的项目中利用 REST，是由杰夫·拉什给出的。和拉什一样，他在很短的时间内讲了大量的信息，而且语速很快。只要你能跟上，他的讲座总是值得去听的。他似乎对他提出的任何主题都有百科全书般的知识，这让我相信他要么大量使用这个项目，要么研究得死去活来。 
我建议拉什和珀金斯都去拿幻灯片，看视频。它将非常值得你花时间。 
 
PyCon 2009 -周六下午会谈 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/29/pycon-2009-saturday-afternoon-talks/ 
 
在 2009 年 PyCon 的第二个正式日下午，我开始了 Ed Leafe 的关于使用达波的 web 应用的达波T2 演讲。他使用了他们创造的一个叫做“跳板”的产品，这是一种类似谷歌齿轮的与网络互动的产品。它给你一个客户端，可以与远程服务器上的数据库连接，你可以在那里编辑数据，也可以从服务器上获取新数据。在我参加的演讲中，这是最好的一次。对我来说，第二好的可能是克里斯托弗·巴克在午餐前的讲话。我建议试试达波和跳板。即使不能直接使用，也很可能从源头学到很多东西。 
我参加的第二个演讲是曼弗雷德·施文丁格的谷歌应用引擎，我觉得很无聊。他有一些很好的统计数据，关于你得到了多少处理能力等等，但那是我从它那里得到的全部。部分问题可能是由于他的母语不是英语，所以他并不总是容易理解。 
我去的最后一个演讲是比尔·格里布的 *数据分析应用的精确状态恢复和重启。*我也没有从这次演讲中得到太多，原因和之前的演讲一样。我肯定这完全是我的错。可能是我的注意力持续时间缩短了什么的。我很确定我从没在这里做过到底如何恢复状态…至少不是一般意义上的。 
我跳过了其他的演讲，这样我就可以去 Steve Holden 的教我 Web 测试开放空间，我会单独写一篇文章。我本来想看伊恩·比金的演讲，但是唉。有太多有趣的事情同时发生，你必须做出牺牲。敬请期待！ 
 
PyCon 2009 -周六上午 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/29/pycon-2009-saturday-morning/ 
 
我没有机会写下周六在 PyCon 上发生的事情，所以这里有一个我早上所见所闻的快速摘要:我和一名来自比利时的研究员从皇冠假日酒店步行到凯悦酒店。然后我参加了上午的闪电讲座和圭多的主题演讲。 
我总是希望 Guido 真的很有趣，但事实上，我发现今年和去年的谈话都相当乏味。他谈到了在未来 5-10 年的某个时候退休，Python 2.7 可能是 2.x 系列的最后一个，Twitter 如何改变了他，以及为什么他认为 Python(及其用户)太聪明了，不会失败。他偶尔会有一些有趣的见解，但我往往会过于频繁地走神。 
最初的计划要求我上午去参加三个会谈。相反，我去了一个面向 Python 作者和编辑的开放空间聚会。史蒂夫·霍尔登、安德鲁·库奇林、道格·海尔曼、、布兰登·罗德斯以及其他各种作者/编辑(和想要成为作者的人)和奥莱利的代表，他们让我错过了前两次会议。我认为这是非常有益的和有趣的。这是一个建立关系网的好地方。接下来，我去听了 Christopher Barker 关于构建一个混合了 wxPython 和网站内容的浏览器界面的演讲。那是我参加的最后一次晨会，然后是午餐时间。 
 
PyCon 2009 -教我网络测试(周六) 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/29/pycon-2009-teach-me-web-testing-saturday/ 
 
从 4 点至 5 点 15 分，史蒂夫·霍尔登举行了一场教我 Web 测试的开放空间会议，他在会上设置了笔记本电脑和投影仪，加载了他的网站，在桌子上放了一些威士忌和伏特加，然后让人群教他如何使用用 Python 编写的工具来测试他的网站。开始时人群很小，但后来增长到 20-30 人左右。我认为增长的主要原因是人们在推特上谈论酒精。总之，人群中的各种开发人员让史蒂夫轻松安装twill 并使用 nose 运行他网站的各个部分。接下来，他们让他试着安装风车，但是他们无法在他的 Vista 笔记本电脑上安装。这似乎是一个问题，由于 UAC 和 Cygwin 的混合物对我来说，但它是相当恼人的。 
他们针对史蒂夫网站运行的最后一个工具是 Selenium IDE Firefox 插件。我认为工具、知识和幽默的结合是一种很好的学习方式。唯一真正的缺点是，各种各样的开发人员一直在争论最好的方式，否则他们就会开始对话，很难听到霍尔登的声音。幸运的是，史蒂夫通常能让他们闭嘴。 
最后，它给了我几个未来测试网站的想法，所以总的来说这是一次相当好的体验。我希望他继续做这些。如果它们也被记录下来就好了。如果你在那里或者因为错过了而有问题，给我写信。 
 
PyCon 2009-Python 的现状(周五演讲) 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/28/pycon-2009-the-state-of-python-friday-talk/ 
 
Steve Holden 在周五下午做了一个关于 Python 在哪里的演讲。对于那些不了解内情的人来说，Holden 是 Python 软件基金会(PSF)的现任主席。他的第一个话题是关于 PSF 和它有多少成员，以及他们如何计划在这次 PyCon 上增加额外的成员。Holden 谈到了谁是 Python 社区，新手们第一次使用 Python 时对它的看法，Python 会议，开发，PSF 资助，Python 网站等等。我认为他对网站改变的想法可能是好的，因为网站从 2006 年 4 月以来一直保持着它的外观。我也很欣赏他关于 Python 核心开发以及如何使其更容易做出贡献的想法。霍尔登给出了一些我以前不知道的关于 PSF 的好信息。因为他的演讲有 45 分钟，而且他涵盖了这么多不同的主题，所以我不想花时间重写他的演讲。我认为这是一个很好的演讲，当它被发布的时候你应该检查一下。 
 
PyCon 2009 辅导日 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/26/pycon-2009-tutorial-days/ 
 
今天 Pycon 2009 辅导日结束了。周三我参加了两个涡轮齿轮课程，然后我去了 Py。周四考试和网络编程课。涡轮 班大多由 TG 核心开发者之一的马克·拉姆带领。不幸的是，他的课堂笔记、幻灯片和代码样本错误百出，因此，这是一堂非常令人沮丧的课。他基于几个月前也有很多错误的 TG2.0 20 分钟 Wiki 。几个月前，我在 wiki 教程中指出了许多这样的问题，但是在被告知去学习 Sphinx 并提交补丁之后，我就不再关注他们的网站了。好的一面是，他们分发了一些印有 TG 2 标志的非常酷的 u 盘。另外，马克·拉姆是一个非常好的人，他的助手克里斯·帕金斯也是如此。 
这个 Py。测试 类非常有用，但不是很有帮助。我从其他 Python 测试框架在线文档中读到了他们谈论的大部分内容。我仍然不明白如何将测试集成到我自己的应用程序中，这很可悲。我想我得用头撞墙一阵子，直到我“明白”为止。我要感谢多尔西和克雷克尔能够如此出色地展示，并能够以连贯的方式相互呼应。 
我最后的教程是 Wesley Chun 的Python 中的互联网编程。他从 socket 编程开始，通过 TCP/IP、FTP、SMTP、POP3、IMAP，进入 Django。几年前，当我阅读他的书《核心 Python》时，我已经完成了他的大部分例子。然而，他非常有条理，我认为这是一个很好的复习。另外，我还认识了新朋友！ 
我期待着 2009 年 PyCon 剩下的时间。如果你在这里，给我写封短信，地址是 python library . org 的迈克。 
 
PyCon 2009 -风车之谈 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/27/pycon-2009-windmill-talk/ 
 
亚当·克里斯蒂安做了风车演讲。它是一个用于在各种浏览器中测试网站的 IDE。它允许开发人员构建、记录和编辑测试。回放和调试在 IDE 中完成。它可以在 Firefox 中启动 Firebug，或者在其他浏览器中使用 Firebug-Lite。它还有一个 DOM explorer。 
风车使用 ipython 从命令行运行调试。它还有一个 Python 代理 API，可能会很酷。他播放了几个视频，演示了 Windmill 是如何工作的，但是文本太小，无法阅读。希望克里斯蒂安会把他的视频放到网上。首先，您可以将错误报告导出到与 JUnit 兼容的地方。虽然我没得到什么好处，但你可能会。至少浏览一下他们的网站。 
 
PyCon 2009 - wxPython BoF 会议 
 
 原文：https://www.blog.pythonlibrary.org/2009/03/28/pycon-2009-wxpython-bof-meeting/ 
 
2009 年 3 月 27 日，我们在 PyCon 2009 上举行了一次物以类聚 (BoF)会议。我把它贴在开放空间板上，我们有大约 18 个人出现，其中一对会在房间里进进出出。我认为这对我来说是一个亮点，因为我去见其他使用 wx 框架的人。克里斯托弗·巴克也在那里，很高兴见到他。 我们讨论了我们在 wxPython 中正在做的各种事情以及我们遇到的问题。我们也给那些有问题或者考虑使用 wxPython 的人一些建议。了解其他人在做什么，接触一点社交网络，这很好。这就是我想说的。希望明年会有更多的人来，我们可以想出更多实质性的东西。 
 
PyCon 2010 和志愿服务 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/28/pycon-2010-and-volunteering/ 
 
今年，我决定在 PyCon 做志愿者。在我之前的两次 PyCons 上，我都打算帮忙，但不知道如何加入。2009 年教程开始的前一天晚上，我在酒店里四处游荡，寻找 PyCon 的工作人员，结果一个人也没找到。一旦教程开始了，我在晚上会感觉很累，因为我上了最大数量的教程，有很多信息要吸收。但是回到今年。我自愿当了一次会议的主席。这是一次有趣的经历。我见到了很多很酷的 Python 人，尽管在会议结束后我真正见到的只有 Stani 和 Nadia。 
我在 PyCon 的第一个晚上，有个家伙找到我，想知道我是否愿意做一些数据录入工作。他有一堆辅导调查需要输入给优雅缝线(又名:WearPython)的创始人格雷格·林德斯特罗姆。所以当我找到 PyCon 无线网络工作的地方后，我就开始做这些事情。 
我下一个善意的举动发生在星期五早上。我刚刚找到一个靠近接入点的位置，这时一个人走过来，问我是否可以帮忙把电源板放在桌子下面。这是在主礼堂。我最后做了大约一半的房间。在此期间，另一个人开始帮助我。目标是将它们从前到后全部插在一起，通常是两个系列。这种方法的最大缺点是，如果有人在后面碰了他们的电源板的开关，那么所有正在使用那个电源板的人都会断电。我从未听说这是否是一个问题。 
我最后的志愿活动是担任会议主席。我实际上在另一篇文章的中谈到了这一点，所以我在这里不再重复。我只想说，除了一些小的音频问题和一些蝴蝶之外，这是一次很好的体验。 
我有点希望得到一个很棒的员工球座，但是主要的员工球座很差劲。它们只是黑底白字和 PyCon 标志。如果你在会议上注意，那么你会知道工作人员和会议 t 恤之间的唯一区别是略有不同的措辞和一个是黑色和另一个是蓝色的事实。哦好吧。我确实喜欢 PyCon 的标志，但是如果他们在背面也有一个大的标志或者什么的，那会更酷。 
明年，我鼓励大家走出去，成为志愿者，让 PyCon 2011 变得更好！ 
PyCon 2010:星期五(2010/02/19) -第一场会议 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/19/pycon-2010-friday-20100219-session-one/ 
 
我上午参加了三个会议:建立 Leafy Chat，一个简短的 Pinax 教程和 Import This，that 和其他东西:自定义进口商。如果你感兴趣，你可以继续读下去，看看我是怎么想的。 
Alex Gaynor 使用 AJAX 和 Comet 在“实时 web”上发言(尽管官方标题是“构建 Leafy Chat、DjangoDose 和 Hurricane，使用 Python 在实时 Web 上的经验教训”)。他谈到的第一个话题是 Leafy Chat ，这是一个在浏览器中使用 django 的聊天程序，他在一次 django 短跑比赛中用 48 小时组建了一个团队。Leafy Chat 使用 JSON 将数据包从客户端传递到服务器(反之亦然)。虽然它可以工作，但是不可扩展。下一个话题是djangode，使用了绕圈、扭转和跺脚。这个 web 应用程序运行得更好，但也不完全稳定，因为它依赖于 Twitter，如果 Twitter 宕机，DjangoDose 也会宕机。 
下一个话题是飓风。它使用 jQuery、 Tornado 服务器和多处理队列来产生和消费数据。它可以提供 twitter feed 和聊天，但问题是应用程序状态完全在内存中。 
最后一个题目是 Redis 。它建立在前面的例子之上，做了许多相同的事情，只是规模更大。幻灯片见 http://us.pycon.org/2010/conference/schedule/event/10/。我真的不明白这和他创造的其他东西有什么不同，但这可能是我的错， 
第二个演讲人是丹尼尔·格林菲尔德(又名:皮丹尼)。我认为他要么是 Pinax 项目的创始人，要么是主要参与者之一。不幸的是，他有严重的技术问题，这使得他的演讲很难继续下去。他甚至没有机会展示任何代码，所以我有点失望。 
第一场会议的最后一位演讲者是 Python 核心开发人员 Brett Cannon。它被评为先进，它是。我不太明白他在说什么，所以我很早就离开去吃午饭了。第二次治疗好多了，所以请尽快查看我的帖子。 
 
PyCon 2010:周五全体会议 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/19/pycon-2010-friday-plenaries/ 
 
这是 PyCon 的第一个正式日子:2010 年 2 月 19 日，星期五。根据我的经验，PyCon 全会要么非常有趣，要么非常无聊。我很少看到中间的。PyCon 的主席是 Van Lindberg(我想是 Python 软件基金会的律师)。史蒂夫·霍尔登是第一位全会发言人。他是 Python 软件基金会的主席，他的演讲题目是“构建 Python 社区”。他介绍了各种工作人员，并感谢志愿者。他谈到了 PSF 的用途(即“铲垃圾，这样我们就可以用 Python 编程”)。然后他把话题转到了多样性和他对 Python 社区将继续在世界范围内发展的希望上。他还告诉我们 Python 社区的人们是如何变得友好和热情的。他从他的演讲到 Guido van Rossums 的演讲有一个奇怪但有趣的转变，包括使用手机电话和一首愚蠢的歌曲。 
Guido 是 Python 的创造者。他用PyCon djangode feed接收和回答问题，很有趣，很好玩。不幸的是，这使得谈话有点不集中。他确实宣布 Python 2.7 将是 2.x 系列的最后一个官方版本。有人问及字典理解的历史和未来。他还收到了姜戈小马的一个问题，问它要跑多少路才能变成一匹马。很奇怪，但是被忽略了。唉。不管怎样，他也收到了几个关于空载燕子和 GIL 的问题。如你所见，他收到了许多愚蠢的询问。 
我认为这些全体会议非常好。这绝对是我迄今为止看到的最好的罗森演讲。霍尔登的有点干，但我的心思没有走神，所以总体来说还不错。敬请关注会谈。 
 
PyCon 2010:周五-第二场会议 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/19/pycon-2010-friday-session-2/ 
 
对于第二场会议，我决定自愿担任会议主席，这意味着我基本上会介绍发言者，并尽量让他们不要讲太长时间。我的第一位演讲者是塔里克·齐亚德，他谈到了 包装的状态 。在开始的 15 分钟里，他大部分时间都在谈论 distutils、setuptools (easy_install)和 pip。然后转投自己支持过、写过或者参与过的各种 PEP(比如 PEP 314、345、386、376)。这个想法是强迫开发者在使用 distutils 或类似的东西时，在他们的脚本的元数据中嵌入更多的信息。他还谈到了每种分配方法的各种问题。总的来说非常有趣。 
第二个演讲是*entthought 如何分发包？* 伊兰·施内尔博士。他为entthought工作，看起来对自己的主题相当了解，但他也相当无趣。唉，我们都没有好的表达技巧。我知道我不是。 
我的最后一次演讲是我最期待的一次，也是我决定主持这次会议的原因之一。这是由斯塔尼·米歇尔和纳迪亚·阿尔拉姆利完成的。演讲题目是 跨平台应用开发与分发 。他们是我在这次会议上见过的准备最充分的演讲者。对于那些不知道的人来说，Stani 是 Stani Python 编辑器 (SPE)和 Phatch 的创造者。他们在演示中使用了后者来展示如何将扩展与二进制文件相关联，将相关联的文件拖到快捷方式上以在其上运行 Phatch，以及其他各种很酷的技巧。 
他们接着讲述了如何为 Windows、Mac 和 Linux 进行开发，他们使用的工具以及需要注意的陷阱。总的来说，它非常好，很受欢迎。 
担任会议主席的其他注意事项。我认为这是学习新事物和结识新朋友的好方法。然而，弄清楚如何使用设备是很困难的，似乎也没有人知道如何去做。如果我明年去 PyCon，我希望他们使用同样的设备，这样如果我成为志愿者，我就能更有效地提供帮助。总的来说，这是一次积极的经历。 
 
PyCon 2010:周五-第三场会议 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/20/pycon-2010-friday-session-3/ 
 
第三次会议只有两次会谈。我决定去看看 Christopher Johnson 和 Chris Moffett 写的《Python 中的:Satchmo 和 GetPaid 简介(#144) 》。我参加这次演讲的主要原因是因为我认为开一家网上商店听起来很有趣，我可能会在工作中使用这些信息，因为我们已经为各种服务进行了大量的在线支付。 
Satchmo 的诞生是因为一群家伙有想创业的女朋友或妻子。他提到玩具反斗城澳大利亚公司是使用 Satchmo 的最大公司之一，这很酷。Satchmo 是一个 Django“插件”。他说这只是普通的 Django 代码和一百多个模板。他展示的唯一例子是一个极其简单化的截图。 
GetPaid 项目是从 BBQ Sprint 开始的。莫菲特不是程序员，但更像是项目背后的组织者，并为其募集支持。GetPaid 是针对 Plone / Zope3 的。两个项目都是“易用”、“灵活”、“易扩展”。奇怪的是，我没有和任何一个主持人订婚。诚然，我被#pycon IRC 频道上关于 Alex Martelli 讲话的抽象性的无聊讨论分散了注意力。 
我很难选择第二个演讲，因为有几个我认为看起来很有趣。最后去了 大型应用是如何嵌入 Python 的？ 出自彼得·辛纳斯。他似乎是在电影或游戏行业，所以他专注于在他们的程序中嵌入 Python 的软件。他提到的例子有 Maya，Nuke，Houdini 和 Blender。我对电脑动画和电影感兴趣已经很久了，但我只听说过这些节目的第一部和最后一部。Shinners 先生重点介绍了 Python 是如何嵌入到每一个组件中的，以及它们之间的差异和相似之处。虽然有趣，但对我来说，这些差异似乎相当微妙。 
总的来说，这是一个体面的会议。我学到了一些新东西，这总是一个优势！ 
 
PyCon 2010 即将到来 
 
 原文：https://www.blog.pythonlibrary.org/2010/01/14/pycon-2010-is-coming/ 
 
 
PyCon USA 2010 将于今年 2 月 19 日至 21 日在佐治亚州亚特兰大举行。在会议前后还有辅导日和冲刺日。去年，我被告知，我的雇主下次不会支付我的费用。所以今年我必须决定我是否要独自去。 
我不喜欢一次花费超过 1000 美元(300 美元的会议费用+800 美元的机票和酒店费用)，但是 PyCon 是一个学习和与志同道合的程序员交流的好地方，所以它非常有吸引力。最后，我硬着头皮决定去。现在你也该走了！查看出席者的列表，看看是否有你想见的人。查看一下的时间表，看看如果你不去的话会错过什么！ 
我不喜欢这次会议的一点是，有太多的东西我想看，但不能。我需要想出一些方法来克隆我自己，让我可以和我的克隆人分享我的想法…与此同时，我想我只要复制范·林德伯格和杰西·诺勒 (Python 杂志作者和多处理模块的创造者维护者)并告诉你我想看的。 
我期待着窃听供应商，并参加比去年更多的开放空间会议。我可能也会试着学习一种新的棋盘游戏！但我肯定你对我的谈话内容更感兴趣，所以我们开始吧: 
布雷特·卡农的“进口这个、那个和其他东西:定制进口商”(Talk #9)塔里克·齐亚当的“包装的现状”(Talk #63)或约瑟夫·李瑟娥的“一条水下巨蟒:巨蟒动力机器人托尔图加”(Talk #175)杰夫·拉什的“一个物体的解构”(Talk #62)克里斯托弗·约翰逊和 
我听说过坎农的演讲，实际上我也去过杰夫·拉什的演讲和辅导课。我知道他真的很棒。其余的演讲者我真的一无所知，但这些演讲本身让我很感兴趣。我肯定我会检查其他的，甚至可能跳过其中的一些。 
希望二月份能在那里见到你！ 
PyCon 2010 开放空间 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/21/pycon-2010-open-spaces/ 
 
PyCon 2010 延续了开放空间的做法(如果你不知道那些是什么，点击这里)。我真的很喜欢去年的空地赛道，今年也非常期待。不幸的是，我只找到了一个，那就是我贴在板上的 wxPython BoF。我今年看到的主要缺陷是有两个矛盾的开放空间板。一个在通向开放空间走廊的门外，具有使用 24 小时时间格式(即 1300 小时)给出的块，然后另一个在门口内，具有相同的房间字母和大部分相同的时间，但是使用正常的美国格式(即下午 1 点、下午 2 点等)。因此，很难知道该遵循哪个委员会。 
比如我想去 Python 作者 BoF (BoF =一丘之貉)。我下去的时候，外面的板子说在某某里面，里面的板子是空白的。我去寻找这个房间，但却发现了其他人(我想他们是姜戈人)。我不知道作者的东西在哪里，也不知道它是否发生过。 
我的 wxPython BoF 也好不到哪里去。我在两块板上都写下了我的时间，希望能减轻困惑，但无论如何，我预订的房间里有很多人。他们在我的 BoF 应该开始后大约 5-10 分钟离开，我认为这导致我们失去了参与者。我们只有 6 个人出席，而去年这个数字接近三倍。 
总而言之，我认为 wxPython BoF 还不错，因为我见到了达波背后的两位主要开发者，艾德·利夫和保罗·麦克内特。而且斯塔尼也出现了，所以我能够和 wxPython 利基市场的一些酷人接触。我们讨论了我们正在做的各种项目，并帮助了一个 wxPython 新手。 
另一个烦恼是，似乎从来没有任何卡片可以方便地填写并张贴在公告板上！。如果我能参加明年的 PyCon，我希望会议的这个领域能得到更多的关注。 
PyCon 2010:周六上午闪电会谈 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/20/pycon-2010-saturday-morning-lightning-talks/ 
 
周六上午，PyCon 主持了约半小时的闪电会谈。以下是主题和作者(当我捕捉到他们的名字时): 
Joseph Tate——一个 web 反模式 
保护 Python 包管理 Justin Samuel 
Python 中的加密技术——Geremy Condra 
Django 的 Haystack，有自定义搜索，包括测试和文档。安装 Solr/Whoose/Xapian，然后安装 Haystack www.haystacksearch.org 
为 Twisted 做贡献—呼吁人们参与 Twisted dev 
还有一个人在没有幻灯片的情况下进行了演示，因为他或那个扭曲的人设法烧断了保险丝，导致投影仪出现故障之类的。我已经忘了他讲的是什么。我认为干草堆是最有趣的，因为它最吸引我。其他人以他们自己的方式很有趣，但是大部分的谈话需要超过 5-10 分钟来真正地引出他们的话题。 
 
PyCon 2010:周六全体会议(迪诺·维兰、马切伊·菲哈尔科夫斯基和马克·舒托沃尔斯！) 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/20/pycon-2010-saturday-plenaries-dino-viehland-maciej-fijalkowski-and-mark-shuttleworth/ 
 
在今天早上结束闪电会谈的技术困难之后，范·林德伯格站起来拖延时间，让他们把它修好，这样他就可以介绍第一次全体会议了。他做得非常好，让我们知道这个 PyCon 创下了两项纪录:首先，它有史以来最大的出席人数约为。1025 人。其次，大约 10-11%是妇女。非常酷！ 
第一次全体会议由 Dino Viehland 在“IronPython 的状态”上完成。他回顾了 IronPython 去年取得的成就，并谈了一点未来。在 IronPython 2.6 中，他们设法增加了对 ctypes，sys 的支持。_getframe 和 sys.settrace，后者允许 pdb 工作(我觉得)。他还说，IronPython 团队已经编写了一个新的解释器，速度要快得多。最后，他为 Visual Studio 2010 演示了一个简洁的 IronPython 插件。文字太小，看不清他在做什么，但听起来让人印象深刻。还要注意，他们终于有了真正的网站！ 
先生 Fijalkowski 是投稿人(可能是创始人？)的 PyPy ，不要和 PyPI 混淆。我不太了解这个项目，但听起来好像是某种新的、更快的 Python 实现。我必须密切关注这份报告，这样我才能弄清楚它到底是关于什么的，以及它是否是我应该关心的事情。 
最后一次全体会议由 Ubuntu 项目的创始人马克·舒托沃尔斯主持。他演讲的题目是“节奏、质量和设计”。由于他应用的一些更新损坏了他的 VGA 端口，他结束了没有投影仪的演讲。无论如何，他提倡的主要话题是经常发布，让你的代码尽可能的高质量，和设计师一起工作。发布通常会导致更集中的发布，并帮助开发人员有目标。质量的原因很明显。设计方面有点微妙，因为大多数程序员通常不与 UI 设计师一起工作。我想这可能是我最喜欢的上午全体会议，它有很多好的建议。 
 
PyCon 2010:周六第一场会议(上午) 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/20/pycon-2010-saturday-session-1-morning/ 
 
对于上午的会议，我去了“装饰者从基础到类装饰者到装饰者库”和“接口、适配器和工厂”，这是在第一和第二部分。我跳过了所有中间的演讲，因为我没有看到任何我认为听起来有趣的东西。不幸的是，开放空间在上午和下午几乎完全没有得到利用，所以没有什么事情可做。无论如何，关于我将要看到的两个演讲的想法。 
查尔斯·梅里亚姆做了《装饰者对话》。他报道得相当深入。我仍然不知道什么时候使用装饰者，什么时候不使用，所以我开始认为我在这个话题上很笨。无论如何，他讲述了混凝土装饰者和普通装饰者之间的区别(混凝土有支撑，而其他的没有)。他还写了一个叫做 dectools 的模块，据说可以让装饰者更容易使用。他还谈到了类装饰器和它们的一些用途，比如字典转换、回调注册、契约编程、非继承混合以及其他我不熟悉的奇怪东西。无论如何，这是一次很好的谈话，因为它有希望让我查阅所有这些新术语。 
这个演讲是由德高望重的杰夫·拉什完成的。这个演讲对我来说也有点难懂，但是拉什先生总是深入地谈论他的话题，而且总是很好。他深入研究了接口、适配器和工厂。我强烈建议你下载他的幻灯片(见链接),并在视频发布后寻找视频版本。 
 
PyCon 2010:周六第二场会议(下午早些时候) 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/20/pycon-2010-saturday-session-2-early-afternoon/ 
 
我设法在中间阶段做了三次演讲。以下是清单:凯蒂·坎宁安的《508 和你:消除可及性的痛苦》、多诺万·普雷斯顿的《演员:什么、为什么和如何》和尼古拉斯·拉拉的《Python 元编程》。跳完之后再见！ 
我之所以去听坎宁安女士的演讲，主要是因为我为政府工作，我认为这个话题可能会有用。她重点讲述了 T2 和姜戈如何打破常规。如果你不知道 508 是什么，那么你应该看看她的谈话。一个简单的解释是，508 是美国网站的可访问性规则。基本上，他们告诉你如何使你的网站兼容，以便盲人、聋哑人可以使用它。这个演讲一点也不晦涩。相反，它更像是一个让你的网站正确设置的技巧和窍门的集合。这比我在这里解释的方式更有趣。参见 www.section508.gov的 
普雷斯顿先生的演讲是关于一个我一无所知的话题。原来 Actors 是一种做线程或者多处理的方式。演讲者写了一个 actor 实现，因为没有内置 actors 模块，尽管他列出了一些第三方脚本。他们是帕利、戏剧家和糖果人。由于我对此了解不多，就交给你看他的笔记了。 
最后一个主题是关于元编程，这是一个我一直想深入研究的主题，并且认为这是一个不错的选择。劳拉先生绝对知道他在说什么。你可以从这个演讲中学到很多好东西。有机会就去看看吧！ 
 
PyCon 2010:周六会议 3(下午晚些时候)-全球思考，本地黑客 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/20/pycon-2010-saturday-session-3-late-afternoon-think-globally-hack-locally/ 
 
我只参加了当天最后一节的两个讲座中的一个。它是由 Leigh Honeywell 女士提出的，名为:“放眼全球，在你的社区中教授 Python 语言”( T3)。 
她在加拿大多伦多发起了“Python 新手之夜”。这是一个非正式的同辈教学的班级，经常用投影仪把代码挂在墙上。他们将通过 Python 书籍“如何像计算机科学家一样思考”来工作，这本书有章节练习(这本书在 http://thinkpython.com 上是免费的)。她在一个黑客空间(她在当地的名字是hacklab.co)并且似乎推荐他们。她列举了一系列教授编程的场所，如社区中心、教堂、养老院、学校、监狱等等。她还提到多伦多大学已经从 Java 转到教 Python 了(我觉得)。 
她谈到了这些课程的成功之处和失败之处。在很大程度上，这个演讲只是讲授 Python 的通用技巧。我为 Pyowa(本地 python 用户组)做了她所说的大部分事情，并且完全同意独自做这件事很糟糕。我也同意教别人 Python 是非常有益的。我认为这是一次很好的非正式谈话，对以前从未做过这种事的人来说，会很有帮助。如果你打算建立一个用户群，看她讲话或读她的幻灯片将是朝着正确方向迈出的一步。 
 
PyCon 2010:周日上午闪电会谈 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/22/pycon-2010-sunday-morning-lightning-talks/ 
 
最后一次晨间闪电会谈是在周日。我没能留下来听下午的简短谈话。这里有一个快速的运行(注意，我不能在很多这样的幻灯片上看到演示者的名字，因为他们只会用几秒钟展示他们的第一张幻灯片): 
 请盗版——这是在闪电会谈预定前半个多小时给出的。我不知道为什么。他演讲的网站是 www.pleasepirate.com。他的前提是人们应该鼓励他人侵犯他们的知识产权。实际上，这很令人困惑。他认为知识共享也做得不够。你可以用 postgres 编写存储过程——这就像一个 60 秒的广告。PyAr 来自阿根廷 Python 用户组的 Natalia 讲述了他们的小组是如何成立的，以及他们的使命/愿景。它有 650 多个成员，每个月有 11000 多封邮件。她还谈到了他们作为一个群体所做的事情，比如 PyCamps 和 sprints (cocos2d，lalita，CDPedia)python Spring clean up——回家，弄清楚如何为 Python 做贡献，在 Python 用户组演示你的东西，让其他人也这样做你在我的 NumPy 中获得了你的Cython——作者 D. Huggins 经历了 k-means 代码的一系列迭代，展示了 cy thon 如何使 Python 代码变得更快。他在最后搞砸了，所以我们从来没有看到它到底有多快。PiCloud 灵感来自 facebook 的照片标签助手，但它变成了某种云计算程序。我真的没有很好地理解这一点，但他们似乎已经创建了一个“云”模块/包，允许您利用亚马逊的资源(EC2？)来做计算。MOX“Django 的移动网络”，由来自英国牛津的 Tim Fernando 介绍，Molly 是一个即将推出的开源项目，专注于为移动设备提供网络内容。例子(我认为)是 m.ox.ac.uk。它也做地图，它是 RESTful 的CCP Gamesguy“customstackless或 socket api(重音很难理解)，使用 cherrypy 的线程测试与他的程序进行比较，以显示他的版本超级快。我看不懂屏幕，所以我不知道他是否证明了什么。
 
PyCon 2010:周日全体会议 
 
 原文：https://www.blog.pythonlibrary.org/2010/02/22/pycon-2010-sunday-plenaries/ 
 
2010 年 PyCon】的最后一次全体会议是在周日。在这篇文章中，Van Lindberg 告诉我们，如果我们包括所有的供应商，我们的会议就有超过 1100 人参加。这意味着，对于 2011 年的 PyCon，他们可能不得不将出席人数限制在 1500 人，这样我们就不会在目前的场地上用完房间。这样好吗？我真的不知道。有时候感觉它已经太大了。时间会证明一切。 
上午的第一次全体会议由酱实验室的 Frank Wierzbicki 主讲，他谈到了“Jython 的现状”，Python 的 Java 实现。事实证明，2.5.1 版本与普通的 Python 实现有很好的兼容性，因此 Jython 通过了几乎所有的 CPython 测试套件。Jython 还可以与大多数 Java 库和应用程序一起工作，因此您可以两全其美。 
Wierzbicki 接着说，任何纯 Python 代码都应该在 Jython 中工作。他给出了 SqlAlchemy、Django、Pylons、pip、web2py 和 distribute 在 Jython 中工作的例子。Jython 目前的计划是争取在今年夏天发布 2.6 版本，让 Jython 达到 Python 2.6 的水平，然后，根据 Python 开发人员将他们的应用程序移植到 Python 3 的进度，他希望 Jython 也开始移植到 3.x。 
他呼吁对 Jython 项目提供帮助，因为他们不再有赞助商。然后他用麻省理工学院的 Joseph Chang 用 Jython 玩 Bejeweled 的脚本做了一个演示。很奇怪，但是很酷！ 
第二次全体会议是关于“空载燕子的状态”，由谷歌工作人员科林·温特主讲。他没有使用幻灯片，因为他说如果我们需要视觉效果，我们可以参考他周六的一次演讲。温特告诉我们他们的解释器如何比 Jython 和 PyPy 更快，但它可以在 wazoo 上进行优化。他宣布 Guido 已经批准将 Unladen Swallow 合并到 Python 3.x 代码库，希望能赶上 3.3。温特说，他希望通过合并代码，他们将获得更多的开发人员，他们可以使优化过程达到最快速度，并做出真正快速的解释器。最后，Winter 指出，Unladen Swallow 与所有当前的 Python 代码 100%兼容，并举例说明 Unladen Swallow 使 Django 的运行速度提高了 20%。 
最后一场全体会议由企业家安东尼奥·罗德里格斯(Antonio Rodriguez)主持，他是 tabblo 的创始人(后来他将该公司卖给了惠普)。以下是我在演讲中的一些笔记: 
success =[e . hack()for e in employees]每台机器都可以运行完整的堆栈。任何人都可以检查完整的树并构建完整的产品。任何人都可以对源代码树的任何部分进行更改。每个人都有承诺位。任何人都可以推进生产。98%的公司从 10 人左右开始商业与技术是一个错误的二分法精益创业应该是瘦弱的创业 
他认为 Python 面临的挑战是让人们承诺移植到 3.x，这样分裂就不会继续，标准库中需要更多的电池以及打包问题。我强烈建议等待视频并观看，因为我没有很好地解释他的演讲，我认为他的演讲是我参加过的最好的演讲。 
PyCon 2011:高级 Python 1 教程 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/09/pycon-2011-advanced-python-1-tutorial/ 
 
 
我今天的第一个教程是 Raymond Hettinger 的高级 Python I 。他很博学，但我被堵在了教室的最后面，跟不上他的课。演讲者讲述了 Python 3.2 的一系列基础知识。他讲述了如何打开一个模块，并使用 IDLE 的内置类浏览器快速浏览代码。 
然后他使用 itertools(他写的？)和 functools 进行矢量化处理。他会显示一些带有大量函数或方法调用的糟糕代码，然后使用 itertools、functools 或两者的一部分来修复代码。我想我知道更多关于最优化的知识，但是我没有准备好第二部分的课程。请注意，这被认为是一个“有经验”的水平教程。我想我还没有足够的经验，但我希望明年会有。 
PyCon 2011:聚合器和创业故事 
 
 原文：https://www.blog.pythonlibrary.org/2011/02/10/pycon-2011-aggregator-and-startup-stories/ 
 
 
2011 年美国 PyCon 还有不到一个月的时间！今年，他们正在推广使用concure作为一种 PyCon 聚合器。我想这意味着去年那个很酷的 DjangoDose 现在已经死了。反正 Convore 和 Twitter 差不多，但是有主题的概念，所以更有条理一点。我想我会把它描述为 Twitter 和 IRC 的混合体。他们目前有三个话题:是否使用这项服务，为 PyCon 和苏格兰 BoF 做志愿者。 
杰西·诺勒宣布了“创业故事”。创业是本次 PyCon 的主题，据我所知，它们将是全体会议的焦点。因此，除了与 Guido 的“炉边谈话”和 Hilary Mason 的谈话，我们还将了解 Dropbox、Disqus 和 Threadless。我用了三分之二，所以我很兴奋。 
还有时间报名来 PyCon 吧。不要等到太晚了！ 
PyCon 2011:周五上午会谈(与 Bicking 和 Beazley) 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/12/pycon-2011-friday-morning-talks-with-bicking-and-beazley/ 
 
 
我参加了迈克尔·福德的模拟演讲，开始了我的晨会，但是由于我的绿色房间志愿者职位，我不得不提前离开。我们有一些幕后问题需要处理。唉！总之，我最终跳过了大部分内容，但是我提到了另一个 Python 名人: 
Ian Bicking 关于 Javascript 的演讲 
Ian Bicking 在 Python 社区和 PyCon 都很受欢迎。今年他在 Pythonistas 的 Javascript 上发言。以下是我从中得到的收获: 
Javascript 到处都有对象，很像 PythonJavascript 有一个类似于 Python 字典的对象，至少从语法上看是这样的他谈到了可变范围，但我没有抓住那张幻灯片的要点undefined 是 falshish，不同于 null，不神奇，是一个对象！typeof 有点神奇原型就像 Python 类？这个就像是 Python 的自我。这个即使没用也总有价值Javascript 中的数组糟透了如果你喜欢 Python，你可能会喜欢 CoffeeScript 
大卫·比兹利的演讲 
David Beazley 做了一个关于使用 Python 3 为我的 Superboard II 构建云计算服务的演讲。他也一直在他的博客上谈论这个项目，我觉得这听起来很有趣。他谈到 Superboard II 是他 12 岁时的第一台电脑。如果我的笔记是准确的，它有以下规格:1 兆赫中央处理器，8k 内存，300 波特磁带。 
他发现他的父母仍然在他们的地下室里有这个东西，所以他把它拿出来，并试图找出如何处理它。他的主意？使用 Python 将它的程序存储在云中！或者类似的东西。它使用录音带来告诉它该做什么，所以他必须将 pyaudio 移植到 Python 3，然后使用他的 Mac 来模拟这些声音。最终，他出于测试目的编写了 Superboard II 的模拟器(我想)。他还谈到用 Python 3 用 500 行左右的代码编写 6502 汇编程序。 
这里的要点是，他必须将大约 6 个库移植到 Python 3(包括 Redis 和 pyPng)。他使用 Redis 创建了他的云，并展示了许多录制的演示，展示了他如何与 Superboard 通信，并最终如何将程序存储在 Redis 云中，甚至将程序从云中恢复到 Superboard 中。总的来说，这个演讲棒极了！我绝对推荐试着找一下这个的视频。 
PyCon 2011:周五全体会议(希拉里·梅森) 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/11/pycon-2011-friday-plenaries-with-hilary-masen/ 
 
 
货车简介 
官方 Python 会议日于今天，2011 年 3 月 11 日星期五开始。范·林德伯格以一个介绍性的演讲开始。他花了一些时间感谢捐赠者，并解释房间将如何分割。然后他给了一些门奖，似乎是 Python 编程书籍。 
史蒂夫·霍尔登的全体会议 
接下来是 PSF 主席史蒂夫·霍尔登，他发表了题为“PSF 年”的演讲。谈话似乎集中在 PSF 如何做 PyCon 背后的背景材料。他说今年的会议有超过 1300 人(或者 1400 人，这取决于你如何计算，不管这意味着什么)注册。他继续告诉听众 PSF 是如何支持其他会议和 Python 项目的。例如，他们资助了 Python 3 中的 OpenSSL 和 Python Miro 社区。霍尔登还提到了一个 PSF Python 宣传册。他也承认 PSF 董事会和成员。(会议也有一页是关于这个主题的) 
杰西·诺勒全体会议 
杰西也进行了一次“抽奖”，但只是为了 PyPy 项目。他给了他们一张一万美元的大支票。 
希拉里·梅森 
bit.ly 的首席科学家希拉里·梅森(Hilary Mason)谈到了她在工作中所做的事情，比如使用 Python 进行机器学习。她用 Python 展示了某种几乎无法读懂的奎因方程。她展示了一个有趣的图表，显示了 Python 在 StackOverflow 的问题数量中的位置。Python 在顶部附近。出于某种原因，她指出 BBC 的一篇文章显示伦敦出租车司机比其他人有更大的河马校园。她做了一个非正式的调查来寻找最流行的 Python 构造。获胜者？列出理解！ 
她谈到的其他话题包括 kinect 黑客、安全性、1700 方程、最喜欢的书籍等。然后，她开始谈论机器学习，以及我们需要如何构建工具来实现机器学习。她展示了一张有趣的地图，显示了点击 PyCon 相关链接的人来自哪里。然后她展示了接下来人们会去的两个网站。一个是“18 岁生日快乐 ro Ruby”，然后是某种赤裸裸的密码网站。 
她在结束讲话时谈到了昨晚袭击日本的海啸。然后她和观众一起进入问答环节。另请参见 keynote 上的召集页面。 
PyCon 2011:我成功了！ 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/09/pycon-2011-i-made-it/ 
 
几个月前，我告诉我的读者，我的组织不会支付我去 PyCon 的费用，我想知道他们是否会帮助我。我得到了不同的结果，并放弃了这个想法。然后我的老板说他会尽力让我去，他去了。所以我最终还是去了皮肯。我是昨天下午到达的，在与奇怪的亚特兰大班车服务发生了很多混乱之后，他们把我放在多辆货车里，最后把我带到了酒店。 
到目前为止，我已经认识了几个人，也见过一些熟人。昨晚我还做了一些志愿者工作。今天我去上了今天的第一节课，但是我会在不同的帖子里写这两个活动。对于这个简短的问题，我想得到反馈和想法。让我知道你想看什么，如果我能把它放进我的时间表，我会去看看并写下来。查看 PyCon 日程获取一些想法。谢谢！ 
PyCon 2011:专访卫斯理·陈 
 
 原文：https://www.blog.pythonlibrary.org/2011/02/21/pycon-2011-interview-with-wesley-chun/ 
 
随着 PyCon 的临近，blogger 社区受邀采访即将参加活动的演讲者。我选择了 Wesley Chun，他是《核心 Python 编程》的作者，也是《与 Django 一起进行 Python Web 开发》的合著者。在这次采访中，我问了 Wesley 关于他的演讲,在 Google App Engine 上运行 Django Apps以及 PyCon 的总体情况。让我们看看他有什么要说的: 
你希望与会者从这次演讲中学到什么？ 
我希望所有与会者带着更大的乐观情绪离开这个演讲，他们可以带着他们的 Django 应用程序，在 Google App Engine 上进行很少或没有修改的情况下运行它们，利用他们需要的可伸缩性，这是靠自己很难实现的。 
这个演讲的一部分是伪营销，以提高 Django-nonrel 的知名度，这是如何让 Django 应用程序在 App Engine 上运行的基础。自从 App Engine 在 2008 年首次亮相以来的几年里，已经有几个工具，称为助手和补丁，来帮助你在 App Engine 上运行 Django 应用程序。不幸的是，这些旧系统要求你修改应用程序的数据模型，以便让它们在 App Engine 上运行。Django-nonrel 则不是这样，当用户希望在任何 NoSQL 或非关系数据库上运行 Django 应用程序时，它应该成为用户应该使用的主要工具。 
除了 Django-nonrel，开发人员还需要相应的 NoSQL 适配器代码，djangoappengine(用于 Google App Engine 的数据存储)，Django-mongodb-engine(用于 mongodb)。他们(和其他人)正在开发其他 NoSQL 数据库的适配器，但比这更令人兴奋的是 NoSQL 的加入！ 
是什么让你决定谈论这个话题的？ 
我想做这个演讲有很多原因…我之前已经提到了意识。另一件事是，人们如此习惯于助手和补丁，以至于他们没有意识到还有更好的工具。 
另一个重要原因是供应商锁定的概念，这种现象是指您无法轻松地将应用程序和/或数据迁移到不同的平台，因为您被当前的供应商“锁定”了。人们抱怨你不能在其他地方运行 App Engine 应用程序，但这不是真的。除了谷歌的原始版本，你还可以选择不同的后端…其中两个后端是 AppScale 和 TyphoonAE。类似地，如果你创建了一个 Django 应用程序，并通过传统的主机运行它，Django-nonrel 可以帮助你把它移植到 App Engine，只需要很少的移植。类似地，如果你写了一个 Django 应用程序，并使用 Django-nonrel 在 App Engine 上运行它，把它转移到传统的主机上应该不需要太多的工作。 
3)在 Google App Engine 上使用 Django 有什么利弊？ 
最明显的优点是可扩展性。这是一件既困难又昂贵的事情。为什么不利用谷歌的聪明人，他们在核心基础设施中构建了可伸缩性来帮助他们…，嗯，谷歌！有了 Django-nonrel，如果谷歌不适合你，你可以带着你的应用去别的地方运行！这里没有供应商锁定。 
一个缺点是，如果你习惯于传统的关系数据库模型，App Engine 的数据存储仍然难以理解。您还不能完全执行原始 SQL 或连接。Google 确实给了你一个被称为 GQL 的简化的 SQL 语法，但是它并不是完整的图片。此外，为了换取它的好处，你必须放弃对你的应用程序的一些控制，让谷歌托管它。您不能使用任何 C/C++扩展，也不能访问您的日志或其他原始文件。 
今年在 PyCon 上你最期待什么？ 
我期待着与我在过去十年参加 Python 和开源会议时遇到的那些熟悉而友好的面孔建立联系。因为我们都在不同的地理位置，这是你可以指望见到一年没见的人并叙叙旧的唯一时间，无论是在展厅还是在有趣的走廊谈话中。 
我也很高兴学习 Python 世界中的新事物。它似乎每年都在增长，所以很难跟上社区的最新发展。我也期待着重复我去年的 Python 3 演讲，部分原因是它每年都变得越来越重要，作为我演讲研究的一部分，我将找出哪些项目已经转移到 Python 3。 
5)往年 PyCon 你最喜欢的部分是什么？ 
Python 生态圈最棒的部分是有一个伟大的编程语言作为后盾，但是下一个最好的部分是人…Python 社区本身。PyCon 是与社区互动的最佳场所。会议是非凡的，因为会谈，能够达到所有技能水平(从初级到高级)，持续两天的高超教程，引人注目的会议会谈，令人惊叹的闪电会谈和开放空间会议，当然，还有走廊对话，更不用说来自这种会议的疯狂迷因和黑客，如 oh war-http://pycon.ohwar.com。 
想想看:你可能会在今年的 PyCon 上见到你最喜欢的 Python 作者，同时了解 Django 和 Google App Engine。如果这还不能让你满意，Wesley 也在做一个关于 Python 3 的演讲。你还在等什么？登陆 PyCon 网站并报名吧！ 
注:本文交叉发布到 PyCon 博客 
PyCon 2011 需要你！ 
 
 原文：https://www.blog.pythonlibrary.org/2011/01/22/pycon-2011-needs-you/ 
 
 
PyCon 是一个几乎完全由志愿者管理的会议。你知道这意味着什么吗？这意味着他们需要你！去年，我成为了志愿者，这是一次很好的经历。你可以查看一下的员工名单，看看有没有你想接手的职责。如果是这样，一定要加入pycon-组织者邮件列表。这里的是主要的志愿者页面，下面是他们目前需要帮助的内容的副本现场: 
登记处志愿者赃物袋填充物会议人员(注意:我们需要为会议人员自愿参加特定会议设置一种方式；敬请期待！)教程日帮助电线锥形/非锥形最后一分钟帮手暴徒 
你可以在这里阅读这些职位。你可以在这里阅读我作为会议主席的经历。我做的时候有点紧张，但是我想一切都很顺利。 
要记住的主要事情是:如果你看到有人明显需要帮助，不要像个傻瓜一样站在那里！去那边帮帮他们！ 
PyCon 2011: Panel - Python 在学校的演讲 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/18/pycon-2011-panel-python-in-schools-talk/ 
 
 
在 PyCon 大会的第一天下午(2011 年 3 月 11 日，星期五)，我参加了一个关于在学校教授 Python 的座谈会。它是由扎克·米勒领导的，他是首先提出建立专家组的人。他介绍了 Brian Brumley，Brian brum ley 讲述了他在学校使用 Python 的经历。 
接下来是玛丽亚·利特温，一位数学和计算机科学老师。目前她在公立学校教 Python。她提到了她尝试过的教学生编程的各种想法。然后我们转到来自弗吉尼亚州阿灵顿的 Jeffrey Elkner，他在那里的高中教 Python。他还大力提倡 Python，以至于他所在地区的 5 所高中甚至中学都使用了 Python。他还提到有人在那里的小学用 Scratch 教学。 
最后，他们介绍了印第安纳州坎特伯雷学校的技术总监 Vern Ceder。他用 Scribbler 机器人教 Python。他还提到一些人也用 turtle 来教编程。 
介绍完每个人后，他们允许观众向小组提问。一个很好的问题是“对孩子来说最难学的是什么？”答案是循环、条件句和句法。 
总的来说，我认为这是一个非常有趣的小组。我希望我在高中学过编程。观众认为学校也应该开设编程课程。你可以点击查看。 
PyCon 2011:后 PyCon 文章 
 
 原文：https://www.blog.pythonlibrary.org/2011/04/01/pycon-2011-post-pycon-articles/ 
 
PyCon 2011(美国)结束了。但是很多人写了关于它的文章。因此，如果你错过了这次行动，你可以看看一些关于发生了什么的文章。如果你在这里没有看到你最喜欢的 PyCon 相关文章，请在评论中告诉我。 
超级纸板拿下 Pycon！参加 PyCon 的十大技巧PyCon 2011 CPython Sprint 新人来自 PyCon 的两个圆周率日谜题PyCon 碎屑【PyCon 2011 上的 Python IDEs 面板视频PyCon 2011 -快速回顾PyCon 发言PyCon 2011 想法PyCon 2011 视频PyCon 2011 年报告道格·海尔曼的 PyCon 幻灯片在线布雷特·卡农的【2011 年 PyCon 总结诺亚礼物的 PyCon 2011:超出我的预期帖子 
PyCon 2011 海报会议 
 
 原文：https://www.blog.pythonlibrary.org/2011/02/05/pycon-2011-poster-session/ 
 
 
去年我真的没有太多的时间来检查第一次 PyCon 海报会议，但今年我会有时间来真正检查一下。据我所知，今年的会议将会有 35 张海报，它们有一些非常令人惊讶的主题。这里有一个例子: 
使用 Kinect 提高可访问性SPM 的技术剖析。Python，Python 的可扩展并行版本用于水文地球化学建模的 Python使用 Python 和 MySQL 捕获、存储、分析和比较超级计算机性能指标协作制书 
我甚至不知道那些东西是什么意思，但我想弄清楚。你呢？你不想看看这个吗？我很确定这次谈话不会被录下来，所以你需要亲自去看看。如果以上没有让你兴奋，那么试着阅读一下完整列表。 
PyCon 2011: PyQt 教程第一期和第二期 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/10/pycon-2011-pyqt-tutorial-sessions-i-and-ii/ 
 
 
今天早上，我参加了保罗·基普斯的使用 Qt I 在 Python 中创建 GUI 应用程序，然后今天下午，我参加了他的关于 PyQt 的第二个教程的一半。演讲者给了我们许多材料，包括相当广泛的例子。我认为我在上午的会议中学到了很多东西，因为我有相当多的实践代码编辑和使用 Qt 的设计器，这是一个用于创建 GUI 的所见即所得编辑器，有点像微软的 Visual Studio。 
第二次会议只是一个讲座，没有任何形式的实际编码。我在这个教程中没有很好地参与，并决定我最好在休息时离开并做些其他事情。稍后我会研究演讲者的笔记和代码示例。期待以后有一些关于 PyQt，或许还有 PySide 的文章！ 
PyCon 2011:周六上午闪电会谈 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/18/pycon-2011-saturday-morning-lightning-talks/ 
 
 
3 月 12 日周六，早上 8:30 以闪电会谈拉开帷幕！Alfredo Deza 在 chapa.vim 上发表讲话，开始了会议。接下来是迪安·霍尔谈论芯片上的 Python。他提到了 PyMite 项目和 Python 到 DryOS 的移植，这使得 Python 可以在佳能 DSLR 相机上运行。 
Ned Batchelder 谈到了 Slippy 的幻灯片，以及他希望 Python 中也有类似的东西(我想)。他意识到他已经写了一个叫做 Cog 的东西，可以做他想做的事情，然后用剩下的时间谈论这个。 
在 Ned 之后，有一个由 PSF 资助写文档的家伙的演讲。他重新编写了 Python 开发指南，Python 3 移植指南，并建立了网站py3ksupport.appspot.com。 
最后，来自印度的 Baiju Muthukadan 谈到了 GetPython3.net。如果我理解正确的话，这个网站是为 Python 3 包提供反馈的。 
你可以在这里观看演讲。 
PyCon 2011:周六全体会议(有 Dropbox 和 Guido) 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/18/pycon-2011-saturday-plenaries-with-dropbox-and-guido/ 
 
 
Dropbox 全会 
2011 年 3 月 12 日的周六全体会议以 Dropbox 的一名工程师开始，他与 Rian Hunter 一起做了一个题为“Dropbox 是如何做到的以及 Python 是如何帮助的”的演讲。他首先向我们讲述了 Dropbox 必须克服的技术难题。他告诉我们，他们写的所有东西都是用 Python 写的，是他把 Dropbox 移植到 Linux 上的。不管怎样，他给了我们一些关于 Dropbox 的很酷的统计数据，比如下面这个: 
Dropbox 是自 Skype 以来下载量增长最快的应用Dropbox 上每天保存的文件比 Twitter 上的 tweets 还多——每 15 分钟就有 100 万个文件被保存！所有这些都没有任何广告。都是口碑 
老实说，这是一次有点无聊的谈话。视频现在上来了，就留给大家发表自己的看法吧。 
与圭多·范·罗森的炉边谈话 
与 Guido(Python 的创造者)的聊天由 Jesse Noller 主持。如果你愿意，可以在这里看视频。给 Guido 的问题由社区投票决定，但最终由 Jesse 选择。我认为对我来说最重要的是了解到 Guido 在某些编程范例上有困难，比如异步回调。Jesse 还提到 Python 3 对 Mark Pilgrim 来说是个问题。我想他说的话引起了一些争议，但不知何故我错过了。全体会议结束时，他们拿出一个蛋糕给 Guido，因为今年是 Python 的 20 岁生日。 
PyCon 2011:需要会议人员 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/04/pycon-2011-session-staff-needed/ 
 
 
PyCon USA 2011 网站终于在本周发布了日程安排，在过去的几天里，他们增加了注册成为会议主席或会议主持人的功能。这些都是相当重要的角色，需要填补，以使会议顺利进行。 
基本上，一个会议主席将主持一系列的会谈。这意味着他或她将介绍发言人，并帮助保持准时。他们还可以帮助组织一个开放的空间，尽管我不认为这在今年的工作描述中有所提及。最后，他们帮助回答观众的问题。 
会议主持人将帮助演讲者从绿色房间走到合适的舞台。他们以任何方式帮助会议顺利进行。换句话说，他们协助会议主席。 
通过申请其中一个或两个职位，你可以认识有趣的人，结交新朋友。只需进入日程页面，点击演讲旁边的“S”符号即可报名。请记住，你报名参加的是一个房间里的 2 或 3 个讲座。一定要在第一次谈话前 15-30 分钟到休息室去拿你的装备。 
更新 : 查看哪些会议有主席/跑步者的另一种方法已发布到 PyCon 网站 
PyCon 2011:周日上午闪电会谈 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/20/pycon-2011-sunday-morning-lightning-talks/ 
 
 
周日(3 月 13 日)开始时，日光节约时间让我们头脑混乱。从前一天晚上开始，我们失去了一个小时的睡眠。无论如何，会议日本身是从早上 8:30 的闪电谈话开始的。他们只有四个人，但是你看: 
一位名叫 Fecundo Batista 的人谈到了 PyAr、阿根廷 Python 集团以及他们一直在做的事情。Ed Abrams 接着讲述了 Adobe 如何使用 Python。他也在 HP 和 Tabblo 上讲了一点。接下来是关于今年十月即将到来的德国皮肯节的演讲。如果你在欧洲，你应该去看看！最后一个演讲是关于 Gunicorn 项目，一个基于 Ruby unicorn 项目的 WSGI 服务器 
即使只有四个人也有各种各样的演讲。当你去参加闪电会谈时，你永远不知道你会得到什么。即使一个很烂，也只会烂五分钟，所以一定要给他们一个机会！ 
PyCon 2011:周日全会，包括 Threadless、Disqus 和 OpenStack 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/20/pycon-2011-sunday-plenaries-with-threadless-disqus-and-openstack/ 
 
 
3 月 13 日，星期天，最后一个正式的 PyCon 会议日(当然，接下来是冲刺日)。总之，他们有三个有趣的全体会议来引导我们进入那天的讨论。请继续阅读，找出它们的内容。 
无聊的全体会议 
芝加哥 Threadless 公司的技术副总裁克里斯·麦卡沃伊(Chris McAvoy)在全体会议上发表了他所谓的全面 Python 化。Threadless 是一家 t 恤公司，已经存在了至少 10 年，并且成为商界的宠儿已经有一段时间了。我猜这也是当地民主党人喜欢的竞选地点。 
无论如何，他们在 PyCon 的原因是因为他们现在从 php 转向使用 django 作为他们的网站。他谈到了该公司的历史，以及他们现在如何支持各种事业，包括通过销售特殊衬衫来支持日本海啸灾难。他讲了许多蹩脚的笑话，我认为他的讲话是其中骂人最多的。你可以在大多数实时发生的召集线程上阅读关于它的评论。你也可以在这里看到的对话。总的来说，我认为这是一次有趣的谈话。 
Disqus 全体会议 
严雨松和大卫·克莱姆做了一个名为 *disqus 的全体会议——世界上最大的 django 系统！*他们有很多有趣的统计数据，比如: 
disqus 服务 5 亿用户成立于 4 年前只有 16 名员工，其中 8 名是工程师流量每月增长 15-20%6 个月内流量翻倍 
他们创建了一个叫做石像鬼的程序，他们开源了这个程序，它是某种功能开关装饰器。虽然我并不真正理解它的用例。他们还提到使用以下项目:Hudson、Open Sentry、Monitor Graphite、pylint 和 pyflakes(我认为)。你可以在这里阅读与会者的发言，或者在这里观看全体会议。 
OpenStack 全体会议 
安迪·史密斯在 OpenStack 上做了一次全体会议，open stack 是一个起源于 NASA 和 Rackspace 的项目。以下是他提到的相关项目列表: 
快速对象存储系统nova -计算规模，调配虚拟机扫视-图像和注册表存储嵌入 erlang 分布式消息仪表板(dash) - django 管理界面 
史密斯先生提到美国宇航局使用 OpenStack 来探测小行星，所以这很好。美国宇航局也用它来拍摄和管理 WYSE(某种卫星，我想)的照片。下面这些也都用 open stack:Citrix，SCALR，cloudkick，OPSCODE，NTT，piston。 
你可以在这里阅读相关的会议主题或者在这里阅读视频。 
PyCon 2011: TiP BoF 
 
 原文：https://www.blog.pythonlibrary.org/2011/03/19/pycon-2011-tip-bof/ 
 
 
备受尊崇的 TiP BoF(测试 Python“一丘之貉”)会议于周六(2011 年 3 月 12 日)晚上 7 点左右举行。迪士尼提供免费披萨和沙拉。有人(我认为)提供了一些流行音乐。这个房间只有后面有站立的地方。当人们在吃饭时，蛙跳乐队的特里·佩珀主持了会议。他告诉我们 TiP BoF 是如何工作的，然后让他的一名员工向我们展示如何做奇怪的手/手臂伸展动作。如果我没记错的话，他的名字是许飞鸿。 
之后，测试相关的闪电会谈开始了。闪电谈话确实是这次活动的主要亮点，尽管在过去的几年里，酒精吸引了许多人前来。今年，酒店取缔了这种做法，几乎看不到任何酒，这对我来说还不错。我只呆了两个小时，所以我就简单介绍一下我的所见所闻: 
甚至在我们吃任何东西之前，就有很多自慰和其他粗俗的笑话，它们持续了我在那里的大部分时间Peppers 以一个名为 Snakes on a domain 开始会谈，这是关于一个名为Nagios的插件接下来，迪士尼授予杰西·诺勒一个迪士尼啤酒杯，以他们的动画电影《纠结》为主题。Alfredo Deza 做了一个叫做 Konira 的 DSL 测试框架的演讲接下来是关于 Cram 的演讲——一个用于命令行测试的 mercurial 测试套件。我不知道是谁送的。我想是这个:https://bitbucket.org/brodie/cram/src然后是关于实验室外套的演讲。他们让演讲者也穿上了实验服。这张不记得是谁做的了(可能是作者？)或者这个项目到底做了什么…Roman Lisagor 做了一个关于“清新”的演讲，这是 Ruby 黄瓜项目的一个克隆。这是一个鼻子的插件，应该类似于莴苣项目。库马尔·麦克米兰做了一个名为用模拟物体捏造事实的演讲。是的，这是另一个模拟库，但是这个是基于一个叫做 Mocha 的项目(我想他说他也使用了 Michael Foord 的模拟库的东西)。你可以在这里查看:【http://farmdev.com/projects/fudge/下一个演讲是 Python 中的科学测试。这一点我的笔记不好，但我认为它与光明项目有关(如果我错了请纠正我)。演讲者还提到了一个叫 goathub.com 的东西，但据我所知，那并不存在。许飞鸿自己也发表了演讲。它有一个很长的标题:我对漫画的痴迷如何催生了一个新的功能工具。基本上这是一个网络抓取项目，用于下载用 Python 翻译成中文的日本漫画。他叉开了 spynner ，把它变得“更笨”，并把他的叉子叫做“Punky Browster”。我觉得这个项目还不可用。 
为了报名参加闪电讲座，他们使用了一个 convore 线程。前排的人都是会向演讲者起哄的起哄者。他们似乎更喜欢用激烈的咒骂来对付诘问。它可能很有趣也很粗糙。我了解了许多我从未听说过的新项目。这绝对是我认为值得至少看一次的东西。 
PyCon 2011:我想看的 5 场会谈 
 
 原文：https://www.blog.pythonlibrary.org/2011/01/22/pycon-2011-what-5-talks-i-want-to-see/ 
 
 
去年，博客写手们很流行写他们最想在 PyCon 上看到的五场演讲。我不知道今年我是否会去 PyCon，但如果我去了，这些是我的选择，没有特别的顺序。 
你想知道但不敢问的关于腌制的一切！理查德·桑德斯。这不仅听起来很有趣，而且当我说我去参加了一个腌渍演讲时，看到人们的表情也会很有趣。懂 Python 的人用的 Javascript】作者 Ian Bicking。这是 Python 杰出人物之一的一次极端演讲。我不喜欢 Javascript，但也许 Bicking 的谈话会改变我的想法。Python 和机器人:高中编程教学Vern Ceder 著。我在很多地方读到过 Ceder 的名字，我听说他教 Python，他也写了一本关于这个主题的书。此外，机器人很酷，让孩子们对编程感兴趣也很棒！使用 Python 3 为我的 Superboard II 构建云计算服务。我最近断断续续地关注 Beazley 的博客，它总是很有趣，即使我并不总是明白他在做什么。他还时不时地教授 Python 课程，所以我希望这个演讲能够很好地组织起来。藏在标准图书馆的宝藏作者道格·海尔曼。我不记得我是否听过赫尔曼的演讲，但我很喜欢他的博客。 
当然，我想看的不止 5 部，而且我肯定会有很多部同时发生，所以我不会全部都去看。Alex Gaynor 有几个我想看的演讲，Martelli 正在做另一个大脑演示。卫斯理·陈也有几个讲座。我没有看到杰夫拉什的任何东西，这是一个耻辱。我过去很喜欢他。不管怎样，你想看什么样的会谈？请在评论中告诉我！ 
PyCon 2011:为什么你应该去！ 
 
 原文：https://www.blog.pythonlibrary.org/2011/01/21/pycon-2011-why-you-should-go/ 
 
 
PyCon 一年只有一次，不管你是新的 Python 程序员还是过去 20 年的核心开发人员，我认为你都应该看看 PyCon。花点时间了解一些 PyCon 信息，这样你就可以告诉你的雇主为什么他们应该付钱让你去，或者为什么你应该把辛苦赚来的钱投资到一个只涉及一种编程语言的会议上！ 
辅导日 
在会议开始之前，PyCon 非正式地开始了两天拥挤的辅导日。教程范围从简单的到高级的。你可以学习桌面 GUI 创建，谷歌应用引擎或者只是学习一些随机的食谱。它们甚至涵盖了文档！ 
会议本身 
大会是大人物出场的地方。它们涵盖了一切: Django ， Javascript ，模拟测试，甚至机器人！今年，他们有一个“极端”的轨道，这是一套把介绍性的材料扔出窗外，给与会者除了熏肉什么都没有的谈话。时间表中任何标有“E”的都是极端的。 
会议也有供应商，一个海报会议和新的创业街，在那里你可以和 Python 创业公司的人面对面交流。如果你想找份新工作，那么这是个好去处！ 
短距离赛跑 
发布会结束后，还有几天的冲刺。你可以加入其中的任何一个，帮助完成各种项目。他们需要文档、简单的 bug、增强，甚至从 2.x 移植到 3.x 的帮助。这意味着他们将接受经验丰富的专业人士，而不是刚刚开始 Python 生涯的人。 
你要去吗？ 
那么，你去吗？我还没有发现我是不是，但是"
KDNuggets 博客中文翻译（六十九）,2024-11-03,"原文：KDNuggets

协议：CC BY-NC-SA 4.0

使用 Gretel 和 Apache Airflow 构建合成数据管道

原文：www.kdnuggets.com/2021/09/build-synthetic-data-pipeline-gretel-apache-airflow.html

评论

作者：Drew Newberry，Gretel.ai 的软件工程师

大家好，我是 Drew，我是 Gretel 的一名软件工程师。我最近一直在思考如何将 Gretel API 集成到现有工具中的模式，以便轻松构建安全性和客户隐私为首要特性的数据管道，而不仅仅是事后的考虑或需要打勾的选项。

在 Gretel 的工程师和客户中，Apache Airflow 是一个很受欢迎的数据工程工具。它也与 Gretel 配合得很好。在这篇博客文章中，我们将展示如何使用 Airflow、Gretel 和 PostgreSQL 构建一个合成数据管道。让我们开始吧！

什么是 Airflow

Airflow 是一个常用于构建数据管道的工作流自动化工具。它使数据工程师或数据科学家能够使用 Python 和其他熟悉的构造程序化地定义和部署这些管道。Airflow 的核心概念是 DAG，即有向无环图。Airflow DAG 提供了一个模型和一组 API，用于定义管道组件、它们的依赖关系和执行顺序。

你可能会发现 Airflow 管道将数据从产品数据库复制到数据仓库。其他管道可能会执行将规范化数据连接到一个适合分析或建模的单一数据集的查询。还有的管道可能会发布每日报告，汇总关键业务指标。这些用例的共同主题是：协调跨系统的数据流动。这正是 Airflow 擅长的地方。

利用 Airflow 及其丰富的 集成 生态系统，数据工程师和科学家可以将任意数量的异构工具或服务编排成一个易于维护和操作的统一管道。了解这些集成功能后，我们将开始讨论 Gretel 如何集成到 Airflow 管道中，以改进常见的数据操作工作流程。

Gretel 如何融入其中？

在 Gretel，我们的使命是让数据处理变得更加简单和安全。在与客户交流时，我们经常听到的一个痛点是，获取敏感数据所需的时间和精力。通过使用 Gretel Synthetics，我们可以通过生成数据集的合成副本来减少处理敏感数据的风险。通过将 Gretel 与 Airflow 集成，可以创建自助服务管道，使数据科学家能够快速获取所需的数据，而不需要每个新数据请求都依赖数据工程师。

为了演示这些功能，我们将构建一个 ETL 管道，该管道从数据库中提取用户活动特征，生成数据集的合成版本，并将数据集保存到 S3。将合成数据集保存到 S3 后，它可以被数据科学家用于下游建模或分析，而不会影响客户隐私。

首先，让我们从鸟瞰图开始了解管道。图中的每个节点代表一个管道步骤，或 Airflow 术语中的“任务”。

示例 Gretel 合成管道在 Airflow 上。

我们可以将管道分为三个阶段，这类似于你在 ETL 管道中可能遇到的情况：

提取 - extract_features 任务将查询数据库，并将数据转换为数据科学家可以用来构建模型的特征集。

合成 - generate_synthetic_features 将提取的特征作为输入，训练一个合成模型，然后使用 Gretel API 和云服务生成合成特征集。

加载 - upload_synthetic_features 将合成特征集保存到 S3 中，在那里可以被下游模型或分析程序使用。

在接下来的几个部分，我们将更详细地探讨这三个步骤。如果你希望跟随每个代码示例，你可以前往 gretelai/gretel-airflow-pipelines 下载本博客文章中使用的所有代码。该仓库还包含启动 Airflow 实例和运行整个管道的说明。

此外，在我们剖析每个组件之前，查看整个 Airflow 管道可能会很有帮助，dags/airbnb_user_bookings.py。以下部分的代码片段是从链接的用户预订管道中提取的。

提取特征

第一个任务，extract_features 负责从源数据库中提取原始数据并将其转换为特征集。这是任何机器学习或分析管道中可能遇到的常见特征工程问题。

在我们的示例管道中，我们将配置一个 PostgreSQL 数据库，并从 Airbnb Kaggle 竞赛 中加载预订数据。

这个数据集包含两个表格，Users 和 Sessions。Sessions 包含一个外键引用，user_id。利用这种关系，我们将创建一个特征集，包含按用户聚合的各种预订指标。下图展示了用于构建这些特征的 SQL 查询。

py` WITH session_features_by_user AS ( SELECT user_id, count() AS number_of_actions_taken, count(DISTINCT action_type) AS number_of_unique_actions, round(avg(secs_elapsed)) AS avg_session_time_seconds, round(max(secs_elapsed)) AS max_session_time_seconds, round(min(secs_elapsed)) AS min_session_time_seconds, ( SELECT count() FROM sessions s WHERE s.user_id = user_id AND s.action_type = ‘booking_request’) AS total_bookings FROM sessions GROUP BY user_id ) SELECT u.id AS user_id, u.gender, u.age, u.language, u.signup_method, u.date_account_created, s.number_of_actions_taken, s.number_of_unique_actions, s.avg_session_time_seconds, s.min_session_time_seconds, s.max_session_time_seconds FROM session_features_by_user s LEFT JOIN users u ON u.id = s.user_id LIMIT 5000 py The SQL query is then executed from our Airflow pipeline and written to an intermediate S3 location using the following task definition. @task() def extract_features(sql_file: str) -> str: context = get_current_context() sql_query = Path(sql_file).read_text() key = f""{context[‘dag_run’].run_id}_booking_features.csv"" with NamedTemporaryFile(mode=“r+”, suffix=“.csv”) as tmp_csv: postgres.copy_expert( f""copy ({sql_query}) to stdout with csv header"", tmp_csv.name ) s3.load_file( filename=tmp_csv.name, key=key, ) return key py The input to the task, sql_file, determines what query to run on the database. This query will be read-in to the task and then executed against the database. The results of the query will then be written to S3 and the remote file key will be returned as an output of the task. The screenshot below shows a sample result set of the extraction query from above. We will describe how to create a synthetic version of this dataset in the next section. ![](https://kdn.flygon.net/docs/img/1f5ce925697015f6d932eac9117b1775.png) *Query result preview.* ## Synthesize Features using Gretel APIs     To generate a synthetic version of each feature, we must first train a synthetic model, and then run the model to generate synthetic records. Gretel has a set of Python SDKs that make it easy to integrate into Airflow tasks. In addition to the Python Client SDKs, we’ve created a [Gretel Airflow Hook](https://github.com/gretelai/gretel-airflow-pipelines/blob/main/plugins/hooks/gretel.py) that manages Gretel API connections and secrets. After setting up a Gretel Airflow Connection, connecting to the Gretel API is as easy as from hooks.gretel import GretelHook gretel = GretelHook() project = gretel.get_project() py For more information about how to configure Airflow connections, please refer to our Github repository [README](https://github.com/gretelai/gretel-airflow-pipelines#2-configure-airflow-connections). The project variable in the example above can be used as the main entrypoint for training and running synthetic models using Gretel’s API. For more details, you can check out our [Python API docs](https://python.docs.gretel.ai/en/stable/projects/projects.html). Referring back to the booking pipeline, we’ll now review the generate_synthetic_features task. This step is responsible for training the synthetic model using the features extracted in the previous task. @task() def generate_synthetic_features(data_source: str) -> str: project = gretel.get_project() model = project.create_model_obj( model_config=“synthetics/default”, data_source=s3.download_file(data_source) ) model.submit_cloud() poll(model) return model.get_artifact_link(“data_preview”) py Looking at the method signature, you will see it takes a path, data_source. This value points to the S3 features extracted in the previous step. In a later section we’ll walk through how all these inputs and outputs are wired together. When creating the model using project.create_model_obj, the model_config param represents the synthetic model configuration used to generate the model. In this pipeline, we’re using our [default model config](https://github.com/gretelai/gretel-blueprints/blob/main/config_templates/gretel/synthetics/default.yml), but many other [configuration options](https://docs.gretel.ai/synthetics/synthetics-model-configuration) are available. After the model has been configured, we call model.submit_cloud(). This will submit the model for training and record generation using Gretel Cloud. Calling poll(model) will block the task until the model has completed training. Now that the model has been trained, we’ll use get_artifact_link to return a link to download the generated synthetic features. ![](https://kdn.flygon.net/docs/img/d55c92a904377264013477a9a1646316.png) *Data preview of the synthetic set of features.*   This artifact link will be used as an input to the final upload_synthetic_features step. ## Load Synthetic Features     The original features have been extracted, and a synthetic version has been created. Now it’s time to upload the synthetic features so they can be accessed by downstream consumers. In this example, we’re going to use an S3 bucket as the final destination for the dataset. @task() def upload_synthetic_features(data_set: str): context = get_current_context() with open(data_set, “rb”) as synth_features: s3.load_file_obj( file_obj=synth_features, key=f""{…_booking_features_synthetic.csv"", ) py This task is pretty straightforward. The data_set input value contains a signed HTTP link to download the synthetic dataset from Gretel’s API. The task will read that file into the Airflow worker, and then use the already configured S3 hook to upload the synthetic feature file to an S3 bucket where downstream consumers or models can access it. ## Orchestrating the Pipeline     Over the last three sections we’ve walked through all the code required to extract, synthesize and load a dataset. The last step is to tie each of these tasks together into a single Airflow pipeline. If you’ll recall back to the beginning of this post, we briefly mentioned the concept of a DAG. Using Airflow’s TaskFlow API we can compose these three Python methods into a DAG that defines the inputs, outputs and order each step will be run. feature_path = extract_features( “/opt/airflow/dags/sql/session_rollups__by_user.sql” ) synthetic_data = generate_synthetic_features(feature_path) upload_synthetic_features(synthetic_data) py If you follow the path of these method calls, you will eventually get a graph that looks like our original feature pipeline. ![](https://kdn.flygon.net/docs/img/9776c937553639c4bd8886936d454f38.png) *Gretel synthetics pipeline on Airflow.*   If you want to run this pipeline, and see it in action, head over to the [accompanying Github repository](https://github.com/gretelai/gretel-airflow-pipelines). There you will find instructions on how to start an Airflow instance and run the pipeline end to end. ## Wrapping things up     If you’ve made it this far, you’ve seen how Gretel can be integrated into a data pipeline built on Airflow. By combining Gretel’s developer friendly APIs, and Airflow’s powerful system of hooks and operators it’s easy to build ETL pipelines that make data more accessible and safer to use. We also talked about a common feature engineering use case where sensitive data may not be readily accessible. By generating a synthetic version of the dataset, we reduce the risk of exposing any sensitive data, but still retain the utility of the dataset while making it quickly available to those who need it. Thinking about the feature pipeline in more abstract terms, we now have a pattern that can be repurposed for any number of new SQL queries. By deploying a new version of the pipeline, and swapping out the initial SQL query, we can front any potentially sensitive query with a synthetic dataset that preserves customer privacy. The only line of code that needs to change is the path to the sql file. No complex data engineering required. ## Thanks for reading     Send us an email at [hi@gretel.ai](https://gretel.ai/blog/running-gretel-on-apache-airflow#) or come join us in [Slack](https://gretel.ai/slackinvite) if you have any questions or comments. We’d love to hear how you’re using Airflow and how we can best integrate with your existing data pipelines.   **Bio: [Drew Newberry](https://www.linkedin.com/in/drew-newberry/)** is a Software Engineer at Gretel.ai. [Original](https://gretel.ai/blog/running-gretel-on-apache-airflow). Reposted with permission. **Related:** * Prefect: How to Write and Schedule Your First ETL Pipeline with Python * 15 Python Snippets to Optimize your Data Science Pipeline * How to Query Your Pandas Dataframe * * * ## Our Top 3 Course Recommendations ![](https://kdn.flygon.net/docs/img/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast track to a career in cybersecurity. ![](https://kdn.flygon.net/docs/img/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up your data analytics game ![](https://kdn.flygon.net/docs/img/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support your organization in IT * * * ### More On This Topic * [5 Airflow Alternatives for Data Orchestration](https://www.kdnuggets.com/5-airflow-alternatives-for-data-orchestration) * [6 Data Science Technologies You Need to Build Your Supply Chain Pipeline](https://www.kdnuggets.com/2022/01/6-data-science-technologies-need-build-supply-chain-pipeline.html) * [How to Build a Scalable Data Architecture with Apache Kafka](https://www.kdnuggets.com/2023/04/build-scalable-data-architecture-apache-kafka.html) * [High-Fidelity Synthetic Data for Data Engineers and Data Scientists Alike](https://www.kdnuggets.com/2022/tonic-high-fidelity-synthetic-data-engineers-scientists-alike.html) * [How to Democratize AI/ML and Data Science with AI-generated Synthetic Data](https://www.kdnuggets.com/2022/11/mostly-ai-democratize-aiml-data-science-aigenerated-synthetic-data.html) * [Data access is severely lacking in most companies, and 71% believe…](https://www.kdnuggets.com/2023/07/mostly-data-access-severely-lacking-synthetic-data-help.html)

用 Python 在 5 分钟内构建文本转语音转换器

原文：www.kdnuggets.com/2022/09/build-texttospeech-converter-python-5-minutes.html

Kelly Sikkema

对于你的早期编程技能，最好的事情就是项目。你可能拥有知识，但将其应用到实际中才是真正的挑战，并保持你的竞争力。

我们的前三个课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业道路。

 2. 谷歌数据分析专业证书 - 提升你的数据分析技能

 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 工作

最近几个月我采访了一些前辈，他们提到的新人常见问题是缺乏将技能应用到项目或现实问题中的经验。所以，我觉得创建这个快速 5 分钟的项目来帮助应用和提升你的技能会很有趣。

我选择了展示如何在 Python 中构建一个文本转语音转换器，这不仅简单，而且有趣且互动。我将向你展示两种用 Python 实现的方法。

那么，让我们开始吧。

使用 pyttsx3
要求

对于这个快速简单的构建，你需要以下模块：pyttsx3。这个模块是一个文本转语音转换库，兼容 Python 2 和 3。

要安装这个模块，请输入以下内容：

pip install pyttsx3

1
导入

现在你需要将库导入到你的环境中：

import pyttsx3

1
引擎实例

我们现在将初始化‘init’函数以获得引擎实例

engine = pyttsx3.init()

1
告诉我们的引擎要说什么

使用引擎上的‘say’方法，我们输入希望被朗读的文本

engine.say('Oh my. I can't believe I did this in less than 5 minutes')

1
现在听听吧

我们现在使用“运行并等待”方法来处理语音命令

engine.runAndWait()

1

就这样。现在另一个…

使用 gTTS API
要求

对于这个文本转语音转换器，我们需要 Google 文本转语音 API。它可以轻松将输入的文本转换为音频，然后保存为 mp3 文件。

这可以用于多种语言，如英语、印地语、泰米尔语、法语、德语等。

要安装 API，请输入以下内容：

pip install gTTS

1
导入

现在你需要将库导入到你的环境中：

from gtts import gTTS

1

你还需要导入 os 以播放音频：

import os

1
输入你的文本
text = 'Learn how to build something with Python in 5 minutes'

1
选择你的语言

选择你喜欢的语言。你可以通过点击这个 链接 查找语言列表。

language = 'en'

1
将文本传递给引擎

你还可以选择音频播放速度是快还是慢。

myobj = gTTS(text=mytext, lang=language, slow=False)

1
将你的音频保存为 .mp3
myobj.save(""mytext2speech.mp3"")

1
该听听了

mpg123 是一个免费的开源音频播放器，我们将添加它来指定我们希望 .mp3 文件在哪个程序中播放。

os.system(""mpg321 mytext2speech.mp3"")

1

哒哒！你选择的媒体播放器应该已经说了：

“学习如何在 5 分钟内用 Python 构建东西”

总结一下

这篇文章完全是为了让你探索你的 Python 技能，然后构建更好更酷的项目。& 为了一点乐趣！

妮莎·阿里亚 是一位数据科学家和自由技术写作人。她特别关注提供数据科学职业建议或教程以及围绕数据科学的理论知识。她还希望探索人工智能如何有益于人类寿命的不同方式。她是一位热衷于学习、寻求拓展技术知识和写作技能的热心者，同时帮助引导他人。

更多相关话题

用 Python 在 5 分钟内构建网页抓取器

FastAPI 教程：用 Python 几分钟内构建 API

5 分钟内构建机器学习网页应用

KDnuggets 新闻 2022 年 3 月 9 日：5 分钟内构建机器学习网页应用

用 Hugging Face 和 Gradio 在 5 分钟内构建 AI 聊天机器人

语音识别指标的演变

用 Python 在 5 分钟内构建网络爬虫

原文：www.kdnuggets.com/2022/02/build-web-scraper-python-5-minutes.html

数据科学家通常需要收集大量数据，以便在组织中提取业务价值。不幸的是，这是一项经常被忽视的技能，因为大多数数据科学课程并不教授如何收集外部数据。相反，大多注重于模型构建和训练。

在本文中，我将向你展示如何从头开始创建一个网络爬虫。

我们的前三个课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业道路。

 2. 谷歌数据分析专业证书 - 提升你的数据分析水平

 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT

如果你还不熟悉这个术语，网络爬虫是一个自动化工具，可以从网站中提取大量数据。借助网络抓取，你可以在几分钟内收集多达数十万条数据点。

在本教程中，我们将抓取一个名为 “Quotes to Scrape” 的网站。这个网站是专门为练习网络抓取而建立的。

在本文结束时，你将熟悉以下内容：

从网站提取原始 HTML

使用 BeautifulSoup 库来解析这些 HTML 并从网站中提取有用的信息

同时从多个网页收集数据

将这些数据存储到 Pandas 数据框中

在我们开始构建爬虫之前，请确保已安装以下库 — Pandas，BeautifulSoup，requests。

完成后，让我们查看一下我们想要抓取的网站，并决定要从中提取的数据点。

这个网站包含了名人名言的列表。页面上显示了三项主要信息——名言、作者及其相关的几个标签。

该网站有十页，我们将抓取其中的所有信息。

让我们首先导入以下库：

import pandas as pd
from bs4 import BeautifulSoup
import requests

1
2
3

然后，使用 requests 库，我们将获取我们想抓取的页面并提取其 HTML：

f = requests.get('[`quotes.toscrape.com/'`](http://quotes.toscrape.com/'))

1

接下来，我们将把网站的 HTML 文本传递给 BeautifulSoup，它将解析这些原始数据，以便轻松进行抓取：

soup = BeautifulSoup(f.text)

1

现在，网站的所有数据都存储在 soup 对象中。我们可以很容易地在这个对象上运行 BeautifulSoup 的内置函数，以提取我们想要的数据。

例如，如果我们想提取网页上所有的文本，可以通过以下代码轻松完成：

print(soup.get_text())

1

你应该能看到网站上的所有文本出现在你的屏幕上。

现在，让我们开始抓取网站上列出的引用。右键点击任何一个引用，选择 “检查元素”。Chrome 开发者工具将会出现在你的屏幕上：

BeautifulSoup 具有像 find() 和 findAll() 这样的函数，你可以用它们从网页中提取特定的 HTML 标签。

在这种情况下，请注意名为 text 的 <span> 类被高亮显示。这是因为你右键点击了页面上的一个引用，所有引用都属于这个 text 类。

我们需要提取这个类中的所有数据：

for i in soup.findAll(""div"",{""class"":""quote""}):
print((i.find(""span"",{""class"":""text""})).text)

1
2

一旦你运行上述代码，将会生成以下输出：

网页上有十个引用，我们的抓取器已经成功地收集了所有这些引用。太棒了！

现在，我们将使用相同的方法抓取作者名字。

如果你右键点击网页上的任何一个作者名字并选择检查，你会看到它们都包含在 <small> 标签内，且类名为 author。

我们将使用 find() 和 findAll() 函数来提取这个标签内的所有作者名字。

for i in soup.findAll(""div"",{""class"":""quote""}):
print((i.find(""small"",{""class"":""author""})).text)

1
2

以下输出将被渲染：

再次，我们成功地抓取了页面上列出的所有作者。我们快完成了！

最后，我们将抓取网站上列出的标签。

如果你右键点击任何一个标签并选择检查元素，你会看到它们都包含在 <meta> 标签内，并用逗号分隔：

同时，请注意 <meta> 标签被一个父 <div> 标签包裹，且类名为 tags。

要提取页面上的所有标签，请运行以下代码：

for i in soup.findAll(""div"",{""class"":""tags""}):
print((i.find(""meta""))['content'])

1
2

屏幕上的输出将如下所示：

我们现在已经成功地抓取了网站单页上的所有数据。

但我们还没有完成！记住，网站有十页，我们需要从所有页面中收集相同的数据。

在我们做这些之前，让我们创建三个空数组，以便存储收集到的数据。

quotes = []
authors = []
tags = []

1
2
3

现在，我们将创建一个范围从 1 到 10 的循环，并遍历网站上的每一页。我们将运行之前创建的完全相同的代码行。唯一的区别是，我们现在将其附加到数组中，而不是打印输出。

for pages in range(1,10):f = requests.get('[`quotes.toscrape.com/page/'+str(pages))`](http://quotes.toscrape.com/page/'+str(pages)))
soup = BeautifulSoup(f.text)for i in soup.findAll(""div"",{""class"":""quote""}):
quotes.append((i.find(""span"",{""class"":""text""})).text)

for j in soup.findAll(""div"",{""class"":""quote""}):
authors.append((j.find(""small"",{""class"":""author""})).text)for k in soup.findAll(""div"",{""class"":""tags""}):
tags.append((k.find(""meta""))['content'])

1
2
3
4
5
6
7

完成！

最后，让我们将所有收集到的数据汇总到一个 Pandas 数据框中：

finaldf = pd.DataFrame(
{'Quotes':quotes,
 'Authors':authors,
 'Tags':tags
})

1
2
3
4
5

查看最终数据框的开头，我们可以看到所有抓取到的数据都已被整理到三列中：

这就是本教程的全部内容！

我们已经成功使用 Python 库抓取了一个网站，并将提取的数据存储到数据框中。

这些数据可以用于进一步分析——你可以构建一个聚类模型来将相似的引用聚集在一起，或者训练一个模型，根据输入的引用自动生成标签。

如果你想练习上述学到的技能， 这里 是另一个相对简单的网站可以抓取。

网页抓取不仅仅是本文中提到的技术。现实中的网站通常有机器人保护机制，这使得从数百个页面中收集数据变得困难。当你想从像上述那样的静态 HTML 网页中提取数据时，使用 requests 和 BeautifulSoup 就足够了。

如果你从需要认证的网站抓取数据，或有验证码等验证机制，或者在页面加载时有 JavaScript 运行，你将需要使用像 Selenium 这样的浏览器自动化工具来帮助抓取。

如果你想学习用于网页抓取的 Selenium，我建议你从 this 初学者友好的教程开始。

Natassha Selvaraj 是一位自学成才的数据科学家，热衷于写作。你可以在 LinkedIn 上与她联系。

更多相关内容

5 分钟构建机器学习网页应用

KDnuggets 新闻 2022 年 3 月 9 日：用 5 分钟构建机器学习网页应用

FastAPI 教程：用 Python 分钟构建 API

用 Python 在 5 分钟内构建文本到语音转换器

用 Hugging Face 和 Gradio 在 5 分钟内构建 AI 聊天机器人

在 3 分钟内理解偏差-方差权衡

创建你的第一个语音助手

原文：www.kdnuggets.com/2019/09/build-your-first-voice-assistant.html

评论

来源：giphy

现在，听到有人跟不存在的人说话已不再令人惊讶。我们问 Alexa 天气情况和调低恒温器的温度。然后，我们询问 Siri 我们今天的日程安排并拨打电话。现在我们通过声音和语音界面技术连接得比以往任何时候都更多。我无法再想象手动完成任务的情景！这真是未来的科技。

— 福布斯

介绍

谁不想拥有一个总是听到你呼唤、预见你每个需求并在必要时采取行动的助手呢？得益于基于人工智能的语音助手，这种奢侈现在已经可以实现。

语音助手体积较小，能够在听到你的命令后执行各种操作。它们可以开灯、回答问题、播放音乐、下在线订单，并做各种基于 AI 的事情。

语音助手不应与虚拟助手混淆，后者是远程工作的人，因此可以处理各种任务。语音助手则是基于技术的。随着语音助手变得越来越强大，它们在个人和商业领域的实用性也会增加。

来源

什么是语音助手？

一个语音助手或智能个人助手是一个可以根据口头命令执行任务或服务的软件代理，即通过解读人类语言并通过合成声音回应。用户可以通过语音提问、控制家庭自动化设备和媒体播放，以及通过口头命令管理其他基本任务，如电子邮件、待办事项列表、打开或关闭应用程序等。

让我给你一个例子，Braina (Brain Artificial) 是一个智能个人助手、人类语言界面、自动化和语音识别软件，用于 Windows PC。Braina 是一个多功能的 AI 软件，它允许你使用语音命令与计算机互动，支持世界上大多数语言。Braina 还允许你在世界上 100 多种不同语言中准确地将语音转换为文本。

语音助手的历史

现代语音助手的历史

近年来，语音助手在苹果公司整合了令人惊叹的虚拟助手 Siri 后获得了主要平台。真正的演变时间线开始于 1962 年的西雅图世博会，IBM 展示了一种名为 Shoebox 的独特装置。它的实际大小是一个鞋盒，可以执行科学功能，能够识别 16 个单词，并用人类可识别的声音发音，包含 0 到 9 的数字。

在 1970 年代，匹兹堡的卡内基梅隆大学的研究人员——在美国国防部及其国防高级研究计划局 (DARPA) 的巨大帮助下——制作了 Harpy。它能够理解大约 1,000 个单词，相当于三岁小孩的词汇量。

像苹果和 IBM 这样的大组织在 90 年代初开始制作利用语音识别的产品。1993 年，Macintosh 开始在其 Macintosh 电脑上通过 PlainTalk 构建语音识别技术。

1997 年 4 月，Dragon NaturallySpeaking 是第一个能够理解约 100 个单词并将其转换为可读内容的持续听写产品。

来源

话虽如此，构建一个简单的基于语音的桌面/笔记本助手，具备以下功能会有多酷呢：

在浏览器中打开子版块。

在浏览器中打开任何网站。

给你的联系人发送一封电子邮件。

启动任何系统应用程序。

告诉你几乎任何城市的当前天气和温度

告诉你当前的时间。

问候

在 VLC 媒体播放器上播放一首歌曲（当然，你需要在你的笔记本/台式机上安装 VLC 媒体播放器）

更改桌面壁纸。

告诉你最新的新闻动态。

告诉你几乎任何你问的问题。

所以在这篇文章中，我们将构建一个基于语音的应用程序，它可以完成上述所有任务。但首先，查看下面我在与桌面语音助手互动时录制的视频，我称她为 Sofia。

Nagesh Chauhan 的新视频

与 Sofia 的互动

我希望你们喜欢我与 Sofia 互动的上面的视频。现在让我们开始构建这个酷炫的东西……

依赖关系和要求：

系统要求：Python 2.7，Spyder IDE，MacOS Mojave（版本 10.14）

安装所有这些 Python 库：

pip install SpeechRecognition
pip install beautifulsoup4
pip install vlc
pip install youtube-dl
pip install pyowm
pip install wikipedia

1
2
3
4
5
6
让我们开始使用 Python 构建我们的桌面语音助手

来源

首先导入所有所需的库：

py` import speech_recognition as sr import os import sys import re import webbrowser import smtplib import requests import subprocess from pyowm import OWM import youtube_dl import vlc import urllib import urllib2 import json from bs4 import BeautifulSoup as soup from urllib2 import urlopen import wikipedia import random from time import strftime ```py ````

为了让我们的语音助手执行上述所有功能，我们必须在一个方法中编写每个功能的逻辑。

所以我们的第一步是创建一个方法来解释用户的语音响应。

py` def myCommand(): r = sr.Recognizer() with sr.Microphone() as source: print(‘说点什么…’) r.pause_threshold = 1 r.adjust_for_ambient_noise(source, duration=1) audio = r.listen(source) try: command = r.recognize_google(audio).lower() print(‘你说了: ’ + command + ‘\n’) # 如果收到无法识别的语音则循环继续监听命令 except sr.UnknownValueError: print(’…') command = myCommand(); return command ```py ````

接下来，创建一个方法，将文本转换为语音。

py` def sofiaResponse(audio): print(audio) for line in audio.splitlines(): os.system(""say "" + audio) ```py ````

现在创建一个循环以继续执行多个命令。在方法 assistant() 中传递用户命令(myCommand()) 作为参数。

py` while True: assistant(myCommand()) ```py ````

我们的下一步是为每个功能创建多个 if 语句。因此，让我们看看如何在 if 语句中为每个命令创建这些小模块。

1. 在浏览器中打开 Reddit 子版块。

用户将给出任何命令以打开 Reddit 的子版块，命令应为 “Hey Sofia! Can you please open Reddit subreddit_name”。 只有斜体粗体短语需要按原样使用。你可以使用任何前缀，只要注意斜体粗体短语。

工作原理 : 如果你在命令中说了短语open reddit，那么它将使用 re.search() 在用户命令中搜索子版块名称。子版块将通过www.reddit.com进行搜索，并使用 Python 的 Webbrowser 模块在浏览器中打开。Webbrowser模块提供了一个高级接口，允许用户显示基于 Web 的文档。

py` if ‘open reddit’ in command: reg_ex = re.search(‘open reddit (.*)’, command) url = ‘https://www.reddit.com/’ if reg_ex: subreddit = reg_ex.group(1) url = url + ‘r/’ + subreddit webbrowser.open(url) sofiaResponse(‘Reddit 内容已经为您打开。’) ```py ````

所以，上面的代码将在默认浏览器中打开你所期望的 Reddit。

2. 在浏览器中打开任意网站。

你可以通过说“打开 website.com”或“打开 website.org”来打开任何网站。

例如：“请打开 facebook.com”或“嘿，你能打开 linkedin.com 吗”，你可以像这样让苏菲亚为你打开任何网站。

如何运作：如果你在指令中说了打开这个词，它将使用 re.search() 在用户的指令中搜索网站名称。接着，它将把网站名称附加到 www. 上，并通过webbrowser模块在浏览器中打开完整的 URL。

py` elif ‘open’ in command: reg_ex = re.search(‘open (.+)’, command) if reg_ex: domain = reg_ex.group(1) print(domain) url = ‘https://www.’ + domain webbrowser.open(url) sofiaResponse(‘您请求的网站已为您打开。’) else: pass ```py ````

3. 发送邮件。

你也可以让桌面助手发送邮件。

如何运作：如果你在指令中说了邮件这个词，那么机器人会询问收件人。如果我的回复是 rajat，机器人将使用 Python 的 smtplib 库。 smtplib 模块定义了一个 SMTP 客户端会话对象，可以用来向任何具有 SMTP 或 ESMTP 监听守护进程的互联网机器发送邮件。发送邮件使用 Python 的 smtplib 和 SMTP 服务器。首先，它会使用smtplib.SMTP() 初始化 Gmail SMTP，然后使用ehlo() 函数识别服务器，然后加密会话starttls()，接着使用login() 登录到你的邮箱，然后使用sendmail() 发送消息。

py` elif ‘email’ in command: sofiaResponse(‘收件人是谁？’) recipient = myCommand()if ‘rajat’ in recipient: sofiaResponse(‘我应该对他说什么？’) content = myCommand() mail = smtplib.SMTP(‘smtp.gmail.com’, 587) mail.ehlo() mail.starttls() mail.login(‘your_email_address’, ‘your_password’) mail.sendmail(‘sender_email’, ‘receiver_email’, content) mail.close() sofiaResponse(‘邮件已成功发送。你可以查看你的收件箱。’)else: sofiaResponse(‘我不知道你说的是什么意思！’) ```py ````

4. 启动任何系统应用程序。

说“启动日历”或“你能启动 Skype 吗”或“苏菲亚启动查找器”等，苏菲亚将为你启动该系统应用程序。

它是如何工作的：如果你在命令中提到launch这个词，它将使用 re.search() 搜索应用程序名称（如果它存在于你的系统中）。然后，它会将后缀“.app”附加到应用程序名称上。例如，你的应用程序名称可能是 calender.app（在 macOS 中，执行文件以 .app 结尾，而在 Windows 中则以 .exe 结尾）。所以执行应用程序名称将通过 python subprocess 的 Popen() 函数启动。subprocess 模块使你能够从 Python 程序中启动新应用程序。

py` elif ‘launch’ in command: reg_ex = re.search(‘launch (.*)’, command) if reg_ex: appname = reg_ex.group(1) appname1 = appname+“.app” subprocess.Popen([“open”, “-n”, “/Applications/” + appname1], stdout=subprocess.PIPE)sofiaResponse(‘我已经启动了所需的应用程序’) ```py ````

5. 告诉你几乎任何城市的当前天气和温度。

Sofia 还可以告诉你世界上任何城市的天气、最高温度和最低温度。用户只需说类似“伦敦当前的天气如何”或“告诉我德里的当前天气”这样的句子。

它是如何工作的：如果你在命令中提到current weather这个短语，它将使用 re.search() 搜索城市名称。我使用了 Python 的 pyowm 库来获取任何城市的天气。get_status() 会告诉你天气状况，如雾霾、阴天、雨天等，而 get_temperature() 会告诉你城市的最高和最低温度。

py` elif ‘current weather’ in command: reg_ex = re.search(‘current weather in (.*)’, command) if reg_ex: city = reg_ex.group(1) owm = OWM(API_key=‘ab0d5e80e8dafb2cb81fa9e82431c1fa’) obs = owm.weather_at_place(city) w = obs.get_weather() k = w.get_status() x = w.get_temperature(unit=‘celsius’) sofiaResponse(‘%s 的当前天气是 %s。最高温度是 %0.2f 度，最低温度是 %0.2f 度’ % (city, k, x[‘temp_max’], x[‘temp_min’])) ```py ````

6. 告诉你当前时间。

“Sofia，你能告诉我当前时间吗？”或者“现在几点了？”Sofia 将告诉你你所在时区的当前时间。

它是如何工作的：非常简单

py` elif ‘time’ in command: import datetime now = datetime.datetime.now() sofiaResponse(‘当前时间是 %d 小时 %d 分钟’ % (now.hour, now.minute)) ```py ````

7. 问候/离开

说“hello Sofia”来打招呼，或者当你想让程序终止时，可以说类似“shutdown Sofia”或“Sofia 请关闭”等。

工作原理：如果你在指令中提到了hello这个词，那么根据一天中的时间，机器人会向用户打招呼。如果时间超过中午 12 点，机器人会回应“Hello Sir. Good afternoon”，同样，如果时间超过下午 6 点，机器人会回应“Hello Sir. Good evening”。当你给出关闭命令时，sys.exit() 将被调用以终止程序。

py` #问候索非亚 elif ‘hello’ in command: day_time = int(strftime(‘%H’)) if day_time < 12: sofiaResponse(‘Hello Sir. Good morning’) elif 12 <= day_time < 18: sofiaResponse(‘Hello Sir. Good afternoon’) else: sofiaResponse(‘Hello Sir. Good evening’)#终止程序 elif ‘shutdown’ in command: sofiaResponse(‘Bye bye Sir. Have a nice day’) sys.exit() ```py ````

8. 在 VLC 媒体播放器中播放歌曲

该功能允许你的语音机器人在 VLC 媒体播放器中播放你想要的歌曲。用户会说“索非亚，给我放一首歌”，机器人会问“Sir，我该播放什么歌？”只需说出歌曲的名称，索非亚会从 YouTube 下载这首歌到你的本地驱动器中，在 VLC 媒体播放器中播放该歌曲，如果你再次要求播放歌曲，之前下载的歌曲将会自动删除。

工作原理：如果你在指令中提到了play me a song这个短语，那么它会询问你要播放什么视频歌曲。你要求的歌曲将在 youtube.com 上进行搜索，如果找到，歌曲将使用 Python 的 youtube_dl 库下载到你的本地目录中。 youtube-dl 是一个用于从 YouTube.com 及其他一些网站下载视频的命令行程序。下载完成后，歌曲会使用 Python 的 VLC 库 播放，play(path_to__videosong) 模块会实际播放这首歌。

如果下次你要求播放任何其他歌曲，本地目录将被清空，并且在该目录中会下载一首新的歌曲。

py` elif ‘play me a song’ in command: path = ‘/Users/nageshsinghchauhan/Documents/videos/’ folder = path for the_file in os.listdir(folder): file_path = os.path.join(folder, the_file) try: if os.path.isfile(file_path): os.unlink(file_path) except Exception as e: print(e)sofiaResponse(‘我该播放什么歌曲，先生？’)mysong = myCommand() if mysong: flag = 0 url = “https://www.youtube.com/results?search_query=” + mysong.replace(’ ', ‘+’) response = urllib2.urlopen(url) html = response.read() soup1 = soup(html,“lxml”) url_list = [] for vid in soup1.findAll(attrs={‘class’:‘yt-uix-tile-link’}): if (‘https://www.youtube.com’ + vid[‘href’]).startswith(“https://www.youtube.com/watch?v=”): flag = 1 final_url = ‘https://www.youtube.com’ + vid[‘href’] url_list.append(final_url)url = url_list[0] ydl_opts = {}os.chdir(path) with youtube_dl.YoutubeDL(ydl_opts) as ydl: ydl.download([url]) vlc.play(path)if flag == 0: sofiaResponse('我在 Youtube 上没有找到任何东西 ') py

9. 更换桌面壁纸。

你们也可以使用此功能更换桌面壁纸。当你说“更换壁纸”或“索非亚，请更换壁纸”时，机器人会从unsplash.com下载随机壁纸，并将其设置为你的桌面背景。

工作原理： 如果你在命令中说了更换壁纸这句话，程序将从 unsplash.com 下载一张随机壁纸，将其存储在本地目录中，并通过 subprocess.call()将其设置为桌面壁纸。我使用了 unsplash API 来访问其内容。

如果下次你要求再次更换壁纸，本地目录将被清空，并且会在该目录下下载新的壁纸。

py` elif ‘更改壁纸’ 在命令中: folder = ‘/Users/nageshsinghchauhan/Documents/wallpaper/’ 对于文件夹中的每个文件: file_path = os.path.join(folder, the_file) 尝试: 如果 os.path.isfile(file_path): os.unlink(file_path) except Exception as e: print(e) api_key = ‘fd66364c0ad9e0f8aabe54ec3cfbed0a947f3f4014ce3b841bf2ff6e20948795’ url = ‘https://api.unsplash.com/photos/random?client_id=’ + api_key # 来自 unspalsh.com 的图片 f = urllib2.urlopen(url) json_string = f.read() f.close() parsed_json = json.loads(json_string) photo = parsed_json[‘urls’][‘full’] urllib.urlretrieve(photo, “/Users/nageshsinghchauhan/Documents/wallpaper/a”) # 我们下载图片的位置 subprocess.call([“killall Dock”], shell=True) sofiaResponse(‘壁纸更改成功’) ```py ````

10. 告诉你最新的新闻动态。

Sofia 还可以告诉你最新的新闻更新。用户只需说 “Sofia，今天的头条新闻是什么？” 或 “告诉我今天的新闻”。

工作原理： 如果你在命令中说了 今天的新闻，那么它会使用 Beautiful Soup 从 Google News RSS() 中抓取数据并为你朗读。为了方便起见，我已将新闻数量限制设置为 15 条。

py` elif ‘今天的新闻’ 在命令中: 尝试: news_url=“https://news.google.com/news/rss” Client=urlopen(news_url) xml_page=Client.read() Client.close() soup_page=soup(xml_page,“xml”) news_list=soup_page.findAll(“item”) 对于前 15 条新闻: sofiaResponse(news.title.text.encode(‘utf-8’)) except Exception as e: print(e) ```py ````

11. 告诉你几乎任何你询问的内容。

你的机器人可以获取几乎任何你询问的详细信息。例如 “Sofia，告诉我关于 Google 的事” 或 “请告诉我关于超级计算机的事” 或 “请告诉我关于互联网的事”。所以你可以看到，你几乎可以询问任何内容。

工作原理： 如果你在命令中说了 告诉我关于，那么它会使用 re.search() 在用户命令中搜索关键字。使用 Python 的 Wikipedia 库，它会搜索该主题并提取前 500 个字符（如果你不指定限制，机器人将为你读取整个页面）。 Wikipedia 是一个 Python 库，使得访问和解析 Wikipedia 数据变得简单。

py` elif ‘tell me about’ in command: reg_ex = re.search(‘tell me about (.*)’, command) try: if reg_ex: topic = reg_ex.group(1) ny = wikipedia.page(topic) sofiaResponse(ny.content[:500].encode(‘utf-8’)) except Exception as e: sofiaResponse(e) ```py ````

让我们把一切放在一起

py` 导入 speech_recognition 作为 sr 导入 os 导入 sys 导入 re 导入 webbrowser 导入 smtplib 导入 requests 导入 subprocess 从 pyowm 导入 OWM 导入 youtube_dl 导入 vlc 导入 urllib 导入 urllib2 导入 json 从 bs4 导入 BeautifulSoup 作为 soup 从 urllib2 导入 urlopen 导入 wikipedia 导入 random 从 time 导入 strftime def sofiaResponse(audio): “说出传递的音频” print(audio) for line in audio.splitlines(): os.system(""say "" + audio) def myCommand(): “监听命令” r = sr.Recognizer() with sr.Microphone() as source: print(‘说点什么…’) r.pause_threshold = 1 r.adjust_for_ambient_noise(source, duration=1) audio = r.listen(source) try: command = r.recognize_google(audio).lower() print(‘你说: ’ + command + ‘\n’) #loop back to continue to listen for commands if unrecognizable speech is received except sr.UnknownValueError: print(’…') command = myCommand(); return command def assistant(command): “执行命令的 if 语句” #打开 subreddit Reddit if ‘open reddit’ in command: reg_ex = re.search(‘open reddit (.)‘, command) url = ‘https://www.reddit.com/’ if reg_ex: subreddit = reg_ex.group(1) url = url + ‘r/’ + subreddit webbrowser.open(url) sofiaResponse(‘Reddit 内容已为您打开。’) elif ‘shutdown’ in command: sofiaResponse(‘再见。祝您有美好的一天’) sys.exit() #打开网站 elif ‘open’ in command: reg_ex = re.search(‘open (.+)’, command) if reg_ex: domain = reg_ex.group(1) print(domain) url = ‘https://www.’ + domain webbrowser.open(url) sofiaResponse(‘您请求的网站已为您打开。’) else: pass #问候 elif ‘hello’ in command: day_time = int(strftime(’%H’)) if day_time < 12: sofiaResponse(‘您好。早上好’) elif 12 <= day_time < 18: sofiaResponse(‘您好。下午好’) else: sofiaResponse(‘您好。晚上好’) elif ‘help me’ in command: sofiaResponse(“”"" 您可以使用这些命令，我会帮助您: 1. 打开 reddit subreddit : 在默认浏览器中打开 subreddit。 2. 打开 xyz.com : 将 xyz 替换为任何网站名称 3. 发送电子邮件/email : 会依次询问收件人姓名、内容等问题。 4. {cityname} 当前天气 : 告诉您当前的天气情况和温度 5. 你好 6. 播放视频 : 在 VLC 媒体播放器中播放歌曲 7. 更改壁纸 : 更改桌面壁纸 8. 今日新闻 : 阅读今日的头条新闻 9. 时间 : 当前系统时间 10. Google 新闻的头条故事 (RSS feeds) 11. 告诉我关于 xyz 的信息 : 告诉您关于 xyz 的信息 “”"") #笑话 elif ‘joke’ in command: res = requests.get( ‘https://icanhazdadjoke.com/’, headers={“Accept”:“application/json”}) if res.status_code == requests.codes.ok: sofiaResponse(str(res.json()[‘joke’])) else: sofiaResponse(‘哎呀！笑话讲完了’) #Google 新闻的头条故事 elif ‘news for today’ in command: try: news_url=“https://news.google.com/news/rss” Client=urlopen(news_url) xml_page=Client.read() Client.close() soup_page=soup(xml_page,“xml”) news_list=soup_page.findAll(“item”) for news in news_list[:15]: sofiaResponse(news.title.text.encode(‘utf-8’)) except Exception as e: print(e) #当前天气 elif ‘current weather’ in command: reg_ex = re.search('current weather in (.)’, command) if reg_ex: city = reg_ex.group(1) owm = OWM(API_key=‘ab0d5e80e8dafb2cb81fa9e82431c1fa’) obs = owm.weather_at_place(city) w = obs.get_weather() k = w.get_status() x = w.get_temperature(unit=‘celsius’) sofiaResponse(‘当前 %s 的天气是 %s。最高温度是 %0.2f 度，最低温度是 %0.2f 度’ % (city, k, x[‘temp_max’], x[‘temp_min’])) #时间 elif ‘time’ in command: import datetime now = datetime.datetime.now() sofiaResponse(‘当前时间是 %d 小时 %d 分钟’ % (now.hour, now.minute)) elif ‘email’ in command: sofiaResponse(‘收件人是谁？’) recipient = myCommand() if ‘rajat’ in recipient: sofiaResponse(‘我应该对他说些什么？’) content = myCommand() mail = smtplib.SMTP(‘smtp.gmail.com’, 587) mail.ehlo() mail.starttls() mail.login(‘your_email_address’, ‘your_password’) mail.sendmail(‘sender_email’, ‘receiver_email’, content) mail.close() sofiaResponse(‘电子邮件已成功发送。您可以查看收件箱。’) else: sofiaResponse(‘我不明白您的意思！’) #启动任何应用程序 elif ‘launch’ in command: reg_ex = re.search(‘launch (.)‘, command) if reg_ex: appname = reg_ex.group(1) appname1 = appname+“.app” subprocess.Popen([“open”, “-n”, “/Applications/” + appname1], stdout=subprocess.PIPE) sofiaResponse(‘我已启动所需的应用程序’) #播放 YouTube 歌曲 elif ‘play me a song’ in command: path = ‘/Users/nageshsinghchauhan/Documents/videos/’ folder = path for the_file in os.listdir(folder): file_path = os.path.join(folder, the_file) try: if os.path.isfile(file_path): os.unlink(file_path) except Exception as e: print(e) sofiaResponse(‘我应该播放什么歌曲？’) mysong = myCommand() if mysong: flag = 0 url = “https://www.youtube.com/results?search_query=” + mysong.replace(’ ', ‘+’) response = urllib2.urlopen(url) html = response.read() soup1 = soup(html,“lxml”) url_list = [] for vid in soup1.findAll(attrs={‘class’:‘yt-uix-tile-link’}): if (‘https://www.youtube.com’ + vid[‘href’]).startswith(“https://www.youtube.com/watch?v=”): flag = 1 final_url = ‘https://www.youtube.com’ + vid[‘href’] url_list.append(final_url) url = url_list[0] ydl_opts = {} os.chdir(path) with youtube_dl.YoutubeDL(ydl_opts) as ydl: ydl.download([url]) vlc.play(path) if flag == 0: sofiaResponse(‘我在 YouTube 上没有找到任何内容’) #更改壁纸 elif ‘change wallpaper’ in command: folder = ‘/Users/nageshsinghchauhan/Documents/wallpaper/’ for the_file in os.listdir(folder): file_path = os.path.join(folder, the_file) try: if os.path.isfile(file_p"
KDNuggets 博客中文翻译（五十八）,2024-11-03,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
贝叶斯深度学习与近期量子计算机：量子机器学习中的警示故事 
 
 原文：www.kdnuggets.com/2019/07/bayesian-deep-learning-near-term-quantum-computers.html 
 
 评论 
由彼得·维特克（阿米尔·费兹普尔和尼克·莫里森编辑） 
这篇博客文章是由《量子计算机上的贝叶斯深度学习》一文的作者撰写的量子机器学习概述。文章探讨了量子计算领域中的机器学习应用。本文的作者希望实验结果能够影响量子机器学习的未来发展。 
由于研究问题、教育项目和人才需求不断增加，机器学习成为当今技术领域最热门的话题之一。与学习算法的成功相并行的是，量子计算硬件的发展在过去几年中加速了。实际上，我们正处于实现量子优势的门槛上，也就是在某些特定应用领域（例如机器学习）相较于经典算法获得速度或其他性能提升。这听起来很令人兴奋，但不要急于丢弃你的 GPU 集群；量子计算机和你的并行处理单元解决的是不同类别的问题。 
如果你查看下图，你将快速了解如何思考量子计算在机器学习中的角色。随着待解决问题复杂性的增长，人工智能堆栈已经开始包含越来越多不同的硬件后端，包括从现成的 CPU 到张量处理单元（TPUs）和神经形态芯片。然而，仍然存在一些问题过于复杂而难以解决，其中一些问题适合用量子计算解决。换句话说，量子计算机可以像过去的 GPU 一样，使创建更复杂的机器学习和深度学习模型成为可能。 
 
图 1。 人工智能已经使用了异构的硬件后端组合。在未来，这种组合将包括量子技术，这些技术能够支持某些学习算法。基于斯蒂夫·朱尔维特森在机器学习与智能市场会议上的讲座。 
量子计算机使用“量子比特”（qubits）：量子比特是 0 和 1 之间的特殊概率分布。量子比特的数量和我们可以执行的操作的质量（保真度）是衡量量子计算机优劣的最重要指标。量子机器学习使用量子计算机来运行学习算法的某些部分。各种量子机器学习提案的近期可行性各不相同。一些量子算法已经可以在当前量子硬件上运行（例如，需要少量量子比特和量子门的算法，称为变分算法），其他算法则需要更好的量子计算机，具有更多且质量更高的量子比特（如最著名的算法如 Shor 算法的因式分解或 Grover 算法的无结构搜索）。到目前为止，仅有少数基准结果可用于在真实硬件上运行实际的量子机器学习算法。 
所有这些基准测试都专注于那些为这些嘈杂、早期量子设备设计的更明显的算法类别。我们则关注另一个极端：让我们选择一个最困难的算法原语（所有步骤都是基本操作，并且不调用其他算法），看看我们能得到什么结果。为什么？一个著名的算法是量子矩阵求逆算法（简称 HHL，取自原始文章的作者），它在矩阵求逆方面相比经典算法提供了指数级的加速，是量子机器学习中最常见的算法原语之一。我们相信使用量子计算机有其优势，但我们也认为必须考虑当前量子硬件的能力。因此，我们开始了一项展示情况可能变得多么糟糕的任务。 
量子矩阵求逆算法存在无数的警告：它需要数千个错误校正量子比特和更多的量子门。换句话说，它需要一个完美的大规模通用量子计算机。这与我们今天拥有的少量嘈杂量子比特和由于量子比特只能维持短时间的叠加状态（也称为相干时间）而能够构建的浅层电路形成鲜明对比。叠加状态是我们前面提到的 0 和 1 之间的特殊概率分布。如果相干时间不好，概率分布不在我们的控制之下：我们无法对其进行变换以表达计算。 
鉴于量子矩阵反演是量子机器学习中的常见子程序，并且它需要非常高质量的量子计算机，我们想要讲述一个警示故事。我们发现了一个机会：在 2017 年末出现了two papers ，它们将深度学习与高斯过程联系了起来。这是一个重要的成就，因为将深度学习与贝叶斯技术（其中高斯过程是一个例子）结合起来，可以从网络中获取更多信息。特别是，它提供了了解预测不确定性的简便方法。不幸的是，贝叶斯学习是困难的，即使在最好的经典硬件上也是如此。这就是为什么利用这一新发现的联系，我们提出了一种新的量子贝叶斯深度学习协议，该协议内部使用量子矩阵反演来实现加速。从表面上看，新协议看起来不错：这是一个在经典计算中很难解决的问题，我们应用了一些数学技巧，通过使用量子算法使其速度显著提升！不幸的是，这种加速需要非常非常高质量的量子计算机。 
我们利用这个机会实现了量子矩阵反演算法，并在现代量子计算机上进行了基准测试。相关的论文刚刚发表（开放版可以在arXiv上找到），还有一个匹配的git 仓库。下图概述了我们提出的内容。 
 
图 2. 量子贝叶斯深度学习的示意图。在某些条件下，神经网络中的一层神经元可以被视为等同于高斯过程。最近的研究表明，如果我们考虑许多层，这种等价关系仍然成立。为了快速训练高斯过程，我们使用了量子辅助协议，并在量子处理单元上运行它。图像来源：Alejandro Pozas-Kerstjens。 
结果证实了尽管量子矩阵反演被广泛使用，但它并不是实现实际量子优势的首选目标。任何门噪声都会立即破坏计算，这很直观，因为即使是一个小的矩阵反演也需要几十个门。量子计算结果的读取是通过对设备的测量来完成的。测量量子比特的设备中的故障也会影响算法的性能，不过，由于需要进行的测量较少，这种误差源的影响较小。 
 
图 3. 反转 4x4 矩阵后的最终状态的保真度。使用不同噪声水平和噪声模型进行的模拟。 
为了在真实硬件上运行，我们需要简化，因为完整算法即使对 2x2 矩阵来说也过于深奥。幸运的是，我们找到了所需的解决方案：一个专门为求逆 2x2 矩阵设计的浅层电路。虽然这对任何实际应用无关，但这个电路有略多于十二个门（包括状态准备）和有趣的结果。我们使用了16 量子比特的 IBM 芯片和8 量子比特的 Agave 芯片。Agave 芯片具有特定的线性拓扑，增加了一些额外的门来按需操作量子状态。IBM QPU 在测量后成功率为 89%，相当于 0.78 的保真度。Agave QPU 的成功率为 50%，相当于零保真度，但我们认为这主要是由于线性拓扑和需要插入的额外门。这些并不多，但足以使算法的执行时间超过量子比特的相干时间。 
为了量子机器学习（QML）算法而创建的内容，如果在 GPU 集群上可以有效完成，则没有意义。由于贝叶斯深度学习在使用经典资源时很难执行，我们通过量子协议进行了攻击。凭借这一理论，我们在允许量子计算机训练深度神经网络方面取得了非平凡的进展。 
在实现方面，我们建立了量子矩阵求逆算法的完整实现，这可以作为未来量子计算机的一个严格基准。我们得出结论，最好远离这些抽象算法，更多关注今天量子硬件的局限性。我们希望经典和量子机器学习社区能发现这些结果具有趣味性，并且希望下一代量子编码者能够从这次实验中学习。 
原文。经许可转载。 
相关内容： 
 预训练、变压器和双向性生成对抗网络也需要关注大规模图像分类器的演变
 
我们的前 3 课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全领域的职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析水平 
 3. 谷歌 IT 支持专业证书 - 支持您的组织的 IT 需求 
 
更多相关话题 
 成为优秀数据科学家所需的 5 个关键技能每个初学者数据科学家应该掌握的 6 种预测模型2021 年最佳 ETL 工具前往又回来的…一个 RAPIDS 故事强化学习：教计算机做出最佳决策停止学习数据科学以寻找目标，而是通过寻找目标…
贝叶斯与频率统计学在数据科学中的对比 
 
 原文：www.kdnuggets.com/2023/05/bayesian-frequentist-statistics-data-science.html 
 
 
作者提供的图片 
在深入探讨贝叶斯统计和频率统计的区别之前，让我们先从它们的定义开始。 
 
我们的前 3 个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织进行 IT 管理 
 
什么是贝叶斯方法？ 
使用统计推断时，你是在根据数据对总体参数做出判断。 
贝叶斯推断考虑了先验知识，并且参数被视为随机变量。这意味着事件发生的概率是存在的。例如，如果我们掷一枚硬币，贝叶斯推断将会说明没有对错之分，硬币落在正面或反面的概率取决于他们的观点。 
贝叶斯观点基于贝叶斯定理，这是一种基于先验知识计算事件概率的公式。公式如下： 
 P(A)：事件 A 发生的概率P(B)：事件 B 发生的概率P(A|B)：在事件 B 发生的情况下 A 的概率P(B|A)：在事件 A 发生的情况下 B 的概率Pr(A|B)：后验概率，即在给定数据的情况下参数的概率。
 
图片来源：维基百科 
拥有贝叶斯思维的人会使用概率来衡量事件发生的可能性。这是他们的信念。假设的概率是通过先验意见和知识计算得出的，并且在新数据随时可用的情况下被认为是正确的。这称为先验概率，在项目开始之前得出。 
这个先验概率随后转化为后验概率，即项目开始后所持的信念。 
先验 + 可能性 = 后验 
什么是频率主义方法？ 
频率主义推断有所不同。它假设事件基于频率，参数不是随机变量——意味着没有概率。使用上述相同的例子，如果你抛硬币，频率主义推断将表明根据频率有一个正确答案。如果你抛硬币并且一半时间都是反面，那么得到反面的概率是 50%。 
设有停止标准。停止规则决定了样本空间，因此对它的了解对频率主义推断至关重要。例如，对于抛硬币，频率主义方法可能会重复测试 2000 次，或者直到出现 300 次反面。研究人员通常不会重复测试这么多次。 
拥有频率主义思维的人将概率视为频率。他们的概率取决于某个事件在无限次重复中发生的情况。 
从频率主义者的角度来看，你用来估计总体的参数被认为是固定的。你将估计一个单一的真实参数，而不是将其建模为概率分布。当有新数据可用时，你将使用它来进行统计测试，并对数据进行概率预测。 
在频率主义统计中，最常见的计算是 p 值，这是用来验证假设的统计测量。它描述了如果原假设（即没有统计关系）是正确的，你找到特定观察结果的可能性有多大。 
下图中的阴影蓝色区域表示 p 值，即观察结果因随机性发生的概率。 
 
图片由作者提供 
它如何应用于数据科学？ 
统计学是数据科学的重要组成部分，如果你是这个领域的一部分，你一定遇到过贝叶斯定理、p 值和其他统计测试。作为数据科学家或数据工作者，了解统计分析和现有工具对你是有益的。你可能会在某个时候需要这些知识。 
在你的团队中，当你讨论项目和下一步计划时，你会开始看到谁拥有贝叶斯思维，谁拥有频率主义思维。数据科学家将从事概率预测，这结合了残差方差和估计的不确定性。这是一个专门的贝叶斯框架。然而，这并不排除一些专家希望使用频率主义方法。 
你采取的方法会反映在你选择的统计方法上。数据科学的许多基础是建立在贝叶斯统计上的，有些人甚至将频率主义的方法视为贝叶斯理论的一个子集。 
然而，当涉及到数据科学时，你的重点是当前的问题。许多数据科学家根据他们要解决的问题选择模型。贝叶斯方法的优势在于，在数据科学领域，拥有关于问题的具体知识始终是一个优势。 
贝叶斯方法被认为更快、可解释、以用户为中心，并且具有更直观的分析方法。 
我将在下面进一步探讨这些以及它们之间的区别。 
更快的学习 
贝叶斯方法从最初的信念开始，这一信念通过收集证据得到支持。这导致更快的学习，因为你有证据来支持你的观点。 
频率学方法基于从数据中获得的事实。尽管他们已经查看了数据，但没有进行分析以确保这是证据。没有计算概率来支持假设。 
可解释的 
贝叶斯方法拥有多种灵活的模型，使其能够应用于复杂的统计问题。这使得贝叶斯方法更易于解释。 
不幸的是，频率学方法并不那么灵活，通常会失败。 
以用户为中心 
这两种方法有不同的途径。贝叶斯方法允许在项目讨论中纳入不同的研究和问题。重点关注可能的效应大小。 
而频率学方法限制了这种可能性，因为它专注于不确定的显著性。 
贝叶斯与频率学总结 
属性：贝叶斯：频率学：是什么？参数周围的概率分布参数是固定的且为单点质疑什么？给定数据，假设的概率是多少？假设是真的还是假的？需要什么？先验知识/信息和任何数据集。停止标准输出什么？关于假设的支持或反对概率。点估计（p 值）主要优点有证据支持，并且可以应用新信息简单易用，不需要先验知识主要缺点需要高级统计学高度依赖样本大小，仅给出“是”或“否”的结果何时使用？有先验知识时限制数据，使用更多计算资源大量数据时 
结论 
我希望这篇博客能让你更好地理解贝叶斯方法和频率学方法之间的区别。两者之间的争论颇多，甚至是否存在彼此。我的建议是坚持让你感到舒适的方式，以及你如何通过个人逻辑解析问题。 
如果你想深入了解，应用你的技能和知识，我推荐：初学者统计学速成课程：使用 Python 的频率学派和贝叶斯统计的理论与应用 
尼莎·阿雅 是一位数据科学家、自由技术作家和 KDnuggets 的社区经理。她特别关注提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能如何有助于延长人类寿命。作为一个热衷于学习的人，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。 
更多相关话题 
 超越猜测：利用贝叶斯统计进行有效的…现代数据科学中的贝叶斯思维KDnuggets 新闻，7 月 6 日：12 个必备的数据科学 VSCode…数据科学的 8 个基本统计概念学习数据科学统计学的顶级资源数据科学、统计学和机器学习词典
使用 PyCaret 中的 tune-sklearn 进行贝叶斯超参数优化 
 
 原文：www.kdnuggets.com/2021/03/bayesian-hyperparameter-optimization-tune-sklearn-pycaret.html 
 
评论 
由Antoni Baum，PyCaret 的核心贡献者及 Ray Tune 的贡献者 
 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升您的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持您的组织 IT 需求 
 
这里有一个每个PyCaret用户都熟悉的情况：在从compare_models()中选择了一个或两个有前景的模型后，是时候使用tune_model()调整其超参数，以充分发挥模型的潜力了。 
from pycaret.datasets import get_data
from pycaret.classification import *

data = get_data(""juice"")

exp = setup(
data,
target = ""Purchase"",
)
best_model = compare_models()
tuned_best_model = tune_model(best_model)
1234567891011 
(如果你想了解更多关于 PyCaret 的信息——这是一个开源的低代码 Python 机器学习库，这个指南是一个很好的起点。) 
默认情况下，tune_model()使用的是经过验证的RandomizedSearchCV，来自 scikit-learn。然而，并非所有人都知道tune_model()提供的各种高级选项。 
在这篇文章中，我将向你展示如何利用tune-sklearn，这是一个用于 scikit-learn 模型选择模块的替代品，提供了前沿的超参数调优技术，来轻松使用其他最先进的算法。我还将报告一系列基准测试结果，展示 tune-sklearn 如何轻松提升分类模型的性能。 
随机搜索与贝叶斯优化 
超参数优化算法的效率差异可能很大。 
随机搜索一直是机器学习的主流方法，这有充分的理由：它易于实现和理解，并且在合理的时间内能给出良好的结果。然而，正如名称所示，它是完全随机的——大量时间可能花在评估不良配置上。考虑到迭代次数是有限的，优化算法专注于它认为有前景的配置，并考虑到已评估的配置，似乎更有意义。 
本质上，这就是贝叶斯优化（BO）的核心思想。BO 算法跟踪所有评估，并利用数据构建一个“替代概率模型”，该模型的评估速度比 ML 模型快得多。评估的配置越多，算法的知识越丰富，替代模型就越接近实际的目标函数。这样，算法可以做出明智的选择，决定接下来评估哪些配置，而不仅仅是随机采样。如果你想了解更多关于贝叶斯优化的知识，可以查看Will Koehrsen 的这篇精彩文章。 
幸运的是，PyCaret 内置了多个优化库的封装器，在本文中，我们将重点关注tune-sklearn。 
PyCaret 中的 tune-sklearn 
tune-sklearn是 scikit-learn 模型选择模块的替代品。tune-sklearn 提供了一个基于 scikit-learn 的统一 API，让你可以访问各种流行的先进优化算法和库，包括 Optuna 和 scikit-optimize。这个统一的 API 允许你通过一个参数在多种超参数优化库之间切换。 
tune-sklearn 由Ray Tune驱动，它是一个用于实验执行和超参数调优的 Python 库，支持任何规模的调优。这意味着你可以在多台机器上扩展调优，而无需更改代码。 
为了更简单，从版本 2.2.0 开始，tune-sklearn 已集成到 PyCaret 中。你只需执行pip install ""pycaret[full]""，所有可选依赖项将自动处理。 
 
如何协同工作 
!pip install ""pycaret[full]""

from pycaret.datasets import get_data
from pycaret.classification import *

data = get_data(""juice"")

exp = setup(
data,
target = ""Purchase"",
)
best_model = compare_models()
tuned_best_model_hyperopt = tune_model(
best_model,
search_library=""tune-sklearn"",
search_algorithm=""hyperopt"",
n_iter=20
)
tuned_best_model_optuna = tune_model(
best_model,
search_library=""tune-sklearn"",
search_algorithm=""optuna"",
n_iter=20
)
123456789101112131415161718192021222324 
只需向tune_model()添加两个参数，你就可以通过Hyperopt或Optuna将随机搜索切换为 tune-sklearn 驱动的贝叶斯优化。请记住，PyCaret 内置了所有包含模型的搜索空间，但你始终可以传递自己的搜索空间（如果你愿意的话）。 
但是，它们与随机搜索相比效果如何呢？ 
一个简单的实验 
为了比较贝叶斯优化与随机搜索的效果，我进行了一个非常简单的实验。使用 Kaggle 房价数据集，我创建了两个流行的回归模型，分别是 随机森林 和 弹性网络。然后，我使用 scikit-learn 的随机搜索以及 tune-sklearn 的 Hyperopt 和 Optuna 搜索器对它们进行了调优（所有方法进行 20 次迭代，最小化 RMSLE）。该过程重复三次，使用不同的种子，并对结果进行了平均。下面是代码的简化版本——你可以在 这里 找到完整代码。 
from pycaret.datasets import get_data
from pycaret.regression import *

data = get_data(""house"")
exp = setup(
data,
target = ""SalePrice"",
test_data=data, # so that the entire dataset is used for cross validation - do not normally do this!
session_id=42,
fold=5
)
rf = create_model(""rf"")
en = create_model(""en"")

tune_model(rf, search_library = ""scikit-learn"", optimize=""RMSLE"", n_iter=20)
tune_model(rf, search_library = ""tune-sklearn"", search_algorithm=""hyperopt"", n_iter=20)
tune_model(rf, search_library = ""tune-sklearn"", search_algorithm=""optuna"", optimize=""RMSLE"", n_iter=20)

tune_model(en, search_library = ""scikit-learn"", optimize=""RMSLE"", n_iter=20)
tune_model(en, search_library = ""tune-sklearn"", search_algorithm=""hyperopt"", n_iter=20)
tune_model(en, search_library = ""tune-sklearn"", search_algorithm=""optuna"", optimize=""RMSLE"", n_iter=20)
123456789101112131415161718192021 
PyCaret 让事情变得如此简单，难道不是很棒吗？无论如何，这里是我在我的机器上获得的 RMSLE 分数： 
实验的 RMSLE 分数 
为了更好地理解，下面是相较于随机搜索的百分比改进： 
相较于随机搜索的百分比改进 
所有这些操作都使用了相同的迭代次数，在相近的时间内完成。请记住，由于过程的随机性，你的效果可能会有所不同。如果你的改进不明显，可以尝试将迭代次数（n_iter）从默认的 10 增加到 20–30，这通常是一个明智的选择。 
Ray 的优点在于你可以轻松地将计算从单台机器扩展到由数十、数百或更多节点组成的集群。虽然 PyCaret 目前尚不完全支持 Ray 集成，但可以在调整之前初始化一个 Ray 集群——并且 tune-sklearn 将自动使用它。 
exp = setup(
data,
target = ""SalePrice"",
session_id=42,
fold=5
)

rf = create_model(""rf"")

tune_model(rf, search_library = ""tune-sklearn"", search_algorithm=""optuna"", optimize=""RMSLE"", n_iter=20) # Will run on Ray cluster!
12345678910 
只要所有必要的配置就位（RAY_ADDRESS 环境变量），就无需其他操作即可利用 Ray 的分布式计算进行超参数调整。由于超参数优化通常是创建 ML 模型中性能最密集的部分，使用 Ray 进行分布式调整可以为你节省大量时间。 
结论 
为了加快 PyCaret 中的超参数优化，你只需安装所需的库并更改 tune_model() 中的两个参数——多亏了内置的 tune-sklearn 支持，你可以轻松利用 Ray 的分布式计算，超越本地机器的限制。 
请务必查看PyCaret、Ray Tune和tune-sklearn的文档，以及PyCaret和tune-sklearn的 GitHub 仓库。最后，如果你有任何问题或想与社区联系，请加入PyCaret 的 Slack和Ray 的 Discourse。 
感谢 Richard Liaw 和 Moez Ali 的校对和建议。 
个人简介: Antoni Baum 是计算机科学与计量经济学硕士生，同时也是 PyCaret 的核心贡献者和 Ray Tune 的贡献者。 
原文。经许可转载。 
相关: 
 高级超参数优化/调优算法轻松的数据科学的 5 种工具你在 PyCaret 中做错的 5 件事
更多相关内容 
 超参数优化：10 个顶级 Python 库使用 PyCaret 进行二分类简介使用 PyCaret 进行 Python 聚类简介宣布 PyCaret 3.0：Python 中的开源、低代码机器学习开始使用 PyCaret使用网格搜索和随机搜索进行 Python 中的超参数调优
贝叶斯机器学习的解释 
 
 原文：www.kdnuggets.com/2016/07/bayesian-machine-learning-explained.html 
 
由 Zygmunt Zając， FastML。 
所以你知道贝叶斯规则了。它与机器学习有什么关系呢？很难掌握这些拼图如何组合在一起——我们知道这花费了我们一段时间。这篇文章是我们当时希望有的介绍。 
虽然我们对这个问题有一些了解，但我们不是专家，因此以下内容可能包含不准确之处甚至明显错误。如有发现，请在评论中或私下指出。 
贝叶斯主义者与频率主义者 
从本质上讲，贝叶斯意味着概率性。这个特定的术语存在是因为概率有两种方法。贝叶斯主义者认为它是信念的度量，因此概率是主观的并且指向未来。 
频率主义者有不同的看法：他们使用概率来指代过去的事件——这样它是客观的，不依赖于个人的信仰。这个名字来源于方法——例如：我们抛硬币 100 次，出现正面 53 次，所以正面的频率/概率是 0.53。 
要深入探讨此主题及更多内容，请参阅 Jake VanderPlas 的 Frequentism and Bayesianism 系列文章。 
先验、更新和后验 
作为贝叶斯主义者，我们从一个称为先验的信念开始。然后我们获得一些数据并用它来更新我们的信念。结果称为后验。如果我们获得更多数据，旧的后验成为新的先验，循环重复。 
这个过程使用了 贝叶斯规则： 

P( A | B ) = P( B | A ) * P( A ) / P( B )

123 
P( A | B )，读作“在 B 的条件下 A 的概率”，表示条件概率：如果 B 发生，那么 A 的可能性有多大。 
从数据中推断模型参数 
在贝叶斯机器学习中，我们使用贝叶斯规则从数据（D）中推断模型参数（theta）： 

P( theta | D ) = P( D | theta ) * P( theta ) / P( data )

123 
这些所有组件都是概率分布。 
P( data ) 是我们通常无法计算的，但由于它只是一个标准化常数，因此不是那么重要。在比较模型时，我们主要关心包含 theta 的表达式，因为 P( data ) 对每个模型都是相同的。 
P( theta ) 是先验，或者说我们对模型参数可能是什么的信念。我们对此的意见通常比较模糊，如果我们有足够的数据，我们根本不在意。推断应趋于可能的 theta，只要它在先验中不是零。先验是通过参数化的分布来指定的——请参见 Where priors come from。 
P( D | theta ) 被称为给定模型参数的数据的似然。似然的公式是特定于模型的。人们经常使用似然来评估模型：一个对真实数据提供更高似然的模型更好。 
最终，P( theta | D )，一个后验，是我们追求的目标。它是一个通过先验信念和数据获得的模型参数的概率分布。 
当使用似然来获取模型参数的点估计时，这叫做最大似然估计，或 MLE。如果还考虑先验，那么就是最大后验估计（MAP）。如果先验是均匀的，MLE 和 MAP 是相同的。 
注意，选择模型可以看作与选择模型（超）参数是分开的。然而，在实践中，它们通常通过验证一起进行。 
模型与推断 
推断指的是你如何学习模型的参数。模型与训练它的方式是分开的，尤其是在贝叶斯世界中。 
考虑深度学习：你可以使用 Adam、RMSProp 或其他一些优化器来训练网络。然而，它们往往彼此相似，都是随机梯度下降的变体。相比之下，贝叶斯推断的方法之间的差异更为深刻。 
两种最重要的方法是蒙特卡罗采样和变分推断。采样是黄金标准，但速度较慢。《大师算法》摘录中有更多关于 MCMC 的内容。 
变分推断是一种明确设计用来在准确性和速度之间进行权衡的方法。它的缺点是模型特定，但隧道尽头有曙光——见下文的软件部分和变分推断：统计学家的综述。 
统计建模 
在贝叶斯方法的谱系中，有两种主要的风格。我们称第一种为统计建模，第二种为概率机器学习。后者包含了所谓的非参数方法。 
建模发生在数据稀缺、珍贵且难以获得的情况下，例如在社会科学和其他难以进行大规模受控实验的环境中。想象一个统计学家用他所拥有的少量数据精心构建和调整模型。在这种情况下，你不遗余力地充分利用可用输入。 
同时，在小数据情况下，量化不确定性非常重要，而这正是贝叶斯方法擅长的。 
贝叶斯方法——特别是 MCMC——通常计算成本高。这与小数据密切相关。 
为了体验一下，可以参考示例来了解回归分析与多级/层次模型一书。这是一本关于线性模型的书。它们从一个简单的线性模型开始，然后经过一个预测变量、两个预测变量、六个预测变量，直到十一种预测变量的多个线性模型。 
这种劳动密集型的模式与当前机器学习的趋势相悖，后者使用数据让计算机自动从中学习。 
概率机器学习 
让我们尝试将“贝叶斯的”替换为“概率的”。从这个角度来看，它与其他方法的区别不大。就分类而言，大多数分类器能够输出概率预测。即使是支持向量机（SVM），它们在某种程度上是贝叶斯的对立面。 
顺便说一下，这些概率只是分类器的信念声明。它们是否对应于真实的概率则是另一回事，这被称为校准。 
LDA，一种生成模型 
潜在狄利克雷分配是一种将数据投入并让它自动排序的方法（与手动建模相对）。它类似于矩阵因式分解模型，特别是非负矩阵分解。你从一个矩阵开始，其中行是文档，列是词，每个元素是给定文档中给定词的计数。LDA 将大小为n x d的矩阵“因式分解”为两个矩阵，文档/主题（n x k）和主题/词（k x d）。 
与因式分解的区别在于，你不能通过相乘这两个矩阵来得到原始矩阵，但由于适当的行/列之和为一，你可以“生成”一份文档。为了得到第一个词，你从一个主题中抽样，然后从这个主题中抽取一个词（第二个矩阵）。对你想要的词数重复这个过程。请注意，这是一种词袋表示，而不是词语的正确顺序。 
上述是一个生成模型的例子，这意味着可以从中抽样或生成示例。与通常根据x区分类别的分类器不同，它们通常建模P( y | x )。生成模型关注的是y和x的联合分布P( y, x )。估计这个分布更困难，但它允许抽样，当然也可以从P( y, x )得到P( y | x )。 
贝叶斯非参数方法 
虽然没有确切的定义，但这个名称意味着模型中的参数数量可以随着数据的增加而增长。这类似于支持向量机（Support Vector Machines），例如，该算法从训练点中选择支持向量。非参数方法包括 LDA 的层次狄利克雷过程版本，其中主题的数量会自动选择，以及高斯过程。 
高斯过程 
高斯过程与支持向量机有些相似——两者都使用核函数并具有类似的可扩展性（这一点通过使用近似方法在这些年里得到了极大改善）。GP 的自然公式是回归，分类则是次要的。对于 SVM 则正好相反。 
另一个区别是 GP 从根本上是概率性的（提供误差条），而 SVM 则不是。你可以在回归中观察到这一点。大多数“常规”方法只提供点估计。贝叶斯对应物，如高斯过程，也会输出不确定性估计。 
 
致谢：Yarin Gal 的异方差丢弃不确定性和我的深度模型不知道的东西 
不幸的是，这并不是故事的结尾。即使像 GP 这样复杂的方法通常也基于同方差性假设，即均匀的噪声水平。实际上，噪声可能在输入空间中不同（具有异方差性）——见下图。 
 
高斯过程的一个相对流行的应用是机器学习算法的超参数优化。数据量很小，无论是维度——通常只有几个需要调整的参数，还是样本数量。每个样本代表目标算法的一次运行，可能需要几个小时或几天。因此，我们希望以尽可能少的样本获取重要信息。 
大多数关于 GP 的研究似乎发生在欧洲。英语国家在简化 GP 使用方面做了一些有趣的工作，最终形成了由 Zoubin Ghahramani 领导的自动化统计学家项目。 
观看这个视频的前 10 分钟，获得关于高斯过程的简明介绍。 
软件 
目前最显眼的贝叶斯软件可能是Stan。Stan 是一种概率编程语言，意味着它允许你指定和训练你想要的任何贝叶斯模型。它可以在 Python、R 等语言中运行。Stan 有一个现代的采样器叫做NUTS： 
 
 Stan中的大部分计算是通过哈密尔顿蒙特卡洛（HMC）完成的。HMC 需要一些调整，因此 Matt Hoffman 编写了一种新的算法 Nuts（“No-U-Turn Sampler”），它自适应地优化 HMC。在许多情况下，Nuts 实际上比最优的静态 HMC 更具计算效率！ 
 
Stan 的一个特别有趣的功能是它具有自动变分推断： 
 
 变分推断是一种可扩展的近似贝叶斯推断技术。推导变分推断算法需要繁琐的特定模型计算；这使得自动化变得困难。我们提出了一种自动变分推断算法，即自动微分变分推断（ADVI）。用户只需提供一个贝叶斯模型和一个数据集；别无其他要求。 
 
这种技术为将小规模建模应用于至少中等规模数据铺平了道路。 
在 Python 中，最受欢迎的包是 PyMC。它不像 Stan 那样先进或精致（开发者似乎在追赶 Stan），但仍然不错。PyMC 具有 NUTS 和 ADVI——这是一个带有 minibatch ADVI 示例 的笔记本。该软件使用 Theano 作为后端，因此比纯 Python 更快。 
Infer.NET 是微软的概率编程库。它主要从 C# 和 F# 等语言中获得，但显然也可以从 .NET 的 IronPython 中调用。Infer.net 默认使用 期望传播。 
除此之外，还有大量的包实现了各种贝叶斯计算的变体，从其他概率编程语言到专门的 LDA 实现。其中一个有趣的例子是 CrossCat： 
 
 CrossCat 是一种通用的贝叶斯方法，用于分析高维数据表。CrossCat 通过在分层的非参数贝叶斯模型中进行近似推断，从数据中估计表中变量的完整联合分布，并为每个条件分布提供高效的采样器。CrossCat 结合了非参数混合建模和贝叶斯网络结构学习的优势：它可以通过假设潜在变量来建模任何联合分布，只要数据足够，但也能发现可观察变量之间的独立性。 
 
还有 BayesDB/Bayeslite 来自同一团队。 
资源 
为了巩固你的理解，你可以阅读 Radford Neal 的 Bayesian Methods for Machine Learning 教程。它与本文的主题一一对应。 
我们发现 Kruschke 的 Doing Bayesian Data Analysis（被称为小狗书）最为易读。作者详尽地解释了建模的所有细节。 
Statistical rethinking似乎是类似的，但更新了。它包含了 R + Stan 的示例。作者 Richard McElreath 在 YouTube 上发布了一系列讲座。 
在机器学习方面，这两本书都只涉及到线性模型。同样，Cam Davidson-Pilon 的Probabilistic Programming & Bayesian Methods for Hackers涵盖了贝叶斯部分，但没有涉及机器学习部分。 
Alex Etz 关于理解贝叶斯的系列文章也是如此。 
对于那些数学爱好者，Kevin Murphy 的Machine Learning: a Probabilistic Perspective可能是一本不错的书。如果你喜欢高难度的内容，那没问题，Bishop 的Pattern Recognition and Machine Learning可以满足你的需求。最近有一个Reddit 帖子简要讨论了这两本书。 
David Barber 的Bayesian Reasoning and Machine Learning也很受欢迎，并且在网上免费提供，另外，Gaussian Processes for Machine Learning这本经典书籍也是如此。 
据我们所知，目前没有关于贝叶斯机器学习的 MOOC 课程，但mathematicalmonk 从贝叶斯的角度解释了machine learning。 
Stan 有一个详尽的手册，PyMC 有一个教程以及相当多的示例。 
个人简介: Zygmunt Zając 喜欢新鲜空气、牵手以及海滩上的长途散步。他运营着FastML.com，这是全球最受欢迎的机器学习博客。除了各种有趣的文章，FastML 现在还提供了一个机器学习职位板块和一个3D 数据集可视化工具。 
原文。经许可转载。 
相关内容： 
 机器学习关键术语解读十大数据挖掘算法解读数据科学难题解读
 
我们的三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯的捷径。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 工作 
 
更多相关主题 
 建立一个稳固的数据团队使用管道编写干净的 Python 代码停止学习数据科学以寻找目标，而是为了找到目标…学习数据科学统计的最佳资源90 亿美元 AI 失败，探讨成功数据科学家的 5 个特征
现代数据科学中的贝叶斯思维 
 
 原文：www.kdnuggets.com/bayesian-thinking-in-modern-data-science 
 
 
编辑图片 | Midjourney 
贝叶斯思维是一种利用概率做决策的方法。它从初始信念（先验）开始，当新的证据出现时进行调整（后验）。这有助于根据数据做出更好的预测和决策。它在人工智能和统计学等领域至关重要，其中准确推理非常重要。 
 
我们的前三个课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业 
 2. Google 数据分析专业证书 - 提升您的数据分析技能 
 3. Google IT 支持专业证书 - 支持您的组织的 IT 工作 
 
贝叶斯理论基础 
关键术语 
 先验概率（Prior）：表示对假设的初始信念。似然性：衡量假设对证据的解释程度。后验概率（Posterior）：将先验概率和似然性结合起来。证据：更新假设的概率。
贝叶斯定理 
该定理描述了如何根据新信息更新假设的概率。它的数学表达式为： 
 贝叶斯定理（来源：Eric Castellanos 博客) 其中： 
P(A|B) 是假设的后验概率。 
P(B|A) 是给定假设的证据的可能性。 
P(A) 是假设的先验概率。 
P(B) 是证据的总概率。 
贝叶斯方法在数据科学中的应用 
贝叶斯推断 
贝叶斯推断在不确定时更新信念。它使用贝叶斯定理根据新信息调整初始信念。这种方法有效地将已知信息与新数据结合起来。这种方法量化了不确定性，并相应地调整概率。通过这种方式，它随着更多证据的收集不断改进预测和理解。在需要有效管理不确定性的决策过程中非常有用。 
例子：在临床试验中，贝叶斯方法估计新治疗的有效性。它们将过去研究或当前数据中的先验信念结合起来。这更新了治疗效果的概率。研究人员可以利用旧信息和新信息做出更好的决策。 
预测建模和不确定性量化 
预测建模和不确定性量化涉及做出预测并了解我们对这些预测的信心。它使用如贝叶斯方法等技术来考虑不确定性并提供概率性预测。贝叶斯建模在预测方面有效，因为它管理不确定性。它不仅预测结果，还指示我们对这些预测的信心。这是通过后验分布来实现的，后验分布量化了不确定性。 
示例：贝叶斯回归通过提供一系列可能的价格来预测股票价格，而不是单一预测。交易者利用这个价格范围来规避风险并做出投资决策。 
贝叶斯神经网络 
贝叶斯神经网络（BNNs）是提供概率性输出的神经网络。它们提供预测结果以及不确定性度量。与固定参数不同，BNNs 使用权重和偏置的概率分布。这使得 BNNs 能够捕捉并在网络中传播不确定性。它们对需要不确定性测量和决策的任务非常有用。它们被用于分类和回归任务。 
示例：在欺诈检测中，贝叶斯网络分析诸如交易历史和用户行为等变量之间的关系，以发现与欺诈相关的异常模式。与传统方法相比，它们提高了欺诈检测系统的准确性。 
贝叶斯分析的工具和库 
有多种工具和库可以有效地实现贝叶斯方法。让我们了解一些流行的工具。 
PyMC4 
这是一个用于 Python 的概率编程库。它有助于贝叶斯建模和推断。它建立在其前身 PyMC3 的优势基础上，通过与 JAX 的集成引入了显著改进。JAX 提供了自动微分和 GPU 加速。这使得贝叶斯模型更快且更具可扩展性。 
Stan 
一种用 C++ 实现的概率编程语言，通过各种接口（RStan、PyStan、CmdStan 等）提供。Stan 擅长高效地执行 HMC 和 NUTS 采样，并以其速度和准确性著称。它还包括广泛的诊断工具和模型检查工具。 
TensorFlow Probability 
这是一个用于在 TensorFlow 中进行概率推理和统计分析的库。TFP 提供了多种分布、双射函数和 MCMC 算法。与 TensorFlow 的集成确保了在各种硬件上高效执行。它允许用户将概率模型与深度学习架构无缝结合。本文有助于进行稳健且基于数据的决策。 
让我们看一个使用 PyMC4 的贝叶斯统计示例。我们将看到如何实现贝叶斯线性回归。 
import pymc as pm
import numpy as np

# Generate synthetic data
np.random.seed(42)
X = np.linspace(0, 1, 100)
true_intercept = 1
true_slope = 2
y = true_intercept + true_slope * X + np.random.normal(scale=0.5, size=len(X))

# Define the model
with pm.Model() as model:
# Priors for unknown model parameters
intercept = pm.Normal(""intercept"", mu=0, sigma=10)
slope = pm.Normal(""slope"", mu=0, sigma=10)
sigma = pm.HalfNormal(""sigma"", sigma=1)

# Likelihood (sampling distribution) of observations
mu = intercept + slope * X
likelihood = pm.Normal(""y"", mu=mu, sigma=sigma, observed=y)

# Inference
trace = pm.sample(2000, return_inferencedata=True)

# Summarize the results
print(pm.summary(trace)) 
1234567891011121314151617181920212223242526 
现在，让我们逐步理解上面的代码。 
 它为截距、斜率和噪声设置初始信念（先验）。它根据这些先验和观测数据定义了一个似然函数。该代码使用马尔科夫链蒙特卡洛（MCMC）采样从后验分布中生成样本。最后，它总结了结果，以展示估计的参数值和不确定性。
总结 
贝叶斯方法将先验信念与新证据结合，以进行信息化决策。它们在多个领域提高预测准确性并管理不确定性。工具如 PyMC4、Stan 和 TensorFlow Probability 提供了强大的贝叶斯分析支持。这些工具帮助从复杂数据中做出概率预测。 
Jayita Gulati 是一位机器学习爱好者和技术作者，因对构建机器学习模型的热情而驱动。她拥有利物浦大学计算机科学硕士学位。 
更多相关主题 
 KDnuggets 新闻，7 月 13 日：数据科学的线性代数；10 种现代…使用基础和现代算法处理计算机科学问题…10 种现代数据工程工具学习现代预测技术，帮助预测未来商业结果…2024 年必须了解的 7 种现代 SQL 数据库数据科学中的贝叶斯与频率学派统计
使用机器学习战胜博彩公司 
 
 原文：www.kdnuggets.com/2019/03/beating-bookies-machine-learning.html 
 
 评论 
作者: Richard Bartels, Vantage AI 的数据科学家。 
“庄家总是赢”是赌博中的一句俗语。它反映了在大多数运气游戏中，赌场（例如赌场或博彩公司）具有统计上的优势。换句话说，假设下注金额为 1，赌场的预期回报高于 1，而赌徒的预期回报低于 1。因此，如果你去赌场，长期来看，你很可能会遭遇净亏损。 
体育博彩也不例外。投注者和博彩公司都可以在预测比赛结果上具备同等的技能，但博彩公司设定了投注规则，从而确保长期获利。他们这样做的方式是控制所谓的支付率。 
这里有一个简单的例子。假设有两个水平相当的飞镖运动员，因此在一场对决中他们都有 50%的获胜机会。博彩公司可以设定赔率，我们将其定义为 O₁和 O₂分别对应于运动员 1 和 2。在这个特定的比赛中，O₁ = O₂ = 1.90 是合理的赔率。这意味着每下注 1 美元，如果你赢了，就能得到 1.90 美元的回报。那么你的预期回报 X 是什么呢？如果你对运动员 1 下注 1 美元，你的预期回报为（记住每个运动员的获胜概率是 50%）： 
 
所以从长远来看，每花费 1 美元会得到 95 美分的回报，你将会亏损！也就是说，博彩公司为这个游戏设置的“支付率”是 95%，这意味着博彩公司预计会从所有投注中获得 5%的利润，前提是他们准确评估了获胜概率。所以我们看到，博彩公司设定的赔率是不公平的，公平的赔率应该是 O₁ = O₂ = 2.0。注意，这些赔率也对应于每个运动员的相等获胜概率，即 P₁ = O₂/(O₁ + O₂)=0.5 和 P₂ = O₁/(O₁ + O₂)=0.5。 
在本博客的其余部分，我们将专注于飞镖这种特定的游戏，其中比赛是对决的，结果在很大程度上取决于运动员的技能。然而，这个讨论也可以推广到其他体育运动。 
 
图片来源: www.1zoom.me/en/wallpaper/517285/z7641.3/. 
如何战胜博彩公司 
我们上面看到的情况是，博彩公司通过控制赔付来获利。为了做到这一点，他们必须相应地设置赔率。为此，他们需要知道概率。一个能准确获取所有概率的全知博彩公司在长期内是无法被击败的。但博彩公司并非全知，因此有两种方法可以在纯粹基于概率估算上击败他们。 
 如果你 consistently 评估概率比博彩公司更准确，并且这个差距足以弥补他们在赔付中预留的余地。如果你只投注那些你知道博彩公司犯了错误且赔率“公平”的比赛。
实际上，策略 1 只是策略 2 的一个特定版本。尽管如此，即使你能够比博彩公司更准确地预测每场比赛，也不太可能获利，因为博彩公司对概率的估算非常接近正确。从图 1 可以看出，博彩公司在准确估算飞镖赔率方面做得相当好。 
 
图 1：从博彩公司赔率得出的胜率与观察到的胜率（蓝线）对比，涵盖了将近 3700 场飞镖比赛。虚线黑线对应于能够在无限场比赛中完美预测概率。深灰（浅灰）带是由于有限样本量的固有噪声所产生的 68%（95%）区间。由于蓝线在灰色带内保持得相当好，似乎博彩公司知道他们在做什么。 
图 1 说明博彩公司在大量比赛中正确评估了概率。但他们在一些个别比赛中仍可能出错。上述提到的策略 2 依赖于识别博彩公司对实际概率的误判。例如，在博彩公司提供相等赔率的非现实事件中，例如 O₁ = O₂ = 1.90，在现任世界冠军迈克尔·范·格尔文与世界第 94 名对手的比赛中，我们的直觉已经告诉我们通过投注范·格尔文可能会获利。目标是识别所有这样的比赛。然而，由于大多数时候很难判断博彩公司何时出错，我们可以尝试让机器学习（ML）算法为我们完成这项工作。 
利用 ML 进行飞镖投注 
为了这个项目，我们使用了飞镖统计数据，包括平均值、结算百分比、180 分（用 3 支飞镖的最高分）和交锋统计等特征。此外，我们使用了历史赔率来评估这个模型是否能够获利。 
首先，为了进一步激励我们仅在一部分比赛中下注的策略，我们来看一下基准准确率。如果我们总是根据博彩公司给出的最高获胜概率的选手下注，我们能达到 70%的准确率。仅基于比赛结果构建的技能评级在相同数据上也能达到 69%的准确率。当训练一个机器学习模型，如随机森林、提升树或经过精心构建特征优化的全连接神经网络，并使用二元交叉熵损失函数进行优化时，我们也达到了 70%的准确率。显然，我们没有超越博彩公司，因此获利的机会很小。 
自定义损失函数 
二元交叉熵损失函数优化了我们正确预测比赛结果的能力，即优化了我们预测比赛的准确性。然而，这并不是我们的最终目标。我们想要的是识别博彩公司误判真实概率的比赛，从而提供有利的赔率。即，我们想要优化投资回报率。以下是一个精确实现这一点的损失函数。 
 
自定义损失函数包含两个元素，方括号中的项是我们分别在玩家 1 或 2 上下注$1 的回报。请注意，这取决于比赛结果（yᵢ），如果我们预测错误，我们会损失资金。 ReLu 函数包含了我们的下注策略。它的特性是当 x ≤ 0 时，Relu(x) = 0，否则 Relu(x) = x。参数是我们的预期回报：赔率乘以我们估计的获胜概率减去 1。鉴于 ReLu 函数的特性，这意味着只有在我们认为赔率对我们有利时，它才会大于 0。换句话说，如果我们的模型认为赔率不公平，它不会下注任何资金。另一方面，赔率越有利，模型下注的金额就会越高。 
这个损失函数确保我们优化的不是预测比赛结果的准确性，而是我们的赢利。注意，由于我们的自定义损失函数，预测的概率并不代表真实的概率，因为当模型认为博彩公司给出的赔率不准确时，它会将概率推向极端（0 或 1），以便下注更多。 
那么它的表现如何？ 
为了测试我们的模型性能，我们构建了一个具有两个隐藏层的密集连接神经网络。最终层是一个 sigmoid 层，用于预测玩家 1 获胜的概率。然后，我们使用博彩公司赔率和比赛结果来计算使用上述自定义损失函数的损失。所有这些都在PyTorch中实现。 
由于这是一个时间序列模型，该模型在给定时间点之前的历史数据上进行训练，然后应用于接下来的 50 场比赛。这个过程会对接下来的 50 场比赛重复进行，依此类推。结果如图 2 所示。 
 
图 2：两种不同投注策略的累计收益随时间的变化。红点表示我们始终对博彩公司赔率中获胜概率最高的玩家下注的策略。从长远来看，这导致了约 5%的损失，对应于博彩公司支付方案。绿点是由我们的机器学习模型进行的投注。它只在预计会获得利润时才下注。总体收益波动在 0 附近。然而，也有一些重大收益，能够弥补较大的损失。在这种情况下，我们的利润约为~10%。 
我们的模型成功地实现了盈利，尽管随着时间的推移波动较大。它经历了一些重大损失，但也取得了一些主要的收益来弥补这些损失。最终的投资回报率约为 10%。作为基准，我们采用了一种策略，即始终对博彩公司认为最有可能获胜的玩家进行投注（这与我们使用二元交叉熵优化预测赢家的模型不会有太大不同）。由于赔率不公平，我们在长期内会损失约 5%。 
总结 
 博彩公司通过控制支付金额来获得利润。对于飞镖比赛，它们通常能够很好地评估获胜概率。这使得仅仅通过预测飞镖比赛结果来获利变得困难。一种具有自定义损失函数的机器学习模型——其目标是识别博彩公司赔率中的不足并获取利润，而不是优化正确预测赢家的准确性——可以提供一种有利可图的投注策略。在任何给定时间点的投资回报率会受到较大波动的影响，只有在较长的时间段内才能期望获得利润。所提出的模型绝不是一个保证盈利的机器。相反，它作为一个概念验证，描述了如何设置你的机器学习模型以击败博彩公司。
致谢 
感谢Guido Tournois与我在这个项目中的合作。 
个人简介：Richard Bartels是荷兰数据科学咨询公司 Vantage AI 的数据科学家。如果你需要帮助创建适用于数据的机器学习模型，请随时通过info@vantage-ai.com与我们联系。 
原文。转载时经许可。 
资源： 
 在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育分析、数据科学、数据挖掘和机器学习的软件
相关: 
 你的机器学习代码可能很糟糕的 4 个原因利用基于代理的模型 (ABM) 和数字双胞胎来防止伤害为什么德国在决赛中没有战胜巴西，或来自世界杯的数据科学课程
 
我们的 3 个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯 
 2. 谷歌数据分析专业证书 - 提升你的数据分析能力 
 3. 谷歌 IT 支持专业证书 - 支持组织的 IT 工作 
 
了解更多内容 
 KDnuggets 新闻，12 月 14 日：3 个免费的机器学习课程…每位机器学习工程师都应该掌握的 5 项机器学习技能…学习数据科学、机器学习和深度学习的稳固计划人工智能、分析、机器学习、数据科学、深度学习…打破数据障碍：零样本、单样本和少样本学习如何转变机器学习联邦学习：协作机器学习教程…
使用 dtreeviz 创建美丽的决策树可视化 
 
 原文：www.kdnuggets.com/2021/03/beautiful-decision-tree-visualizations-dtreeviz.html 
 
评论 
由埃里克·刘文森，高级数据科学家 
决策树是机器学习模型中非常重要的一类，它们也是许多更高级算法的构建模块，例如随机森林或著名的 XGBoost。这些树也是基准模型的良好起点，我们随后尝试通过更复杂的算法来改进。 
决策树最大的优点之一是它们的可解释性——在拟合模型后，它实际上是一组可以用来预测目标变量的规则。这也是为什么可以轻松绘制规则并向利益相关者展示它们，以便他们能轻松理解模型的基本逻辑。当然，前提是树不太深。 
使用scikit-learn和matplotlib的组合可以非常简单地可视化决策树。然而，还有一个很棒的库叫做dtreeviz，它提供了更多功能，创建的可视化不仅更美观，还能传达更多有关决策过程的信息。 
在本文中，我将首先展示“旧方式”绘制决策树，然后介绍使用dtreeviz的改进方法。 
设置 
一如既往，我们需要首先导入所需的库。 
然后，我们从scikit-learn加载 Iris 数据集。我们还会讨论一个回归示例，但稍后我们会加载波士顿住房数据集。 
“旧方式” 
下一步是创建训练/测试集，并将决策树分类器拟合到 Iris 数据集上。在本文中，我们纯粹关注于决策树的可视化。因此，我们不会关注模型拟合或寻找合适的超参数（这些主题有很多文章）。我们将“调整”的唯一内容是树的最大深度——我们将其限制为 3，以便树可以适应图像并保持可读。 
现在我们已经有了一个拟合的决策树模型，可以继续可视化这棵树。我们从最简单的方法开始——使用scikit-learn中的plot_tree函数。 
tree.plot_tree(clf);
1 
 
图片由作者提供 
好吧，对于一行代码来说还不错。但它不是很易读，例如，没有特征名称（只有它们的列索引）或类别标签。我们可以通过运行以下代码轻松改进这一点。 
 
图片由作者提供 
好多了！现在，我们可以相对容易地解读决策树。也可以使用graphviz库来可视化决策树，不过，结果非常相似，包含与上图相同的元素。因此，我们在这里跳过这部分，但你可以在GitHub 上的 Notebook中找到实现。 
dtreeviz 的实际应用 
既然已经看到旧的决策树绘制方法，现在让我们直接进入dtreeviz的方法。 
代码片段几乎可以自我解释，因此我们可以继续讨论结果。首先，让我们花一点时间来承认这是一项多么巨大的改进，尤其是考虑到函数调用非常相似。 
 
作者提供的图片 
让我们逐步查看这个图表。在每个节点，我们可以看到用于分裂观察结果的特征的堆叠直方图，以类别颜色标记。这样，我们可以看到各个类别是如何通过每次分裂进行分隔的。x 轴上的值和小三角形是分裂点。在第一个直方图中，我们可以清楚地看到所有setosa类别的观察数据的花瓣长度小于 2.45 cm。 
树的右侧分支表示选择大于或等于分裂值的值，而左侧分支则表示小于分裂值的值。叶节点用饼图表示，显示叶节点内观察结果属于哪个类别的比例。这样，我们可以轻松看出哪个类别是多数，从而也能了解模型的预测。 
在这个图表中，我们没有看到每个节点的基尼系数值。在我看来，直方图提供了更多关于分裂的直观信息，而系数的值在向利益相关者展示时可能也并不是那么重要。 
**注意：**我们还可以为测试集创建类似的可视化，只需在调用函数时替换x_data和y_data参数即可。 
如果你不喜欢直方图并且想简化图表，可以指定fancy=False来获得以下简化的图表。 
 
作者提供的图片 
dtreeviz的另一个有用功能是对图中的特定观察结果进行路径高亮显示，从而提高模型的可解释性。这样，我们可以清楚地看到哪些特征对类别预测有贡献。 
使用下面的代码片段，我们高亮显示了测试集第一条观察数据的路径。 
该图与之前的非常相似，但橙色高亮清楚地显示了观察值遵循的路径。此外，我们可以在每个直方图中看到橙色三角形。它代表了所示观察值的特征值。在最后，我们看到该观察值的所有特征值，其中用于决策的特征用橙色突出显示。在这种情况下，只有两个特征被用来预测该观察值属于versicolor类别。 
 
图片由作者提供 
提示： 我们还可以通过设置orientation=”LR”将图表的方向从自上而下更改为自左而右。由于图表在较窄屏幕的设备上显示效果不佳，因此本文未展示此选项。 
最后，我们可以用简单的英语打印出用于该观察值预测的决策。为此，我们运行以下命令。 
这样，我们可以清楚地看到该观察值满足的条件。 
回归示例 
我们已经介绍了一个分类示例，这展示了库的大多数有趣功能。但为了完整性，我们还讨论了回归问题的示例，以展示图表的不同。我们使用了另一个流行的数据集——波士顿住房数据集。这是一个问题，其中我们使用一组不同的区域来预测波士顿某些区域的中位数房价。 
代码已经相似。唯一的变化是我们添加了show_node_labels = True。这对于较大的决策树特别有用。因此，在与小组讨论图表时，很容易通过节点编号来指示我们讨论的分割。 
 
图片由作者提供 
让我们深入探讨分类树和回归树之间的差异。这一次，我们不看直方图，而是检查用于分割的特征与目标的散点图。在这些散点图上，我们看到了一些虚线。它们的解释如下： 
 水平线是决策节点中左侧和右侧桶的目标均值。垂直线是分割点。这与黑色三角形表示的信息完全相同，但它使得比较水平线更容易 -> 便于分隔两侧。
在叶子节点中，虚线表示叶子中的目标均值，这也是模型的预测。 
我们已经展示了如何突出显示某个观察值的决策路径。我们可以更进一步，只绘制用于该预测的节点。我们通过指定show_just_path=True来实现这一点。以下图只显示了上面树中的选定节点。 
 
图片由作者提供 
结论 
在这篇文章中，我展示了如何使用dtreeviz库来创建优雅且富有洞察力的决策树可视化。经过一段时间的使用，我将继续把它作为可视化决策树的首选工具。我确实相信，使用这个库创建的图表对于那些不日常从事机器学习的人来说更容易理解，并且有助于向利益相关者传达模型的逻辑。 
还值得一提的是，dtreeviz支持一些 XGBoost 和 Spark MLlib 树的可视化。 
你可以在我的GitHub上找到这篇文章中使用的代码。像往常一样，欢迎任何建设性的反馈。你可以在Twitter上或在评论中联系我。 
如果你喜欢这篇文章，你也许会对以下内容感兴趣： 
利用哈希函数改进训练-测试分割 
确保在更新数据集时训练集和测试集从未混合的最佳方法 
Lazy Predict：用一行代码拟合和评估 scikit-learn 中的所有模型 
最简单的方法来查看哪些模型最适合你的数据集！ 
通过随机森林示例解释特征重要性 
学习在 Python 中确定特征重要性的最流行方法 
参考文献 
 github.com/parrt/dtreevizexplained.ai/decision-tree-viz/index.html
个人简介：Eryk Lewinson 目前在荷兰最大的电子商务平台担任高级数据科学家。在他的职业生涯中，他在为两家四大会计师事务所和一家荷兰金融科技初创公司工作时获得了数据科学方法的实际应用经验。在工作中，他专注于使用机器学习为公司提供商业价值。 
原始文章。经许可转载。 
相关： 
 2021 年你应该知道的所有机器学习算法11 个完整 EDA（探索性数据分析）的必要代码块讲述精彩的数据故事：可视化决策树
 
我们的前 3 个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT 工作 
 
了解更多相关主题 
 用 Pandas 制作美观互动可视化的最简单方法通过 Python 和 Scikit-learn 简化决策树解释决策树算法解析通过实现理解：决策树讲述精彩数据故事：可视化决策树随机森林与决策树：关键区别
如何成为 10 倍数据科学家，第一部分 
 
 原文：www.kdnuggets.com/2017/09/become-10x-data-scientist-part1.html 
 
评论 
由 Stephanie Kim， Algorithmia。 
最近我在PyData Seattle上进行了一个讲座，讨论如何通过借鉴开发者社区的技巧来提升你的数据科学技能。这些建议将帮助你成为一个受团队成员和利益相关者喜爱、更高效的数据科学家。 
本文分为五部分，包括： 
 10 倍开发者的历史与争议。项目设计。代码设计。工作所需的工具。模型生产化。
 
如果你想观看完整的原始讲座，请点击这里。 
 
10 倍开发者是指生产力比普通开发者高 10 倍的人。 
10 倍开发者不仅每小时编写的代码比平均开发者多，而且调试能力强，测试代码减少了错误，指导初级开发者，撰写自己的文档，还具备许多超越编程知识的广泛技能。 
 
1968 年，由H. Sackman、W. J. Erikson和E. E. Grant进行的实验“探索性实验研究比较了在线和离线编程性能”发现程序员完成编码任务的时间差异很大。 
研究聚焦于平均有 7 年经验的程序员，并发现程序员之间存在 20:1 的差异。 
尽管实验中发现了缺陷（如将使用低级语言的程序员与使用高级语言的程序员混合），但后来有更多研究得出了类似的结果。 
虽然关于 10 倍开发者是否存在的辩论广泛，但本文将重点介绍如何通过借鉴经验丰富的开发者的技巧和窍门，成为更高效的数据科学家，这些开发者被认为比同行显著更快。 
 
了解业务 
无论你是为教育、生物技术还是金融公司工作，你都应该至少对你所解决问题的业务有一个高层次的理解。 
为了有效地传达数据分析背后的故事，你应该找出推动业务发展的因素，并了解业务的目标。 
例如，如果你专注于优化食品车的位置，你需要了解人流量、竞争、区域内发生的事件，甚至是天气。你需要了解为什么业务要优化位置，可能是为了增加现有食品车的销售额，或者他们可能正在考虑增加食品车。 
即使你今天可能是求职网站的数据科学家，明天可能是在金融公司，你也应该知道是什么让业务运转，以使你的分析对利益相关者相关。 
你还应该了解你项目的业务流程，例如谁需要对最终结果进行签署，数据模型在你的部分完成后将传递给谁，以及预期的时间框架是什么。 
最后，你应该确保知道谁是利益相关者，并向非技术利益相关者介绍现实的期望。你需要做好教育者的角色，教导非技术利益相关者为什么实现他们的目标可能需要比他们预期更多的时间或资源。 
当你理解了利益相关者的目标，并确保你传达了实现他们解决方案所需的技术、专业知识和时间时，你将成为公司更有价值的资产。 
 
了解数据 
尽管理解业务很重要，但理解数据更为重要。你需要知道数据是如何提取的，何时提取的，谁负责质量控制，为什么数据中可能会有缺口（例如供应商更换或提取方法更改），可能缺少什么以及还有哪些其他数据源可以添加以创建更准确的模型。 
关键在于与不同团队沟通并提出大量问题。不要害怕询问别人正在做什么，并讨论你自己正在做的工作，因为你永远不知道别人是否在做重复工作，或者他们是否有你需要访问的更清洁的数据版本。例如，能够查询数据库将节省你大量时间，而不是对 SiteCatalyst 进行多次 API 调用。 
 
为什么在项目设计中花时间和精力会让你成为一个 10 倍的数据科学家？ 
 你只会做需要做的工作（在编码之前考虑一下），因此你会更快地完成项目，因为你会做更少的工作！通过发现客户/用户认为他们需要的东西与他们真正需要的东西之间的误解，你将能够将自己定位为专家和共识的建立者。你将加深对需求的理解，从而避免犯下昂贵的错误。
代码设计 
尽管在设计代码时有许多最佳实践，但有一些特别突出的做法可以显著提高你的 x 值。 
我第一次听到“清晰度胜过巧妙”这个想法是在大学的写作课程中。很容易被自己的巧妙性所吸引，使用最新的流行词来表达你的想法，但就像编程一样，你不仅可能会让自己困惑，还会让他人困惑。 
 
在上面的 Scala 示例中，第一行展示了 sortBy 方法的简写语法。虽然它很简洁，但很难理解下划线代表什么。尽管这是许多人在匿名函数中作为参数名称使用的常见模式，对于较少经验的开发者（或当你有一段时间没有查看你的代码时），理解代码的功能会变得繁琐。 
在第二个示例中，我们至少使用了一个参数名，并且它展示了赋值，我们可以看到它是按序列 x 中倒数第二个元素进行排序的。 
当代码的抽象程度较低时，后续调试会更容易，因此在第三个示例中，我将明确命名我的参数，使其代表数据。 
当你的大脑必须经过每一步，并查找或回忆简写代码的功能时，调试的时间会更长，添加新功能也会更费时。因此，即使使用如上例中的简写最初更简洁、打字更快，但从长远来看，避免过于巧妙会对你和其他人都有好处。 
 
虽然我们不会深入探讨缓存，但我们会讨论命名的重要性。想象一下你在查看一些旧代码时，你看到一个像 Scala 示例中的序列被排序： 
py` .sortBy(x => -x._2) ```py ```` 
使用单个字母来命名序列完全没有提供有用的信息，因为你可能是从 API、数据库或 Spark 中的数据流中提取数据，在这种情况下，你需要运行代码才能知道“x”是什么。 
所以继续之前的 Scala 示例： 
py` sortBy(clothesCount => -clothesCount._2) ```py ```` 
你可以在不运行代码的情况下了解我们正在排序的内容。 
然而，有时使用 X 作为变量名是完全有理由的。例如，X 经常在机器学习库中使用，其中 X 通常表示观察到的数据，而 y 是尝试预测的变量。在这种情况下，最好使用你所在领域的惯例，其中“model”、“fit”、“predicted”，以及“x”和“y”对该领域的每个人来说都有相同的含义。 
在数据科学之外，你需要遵循所使用编程语言的规范。例如，我建议你查看 Python 的 PEP 文档，以了解最佳实践，或者 
通过注意命名规范，并且在编码时保持清晰而不是聪明，这将使重构和调试变得更简单、更快捷。遵循这两条编码设计原则，你将更接近成为一个 10 倍的数据科学家。 
这里是 如何成为 10 倍数据科学家，第二部分。 
原文。经授权转载。 
简介: Stephanie Kim 是 Algorithmia 的开发者倡导者。 
相关: 
 机器学习项目中最重要的步骤是什么？深度学习从零到一：初学者的 5 个令人惊叹的演示及代码，第二部分数据版本控制：迭代机器学习
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业轨道。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你所在的组织进行 IT 工作 
 
相关话题 
 成为伟大的数据科学家需要的 5 项关键技能每个初学者数据科学家都应掌握的 6 种预测模型成功数据科学家的 5 个特征每个数据科学家都应了解的三种 R 库（即使你使用 Python）停止学习数据科学以寻找目标，并找到目标以…构建一个坚实的数据团队
在不到 6 个月内成为商业智能分析师 
 
 原文：www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months 
 
 
作者提供的图像 
许多从事技术行业的人会转向不同的领域。商业智能就是其中之一。没有软件开发或编码经验的人喜欢分析数据，以帮助企业做出正确的决策。 
 
我们的前 3 名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织 IT 
 
你找到这篇文章是有原因的，这可能是上述两个原因之一。 
在商业智能分析方面，主要的是学习基础知识并成为使用工具的专家。这就是为什么我创建了一个顶级提供商的课程列表，其中一些甚至所有课程目前主导了商业智能市场。 
本文的目的是让你可以根据想学习的工具选择适合你的证书。例如，是选择 Looker 还是 Tableau？ 
Google 商业智能证书 
链接: Google 商业智能专业证书 
Google 以其全面的证书课程而闻名，这些课程帮助了许多人获得高薪职位，而无需回到大学。 
由 3 门课程组成，面向寻求高级知识的人，如果每周投入 10 小时，可以在 2 个月内完成。 
在这个商业智能证书课程中，你将探讨商业智能专业人员的角色，学习和实践数据建模、提取、转换和加载 (ETL)。然后你将深入挖掘数据发现，并创建数据可视化和仪表盘，以回答可以有效传达给利益相关者的业务问题。 
IBM 商业智能 (BI) 分析师证书 
链接: IBM 商业智能 (BI) 分析师证书 
IBM 也是一个以多种证书著称的组织。这个证书针对初学者，由 10 门课程组成，可以按照自己的节奏完成。 
在这个全面的 10 门课程认证中，你将掌握 SQL 查询和关系数据库的技能，学习数据收集和清洗，并了解数据仓库。你将应用统计分析方法来识别趋势，创建数据可视化，并使用流行的工具如 Tableau、Excel、Cognos 和 Looker 构建仪表板。通过这些工具，你将生成有价值的见解，帮助决策过程。 
微软 Power BI 数据分析师证书 
链接: 微软 Power BI 数据分析师专业证书 
另一个了不起的提供者——微软！这门专业证书面向初学者，由 8 门课程组成，如果每周投入 10 小时，可以在 5 个月内完成。 
专门针对微软，在这门课程中你将学习如何使用 Power BI 连接数据源，并将其转化为有价值的见解。你将从准备 Excel 数据开始，通过学习常用公式和函数的最佳实践，为分析过程做好准备。接着，你将学习 Power BI 的功能，创建数据可视化、报告和仪表板。 
如果你想做好准备，可以通过项目和微软 PL-300 认证考试来展示你的技能。 
Tableau 商业智能分析师证书 
链接: Tableau 商业智能分析师专业证书 
一门来自最优秀商业智能工具之一的课程！这门课程面向初学者，由 8 门课程组成，如果每周投入 10 小时，可以在 8 个月内完成。 
在这门专注于 Tableau 的课程中，你将获得每个初级商业智能分析师所需的基本技能。你将学习如何使用 Tableau Public 准备和处理数据进行分析，然后创建数据可视化，以揭示有意义和可操作的见解。 
总结一下 
这就是你所需的一切。这 4 门课程将为你提供成为成功商业智能分析师所需的技能和经验。你只需要确定想要成为哪种工具的专家。 
祝学习愉快！ 
****Nisha Arya****是一名数据科学家、自由技术作家、以及 KDnuggets 的编辑和社区经理。她特别感兴趣于提供数据科学职业建议或教程，并讲解数据科学的理论知识。Nisha 涉及广泛的主题，希望探索人工智能如何有益于人类寿命的不同方式。作为一个积极学习者，Nisha 寻求拓展她的技术知识和写作技能，同时帮助指导他人。 
了解更多相关话题 
 一分钟内解释的机器学习算法KDnuggets 新闻，7 月 20 日：机器学习算法解释…少于 15 行代码的多模态深度学习黑色星期五优惠 - 以更低的价格通过 DataCamp 精通机器学习你是否应该成为一名自由职业人工智能工程师？通过 DataCamp 的分析师接管更快地成为数据驱动
使用 Phraser 和 Stable Diffusion 成为 AI 艺术家 
 
 原文：www.kdnuggets.com/2022/09/become-ai-artist-phraser-stable-diffusion.html 
 
 
图片由作者提供 | DreamStudio | Phraser | Stable Diffusion 
我们生活在一个激动人心的时代，每周都有关于前沿技术的公告。几个月前，OpenAI 推出了最先进的文本到图像模型DALL·E 2。只有少数人获得了早期访问体验这个新 AI 系统，它可以根据描述使用自然语言创建逼真的图像。它仍然对公众关闭。 
几周后，Stability AI 推出了名为 Stable Diffusion 模型的开源版本 DALLE2。这次发布改变了一切。因为互联网上的人们纷纷发布提示结果，对逼真的艺术作品感到惊叹。 
模型的权重可以在 Hugging Face 的CompVis/stable-diffusion-v1-4上获得。你还可以查看源代码和模型卡。它在The CreativeML OpenRAIL M许可证下对公众开放。 
在这篇文章中，我们将了解 Stable Diffusion，并理解一个优秀提示生成器的必要性。 
Stable Diffusion 
Stable Diffusion 模型是一个开源的最先进文本到图像模型，用于使用自然语言生成艺术作品。它使用潜在扩散来识别形状和噪声，并将与提示同步的所有元素集中到核心焦点上。 
该模型在一个LAION-5B图像集上进行了训练，该图像集包含了互联网上 50 亿张公开可用的图像。这些图像附带有标题和标签。 
训练该模型花费了数百个高端 GPU（Nvidia A100），Stable Diffusion 的训练成本约为$660,000。在训练过程中，模型使用CLIP（对比语言–图像预训练）将词汇与图像相关联。 
你不必自己训练模型。你可以在 Hugging Face Spaces 和 DreamStudio 上免费体验它。你甚至可以下载模型权重并在本地运行。 
Hugging Face Spaces 
Hugging Face - Stable Diffusion非常棒。只需写一个简单的描述，然后点击生成图像按钮。几秒钟后，你将看到 4 张与你的提示相关的生成图像。 
 
图片由作者提供 | StableDiffusion | Hugging Face Spaces 
有时，图像生成可能需要几分钟，甚至因需求量大而排队。这对于无限制的免费试用是可以接受的，但你始终可以查看名为 DreamStudio 的官方 Demo 应用程序。 
DreamStudio 
注册一个免费的 DreamStudio 账户后，你会获得 2 美元或 200 次生成机会。生成速度很快，你可以玩转其他选项，如尺寸、Cfg 规模、种子、步骤和图像数量。你生成的图像始终保存在历史记录中，你可以使用 API 将其集成到现有应用程序中。 
如你所见，使用提示生成完全新图像只需几秒钟。 
 
DreamStudio | 作者 
这是另一个示例。我是《魔戒》和霍比特人的忠实粉丝，所以我想，为什么不生成一张 3D 渲染的图像呢？ 
 
DreamStudio | 提示：3D 哈比特人的世界 
你可以在提示中添加风格甚至平台名称。你可以尝试生成特定图像的许多方法。你甚至可以编写详细描述所有细节的长提示。 
 
DreamStudio | 提示：Artstation HQ 上红发女孩的梦 
但如何像下面这样创建详细和高质量的图像呢？真正的艺术家现在是那些想象新角色和"
KDNuggets 博客中文翻译（二百四十二）,2024-11-02,"原文：KDNuggets

协议：CC BY-NC-SA 4.0

TheWalnut.io：一种轻松创建算法可视化的方式

原文：www.kdnuggets.com/2015/07/thewalnutio-algorithm-visualizations.html

作者： Daniel Moisset，（Machinalis.com）。

我们发布了 TheWalnut.io 的初始版本，这是一款允许创建和分享算法可视化的 web 应用程序。我们不仅仅是制作漂亮的算法可视化画廊，而是构建一个人们可以在其中学习、讨论并以视觉方式交流代码的地方。我们在路线图中有许多有趣的功能（和 bug 修复），但我们希望发布，让你能看到我们正在做的事情。

Walnut 允许用户用 Python 或 Javascript 编写程序，并使其与用户设计的虚拟“世界”互动。这些世界定义了共享状态、可能的动作、每个程序可见的状态部分等。可以在一个世界中运行单个或多个程序，然后查看结果。然后，你可以使用简单的声明式 DSL 定义如何表示执行结果（程序所做的操作，以及世界和程序的状态）。你可以对单次运行的不同方面进行多种可视化。

以快速排序为例。有一个 排序 世界，它定义了状态为一组数字，程序可以查看这些数字并对其进行交换，目标是将其排序。然后，你可以添加许多可以在该世界中运行的程序（任何基于交换的排序算法，如 快速排序 或 Shellsort）。你可以定义 场景 以及初始数据（以展示平均和最坏情况，或不同的数组大小）。可视化 可以将这些信息映射到显示上：可能只是移动的条形图，或者是像快速排序这样的递归算法的调用栈，或者是交换宽度随时间变化的图表：

如果你访问我们的探索部分，你将能够看到开发团队构建的一些示例作为起点。但我们的愿景是让你创建自己的可视化并分享它们。仅仅通过代码学习算法是很困难的！破解这个难题，让大家看到里面的内容！加入 Walnut 革命吧！

简介： Daniel Moisset @dmoisset 是一位企业家、计算机科学教师和软件开发人员。

相关

50+ 数据科学和机器学习备忘单

开源支持的互动分析：概述

21 个必备的数据可视化工具

我们的前三大课程推荐

 1. Google 网络安全证书 - 快速进入网络安全职业生涯

 2. Google 数据分析专业证书 - 提升你的数据分析技能

 3. Google IT 支持专业证书 - 支持你的组织 IT 部门

更多相关话题

使用 Pandas 制作美丽的互动可视化的最简单方法

选择正确机器学习算法的简单指南

了解不同数据可视化的工作原理

2023 年你应该了解的 10 个惊人的机器学习可视化

用 Python 图形画廊制作惊人的可视化

管理深度学习数据集的新方法

数据科学与数据工程之间的细微差别

原文：www.kdnuggets.com/2019/09/thin-line-between-data-science-data-engineering.html

评论

编辑注：这是“Climbing the Data Science Ladder”播客系列的第四集，由 Jeremie Harris、Edouard Harris 和 Russell Pollari 主持。他们共同经营一个名为SharpestMinds的数据科学辅导初创公司。你可以在下面收听播客：

我们的三大课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。

 2. 谷歌数据分析专业证书 - 提升你的数据分析技能

 3. 谷歌 IT 支持专业证书 - 支持你所在组织的 IT

如果你在过去几年关注数据科学的发展，你会知道这个领域自 2010 年代初期/中期的“狂野西部”阶段以来，已经发生了很大的变化。那时，几份包含半成品建模项目的 Jupyter 笔记本就能让你在一家体面的公司找到工作，但现在情况已经发生了很大变化。

今天，随着公司们终于认识到数据科学所能带来的价值，对数据科学在生产系统中实施的重视程度越来越高。而这些实施需要能够在实时处理越来越大数据集的模型，因此，很多数据科学问题已变成工程问题。

这就是为什么我们与 Akshay Singh 坐下来讨论，他曾在亚马逊、League 和 Chan-Zuckerberg Initiative（前身为 Meta.com）工作和管理数据科学团队等多个领域。Akshay 在数据科学和数据工程的交汇处工作，并向我们讲解了数据分析与数据科学之间的细微差别、该领域的未来，以及他对那些未受到足够重视的最佳实践的看法。以下是我们的主要收获：

在数据工程中最容易犯的一个错误就是没有深入考虑工具的选择。你为什么使用 S3 作为数据仓库？为什么不使用 Redshift 或 BigQuery？逼迫自己理解这些问题的答案，而不是接受工具的现成配置，是一个很好的成长方式，而且如果你想给潜在雇主留下深刻印象，这一点至关重要。

总是要假设你现在构建的任何东西都会在一年或更短时间内被替代。生产系统不是静态的，你或其他人迟早会不得不重新审视代码库的大部分。这就是为什么学习如何使用文档字符串、使用清晰的函数和变量名称，以及理解关于内联注释的最佳实践如此重要。

数据会随着时间的推移发生漂移，一个在今天的数据上表现良好的模型可能在下周表现不佳。这可能由于许多因素：季节性是一个，但用户行为也可能会发生变化。Akshay 建议说，实时收集用户反馈是解决这个问题的关键：如果你发现他们的反馈意外地变得负面，构建一个警报系统，让你知道有些地方不对劲。

大局是需要记住的最重要的事情。很容易陷入技术问题，但伟大的数据科学家的标志是能够停下来问这个问题是否值得解决。你真的需要一个推荐系统，还是一个简单的基于规则的系统同样有效？如果你无法访问监督学习模型所需的确切训练标签，你能否拼凑出一个不错的替代品？现实世界是混乱的，通常要求你用比 Kaggle 竞赛更多的创造力来处理数据科学问题。

看到大局的重要性随着时间的推移只会越来越显著，因为数据科学家的工作量正通过越来越强大的工具被逐步抽象化。数据科学正慢慢地变成一种产品角色。

TDS 播客 — 剪辑

如果你在 Twitter 上，随时可以联系我 @jeremiecharris!

原文。经许可转载。

相关内容:

关于 AI、分析、数据科学、机器学习的十大最佳播客

构建高效数据科学团队

数据工程师新手在 Google BigQuery 中的 7 个“陷阱”

更多相关内容

数据编排：生成 AI 成功与失败的分界线…

梦想与现实之间：生成文本与幻觉

数据科学与命令行：免费电子书

数据科学的 5 种命令行工具

数据分析师与数据科学家之间的区别是什么？

机器学习中训练数据与测试数据的区别

事物并不总是正态的：一些“其他”分布

原文：www.kdnuggets.com/2023/01/things-arent-always-normal-distributions.html

图片来源：Unsplash

关键要点
我们的前三名课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。

 2. 谷歌数据分析专业证书 - 提升你的数据分析技能

 3. 谷歌 IT 支持专业证书 - 支持你的组织进行 IT 管理

概率分布在数据科学和统计学中扮演着重要角色。尽管正态（高斯）分布是最流行的概率分布，但在数据科学中也可以使用其他概率分布：

伽马分布用于建模代表事件之间时间间隔的连续变量

贝塔分布用于建模代表比例或概率的连续变量

伯努利分布用于建模二元结果

概率分布是描述随机变量行为的数学函数。在数据科学和机器学习中，概率分布通常用于描述数据集的潜在分布，预测未来事件，并评估机器学习模型的性能。例如，高斯分布是一种参数分布，它依赖于两个变量，即均值和标准差。因此，一旦均值和标准差参数已知，就可以创建一个正态分布的数据集。举例来说，下面的代码创建了一个包含 1000 个值的数据集，这些值服从均值为 0、标准差为 0.1 的正态分布。

import numpy as np

import matplotlib.pyplot as plt

mu, sigma = 0, 0.1 # mean and standard deviation

s = np.random.normal(mu, sigma, 1000)

count, bins, ignored = plt.hist(s, 30, density = True)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
 np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
 linewidth=2, color='r')
plt.show() 

1
2
3
4
5
6
7
8
9
10
11
12
13

图 1. 高斯分布的可视化。

我们还可以绘制不同均值和标准差组合的正态分布，如下所示：

import numpy as np
import scipy.stats as stats 
import matplotlib.pyplot as plt

#define three Gamma distributions
x = np.linspace(-10, 10, 101)
y1 = stats.norm.pdf(x, 0, 2)
y2 = stats.norm.pdf(x, 0, 4)
y3 = stats.norm.pdf(x, 2, 2)

#add lines for each distribution
plt.plot(x, y1, label='mu=0, sigma=2')
plt.plot(x, y2, label='mu=0, sigma=4')
plt.plot(x, y3, label='mu=4, sigma=1')

#add legend
plt.legend()

#display plot
plt.show() 

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

图 2. 高斯分布在不同均值和标准差下的可视化。

概率分布在数据科学和机器学习中非常重要，因为它们提供了一种量化和分析不确定性的方法，而不确定性是许多现实世界过程的固有部分。它们在统计推断中也发挥着关键作用，统计推断是使用数据对总体或过程进行推测的过程。

在本文中，我们将解释三种用于机器学习的概率分布，即 伽马分布、贝塔分布 和 伯努利分布。

伽马分布

伽马分布是一种连续概率分布，常用于建模在以恒定速率发生的过程中事件之间的时间。它由形状参数（k）和率参数（ϴ）来定义，其概率密度函数（PDF）定义为

其中 Γ(k) 是伽马函数，ϴ 是尺度参数，k 是形状参数。

图 3. 伽马分布的可视化。

伽马分布通常用于建模表示事件之间时间间隔的连续变量的分布。例如，它可以用于建模顾客到达商店的时间间隔，或者设备故障的时间间隔。

代码示例： 在 Python 中，可以使用 scipy.stats 模块中的“gamma”函数生成伽马分布。例如，下面的代码将生成一个具有伽马分布的随机变量 x，并绘制该分布的概率密度函数。k 和 theta 参数分别指定伽马分布的形状参数和率参数。

import numpy as np
import scipy.stats as stats 
import matplotlib.pyplot as plt

#define three Gamma distributions
x = np.linspace(0, 40, 100)
y1 = stats.gamma.pdf(x, a=5, scale=3)
y2 = stats.gamma.pdf(x, a=2, scale=5)
y3 = stats.gamma.pdf(x, a=4, scale=2)

#add lines for each distribution
plt.plot(x, y1, label='shape=5, scale=3')
plt.plot(x, y2, label='shape=2, scale=5')
plt.plot(x, y3, label='shape=4, scale=2')

#add legend
plt.legend()

#display plot
plt.show() 

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

图 4. 不同形状和尺度参数的伽马分布的可视化。

除了生成随机变量外，scipy.stats 模块还提供了从数据中估计伽马分布参数的函数、检验拟合优度的函数，以及使用伽马分布进行统计检验的函数。这些函数对分析被认为遵循伽马分布的数据很有用。

贝塔分布

贝塔分布是一种定义在区间 [0, 1] 上的连续概率分布。它常用于建模比例或概率，并由两个形状参数定义，通常记作 α 和 β。贝塔分布的概率密度函数（PDF）定义为

PDF 也可以表示为

其中

是贝塔函数。

图 5. 贝塔分布的可视化。

Beta 分布常用于建模代表比例或概率的连续变量的分布。例如，它可以用来建模在某些营销努力下客户进行购买的概率，或机器学习模型做出正确预测的概率。

代码示例：在 Python 中，可以使用 scipy.stats 模块中的 “beta” 函数生成 Beta 分布。例如：

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

# Set the shape paremeters
a, b = 80, 10

# Generate the value between
x = np.linspace(beta.ppf(0.01, a, b),beta.ppf(0.99, a, b), 100)

# Plot the beta distribution
plt.figure(figsize=(7,7))
plt.xlim(0.7, 1)
plt.plot(x, beta.pdf(x, a, b), 'r-')
plt.title('Beta Distribution', fontsize='15')
plt.xlabel('Values of Random Variable X (0, 1)', fontsize='15')
plt.ylabel('Probability', fontsize='15')
plt.show()

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

这将生成一个具有 Beta 分布的随机变量 x 并绘制该分布的概率密度函数。a 和 b 参数分别指定 Beta 分布的形状参数。

scipy.stats 模块具有从数据中估计 Beta 分布参数、评估拟合优度以及使用 Beta 分布进行统计测试的函数，除此之外，还可以生成随机变量。

伯努利分布

伯努利分布是一种离散概率分布，描述了单个二元事件的结果，例如抛硬币。它由一个参数 p 特征，该参数是事件发生的概率。伯努利分布的概率质量函数定义为

其中 n 是 0 或 1，代表事件的结果。

图 6. 伯努利分布的可视化。

该分布通常用于建模二元结果的概率，例如客户进行购买的概率或机器学习模型做出正确预测的概率。

代码示例：在 Python 中，可以使用 scipy.stats 模块中的 “Bernoulli” 函数生成伯努利分布。例如：

from scipy.stats import bernoulli
import seaborn as sb

data_bern = bernoulli.rvs(size=1000,p=0.6)
ax = sb.distplot(data_bern,
kde=True,
color='crimson',
hist_kws={""linewidth"": 25,'alpha':1})
ax.set(xlabel='Bernouli', ylabel='Frequency')

1
2
3
4
5
6
7
8
9

这将生成一个具有伯努利分布的随机变量 x 并绘制该分布的概率质量函数。p 参数指定事件发生的概率。

除了生成随机变量，scipy.stats 模块还提供了从数据中估计伯努利分布概率参数、测试拟合优度以及使用伯努利分布进行统计测试的函数。在评估可能遵循伯努利分布的数据时，这些函数可能会很有用。

总之，Gamma 分布用于建模表示事件间时间间隔的连续变量，Beta 分布用于建模表示比例或概率的连续变量，而 Bernoulli 分布用于建模二元结果。了解这些概率分布背后的概念对于你的机器学习旅程非常有帮助，因为它们能帮助你建模解决数据科学和机器学习中的各种问题。

本杰明·O·塔约 是一位物理学家、数据科学教育者和作家，也是 DataScienceHub 的所有者。之前，本杰明曾在中央俄克拉荷马大学、大峡谷大学和匹兹堡州立大学教授工程学和物理学。

更多相关内容

为什么我们将永远需要人类来训练 AI——有时需要实时训练

持续学习：AI 如何防止数据泄露

MLOps 思维模式：始终保持生产就绪

StarCoder：你一直想要的编码助手

区分数据科学家与其他职业的 5 个特点

正态分布综合指南

扩展您的网络数据驱动产品时需要了解的事项

原文：www.kdnuggets.com/2023/08/things-know-scaling-web-datadriven-product.html

图片由 Getty Images 提供，来自 Unsplash+

当你环顾今天的商业环境时，你很可能会看到一个数据不仅是石油，而是大多数行业的燃料、引擎和轮子时代。

我们的前三大课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业道路

 2. 谷歌数据分析专业证书 - 提升你的数据分析技能

 3. 谷歌 IT 支持专业证书 - 支持你所在组织的 IT 工作

所以，如果你从事网络数据驱动产品的业务，你的未来部分依赖于扩展。每个决策、每个策略、每个产品都依赖于数据。

那么，你如何成功地扩展你的产品？

本文旨在通过关键考虑因素和实用技巧为扩展照亮道路。无论你在运营招聘平台、潜在客户生成平台，还是任何数据驱动的产品，你都能在这里找到所需的指导。

理解数据驱动产品扩展的基础

首先谈谈可扩展性。它是什么？想象你的产品是一个气球。随着需求的增长，你希望你的气球能够膨胀而不爆裂。

这就是可扩展性的意义。它是平稳处理增加负载的能力，无论是更多的数据、更多的用户，还是更多的交易。

那么，在计划扩展时你应该注意哪些问题？

数据收集和管理策略

首先是数据。它是你产品的核心。但当你的产品扩展时，你如何保持数据收集的一致性和质量？你如何有效整合和使用这些数据？

成功扩展的核心在于高效管理这些方面。让我们解析一下数据收集和管理策略的组成部分：

持续验证。 定期检查数据来源，确保收集的数据仍然相关和准确。

严格清理。 使用强大的算法清理数据，去除任何不一致、错误或重复项。

智能整合。 以保持数据质量和可用性的方法融合数据集。

通过改进这三个领域，你在为数据驱动产品的成功扩展奠定基础。这全在于精准、干净和智能地管理数据流。

数据隐私与合规

扩展不仅仅是关于增长；它还涉及责任。当你处理更多数据，特别是个人数据时，你必然会遇到伦理和法律的考量。

那么，你如何确保数据隐私并满足法规要求呢？

对聪明人的建议：尽可能地匿名化数据，保持对所在地区最新数据法规的了解，并定期进行审计以确保合规。

不同行业数据驱动产品扩展策略

在扩展数据驱动的产品时，具体情况将取决于行业和产品的性质。

让我们看一些具体的例子，了解如何利用网络数据在不同领域实现扩展。

招聘平台

假设你正在运行一个招聘平台。随着平台的增长，越来越多的公司和求职者加入，你将不得不获取和管理更多的职位发布数据和员工数据。

在这种情况下，基于 AI 的匹配算法可能是你扩展的关键。该算法将分析职位描述、技能要求和候选人档案，提供准确的匹配建议。

随着更多数据的加入，算法会学习和改进，提供更好的匹配结果。

一个例子是像 LinkedIn 这样的平台如何利用数据来优化其“你可能感兴趣的职位”功能。

潜在客户生成平台

在潜在客户生成平台的背景下，扩展意味着高效处理和分析更广泛的公司数据、员工数据和职位发布数据，以生成高质量的潜在客户。

例如，你可以通过整合更多数据来扩展你的平台，这会丰富潜在客户数据，帮助企业更好地理解其潜在客户，并更有效地针对其市场营销工作。

随着平台的增长，可以使用预测分析工具来预测客户行为，基于以前的数据模式，改善潜在客户评分，推动更多转化。

预见并克服扩展挑战

扩展并不总是一帆风顺。你将面临挑战，从基础设施限制和数据管理问题到保持数据质量和安全。

基础设施限制。 随着扩展，你现有的基础设施可能难以应对增加的数据负荷和用户请求。你可能会遇到处理时间变慢或甚至系统崩溃的情况。解决这一问题的关键是从一开始就投资于可扩展的基础设施。考虑像云服务器或数据库这样的解决方案，它们可以根据你的需求进行扩展（或收缩）。像亚马逊网络服务（AWS）或谷歌云这样的供应商提供的托管服务可以帮助缓解这些挑战，提供强大、可扩展的基础设施。

数据管理问题。 数据量的增加带来了更多的复杂性。你将不得不处理各种数据格式、集成挑战，可能还会遇到不完整或不一致的数据。自动化的数据管理工具在这里可以大显身手，帮助你系统地收集、清理、集成和维护数据。

维护数据质量。 随着规模的扩大，数据错误、重复或不一致的风险增加。为了保持数据质量，你需要实施复杂的数据验证和清理过程。这些过程可能包括简单的检查和去重，也可能涉及更复杂的机器学习算法。

数据安全。 随着数据集和用户基数的增加，数据泄露的潜在风险也在增加。实施强有力的安全措施至关重要。这可能包括加密敏感数据、进行定期的安全审计，以及确保你的平台符合相关的数据保护法规。

在扩展过程中，挑战是自然的。关键在于预见潜在问题，为其做准备，并制定应对策略以应对这些问题。

为数据驱动产品的未来做准备

数据领域快速发展，变化无常。为未来做准备不仅仅是保持现状，更是为了在进步的浪潮中站稳脚跟。你如何确保你的数据驱动产品为未来的挑战做好准备？

持续学习。 未来将带来新的技术、新的方法论和新的数据理解与利用方式。在你的团队中培养持续学习和好奇心的文化至关重要。保持对数据科学和技术最新进展的关注。参加研讨会、网络研讨会和行业活动。鼓励你的团队寻找新的认证和教育机会。

投资先进技术。 人工智能（AI）和机器学习（ML）不仅仅是流行词汇——它们正在塑造数据驱动产品的未来。这些技术可以自动化数据处理任务，从复杂的数据集中提取见解，并提高产品的效率和可扩展性。此外，区块链技术正被越来越多地用于提升数据安全性和透明度。考虑如何将这些先进技术集成到你的平台中。

灵活性和适应性。 随着数据驱动产品的规模扩大，你需要对策略和流程进行调整——可能是重大的调整。培养灵活的思维方式可以帮助你更顺利地适应变化。尝试不同的策略，从成功和失败中学习，并在需要时勇于调整方向。

伦理和合规。 随着公众对数据隐私的关注增加和监管重点的加强，确保道德的数据实践和合规性比以往任何时候都更加重要。这不仅仅是为了避免处罚——也是为了建立与用户的信任。定期审查和更新你的数据隐私政策，并考虑进行第三方审计以确保合规。

预测分析。 未来就是关于预测趋势和做出前瞻性决策。预测分析工具可以分析过去的数据来预测未来的趋势，帮助你保持领先一步。它们还可以帮助进行风险管理、客户行为预测和性能预测。

为未来做准备不是一次性的任务，而是一个持续学习、适应和预测的过程。以未来为导向的思维方式可以确保你的数据驱动产品始终保持相关性和竞争力。

但你究竟如何保持准备？

投资于人才。 数据相关的技能集不断发展。投资于团队的持续学习，确保他们跟上新兴趋势和技术。

拥抱人工智能和机器学习。 这些技术将继续塑造数据驱动产品的未来。探索它们如何提升你的产品的可扩展性和有效性。

培养敏捷性。 快速变化是技术世界中的常态。培养敏捷的思维方式，准备好根据需要调整或改变你的策略。

结论

在一个越来越依赖数据的世界中，缩放你的网络数据驱动产品不再是选择，而是必要。

无论你处理的是公司数据、员工数据、招聘数据，还是其他数据，你的缩放成功将取决于数据收集和管理策略、隐私和合规性遵守、行业特定的缩放策略以及对未来的准备情况。

Karolis Didziulis 是 Coresignal 的产品总监，该公司是业界领先的公共网络数据提供商。他的专业知识来源于超过 10 年的 Bh1B 业务发展经验和超过 6 年的数据行业经验。目前，Karolis 的主要重点是领导 Coresignal 的工作，帮助数据驱动的初创公司、企业和投资公司通过提供来自最具挑战性来源的最大规模和最新的公共网络数据，从而在其业务中脱颖而出。

更多相关话题

7 种你不知道的低代码工具用途

使用 Python 进行数据缩放

通过 Apache Gobblin 扩展数据管理

KDnuggets 新闻，4 月 13 日：数据科学家应该知道的 Python 库…

你需要了解的 6 件数据管理的事以及为什么它很重要……

关于 SAS 数据科学学院的 3 件你不知道的事

从 fast.ai 课程 V3 中学到的 10 个新知识

原文：www.kdnuggets.com/2019/06/things-learnt-fastai-course.html

 评论

由Raimi Bin Karim，AI Singapore

大家都在谈论fast.ai的大规模开放在线课程（MOOC），所以我决定尝试一下他们 2019 年的深度学习课程实用深度学习（程序员版）v3。

我一直了解一些深度学习概念/想法（我已经在这个领域待了一年，主要处理计算机视觉），但从未真正理解过一些直觉或解释。我也了解到Jeremy Howard、Rachel Thomas和 Sylvain Gugger（关注他们的 Twitter！）是深度学习领域的影响力人物（Jeremy 有丰富的 Kaggle 竞赛经验），所以我希望从他们那里获得新的见解和直觉，并得到一些模型训练的技巧和窍门。我有很多东西要向这些人学习。

所以，我在观看视频 3 周后（我没有做任何练习 ???)，写下这篇文章来将我学到的 新知识进行整理与分享。当然，也有一些我一头雾水的内容，所以我做了一些额外的研究，并在这篇文章中呈现。最后，我也写了一下对这门课程的感受（剧透：我喜欢它❣️）。

免责声明 不同的人会有不同的学习要点，这取决于你有何种深度学习背景。此文不适合深度学习初学者，也不是课程内容的总结。这篇文章假设你已具备神经网络、梯度下降、损失函数、正则化技术和生成嵌入的基础知识。以下经验也很有帮助：图像分类、文本分类、语义分割和生成对抗网络。

我将我的 10 个学习要点内容组织如下：从神经网络理论，到架构，再到与损失函数（学习率、优化器）相关的内容，到模型训练（及正则化），再到深度学习任务，最终到模型可解释性。

目录：10 个新知识点

通用逼近定理

神经网络：设计与架构

理解损失景观

梯度下降优化器

损失函数

训练

正则化

任务

模型可解释性

附录：Jeremy Howard 谈模型复杂性与正则化

0. Fast.ai & 转移学习

“如果可以，使用转移学习[来训练你的模型]总是好的。” — Jeremy Howard

Fast.ai 等同于迁移学习，并在短时间内取得出色成果。这个课程真的名副其实。迁移学习和实验主义是 Jeremy Howard 强调的两个关键理念，以便成为高效的机器学习从业者。

1. 通用逼近定理

图片来源：Vincentiu Solomon 在 Unsplash

通用逼近定理 说明你可以用一个隐藏层的前馈神经网络来逼近任何函数。由此可见，你也可以对任何更深层的神经网络实现相同类型的逼近。

我的意思是，哇！我刚刚才知道这个，现在才知道。这 是 深度学习的基础。如果你有堆叠的仿射函数（或矩阵乘法）和非线性函数，最终得到的结果可以逼近任何函数。这是追求不同仿射函数和非线性组合的原因。这也是架构越来越深的原因。

2. 神经网络：设计与架构

在这一部分，我将重点介绍在课程中受到关注的架构，以及像丢弃法这样的最先进（SOTA）模型中纳入的一些设计。

ResNet-50 几乎是最先进的，因此你通常会希望在许多图像相关任务中使用它，比如图像分类和目标检测。这个架构在课程的 Jupyter 笔记本中使用得很频繁。

U-net 几乎是图像分割任务中的最先进技术。

对于卷积神经网络（CNNs），前几层常见使用步幅=2 的卷积。

DenseNet 在构建块的最终操作中使用了拼接，而 ResNet 使用了加法操作。

丢弃法

我们随机丢弃 激活。直观理解：这样没有任何激活可以记住输入的任何部分。这有助于解决过拟合问题，其中模型的某一部分基本上是在学习识别特定图像，而不是特定特征或项目。还有 嵌入丢弃，但这一点只是略微提及。

批量归一化（BatchNorm）

BatchNorm 做了 2 件事：（1）规范化激活值，（2）为每个规范化的激活值引入缩放和平移参数。然而，结果表明（1）不如（2）重要。在论文 Batch Normalization 如何帮助优化？ 中提到，“[BatchNorm] 重新参数化了底层优化问题，使其景观显著平滑。” 直观上讲：因为它现在不那么崎岖，我们可以使用更高的学习率，因此收敛更快（见图 3.1）。

3. 理解损失景观

图 3.1：损失景观；左侧景观有很多凸起，右侧则是平滑的景观。来源：arxiv.org/abs/1712.09913

损失函数通常有崎岖和平坦区域（如果你将其可视化为二维或三维图）。看看图 3.2。如果你进入一个 崎岖 区域，该解决方案通常不会很好地泛化。这是因为你找到的解决方案在一个地方很好，但在其他地方不太好。但如果你在 平坦 区域找到了解决方案，你可能会有很好的泛化能力。这是因为你找到的解决方案不仅在一个点很好，而且在 周围 也很好。

图 3.2：在二维图中可视化的损失景观。截取自 course.fast.ai。以上大部分内容引自 Jeremy Howard。这是一个简单而美丽的解释。

4. 梯度下降优化器

我学到的新知识是 RMSprop 优化器充当了“加速器”。直观理解：如果你的梯度在过去几步中很小，很明显你现在需要加快一点速度。

（有关梯度下降优化器的概述，我写了一篇题为 10 Gradient Descent Optimisation Algorithms 的文章。）

5. 损失函数

学习了 2 个新的损失函数：

像素均方误差 (Pixel MSE)。这可以用于语义分割，这是课程内容之一，但本文没有涵盖。

特征损失 ???。这可以用于图像恢复任务。见任务：图像生成。

6. 训练

图片由 Victor Freitas 提供，自 Pexels

本节探讨了一些调整的组合：

权重初始化

超参数设置

模型拟合/微调

其他改进

迁移学习

模型权重可以是（i）随机初始化的，或者（ii）通过一种称为 转移学习 的过程从预训练模型中转移来的。转移学习利用了预训练的权重。预训练的权重 包含有用的信息。

转移学习中的常见模型拟合方法如下：训练靠近输出的权重，并冻结其他层。

对于转移学习，使用 与预训练模型相同的‘统计数据’ 是很重要的，例如，用某种偏差校正图像的 RGB 值。

❤️ 1cycle 策略 ❤️

这是我在这门课程中学到的最重要的知识。我一直以来对学习率掉以轻心。找到一个好的学习率 非常重要，因为我们至少可以为梯度下降提供一个经过深思熟虑的学习率，而不是一个可能只是次优的直觉值。

Jeremy Howard 在他的代码中持续使用 lr_finder() 和 fit_one_cycle()，这让我困惑，因为它虽然效果很好，但我不知道为什么它能有效。所以我阅读了 Leslie Smith 的 论文 和 Sylvain Gugger 的 博客文章（推荐阅读！），这就是 1cycle 的工作原理：

1. 执行 LR 范围测试：用从小到大的（线性）学习率训练模型，从一个小值（10e-8）增加到一个大值（1 或 10）。绘制损失与学习率的图表，如下所示。

图 6.1：损失与学习率的关系。来源： https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html

2. 选择最小和最大学习率。选择最大学习率时，查看图表并选择一个足够高的学习率，以获得较低的损失值（既不要太高也不要太低）。在这里，你可以选择 10e-2。选择最小学习率时，可以选择约低十倍的值。在这里，它是 10e-3。有关如何选择这些值的更多信息。

3. 根据 周期性学习率 的周期数来拟合模型。一个周期是指你的训练过程通过从选择的最小学习率到选择的最大学习率，然后再返回到最小值的学习率。

来源： https://sgugger.github.io/the-1cycle-policy.html

那么我们为什么这样做呢？整个想法是这样的。在损失地形中，我们希望跳过凸起（因为我们不想陷入某个沟壑）。因此，在开始时增加学习率有助于模型跳出沟壑，探索函数表面，并尝试找到损失较低且区域不崎岖的地方（因为如果地形崎岖，模型会再次被推出）。这使得我们可以更快地训练模型。我们还往往会得到更具泛化能力的解决方案。

图 6.2：来自 course.fast.ai 的截图

用于预训练模型的判别学习率

使用超低学习率训练早期层，使用较高学习率训练后期层。这个想法是除了微小的调整外，不要剧烈改变几乎完美的预训练权重，对接近输出层的层进行更激进的训练。判别学习率是在 ULMFiT 中引入的。

一个神奇的数字除数

在 1cycle 拟合中，要获取最小学习率，将最大值除以 2.6⁴。这个数字适用于 NLP 任务。有关更多信息，请参见 https://course.fast.ai/videos/?lesson=4 的 33:30。

用于超参数搜索的随机森林

提到可以使用随机森林来搜索超参数。

使用默认值

使用库或实现论文代码时，使用默认超参数值，“不要做英雄”。

针对预训练模型的模型微调

我注意到 Jeremy 的风格：在训练最后一层之后，解冻所有层并训练所有权重。然而，这一步是实验性的，因为它可能会或可能不会提高准确性。如果没有提高，我希望你保存了最后训练的权重 ???。

渐进式调整图像大小

这最适用于图像相关任务。首先使用较小版本的图像进行训练。然后，使用较大版本的图像进行训练。为此，使用迁移学习将训练好的权重迁移到具有相同架构但接受不同输入大小的模型中。真是天才。

混合精度训练

简化版的解释是：使用单精度（float32）数据类型进行反向传播，但使用半精度（float16）进行前向传递。

7. 正则化

照片由 Rosemary Ketchum 提供，来源于 Pexels

使用神奇的数字 0.1作为权重衰减。如果使用过多的权重衰减，你的模型训练得不够好（欠拟合）。如果使用过少，你可能会过拟合，但没关系，因为你可以提前停止训练。

8. 任务

请注意，并非所有课程中涉及的任务都在此处提及。

多标签分类

语言建模

表格数据

协同过滤

图像生成

a) 多标签分类

我一直在想如何进行[图像]分类任务，其标签数量可能会变化，即多标签分类（不要与多类分类/多项式分类混淆，其相关概念是二分类）。

没有详细说明多标签分类的损失函数如何工作。但经过搜索，我发现标签应该是多热编码的向量。这意味着每个元素必须在最终模型输出中应用 sigmoid 函数。损失函数，即输出和真实值的函数，是使用二元交叉熵来计算的，以独立地惩罚每个元素。

b) 语言建模

对于这个语言建模任务，我喜欢“语言模型”如何被定义（重新表述）：

语言模型是一个学习预测句子下一个词的模型。为了做到这一点，你需要对英语和世界知识有相当多的了解。

这意味着你需要用大量的数据来训练模型。这部分课程介绍了ULMFiT，这是一个可以基于预训练（换句话说，就是迁移学习）进行重用的模型。

c) 表格数据

这是我第一次遇到使用深度学习处理具有分类变量的表格数据！我之前不知道可以这样做？无论如何，我们可以做的是我们可以从分类变量中创建嵌入。如果我没有上过这门课程，我可能不会想到这一点。稍微搜索了一下，我找到了 Rachel Thomas 写的关于表格数据深度学习入门的帖子，介绍了这种嵌入的使用。

那么，问题是如何将(a)连续变量的向量和(b)来自分类变量的嵌入结合起来？课程中没有提到这一点，但这个 StackOverflow 的帖子强调了三种可能的方法：

2 个模型——一个用于(a)，一个用于(b)。将它们进行集成。

1 个模型，1 个输入。这个输入是(a)和(b)的连接。

1 个模型，2 个输入。这两个输入分别是(a)和(b)。你将这两个输入在模型内部连接起来。

d) 协同过滤

协同过滤是当你需要预测一个用户对某个项目的喜好程度时（在这个例子中，我们以电影评分为例）。课程中介绍了使用嵌入来解决这个问题。这是我第一次遇到使用深度学习的协同过滤（就像我之前对协同过滤的经验很丰富一样）！

目标是为每个用户和项目创建大小为n的嵌入。为此，我们随机初始化每个嵌入向量。然后，对于每个用户对电影的评分，我们将其与各自嵌入的点积进行比较，例如，使用均方误差（MSE）。接着我们进行梯度下降优化。

e) 图像生成

这是我学到的一些东西：

‘劣化’用于生成数据，按我们希望的方式。这个术语我特别喜欢。

生成对抗网络（GANs）讨厌 动量，所以将其设置为 0。

仅仅通过观察损失很难了解模型的表现。必须亲自查看生成的图像（尽管在训练结束时，判别器和生成器的损失大致应保持不变）。

提高生成图像质量的一种方法是在我们的损失函数中包含感知损失（也称为 fast.ai 中的特征损失）。特征损失通过取网络中间某处张量的值来计算。

9. 模型可解释性

照片由Maria Teneva拍摄，来自Unsplash

在其中一节课中，Jeremy Howard 展示了一个激活 热图，用于图像分类任务。这个热图显示了被“激活”的像素。这种可视化将帮助我们理解哪些特征或图像的部分导致了模型的输出???。

10. 附录：Jeremy Howard 谈模型复杂度与正则化

照片由NEW DATA SERVICES拍摄，来自Unsplash

我转录了这部分课程（第 5 课），因为直觉实在太引人入胜了❤️。这里 Jeremy 首先总结了那些认为增加模型复杂度不是解决办法的人的观点，然后改变了他们的看法，接着带他们了解L2 正则化。

哦，我以前学的是统计学，所以他在这方面让我措手不及???。

所以，如果你们中的任何人不幸地被统计学、心理学、计量经济学或类似课程的背景洗脑了，你们需要重新认识你们需要更少参数的想法，因为你们实际上需要意识到的是，你们会适应这种谎言，即你们需要更少的参数，因为这是一个方便的虚构，真实的情况是你们不希望你的函数过于复杂。而拥有更少的参数是使其不那么复杂的一种方式。

但是如果你有一千个参数，其中 999 个参数是1e-9呢？或者如果有 0 呢？如果是 0，那它们实际上并不存在。或者如果它们是1e-9，那它们几乎也不存在。

那么如果很多参数都非常小，我为什么不能有很多参数呢？答案是你可以。实际上，用[参数数量]来限制复杂性的做法是极其有限的。这是一个有很多问题的虚构方法，对吧？因此，如果在你脑海中复杂性是通过参数数量来衡量的，那你是完全错误的。要正确地衡量它。

那么我们为什么在乎呢？我为什么要使用更多的参数？

因为更多的参数意味着更多的非线性、更复杂的交互、更曲折的部分，对吧？而现实生活（损失景观）充满了曲折的部分。现实生活并不像这个[欠拟合的直线]。但我们不希望它们比必要的更加曲折，或更多地交互。

所以我们使用大量参数，然后惩罚复杂性。

好的，一种惩罚复杂性的方法是，正如我之前建议的那样：让我们求出参数值的总和。不过这并不完全有效，因为有些参数是正的，有些是负的，对吧？那我们可以求出参数平方的总和。

这实际上是一个非常好的主意。

让我们实际创建一个模型，并在损失函数中添加参数平方的总和。不过这里有一个问题。也许那个数字太大，以至于最佳的损失是将所有参数设置为 0。那样就不好了。因此，我们实际上希望避免这种情况发生。因此，不仅仅是将参数平方的总和添加到模型中，而是将其乘以我们选择的某个数字。在 fastai 中，我们选择的那个数字叫做wd。

你可能也想看看我的文章《L1 和 L2 正则化的直观解释》，我在这里解释了这两种正则化技术。

结论

我真的很喜欢这门课程。以下是一些原因：

他们提供了直观且易于理解的解释。

他们用很棒的资源来补充他们的课程。

他们鼓励你将深度学习应用到你各自的领域中去构建事物。

他们似乎总是跟上有趣且新颖的出版物，并在适当的地方将它们融入 fastai 库中。

他们还进行大量关于深度学习的研究（请参阅：ULMFiT）。

他们围绕 fastai 库建立了一个社区，因此你将容易获得支持。

他们的技巧和窍门对 Kagglers 和以准确性为驱动的建模非常有用。

期待课程的下一部分！

简历：Raimi Bin Karim 是 AI Singapore 的 AI 工程师

原文。经许可转载。

相关：

fast.ai 深度学习第一部分完整课程笔记

使用 fast.ai 进行快速特征工程

深度学习 3 门热门课程概述

我们的前三名课程推荐

 1. 谷歌网络安全证书 - 快速入门网络安全职业

 2. 谷歌数据分析专业证书 - 提升你的数据分析技能

 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT

更多相关内容

BERT 在稀疏性下的最快速度是多少？

使用快速克里金（FKR）加速机器学习

如何让 Python 代码运行得极快

通过快速 Python 提升你的数据科学技能！

Practical Deep Learning from fast.ai 回来了！

机器学习项目的简单快速数据流处理

如何像数据科学家一样思考以成为数据科学家

原文：www.kdnuggets.com/2017/03/think-like-data-scientist-become-one.html

评论

我们都读过头条新闻——数据科学家是最性感的工作，数量不足，薪资非常高。这个角色被销售得非常成功，以至于数据科学课程和大学项目的数量疯狂增长。在我之前的博客文章之后，我收到了很多人询问如何成为数据科学家——哪些课程最好，应该采取哪些步骤，最快的获得数据科学工作的方式是什么？

我尝试深入思考这个问题，并回顾了我的个人经历——我怎么来到这里？我如何成为一名数据科学家？我 是数据科学家吗？我的经历非常复杂——我起初是一个证券分析师，在一家投资公司主要使用 Excel。随后，我逐渐转向银行业的商业智能和多个咨询项目，最终做到了真正的“数据科学”——建立预测模型，处理大数据，处理大量数据并编写代码进行数据分析和机器学习——这在早期被称为“数据挖掘”。

当数据科学的热潮开始时，我试图理解它与我迄今为止所做的有什么不同。我是否应该学习新技能，成为数据科学家，而不是一个在“分析”领域工作的人？

像每个对它着迷的人一样，我开始参加多个课程，阅读数据书籍，进行数据科学专业化（但没有完成……），大量编写代码——我想成为那张（不）著名的数据科学维恩图的中间交集中的那一个。我学到的现实是，这些“数据科学”独角兽（维恩图中传奇般的人物）很少存在，即使存在——他们通常是对这些领域都有了解的通才，但却是“样样通，样样松”。

尽管我现在认为自己是一名数据科学家——我在亚马逊领导一个极其有才华的数据科学团队，建立机器学习模型，处理“大数据”——我仍然觉得这个行业周围的混乱太多，清晰度却少得多，尤其是对于那些考虑职业转换的人。不要误解我的意思——数据科学有很多非常复杂的分支——如人工智能、机器人技术、计算机视觉、语音识别等——这些领域需要非常深厚的技术和数学知识，甚至可能需要一个……或两个博士学位。但如果你对进入一个几年前被称为商业/数据分析师的数据科学角色感兴趣——这里有四条规则帮助我进入并继续在数据科学领域生存。

规则 1 – 确定你的优先级和动机。

对你现在拥有的技能和你希望达到的目标要非常现实——数据科学中有很多不同类型的角色，了解并评估你当前的知识基础是很重要的。比如说，你现在在 HR 部门工作，想要换职业——学习 HR 分析！如果你是律师——了解法律行业中的数据应用。事实是，对洞察力的渴望如此巨大，以至于所有行业和业务职能都开始使用它。如果你已经有了工作，试着理解哪些方面可以通过数据优化或解决，并学习如何自己做到这一点。这将是一个渐进而漫长的转变，但你仍然会有工作，并通过实际操作进行学习。如果你是刚毕业的学生或在校生——你有一个绝佳的机会来发现自己对什么充满热情——也许是电影，也许是音乐，或者是汽车？你无法想象这些行业中雇用了多少数据科学家——他们都对自己所从事的领域充满热情。

规则 2 – 很好地掌握基础知识。

尽管每个数据科学领域的具体内容非常不同，但基础知识是相同的。你应该在三个领域建立扎实的基础——基本的数据分析、入门统计学和编码技能。

数据分析。 你应该理解并大量练习基本的数据分析技术——什么是表格，如何连接两个表格，分析以这种方式组织的数据的主要技术是什么，如何构建数据集的汇总视图并从中得出初步结论，什么是探索性数据分析，哪些可视化方法可以帮助你理解和学习数据。这是非常基础的，但相信我——掌握了这些，你将具备一项在工作中绝对必需的基本技能。

统计学。 同样，要非常好地掌握入门统计学——什么是均值、 медиан、何时使用其中一种而不是另一种，什么是标准差以及在什么情况下使用它没有意义，为什么平均值“欺骗”但仍然是最常用的汇总值等等。当我说“入门”时，我真的指的是“入门”。除非你是一名数学家，并计划成为一名经济计量学家，应用高级统计和计量经济学模型来解释复杂现象——否则，学习高级统计学。如果你没有数学博士学位，就慢慢来，耐心一点，扎实掌握基础统计学和概率论。

编码。 当然 – 学习如何编码。这是最常被提及的陈词滥调建议，但实际上它是一个有效的建议。你应该从学习如何使用 SQL 查询数据库开始 – 不管你信不信，大多数数据科学团队花费的时间都用于数据提取和准备，其中大量工作是用 SQL 完成的。所以打好基础– 构建你自己的小型数据库，编写一些“select * from my_table”的代码行，掌握 SQL 的基础知识。你还应该学习一种（开始时只学习一种）数据分析语言 – 无论是 R 还是 Python。两者都很出色，掌握它们确实会有所不同，因为许多（虽然不是全部）职位都要求掌握它们。首先学习你选择的语言的基础（快速提示 – 从学习 R 的 dplyr 和 ggplot2 包，或 Python 的 pandas 和 Seaborn 库开始），并学习如何用它进行数据分析。你不需要成为程序员来在这个领域取得成功，关键在于知道如何使用语言进行数据分析 – 你不必成为世界级的黑客才能获得数据科学工作。

规则 3 – 数据科学是解决问题的过程 – 找到并解决一个问题。

多年来我学到的一件事是，数据科学家的一个基本要求是总是提出问题和寻找问题。现在我并不建议你全天候这样做，因为你肯定会发疯，但要做好准备，成为问题解决者并不断寻找问题。你会惊讶于外面有多少可用的数据 – 也许你想分析你的支出模式，识别你的电子邮件的情感模式，或者仅仅是构建漂亮的图表来跟踪你所在城市的财务状况。数据科学家负责质疑一切 – 这个活动是否有效，有没有令人担忧的趋势，也许一些产品表现不佳应该被下架，折扣是否合理或者是否过大 – 这些问题成为假设，然后由数据科学家验证或驳回。它们是原材料，也是工作成功的关键，因为你解决的问题越多，你的工作表现就会越好。

规则 4 – 从“做”开始，而不是计划你将来“何时”做的事情。 这适用于任何学习行为，但在数据科学中尤其如此。确保你从学习的第一天就开始“做”。仅仅通过阅读“关于”数据科学的内容、了解它“应该”如何进行、从书中复制粘贴数据分析代码并在非常简单的数据集上运行，实际上很容易将实际学习拖延到以后，这些简单的数据集在现实世界中是不会遇到的。

在你学习的每一件事上，一定要确保将其应用到你热爱的领域。这就是魔法发生的地方 – 写下你的第一行代码并看到它失败，陷入困境而不知道接下来该做什么，寻找答案，发现许多不同的解决方案却都无效，努力构建自己的解决方案，最终跨越一个里程碑 – “啊哈！” 时刻。这就是实际学习发生的地方。通过实践学习是学习数据科学的唯一途径 – 你不会仅通过阅读来学会骑自行车，对吧？同样的道理，无论你学到了什么，确保你立即应用它，并用真实的数据解决实际问题。

“如果你花太多时间思考一件事，你永远无法完成它。” – 这句来自著名武术家布鲁斯·李的名言抓住了这篇文章的精髓。你必须应用你所学到的，并确保你犯自己的错误。

感谢阅读！订阅我的博客 www.cyborgus.com 并获取最新更新：

在 Facebook 上关注我的博客更新 – www.facebook.com/cyborguscom/

在 LinkedIn 上查找我 – www.linkedin.com/in/karolisurbonas/

个人简介： Karolis Urbonas，是亚马逊数据科学部负责人，是一位充满活力的数据高管，拥有建立高绩效数据科学团队和交付战略分析项目的丰富经验。他的博客地址是 cyborgus.com。

相关：

数据科学家的思维 – 第一部分

如何成为数据科学家 – 第一部分

追求卓越的数据科学学习，而不仅仅是为了考试

我们的前三大课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业道路。

 2. 谷歌数据分析专业证书 - 提升你的数据分析技能

 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 需求

更多相关内容

机器学习不像你的大脑 第一部分：神经元是慢的，……

SQL LIKE 操作符示例

用这些课程构建一个类似 ChatGPT 的聊天机器人

机器学习不像你的大脑 第二部分：感知器与神经元

像老板一样进行 MLOps：无泪机器学习指南

像专业人士一样测试：Python Mock 库的逐步指南

如何像数据科学家一样思考

原文：www.kdnuggets.com/2020/05/think-like-data-scientist-data-analyst.html

评论

由 Jo Stichbury，自由技术写作人

数据科学是一个新兴且不断成熟的领域，各种工作职能不断出现，从数据工程和数据分析到机器学习和深度学习。数据科学家必须结合科学、创造性和调查思维，从各种数据集中提取意义，并解决客户面临的根本挑战。

各个领域产生的数据量不断增加——从零售、交通和金融到医疗保健和医学研究。

可用计算能力的增加和人工智能的最新进展将数据科学家——那些将原始数据分析并使其有用的人员——推到了聚光灯下。

自 2016 年以来，数据科学在北美的 50 种最佳职业中排名首位，依据如收入潜力、报告的工作满意度以及 Glassdoor 上的职位空缺数量等标准。

像数据科学家一样思考

图片来自 svgsilh.comCC0 1.0

那么，成为一个数据科学家需要什么呢？成为数据科学家

关于成功所需技能的一些提示，我采访了 Ben Chu，他是 Refinitiv Labs的高级数据科学家。

Chu 在人工智能领域有背景，特别是在语言学、语义学和图形学方面，并在新加坡的 Refinitiv Labs 工作了两年。

保持好奇

Chu 在我们采访开始时提到，数据科学家应该像调查员一样思考。

你需要保持好奇和兴奋，提出“为什么？”。“这有点像当侦探，连接点滴，发现新的线索。”

在金融领域，数据科学家从各种数据集中提取意义，以指导客户并帮助他们做出关键决策。

数据科学家必须聚焦于客户希望解决的挑战，并从他们处理的数据中寻找线索。

从与 Chu 的交谈中，我了解到能够调整重点并考虑调查背景是多么重要。

如果完美的分析不能解决根本问题，那就没有用处。有时你需要回过头来，尝试新的方法，并重新审视你要回答的问题。其核心是 好奇心。你需要热爱问题！

保持科学

数据科学家使用各种工具来管理他们的工作流程、数据、注释和代码。

“我必须非常勤奋。我需要测量和跟踪我的进展，以便在需要时进行回顾并尝试新的方向，重用先前的工作，并比较结果。”

“保持科学性很重要，要进行观察、实验并记录好进展，以便能够重复你的发现。我需要组织我的观察记录，因此我使用 Notion 作为我的主要工具，将所有的笔记、论文和可视化内容集中在一个地方。”

Chu 强调了记录不仅要涵盖他当前的调查，还要包括所有先前的发现。

“这就像是数据科学的日记。我保持良好的参考点，并在遇到类似情境时回顾它们，以指导我的下一步行动。”

发挥创造力

数据科学不仅仅是采用科学的方法。职位名称可能会产生误导；你不一定需要具备科学背景，但你需要能够进行创造性思考。通常，另类思维是解决挑战的关键。

“我必须在解决问题时切换科学思维，并运用创造性思维引导我探索新的和不同的路径，探索新方向。”

“逻辑性和科学性思维对帮助我得出结论至关重要，但发挥创造性思维同样重要：我使用成功和失败的例子作为线索来观察新的模式。这一切都与‘编码智能’有关。”

学会编码

图片来源于svgsilh.com

你需要扎实的编码技能，以便能够预处理不同的数据源，使用各种数据处理技术来解决噪声或不完整的数据问题。

你还需要能够创建机器学习管道，这要求你知道如何构建模型，并使用工具和框架来评估和分析其性能。

Chu 使用 Python，就像大多数数据科学家一样，因为 Python 提供了大量优秀的包来处理和建模数据。

实际上，Glassdoor 在 2017 年上半年对其网站上发布的 10,000 个数据科学家职位进行了抽样调查，发现三个特别的技能——Python、R 和 SQL——构成了数据科学大多数职位的基础。

Ben Chu 的团队依赖开源机器学习包，如 Tensorflow、Pytorch 和 BERT。

“我们主要使用 Confluence 作为文档工具；使用 MLFlow、Amazon Sagemaker、Scikit-Learn、Tensorflow、PyTorch 和 BERT 进行机器学习；使用 Apache Spark 在大型数据集上构建高速数据管道；使用 Athena 作为我们的数据库来存储处理后的数据。”

“我们还使用 Superset 连接数据，更方便地构建仪表板以输出图表，这使得操作更直观。”

不要担心‘冒名顶替综合症’。

朱目前是 Refinitiv Labs 的高级数据科学家，但他小时候想成为一名音乐家，对语言充满兴趣。“在自然语言处理领域，我需要对语言学有深入了解，特别是语义学和语言的细微差别。”

他解释说，数据科学团队需要各种技能——他和他的同事们从不同的背景中发展出了重叠的技能。

“你需要的技能将取决于你所工作的领域。例如，我需要对金融有良好的理解。

“例如，通过建立异常检测方法，数据分析正被用于减少欺诈，以检测交易数据中的欺诈‘行为’作为不规则模式。

“像我这样的数据科学家需要熟练掌握如何处理各种孤立的金融数据。了解如何组合这些数据至关重要，因为没有这种理解，我无法构建成功的模型。”

进入数据科学领域并不一定需要成为计算机科学家或数学家。没有人能在所有领域都有所有的专业知识。你可以来自法律、经济或科学背景。关键在于你的思维方式。

如果你能灵活和系统地处理问题，你将能够在使用工具、框架和数据集的过程中逐渐熟悉这些具体细节。

如何开始

对于那些希望提升数据科学技能的人，朱提供了一些实用的建议，尽管 COVID-19 带来了干扰，但这些建议你仍然可以轻松采纳。

你可以寻找研究社区，参加网络研讨会，并在线找到培训课程。一旦面对面的网络交流再次成为可能，朱建议你积极参与 数据科学社区。

“参加 Meetups 和黑客马拉松，这将帮助你建立强大的网络，讨论你的想法，激发你的研究灵感，并解答你的问题。”

此外，请记住，数据科学领域是新兴的，仍在发展中。

各种不同的职位名称正在出现，例如数据科学家、数据工程师和数据分析师，还有机器学习和深度学习工程师。你 可能会发现某些角色更适合你的兴趣和技能。

发掘你的好奇心和创造力，提升你的 Python 技能，进入数据科学领域吧！

本文最初发表于 Refinitiv Perspectives 于 2020 年 4 月初。

个人简介：Jo Stichbury 是一名自由职业技术作家。

原文。经许可转载。

相关内容：

用 NLP 发现争议

数据科学家的 4 种现实职业选择

如何在理想公司获得数据科学家职位

相关主题

成为优秀数据科学家所需的 5 项关键技能

每个初学者数据科学家应掌握的 6 种预测模型

2021 年最佳 ETL 工具

成功数据科学家的 5 个特征

每个数据科学家都应该了解的三个 R 语言库（即使你使用 Python）

停止学习数据科学以寻找目标，并寻找目标以……

第三波数据科学家

原文：www.kdnuggets.com/2019/05/third-wave-data-scientist.html

 评论

由 Dominik Haitz 提供，IONOS。

介绍

德鲁·康威的数据科学技能集可视化 是一个经常被引用的经典。不同的观点和角色的多样性催生了numerous variations：

我们的三大课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业道路。

 2. 谷歌数据分析专业证书 - 提升你的数据分析技能

 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT

各种数据科学维恩图。图片来源于谷歌图片。来源：sinews.siam.org/Details-Page/a-timely-focus-on-data-science

关于数据科学技能集似乎没有共识。此外，随着领域的发展，缺陷变得明显，新挑战不断出现。我们如何描述这种演变？

第一波数据科学家发生在数据尚未大规模和数据科学尚未成为实际存在之前（2010 年之前）：统计学家和分析师，他们一直存在，做着很多现代数据科学家在做的事情，但伴随的炒作较少。

第二波：大规模数据收集创造了对聪明人才的需求，他们能够将大数据转化为大钱。公司仍在摸索应聘人员的类型，往往转向科学专业毕业生。虽然第二波数据科学家做了很多正确的事，但他们精心制作的模型通常只是概念验证，并未带来实际的改变。

现在，在 2010 年代末，随着对深度学习和人工智能的炒作，进入了第三波数据科学家：进行实验和创新，效率地寻找商业价值，并弥合部署差距以创建出色的数据产品。在这里需要哪些技能？

第三波数据科学家的技能组合。

1. 商业思维

商业思维是数据科学技能组合的核心，因为它设定目标并运用其他技能来实现这些目标。Patrick McKenzie 在这篇博客文章中指出：

工程师被聘用是为了创造商业价值，而不是编程：企业总是因为非理性和政治原因做事，但主要还是集中在增加收入或降低成本上。

同样，数据科学家被聘用是为了创造商业价值，而不仅仅是构建模型。问问自己：我的工作成果将如何影响公司决策？我需要做什么来最大化这一效果？凭借这种企业家精神，第三波数据科学家不仅提供可操作的见解，还寻求实现真正的变化。

关注组织中的资金流向——拥有最大成本或收入的部门可能会提供最大的财务杠杆。然而，商业价值是一个模糊的概念：它超越了当前财政年度的成本和收入。实验和创建创新的数据文化将提高公司的长期竞争力。

优先处理你的工作和知道何时停止是效率的关键。想想递减收益：花费数周时间来调整模型以提高 0.2%的精度是否值得？通常，足够好才是真正的完美**。

领域专长，这是 Conway 技能组合的三分之一，绝对不能被忽视——然而，你几乎在所有地方都必须在工作中学习。这包括对你所在行业以及公司流程、命名方案和特殊情况的了解。这些知识不仅为你的工作设定了框架条件，而且通常对于理解和解释你的数据是不可或缺的。

保持简单，傻瓜

https://twitter.com/matvelloso/status/1001899539484192768

寻找容易实现的目标和快速胜利。对现有数据仓库进行一个简单的 SQL 查询可能会发现产品经理或高层管理人员未知的宝贵见解。**不要陷入“流行词驱动的数据科学”**的陷阱，专注于最先进的深度学习，而一个简单的回归模型就足够了——而且构建、实施和维护的工作量要少得多。了解复杂的事物，但不要使事情过于复杂。

2. 软件工程工艺

对（第二波）数据科学家只需“黑客技能”而不是适当的软件工程的观点已被多次批评。缺乏可读性、模块化或版本控制阻碍了协作、可重复性和生产化。

相反，向专业的软件工程师学习工艺。测试你的代码并使用版本控制。遵循既定的编码风格（例如 PEP8）并学习如何使用 IDE（例如 PyCharm）。尝试对编程进行配对。模块化和文档化你的代码，使用有意义的变量名称并进行重构，重构，重构。

弥合敏捷原型数据产品的部署差距：学习使用日志记录和监控工具。知道如何构建 REST API（例如使用 Flask）以将结果提供给他人。学习如何将工作打包到 Docker 容器中，或将其部署到像 Heroku 这样的平台上。不要让你的模型在笔记本电脑上腐烂，而是将其包装成适合你公司 IT 环境的数据驱动服务。

3. 统计学和算法工具箱

数据科学家必须彻底理解统计学基础概念和特别是机器学习（STEM 大学教育可能是获得这种基础的最佳途径）。关于重要内容有很多资源，所以我不会在这里进一步探讨。你常常需要向客户解释算法或概念，如统计不确定性，或者因为混淆相关性和因果关系而标记出问题。

4. 软技能

由于人际交往技能对生产力的重要性与技术技能相当，第三波数据科学家在这些领域上会有意识地努力提高。

与他人合作良好

咨询你的同事——大多数人乐于帮助或提供建议。平等对待他人：你可能有一个很好的学位和对复杂算法的理解，但其他人拥有你没有的经验（这听起来像基本的社交建议，但谁没遇到过傲慢的 IT 专业人士呢？）。

理解你的客户

问对后续问题。如果客户或你的老板希望你计算一些关键数据或创建一些图表，问“为什么？目的是什么？你想达成什么？根据结果你会采取什么行动？”以更好地理解问题的核心。然后一起找出如何达到目标——是否有比提出的更好的方法来实现目标？

处理公司政治

建立网络，不是因为你期望他人在你的职业生涯中帮助你，而是因为你是一个容易接近的人。与有类似工作主题的人建立联系。如果公司内没有这样的平台，自己创建。识别关键利益相关者，并找出如何帮助他们解决问题。及早邀请他人，并使他们成为变革过程的一部分。记住：公司不是一个理性的实体，而是由经常不理性的个体组成。

传达你的结果

提升你的可视化和演示技巧。从客户的角度进行沟通：我如何才能准确回答他们的问题？学会在不同层次上沟通并总结你的工作细节。人们很容易被华丽的多维图表所吸引，但通常简单的柱状图更能有效传达信息。展示你的成果：当人们看到你正在做的工作并且发现你做得很好，他们会信任你。

自我评估

沟通你的目标和问题，并积极寻求建议。在数据科学社区内外寻找榜样，并向他们学习。

原文。经许可转载。

个人简介：多米尼克·海茨是数据科学家@ionos_de，同时也是博客作者，towardsdatascience.com/@d_haitz。

资源：

在线和基于网页的：分析、数据挖掘、数据科学、机器学习教育

分析、数据科学、数据挖掘和机器学习的软件

相关：

数据科学中最受欢迎的技能

2019 年典型的数据科学家是谁？

数据科学家与数据工程师的区别

更多相关内容

成为伟大的数据科学家所需的 5 项关键技能

每个初学者数据科学家应该掌握的 6 种预测模型

2021 年最佳 ETL 工具

成功数据科学家的 5 个特征

每个数据科学家都应该知道的三个 R 库（即使你使用 Python）

停止学习数据科学以寻找目的，寻找目的去……

本周 AI，7 月 31 日：AI 巨头承诺负责任的创新 • 白鲸入侵

原文：www.kdnuggets.com/2023/07/this-week-ai-2023-07-31.html

由编辑使用 BlueWillow 创建的图像

欢迎来到 KDnuggets 首期的“本周 AI”专刊。此精选的每周更新旨在让您了解人工智能领域迅速发展的最重要进展。从塑造我们对 AI 在社会角色理解的突破性头条新闻，到引发深思的文章、深刻的学习资源，以及推动知识边界的重点研究，这篇文章提供了 AI 当前格局的全面概述。虽然尚未深入细节，但请期待探索反映 AI 广泛和动态本质的众多话题。请记住，这只是许多未来每周更新中的第一个，旨在让您在这一不断发展的领域中保持更新和知情。敬请关注，祝您阅读愉快！

我们的前三个课程推荐

 1. 谷歌网络安全证书 - 快速入门网络安全职业。

 2. 谷歌数据分析专业证书 - 提升您的数据分析能力

 3. 谷歌 IT 支持专业证书 - 支持您的组织的 IT 工作

头条新闻

“头条新闻”部分讨论了过去一周在人工智能领域的最新新闻和发展。信息涵盖从政府的 AI 政策到技术进步以及 AI 领域的企业创新。

AI 巨头在拜登-哈里斯政府下承诺负责任创新

拜登-哈里斯政府已从七家领先的 AI 公司——亚马逊、Anthropic、谷歌、Inflection、Meta、微软和 OpenAI——那里获得了自愿承诺，以确保 AI 技术的安全、保障和透明发展。这些承诺强调了未来 AI 的三个基本原则：安全、保障和信任。这些公司同意在发布前对其 AI 系统进行内部和外部安全测试，分享关于管理 AI 风险的信息，并投资于网络安全。他们还承诺开发技术机制，确保用户知道何时内容是 AI 生成的，并公开报告其 AI 系统的能力、局限性以及适当和不适当使用的领域。这一举措是拜登-哈里斯政府致力于确保 AI 的安全和负责任发展，以及保护美国人免受伤害和歧视的更广泛承诺的一部分。

Stability AI 揭示 Stable Beluga：开放访问语言模型的新强者

Stability AI 和其 CarperAI 实验室宣布推出 Stable Beluga 1 和 Stable Beluga 2 两个强大的开放访问大语言模型（LLMs）。这两个模型在各种基准测试中展现了卓越的推理能力，分别基于原始的 LLaMA 65B 和 LLaMA 2 70B 基础模型。两个模型均通过使用标准 Alpaca 格式的有监督微调（SFT）方法，在新的合成生成数据集上进行了微调。Stable Beluga 模型的训练灵感来源于微软在其论文“Orca: Progressive Learning from Complex Explanation Traces of GPT-4”中使用的方法。尽管训练样本量仅为原始 Orca 论文的十分之一，但 Stable Beluga 模型在各种基准测试中表现出色。截至 2023 年 7 月 27 日，Stable Beluga 2 是排行榜上的顶级模型，而 Stable Beluga 1 排名第四。

Spotify CEO 暗示未来 AI 驱动的个性化和广告功能

在 Spotify 第二季度的财报电话会议上，首席执行官 Daniel Ek 暗示了可能会为流媒体服务引入更多 AI 驱动的功能。Ek 认为 AI 可以用来创建更个性化的体验，总结播客内容和生成广告。他强调了最近推出的 DJ 功能的成功，该功能提供了精选音乐以及关于曲目和艺术家的 AI 驱动评论。Ek 还提到可能会使用生成性 AI 来总结播客内容，使用户更容易发现新内容。此外，Ek 讨论了 AI 生成音频广告的可能性，这可能显著降低广告商开发新广告格式的成本。这些评论正值 Spotify 寻求为 AI 驱动的“文本到语音合成”系统申请专利，该系统可以将文本转换为具有情感和意图的人声音频。

文章

“文章”部分展示了一系列发人深省的人工智能相关文章。每篇文章深入探讨了特定话题，为读者提供了关于 AI 各个方面的见解，包括新技术、革命性方法和开创性工具。

??? ChatGPT 代码解释器：几分钟内完成数据科学工作

这篇 KDnuggets 文章介绍了 ChatGPT 的代码解释器插件，这是一种可以分析数据、编写 Python 代码和构建机器学习模型的工具。作者 Natassha Selvaraj 演示了如何使用该插件来自动化各种数据科学工作流程，包括数据总结、探索性数据分析、数据预处理和构建机器学习模型。代码解释器还可以用于解释、调试和优化代码。Natassha 强调，虽然这个工具功能强大且高效，但它应该作为数据科学任务的基础使用，因为它缺乏领域特定的知识，不能处理存储在 SQL 数据库中的大型数据集。Natassha 建议初级数据科学家和有志于成为数据科学家的人应学习如何利用像代码解释器这样的工具来提高工作效率。

??? 教材就是你所需：一种革命性的 AI 培训方法

这篇 KDnuggets 文章讨论了微软研究人员提出的一种新型 AI 训练方法，该方法涉及使用合成教材而非大量数据集。研究人员将一个名为 Phi-1 的模型完全在自制教材上进行训练，发现它在 Python 编程任务中的表现非常出色，尽管它的规模明显小于 GPT-3 等模型。这表明，训练数据的质量可能与模型的规模同样重要。Phi-1 模型在通过合成练习和解决方案进行微调后，性能也有所提升，表明有针对性的微调可以提高模型的能力超越其专门训练的任务。这表明，这种基于教材的方法可能会通过将重点从创建更大模型转向策划更好的训练数据，从而彻底改变 AI 训练。

最新的提示工程技术巧妙地将不完美的提示转化为卓越的生成式 AI 互动

这篇文章讨论了一种新型的提示工程技术，鼓励使用不完美的提示。作者认为，追求完美的提示可能适得其反，往往更实际的做法是追求“足够好”的提示。生成式 AI 应用程序使用概率和统计方法解析提示并生成响应。因此，即使使用相同的提示多次，AI 也可能每次生成不同的响应。作者建议，与其努力寻求完美的提示，不如利用不完美的提示并将它们汇总以创建有效的提示。文章引用了一项名为“随便问我什么：一种简单的提示语言模型策略”的研究，该研究提出了一种通过汇总多个有效但不完美的提示的预测来将不完美提示转化为强健提示的方法。

学习资源

“学习资源”部分列出了对那些渴望扩展 AI 知识的人员有用的教育内容。这些资源包括从全面的指南到专业课程，适用于 AI 领域的初学者和经验丰富的专业人士。

Cohere 的 LLM 大学：通往大型语言模型世界的门户

Cohere 的 LLM 大学是一个为对自然语言处理（NLP）和大型语言模型（LLMs）感兴趣的开发者提供的综合学习资源。课程旨在提供坚实的 NLP 和 LLMs 基础，并在此基础上构建实际应用。课程分为四个主要模块：“大型语言模型是什么？”，“使用 Cohere 端点进行文本表示”，“使用 Cohere 端点生成文本”和“部署”。无论你是新的机器学习工程师还是希望扩展技能的经验丰富的开发者，Cohere 的 LLM 大学都提供了一个全面的 NLP 和 LLMs 世界指南。

??? 来自 Google 的免费生成式 AI 学习路径

Google Cloud 发布了生成式 AI 学习路径，这是一个包含从生成式 AI 基础知识到更高级工具（如生成式 AI 工作室）的免费课程合集。学习路径包括七门课程：“生成式 AI 概述”、“大型语言模型简介”、“图像生成简介”、“注意力机制”、“Transformer 模型和 BERT 模型”、“创建图像字幕模型”和“生成式 AI 工作室简介”。这些课程涵盖了包括大型语言模型、图像生成、注意力机制、Transformer 模型、BERT 模型和图像字幕模型在内的多个主题。

研究聚焦

“研究聚焦”部分突出了人工智能领域的重要研究。该部分包括突破性的研究、探索新理论以及讨论人工智能领域的潜在影响和未来方向。

??? 大型语言模型在数据科学教育演变中的作用

题为“大型语言模型在数据科学教育演变中的作用”的研究论文讨论了大型语言模型（LLMs）对数据科学家角色和职责的变革性影响。作者认为，大型语言模型的崛起正在将数据科学家的重点从动手编码转移到管理和评估由自动化人工智能系统执行的分析。这种转变需要数据科学教育的重大演变，强调在学生中培养多样化的技能，包括由 LLMs 启发的创造力、批判性思维、受 AI 指导的编程以及跨学科知识。

作者们还提出，大型语言模型（LLMs）在课堂上可以作为互动教学和学习工具发挥重要作用。它们可以促进个性化教育，丰富学习体验。然而，将 LLMs 整合到教育中需要谨慎考虑，以平衡 LLMs 的好处，同时促进人类专业知识和创新。论文建议，数据科学教育的未来很可能涉及人类学习者与 AI 模型之间的共生关系，其中双方相互学习并提升各自的能力。

更多相关内容

学习如何设计和部署负责任的 AI 系统

学习如何设计和部署负责任的 AI 系统

免费 4 周数据科学课程：AI 质量管理

本周提升搜索应用的 8 种方法

本周 AI 动态，8 月 7 日：生成式 AI 进入 Jupyter 和 Stack…

回到基础 第 1 周：Python 编程与数据科学基础

本周 AI 动态，8 月 7 日：生成性 AI 进入 Jupyter 和 Stack Overflow • ChatGPT 更新

原文：www.kdnuggets.com/2023/mm/this-week-ai-2023-08-07.html

图片由编辑使用 Midjourney 创建

欢迎来到本周的“本周 AI 动态”版块，由 KDnuggets 提供。这个精心策划的每周帖子旨在让你了解人工智能快速发展的世界中的最引人注目的进展。从塑造我们对 AI 在社会中角色的突破性头条新闻，到发人深省的文章、深刻的学习资源和推动我们知识边界的研究亮点，这个帖子提供了对 AI 当前格局的全面概述。此每周更新旨在让你在这一不断发展的领域中保持更新和知情。

我们的前三名课程推荐

 1. 谷歌网络安全证书 - 快速进入网络安全职业的捷径。

 2. 谷歌数据分析专业证书 - 提升你的数据分析能力

 3. 谷歌 IT 支持专业证书 - 支持你所在组织的 IT 工作

标题

“标题”部分讨论了过去一周在人工智能领域的主要新闻和进展。信息涵盖了从政府 AI 政策到技术进步和企业 AI 创新的各个方面。

??? Jupyter 中的生成性 AI

开源项目 Jupyter 团队发布了 Jupyter AI，这是一种新扩展，将生成性 AI 功能直接引入 Jupyter 笔记本和 JupyterLab IDE。Jupyter AI 允许用户通过聊天互动和魔法命令利用大型语言模型来解释代码、生成新代码和内容、回答有关本地文件的问题等。它在设计时考虑了负责任的 AI，允许对模型选择和 AI 生成输出的跟踪进行控制。Jupyter AI 支持像 Anthropic、AWS、Cohere 和 OpenAI 这样的提供商。其目标是以伦理的方式使 AI 更易于访问，以提升 Jupyter 笔记本的体验。

??? 宣布 OverflowAI

Stack Overflow 宣布了 OverflowAI，这是他们将 AI 能力整合到公共问答平台、Stack Overflow for Teams 以及 IDE 扩展等新产品中的项目。其功能包括语义搜索以寻找更相关的结果、摄取企业知识以加快内部问答的启动、一个访问 Stack Overflow 内容的 Slack 聊天机器人，以及一个在开发者工作流程中呈现答案的 VS Code 扩展。他们旨在利用其社区中超过 5800 万个问题，同时通过对 AI 生成内容的归属和透明度来确保信任。目标是通过负责任地使用 AI 来提升开发者的效率，将他们与上下文中的解决方案相连接。

??? ChatGPT 更新

在过去的一周中，我们推出了若干小更新，以提升 ChatGPT 的使用体验。这些更新包括引入提示示例以帮助用户开始聊天、建议回复以促使更深层次的互动，以及为 Plus 用户提供默认使用 GPT-4 的偏好设置。此外，还引入了多文件上传功能（在 Plus 用户的代码解释器测试版中）、新的保持登录功能以及一系列键盘快捷键，以提高可用性。

文章

“文章”部分展示了一系列引人深思的人工智能相关作品。每篇文章深入探讨一个特定主题，为读者提供关于 AI 的各种见解，包括新技术、革命性方法和开创性的工具。

??? 我在 3 天内创建了一个 AI 应用

作者通过 ChatGPT 提示实验，创建了一个"
Domino 博客中文翻译（五）,2024-10-31,"原文：Domino Blog

协议：CC BY-NC-SA 4.0

Voicebox 的众包语言理解

原文：https://www.dominodatalab.com/blog/data-science-at-voicebox

我们最近采访了 Voicebox 的数据科学和众包总监 Daniela Braga 博士。

丹妮拉，谢谢你的面试。让我们从谈论你的背景开始我们在一起的时间。

我的背景是言语科学。我在这个地区工作了十五年。我在葡萄牙和西班牙的学术界呆了六年。在过去的九年里，我一直在这个行业工作，包括在葡萄牙、北京和华盛顿州雷德蒙的微软演讲小组工作了七年。最近，我一直专注于数据科学，在微软集团和最近的 Voicebox Technologies 将众包引入了演讲。这些努力为两家公司节省了近 50%-80%的成本。我一直致力于在数据驱动的技术世界中构建高质量的控制工作流程和可扩展的吞吐量，这需要无止境的数据供应。

你是如何对语音识别感兴趣的，更具体地说，是对自然语言理解感兴趣的？

这是一个有趣的故事，因为它是偶然发生的。当我获得语言学学士学位后，我在葡萄牙的大学里看到了一份招聘启事。Porto 的工程学院正在寻找一位语言学家来帮助他们开发欧洲葡萄牙语的首个文本到语音和自动语音识别系统。这发生在 2000 年 7 月中旬。求职截止日期已过，大学里空无一人。我的一个朋友鼓励我无论如何都要打这个号码，当场就打。我以为这毫无意义，但我做到了。一个低沉的男声拿起电话，原来还没人申请。所以我得到了这份工作。我采访了波尔图大学语音实验室的负责人，他告诉我，“在这个新领域(语音技术)，聘请语言学家与工程师合作已经成为一种新趋势。整个欧洲都在发生。我们还不知道语言学家如何以及为什么有用，但如果你能帮助我们搞清楚，你就有这份工作。”我在这里，仍然在语音技术领域工作。我想我找到了答案。

你有非常丰富的学术和专业背景，能分享一些职业亮点吗？

让我看看。我相信第一个是我博士论文的主题。这是在数据驱动技术完全成为语音科学主流之前。即使在今天，文本到语音(TTS)技术也有许多基于规则的技术。我是第一个为文本到语音转换开发完全基于规则的前端的人，并成功地将其应用于欧洲葡萄牙语、巴西葡萄牙语和加利西亚语，加利西亚语是西班牙使用的一种语言，与葡萄牙语有着共同的过去。这是非常独特的，考虑到在那之前，已经用数据驱动技术进行了几次尝试，但没有一次取得相同的结果。

第二个是能够说服微软的语音团队来处理 TTS 系统的开发和葡萄牙大部分欧洲语言的重要语言组件。在两年内，我的团队能够向他们发布 Project Exchange 14(包含 26 种语言)。

最新的亮点是能够说服 Voicebox Technologies，与机器学习相关的众包是扩大语音技术产品规模的最佳方式，可以更快、更具成本效益地覆盖全球，同时保持质量。

你在 Voicebox 做什么工作？

我已经重组了 Voicebox 的数据团队。我刚来的时候，这家公司曾经依赖客户提供数据。我改变了公司在数据方面的能力，使我们能够通过网络服务和 API 向客户公开数据服务，以及我们的对话理解产品。在过去的两年里，我一直在发展一个由 15 人组成的团队，服务于公司对多语言数据收集、数据清理/处理/标记、机器学习、数据评估、摄取、存储、数据库、安全、报告和分析的需求。

你发现的最令人惊讶的见解或发展是什么？

出于基于规则的思维模式(因为我的语言学背景)，我目睹的最令人惊讶的发展是语音技术的科学范式从基于规则到数据驱动技术的转变。现在这说得通了，但它并不总是显而易见的。我很快适应了从基于规则到众包的转变，这一切都是为了服务于数据驱动技术日益增长的需求。

数据科学和语音识别的未来会是什么样子？众包如何融入这一切？

它不再仅仅是语音识别；是语言理解。语音识别在语音层面处理命令和控制，而语言理解解释用户的意图，并以有意义的结果做出响应。语音技术需要大量的数据。虽然我们可以说自然语言理解在美国英语中相对成熟，但对于世界上其余的 7000 种口语来说，情况并非如此。

众包为语音技术使用的机器学习技术带来了必要的人类判断

国际上对支持语音的技术的市场需求正在增长。当前的数据驱动技术需要大量的数据来训练和测试语音识别和自然语言理解模块。数据永远不会完整，因为它需要持续更新的企业、兴趣点、名人、书籍和音乐发行等列表。众包为语音技术使用的机器学习技术带来了必要的人类判断。例如，为了训练一个声学模型(这是一个语音识别组件)，您需要至少 1000 个扬声器，每个扬声器讲一个小时。这些发言者需要在性别、年龄和地区方面保持平衡。你可以训练一个系统来识别不同的方言和社会学家。这就是众包的用武之地，在这种类型的变化中，仍然没有办法取代人类。

今年你在做什么个人/专业项目，为什么/如何让你感兴趣？

自动化我们的众包工作流程，通过报告和分析功能以及数据可发现性创建数据可视化。这是一项新的服务，允许内部和外部客户浏览和查询我们所有不同的数据库和数据类型，从声音到文本数据。

哪些出版物、网站、博客、会议和/或书籍对你的工作有帮助？

我关注并参加传统的语音技术活动，在这些活动中，您可以看到该领域的最新和最大的发展(如 Interspeech、ICASSP、ACL、LREC 等会议)。我阅读了谷歌研究博客(T0)和微软研究博客(T1)，Crowdsourcing.org 新闻，以及《企业家》、《商业内幕》和 TechCrunch 等出版物。我也一直在关注数据科学领域正在发生的事情，但我的注意力仍然非常分散。

你发现了哪些最有帮助的机器学习方法？你认为最有帮助的是什么？你最喜欢使用的工具/应用程序是什么？

目前，深度神经网络是语音技术的 ML 技术的圣杯。如果你将 DNNs 应用于你的主题，你可以在任何地方得到一篇被接受的论文。在我们的领域中，Word2vec 和其他技术也显示出非常有趣。

对数据科学/机器学习的学生或从业者有什么忠告吗？

在大数据世界中，开源数据正变得越来越流行，构建机器学习包并将其作为服务提供似乎是将你的技能集货币化的最佳方式之一。

丹妮拉，非常感谢你抽出时间！我们很高兴了解你和你在 Voicebox 所做的革命性工作。

在 Linkedin 上关注丹妮拉，并访问【语音信箱】(丹妮拉·布拉加语音信箱)。

AWS 上的数据科学:优势和常见陷阱

原文：https://www.dominodatalab.com/blog/data-science-aws

两年多前，我们写了许多企业对云的错误恐惧。事情变化得多快啊！如今，我们合作的每一家企业要么是在使用云技术的 T2，要么正在迁移到那里。我们与一些公司合作，这些公司仅在两年前还坚持认为他们“不能使用云”，现在正在采取战略举措，“到 2017 年底在 AWS 中开展实际工作。”我们看到这种情况发生在各个行业，包括金融、保险、制药、零售，甚至政府。

AWS 对于数据科学工作负载尤其具有吸引力，这些工作负载受益于针对计算密集型实验的弹性计算的爆发，并且通常受益于 GPU 等专用硬件。例如，对冲基金和银行公司可以通过在机器上分散工作来更快地回溯测试投资策略。制药研究团队可以运行更快的蒙特卡罗模拟来加速药物设计。试验驾驶员遥测模型或图像识别的保险公司可以更容易地从深度学习技术中受益。

为了实现云在数据科学方面的全部潜力，组织必须解决一系列外围用例，而不仅仅是提供对基础架构的访问。因为我们从第一天起就用一流的 AWS 集成构建了 Domino ，它为云中的数据科学工作流提供了一个集成的、完整的解决方案。下面，我们介绍了过去两年观察数据科学团队向云迁移时遇到的一些常见问题和陷阱。

DevOps 自动化

您不希望您的数据科学家在开发运维任务上花费时间，比如创建 ami、定义安全组和创建 EC2 实例。数据科学工作负载受益于 Jupyter 或 RStudio 等工具中用于探索性分析的大型机器，以及支持团队突发需求的弹性可扩展性，或数据科学实验的并行执行，这些实验通常是计算密集型的。

成本控制、资源监控和报告

数据科学工作负载通常受益于价格昂贵的高端硬件。当数据科学家能够更多地访问可扩展计算时，您如何降低成本失控的风险、强制实施限制以及在多个小组或团队之间进行分配？

环境管理

数据科学家需要敏捷性来试验新的开源工具和软件包，这些工具和软件包的发展速度比以往任何时候都快。系统管理员必须确保环境的稳定性和安全性。如何在紧张中平衡这两点？

绘图处理器

神经网络和其他有效的数据科学技术受益于 GPU 加速，但配置和利用 GPU 仍然是说起来容易做起来难。如何为您的数据科学家提供对 GPU 的高效访问，而不使他们陷入 DevOps 配置任务中？

安全性

AWS 在其环境中提供了世界一流的安全性，但是您仍然必须选择如何为运行在 AWS 上的应用程序配置安全性。当您的数据科学家转移可能代表您最有价值的知识产权的逻辑(源代码)和数据集时，这些选择可以在降低风险方面发挥重要作用。

了解更多信息

Domino 本机运行在 AWS 中，可以安装在您的 VPC 中，或者在我们的托管环境中使用。包括孟山都、Mashable、Eventbrite、DBRS 在内的公司，以及对冲基金、银行和保险公司都在 AWS 中使用 Domino 作为其数据科学家的力量倍增器。如果你想学习如何操作，请告诉我们。

银行业中的数据科学:过去、现在和未来

原文：https://www.dominodatalab.com/blog/data-science-banking

这是德克萨斯州奥斯汀数据科学论坛上众多精彩会议之一。不要错过 2016 年 9 月 14 日在洛杉矶数据科学弹出式窗口向世界领先的数据科学家学习的下一次机会。查看演讲者阵容并立即获取门票。

利用 Docker 实现数据科学敏捷性

原文：https://www.dominodatalab.com/blog/data-science-docker

这篇文章描述了 Domino 如何使用 Docker 为数据科学家和研究人员解决许多相互关联的问题，这些问题与环境敏捷性和工作的可重复性有关。

背景

在 Domino，我们非常依赖 Docker。但是，虽然大多数关于 Docker 的讨论都集中在工程师如何使用 Docker 来发布他们的软件，但我们在我们的产品本身中使用 Docker，以允许数据科学家轻松控制什么样的环境(包、库等)。)工作时使用。换句话说，我们将 Docker 公开为一个面向用户的特性(除了在内部将其用作 DevOps 工具之外)。

对于那些不熟悉 Domino 的人来说，快速了解一下背景:这是一个平台，数据科学家在这里运行他们的代码(例如 R、Python 等)，跟踪他们的实验，分享他们的工作并部署模型。集中式基础架构和知识管理使数据科学家更具生产力，并支持更具可重复性的协作式数据科学。

环境管理问题

研究人员和数据科学家的一个共同难题是“环境管理”我用这个术语来描述与安装软件和软件包、配置设置、个人或团队进行分析工作相关的一系列问题。我们最常看到的一些问题是:

研究人员在安装新的软件包时经常受到中央 IT 资源的限制。如果这个过程太慢或太费时，组织就不能停留在现代技术和工具的前沿。开源生态系统比以往任何时候都发展迅速，组织需要测试和采用新技术和方法的灵活性。
撇开新的包不谈，缓慢或官僚的过程会延迟对标准工具和包(新版本的 R、Python 等)的更新，导致关键组件过时多年。
对于不同的人，代码的工作方式是不同的(或者有时根本不起作用)，因为他们的环境是不同的(例如，不同版本的包)。
新员工或团队成员难以入职，因为设置他们的环境需要很长时间。
旧的项目变得不可用，旧的结果变得不可能重现，因为自代码编写以来，环境已经发生了变化。
升级(例如，更改 R 或 Python 的底层版本，或者升级特定的包)是有风险的，因为很难知道什么会停止工作。

通常，公司在环境管理方面遵循两种途径之一。其中的每一个都解决了上面的一些问题，但是加剧了其他问题:

公司让用户控制他们自己的环境。这给用户带来了灵活性和敏捷性，但不可避免地会导致不一致的环境。通常，这意味着工作被孤立在研究人员的机器上，使得共享和协作更加困难，也限制了用户的计算资源。
公司锁定中央环境，例如大型共享“研究服务器”。它控制这些机器。用户可以访问一致的环境，但是这些环境很难改变。
基于 Docker 构建的计算环境

Domino 定义了计算环境的一级概念:一个包含一组软件和配置文件的环境，当分析代码运行时，这些文件应该在适当的位置。这是强大的，原因有几个:

环境附加到您的代码所在的项目。因此，如果有人在 12 个月后重访一个项目，或者一个新员工加入一个项目，您可以确保代码将按预期工作，不管您的计算机上安装了什么。
研究人员可以改变环境，升级或安装新的图书馆，而不需要通过它，关键是，不影响任何其他人的环境。
与研究人员操纵他们自己的机器的设计不同，Domino 的环境是集中存储和管理的。研究人员可以很容易地共享环境，并且环境可以一次为每个人升级。

实际上，计算环境是基于 Docker 映像构建的。当研究人员运行代码时，他们的代码是在一个从该图像开始的容器中运行的。

使用 Docker 是特别强大的，因为它让我们保存图像的过去版本，这样我们就可以记录用于生成任何过去结果的确切环境。

新功能

让我们来看看这种设计支持的一些工作流。

使用不同版本的常用工具

假设您的一些用户使用 Python 3，一些用户使用 Python 2。或者有些使用标准 R 分布，有些使用 R 的 Revolution Analytics 分布。您可以为每种配置创建环境，数据科学家可以选择他们工作时想要使用的环境:

创建尽可能多的环境，并管理它们的共享和许可方式，以便在您的组织内创建各种标准环境选择。

升级重要的库

在幕后，Domino 允许您通过运行 Docker 命令来指定环境的内容。要了解这一点，假设默认环境是 R 3.2，而您想升级 R 3.3。因为我们很谨慎，假设我们想在我们自己的项目中测试这种变化，以验证它在向所有人推广之前是否正常工作。

我们可以复制“默认”环境，并在其上添加我们想要运行的任何 Docker 命令。

现在，我们只需更改我们的项目以使用这个新环境，并重新运行我们的代码。

我有一个过去运行过的回归测试脚本，“test。r”。一旦我更改并选择了新环境，我就可以重新运行该脚本。至关重要的是，我可以重新运行上次运行的确切版本，因此我可以肯定，我在结果中看到的任何差异都将完全是由于 R 升级，而不是我的代码中的更改。

我们甚至可以比较在一个环境和另一个环境中运行的结果，以确保没有任何变化。

要深入了解如何比较 Domino 中不同实验的结果，请查看我们过去关于数据科学的“单元测试”和模型调优和实验的帖子。

复制过去的作品

除了给数据科学家带来更多的灵活性，Domino 为代码的每次执行保留了一个不可变的记录，包括它在哪个环境中运行的信息。因此，对于任何过去的结果，很容易找到并恢复用于重现任何过去结果的确切软件集。

我们可以更进一步。除了在改变环境后比较实验结果之外，还可以配置 Domino 在运行代码时保存环境状态的文本快照。这让您可以使用我们的区分功能来检查在同一个实验的两次不同运行之间哪些包或软件可能发生了变化。

在这个例子中，我们有一个在不同时间运行的 python 脚本，我们感兴趣的是识别底层环境中的任何变化。比较也能告诉我们这一点。

结论

Docker 是一项非常强大的技术。除了为 DevOps 团队启用新的工作流之外，它还可以优雅地解决数据科学家日常面临的许多问题。以 Docker 为基础，Domino 的“计算环境”功能使研究人员能够灵活地轻松试验新的包，而不会破坏同事环境的稳定性。与此同时，计算环境使研究人员能够重现结果，并审核已安装软件和软件包的变更。

企业中的数据科学:来自易贝、Stitch Fix、Teleon Health 和 RISELab 的见解

原文：https://www.dominodatalab.com/blog/data-science-enterprise

最近，我们与几位数据科学领导者就企业数据科学的组织设计和工具举行了一次小组讨论。观看完整视频:

面向鸟类(和所有人)的数据科学

原文：https://www.dominodatalab.com/blog/data-science-for-the-birds-and-everyone

奥杜邦的数据科学工作是如何产生影响的

编者按:这是一系列文章的一部分，分享公司在成为模型驱动的道路上的最佳实践。一些文章将包含关于他们使用 Domino 的信息。

我是一名训练有素的科学家。我有生态学博士，保护生物学硕士，生物学本科。今天，我是一个已经有 100 多年历史的非营利组织的临时首席科学家:国家奥杜邦协会。

你可能听说过我们。我们关注整个美洲的鸟类保护。为什么是鸟？生态学家早就观察到，鸟类繁衍的地方，人类也繁衍生息。因此，如果我们能保护鸟类，我们就能更好地保护地球。

在过去的六年里，我们的数据科学实践显著增长。我们希望了解气候变化对鸟类数量的影响，并为我们的保护和宣传工作提供信息。像大多数组织一样，我们经历了通过使用移动电话和网络服务捕获的数据量的急剧增加。这些技术扩展了我们所谓的“社区科学”，志愿者可以分享观察和其他相关数据。我们构建了 180，000 个模型，分析了来自科学家和社区成员的超过 1.4 亿次观察，并预测了不同变暖情景对 604 种鸟类的影响。然后我们建立了鸟类和气候可视化工具，这是一个基于邮政编码的在线工具，任何人都可以看到气候变化对他们社区的影响。

从表面上看，在 Audubon 建立一个数据科学团队似乎与在盈利世界建立一个团队不同。许多人可能认为在这里扩展数据科学会很容易，因为毕竟我们的组织是建立在科学和数据之上的。但在现实中，我们面临着许多相同的文化和组织挑战，各行各业的公司在跨组织扩展工作时都会面临这些挑战。奥杜邦有“业务单元”，专注于不同的优先事项，如海岸、耕地或气候。此外，还有州政府的实地办公室，致力于解决更多的地方性保护问题。正如我提到的，我们是一个有 100 多年历史的科学组织。已经有很多伟大的量化工作在进行。我们必须展示机器学习模型将如何为我们已经在做的事情增加价值。此外，我们不得不从一个孤立运作的科学团队转变为一个与业务部门整合以解决其问题的团队。

为了打破孤岛、简化流程和留住人才，我们的科学团队像数据科学卓越中心(CoE)一样运营。在这篇博客中，我将讨论我们建立一个强大的科学团队的旅程，以及我们采取的步骤，以确保我们拥有正确的技能、流程和技术，将模型驱动的方法纳入鸟类保护工作。

我们从哪里开始？

在最近的数据科学弹出窗口中，Domino 首席数据科学家 Josh Poduska 和来自 Slalom Consulting 的代表讨论了定义 CoE 的能力。我们的科学团队今天解决了他们描述的许多相同的元素。

我们正处于工作的中游，但我们的努力已经在产出和最终用户采用方面产生了重大影响。

以下是我们创建科学团队时采取的五个步骤，旨在建立一个功能卓越的中心:

**围绕我们的“客户”**调整我们的工作，使所有数据科学工作与组织目标保持一致。我们花了大量时间会见奥杜邦保护战略的领导者，讨论每个小组的优先事项，并展示了与我们工作的明确联系。这是关键的第一步，涉及大量的文化变革和交流。我们建立的牢固关系和取得的成功带来了更多的机会。例如，我们为我们的一个州办公室开发了一个模型，这是第一个州办公室的数据科学项目。项目成功的消息不胫而走，其他国家领导人希望与我们合作的机会开始像滚雪球一样越来越多。
**阐明了我们可以提供的能力。**我们还确定了哪些类型的数据科学将为组织提供最大的价值。因此，我们现在关注三个关键领域:

预测模型帮助团队确定可能影响鸟类的问题或威胁，预测未来的挑战，并确定成功缓解这些威胁的政策。

显示鸟类数量下降和增加的趋势，以便团队可以衡量他们的计划是否成功。

跨数据进行空间优化，帮助我们的组织找到开展保护工作的最佳地点。

**制定核心价值观指导人才管理。我们的数据科学团队已经从 2 人发展到 16 人。为了帮助吸引和留住顶尖人才，并确保我们在扩大工作规模时拥有技能，我们制定了核心价值观来指导我们的团队。当然，我们寻找强大的数据科学技能来创造出版质量的科学。但除此之外，我们的价值观强调软技能，如谦逊、同理心和耐心，这对于赢得最终用户对我们工作的认可至关重要。我们还在我们的核心价值观中加入了成长思维，以确保作为一个团队，我们不断学习和应用新技能。拥有增长思维有助于我们进行更多创新，并提高我们的数据科学家的工作满意度。我们有很高的保留率，我相信给人们一个在他们的岗位上学习和成长的机会是关键。 投资技术基础设施、工具和数据合作伙伴关系，促进大规模数据科学。例如，我们评估了不同的数据集，这些数据集将有助于确定对保护最重要的地方。我们现在从一些收集观鸟信息的组织中获取数据，比如 eBird、iNaturalist 和 Movebank。我们仍然有数据缺陷，但我们有程序来帮助我们确定新的数据源来填补空白。我们还希望确保我们的数据科学团队尽可能高效。这将我们带到了 Domino 数据科学平台，它允许我们更快地协作、迭代和探索不同的参数设置。我们为 604 种物种中的每一种建立了 160 个机器学习模型，覆盖了两个不同的季节——冬季和夏季——在短短几个月内总共超过 18 万个模型。能够复制和利用结果对我们的产出产生了相当大的影响。最近，我们开始重新评估我们的技术基础设施。我们有很多 CSV 文件的数据和各种操作系统。数据仓库将允许我们更有效地向最终用户部署数据服务。 **确定成功指标以显示我们的影响力。**在营利性组织中，Coe 通常会确定成本节约或收入增长的关键绩效指标(KPI ),以衡量其数据科学工作的成功与否。在这里，我们做同样的事情，但是我们谈论的不是成本和收入，而是栖息地。具体来说，我们保护了多少栖息地，拯救了多少鸟类？

数据告诉我们什么？

今天，气候变化是鸟类数量的最大威胁。通过我们的工作，我们已经能够展示在不同的变暖情景下，鸟类的活动范围会发生怎样的变化。如果一个物种现有的范围不再适合居住，它会扩展到一个新的区域吗？或者，它们会面临灭绝吗？最脆弱的物种是那些面临其现有分布范围严重丧失的物种，它们不太可能迁移到新的区域。预后不佳。根据我们关于鸟类和气候变化的最新报道，，如果气温上升 3 摄氏度，将近三分之二的北美鸟类将面临灭绝的危险。但随着我们近年来获得的深刻见解，我们现在也可以看到潜在政策变化的影响:立即采取积极行动，将全球气温上升控制在 1.5 摄氏度以内，可以提高 76%的濒危物种的机会。我们的数据科学工作将继续为这一对话提供有价值的见解，并帮助动员我们近 100 万会员和支持者采取行动。随着我们在 CoE 模式下建立我们的科学团队，我们相信我们作为一个组织的影响力将会增长。

欲了解更多信息
国家奥杜邦学会案例研究
新闻稿:新奥杜邦科学:由于气候变化，三分之二的北美鸟类濒临灭绝
鸟类和气候可视化器
奥杜邦杂志的气候问题
感官中的数据科学

原文：https://www.dominodatalab.com/blog/data-science-in-the-senses

By Paco Nathan, Managing Partner, Derwen, Inc. on May 10, 2019 in

本博客最初发布于 KDnuggets 这里。

今年 Rev 大会的晚间活动将是意义上的数据科学。那是在纽约，5 月 23 日星期四。

今天就注册 Rev 并使用此代码获得折扣: PACORev25 —参见本文末尾了解更多关于会议的详细信息。

感官中的数据科学活动将展示目前在人工智能艺术领域掀起波澜的惊人项目——博特尼克、福克·RNN、本·斯内尔、乔希·厄本·戴维斯等——利用数据和机器学习实现感官体验的项目。下面先睹为快:

Botnik 是一个由作家、艺术家和软件开发人员组成的社区，他们使用机器来创造有趣的东西。例如，在网上查看他们的画廊，比如由一个经过 70，000 多首现有乡村歌曲训练的模型生成的新鼻音曲调。或者深度学习生成的言情小说已经在超过 20，000 个丑角言情标题上进行了训练。谁能否认这样不可抗拒的罪恶快感，如助产士牧牛人或医生丈夫的男人，出于艾的礼貌？或者(我最喜欢的)这个超级有用的整理，把玛丽·近藤所有杂乱的剧集整理成一个浓缩的剧本:

现在，这无疑点燃了欢乐的火花。他们的作品是大数据、一些文本分析、概率和大量讽刺的结果，Botnik 将其作为现场表演，有时甚至是卡拉 ok 式的人工智能增强滑稽动作。在我们的感官中的数据科学活动中，他们将发表演讲来解释魔法并现场表演！

民谣 RNN ，总部位于伦敦金斯敦大学，利用深度学习从乐谱中学习凯尔特民谣音乐，然后生成新的乐谱供人演奏。查看他们的开源 GitHub repo 。O’Reilly AI London 2018 大会在圣詹姆斯苏塞克斯花园举行的激动人心的 AI 音乐:音乐制作的深度学习活动中展示了民谣 RNN。在感官数据科学上，民谣 RNN 将放映一段介绍他们作品的短片，此外他们还为活动的 DJ 提供了一份播放列表。他们还会有一个带耳机和显示器的展台来演示 https://folkrnn.org/软件，Rev 的与会者可以在那里创作他们自己的歌曲。

有传言说，博特尼克和民间 RNN 一直在合作的基础上介绍了通过 Rev 会议。一个用 AI 生成新的乐谱，另一个写歌词。我们渴望看到接下来会发生什么！

本·斯内尔 ，住在纽约，是一位研究计算的物质性和生态的人工智能艺术家。举一个有趣的例子，本训练他的电脑成为一名雕塑家:花几个月的时间筛选博物馆藏品，广泛研究大师作品，磨练技术，试图根据记忆重现它曾见过的每一件古典雕塑。机器学习应用程序被亲切地命名为 Dio(以希腊神狄俄尼索斯的名字命名)，它已经将其研究转化为组件的视觉词汇，并从中构建更复杂的形状和表情。作为所有这些工作之后的顶点项目，本向迪奥提出了一个简单的挑战:“闭上你的眼睛，然后梦见一种新的形式，一些以前从未存在过的东西。”

“计算机无形处理能力的痕迹存在于它的身体形态和包含其思想和记忆的物质碎片中。”在我们的感官数据科学活动中，来看看 Ben 的展览，体验 Dio 的梦想成真的物质性，这是 AI 创作的第一批雕塑之一。

**

Josh 将介绍一个关于创建动画的简短演讲，并有一个以他的 GAN 生成的动画合成为特色的装置，在整个晚上循环播放。

声纹——上一次你在餐馆或酒吧时，你是否几乎听不到同桌其他人的声音？你一定要大喊大叫，只是为了互相交谈吗？是啊，我也是。我们都应该感谢臭名昭著的“顶级厨师”马雷欧·巴塔利在美国餐馆建立了震耳欲聋的趋势。

作为回应，SoundPrint 开始开发一款智能手机应用来测量和分析声音水平。他们建立了一个声纹“大使”网络(真的包括你的)，悄悄收集世界各地餐馆、酒吧、咖啡馆和其他公共场所的分贝测量值——收集到一个声音估计的地理空间数据库中，“像 Yelp！，而是为了噪音。”他们的众包解决方案非常适合检查一个场所是否适合谈话，是否对听力健康安全，非常适合寻找一个更安静的地点进行约会或商务会议。

最近，SoundPrint 一直在扩展他们的数据库工作，以包括听力健康分析、办公室和医院环境分析，以确保符合噪音指南、预测评分、声学取证等。SoundPrint 将在 Data Science in the Senses 上展示，此外，他们还将为与会者提供一个展台，让他们了解如何使用该应用程序及其数据集。顺便说一句，SoundPrint 正在寻找数据科学团队的合作伙伴——那些对数据有兴趣的用例的人。在 Rev 抓住他们！

分析风味系统—美食图 AI 的制作者——利用机器学习为食品和饮料生产商创造服务，帮助他们为目标消费者人口统计数据建模、理解和优化风味/香气/质地。他们的创新量化了产品的风味特征及其原料成分。

在感官中的数据科学活动中，分析风味系统公司将设立一个展位，与会者可以使用一个应用程序根据他们的口味设置偏好，然后配制个性化的饮料。双份的！

Kineviz 利用其 GraphXR 平台开发视觉分析软件和解决方案，用于执法、医疗研究和知识管理等领域。GraphXR 使用 Neo4j 在统一的动画上下文中分析链接、属性、时间序列和空间数据。在感官数据科学活动上，Kineviz 将展示他们对 权力的游戏 数据的分析和可视化。说真的，托蒙德·巨人贝恩会是莱安娜·莫尔蒙的父亲吗？# gameoftrones

我对 Rev 会议、我们的感官中的数据科学晚间活动(作为会议讲座后的社交活动)、我们的优秀演讲者阵容介绍数据科学领域的领导力以及企业团队可以相互学习的内容感到非常兴奋。此外，我们将邀请诺贝尔奖获得者 丹尼尔·卡内曼 作为我们的主题演讲，展示“直觉判断和选择心理学”——通过现代行为经济学和长期以来关于人类理性在决策中的假设角色的假设，详细探索机器学习。

今天注册 Rev 并使用此代码享受折扣: PACORev25 。

那里见！

Twitter Facebook Gmail Share**

Instacart 的数据科学:按需盈利

原文：https://www.dominodatalab.com/blog/data-science-instacart

在本次数据科学弹出式会议中，Instacart 数据科学副总裁 Jeremy Stanley 深入探讨了 Instacart 如何利用数据科学实现利润最大化。

数据科学采访:首席科学家 d stillery Claudia per lich

原文：https://www.dominodatalab.com/blog/data-science-interview-claudia-perlich-chief-scientist-dstillery

我们最近采访了 Dstillery 的首席科学家 Claudia Perlich。在加入 Dstillery 之前，她赢得了许多数据挖掘比赛，以及来自行业和学术界的多个奖项。

Claudia，请简单介绍一下你自己，以及你是如何对数据工作感兴趣的。

我在东德长大，这很有趣，因为那里根本没有广告。所以我直到 15 岁左右才看到自己的第一个广告。早期，我对数学非常感兴趣，并决定在德国学习几年计算机科学，之后我作为交换学生来到科罗拉多州的博尔德，在那里我上了第一门课程，即现在所知的数据科学。这门课是关于人工神经网络的，那时我就迷上了。我真的很喜欢看数据和故事中令人着迷的部分，试图对其进行建模。从那时起，我继续走这条路；我在德国完成了我的学位，最终回到了纽约市的商学院。我获得了信息系统的博士学位，但主要还是在追求我对机器学习、预测建模和数据挖掘的兴趣。

后来，我加入了沃森中心的 IBM 研究机构，该机构因制造沃森计算机而闻名，该计算机最近赢得了一场危险游戏。在那里，我是预测建模小组的一员，使用数据和预测建模处理各种事情，或者在内部为 IBM 工作，或者在外部为他们的客户工作，这些客户是通过咨询服务进来的，并且有非常具体的数据相关问题。我还花了很多时间参加数据挖掘比赛，并赢得了很多比赛:)。我一直喜欢接触非常不同的数据集，并挑战自己构建最佳解决方案。

我发现我可以传播我学到的一些东西，并帮助把我的发现带到医学和其他与生活相关的领域。

你是如何开始使用 Dstillery 的？

2010 年，Media6Degrees 找到了我，现在叫做 Dstillery ，一家数字广告领域的公司。他们通过我在 NYU 的学术顾问找到了我，他向我推荐了一些我在博士期间研究的用于广告定位的技术。他们邀请我加入他们的团队，从那以后我一直担任 Dstillery 的首席科学家。这是一个很好的场所，提醒我为什么喜欢在广告行业工作，这真的是尝试不同事情的最终机会，并找出什么在数据科学中行得通，什么行不通。我们拥有难以置信的数据访问权限，这既具有挑战性，也非常有益。我发现我可以传播我学到的一些东西，并帮助把我的发现带到医学和其他与生活相关的领域。

您记得使用的第一个数据集是什么？你用它做了什么？

我工作的第一个数据集，对我个人来说并不十分鼓舞人心，然而，从数学难题的角度来看，它非常鼓舞人心。这可能是在 1995 年左右，人工神经网络的全盛时期，现在被改造成“深度学习”，但那时我们还没有计算能力。我从事的项目使用了教育数据；我们试图看看是否可以根据学生的成绩和考试分数来预测他们的成功。我最难忘的学习经历是:即使我们真的想打败线性模型，你也无法打败它。我们真的很努力地让神经网络看起来很好，我们只是无法击败一个相对简单的模型，它的性能至少与标准相当，甚至更好。在我的工业生涯中，我注意到，如果你能很好地使用它们，总是更简单的模型具有不可思议的力量。经过这次实验，我意识到 50 年代的好的旧统计数据可以让你走得更远。

如果你能很好地使用它们，总是简单的模型拥有不可思议的力量。

【Dstillery 解决什么具体问题？你会如何向不熟悉它的人描述它？

Dstillery 是围绕使用机器学习和预测建模的原则建立的，以帮助放大正确的人群，让他们看到正确的广告。当我加入 Dstillery 时，我没有任何营销经验，因为我上过商学院，所以知道一些基本知识。然而，数据给营销带来的根本转变是能够将许多“粗糙的概念”抛在身后。营销人员倾向于以某种方式思考和谈论他们的受众，例如:“中年足球妈妈。”事实是，尽管我可能在技术上合格，但我讨厌被归类为中年足球妈妈…！对我来说还有很多，我的兴趣是如此广泛，以至于营销人员长期以来因为缺乏更好的数据而使用的这些非常“粗糙”的受众描述是非常不够的。

现代看待人口统计的方式是弄清楚，你到底想和谁交往？你到底想给谁看广告？我们能让机器帮助我们反映人类生存的全部复杂性吗？我们可以找到点击广告的人，但是我们如何解决如何吸引他们并改变他们的行为呢？这就是 Dstillery 正在尝试做的事情，查看精细的信息:你查看的 URL，你正在使用的应用程序，可能是我昨天看到你的物理位置，并以匿名方式解析所有这些数据， 将它输入一个算法，然后说:“根据我在这个非常精细的层面上对你的了解和观察，你对这个特定营销商的这个特定报价感兴趣的概率是多少——以及利用数据和机器学习的能力来说，“根据对产品的兴趣排名，我只会向 1%的人显示广告”。

你想解决的最大的机会/问题是什么？

发展数据科学技术的另一个重要领域是测量因果关系。我们需要超越相关性与因果性的争论，并且说我们实际上可以了解这一点，如果某种互动真的有某种影响以及为什么。如果这些技术被证明是可行的，我们可以把它们带到医学领域，并在几乎个人的基础上应用于测试环境。如果你想到个性化医疗，这是非常相似的。过去，一种诊断只有一种药物，如果你看看人们今天在癌症研究中所做的事情，就会发现为你个人开发的特定药物——对你最有效的药物。我在许多不同的行业中都看到了这一点。我们正从宽泛的消费者描述转向关注个人以及我们能为他们做的最好的事情。这反映在我们处理更多数据的能力上。Dstillery 在广告中是这样做的，但是我们正在创造的技术有更广泛的应用。

我们正在创造的技术有着更广泛的应用。

你最喜欢使用哪些工具/应用程序？

我们使用了很多非 SQL 解决方案，比如 Cassandra。在数据科学的另一端，我们有一个基于 Hive 的 Hadoop 堆栈，它为数据团队提供对数据的访问。我们记录每一条到达我们服务器的数据；数量级大约是一万亿字节。我们希望让我们的数据科学团队能够轻松访问这些传入的数据，以便他们能够在此基础上构建模型。我们的整个系统几乎都是自动化的。建模部分是添加了新技术的逻辑回归。我们通常从学术研究中提取，并在产品中实现我们自己的版本。在工业环境中使用它的贡献是真正实现鲁棒性，并找出我们如何自动化整个过程。我们一天要制作成千上万个这样的模型。在早期，当我还是 IBM 的数据科学家时，我有大约三周的时间来构建一个模型，而现在我们每周都在构建数千个模型。我正在设计一个基于机器学习构建解决方案的流程。我们的数据科学团队在制作原型时使用 Python、R 和 Perl，并使用 Hive 和 Cassandra 从 Hadoop 中提取数据。

哪些出版物、网站、博客、会议和/或书籍对你的工作有帮助？

我个人很喜欢面对面的互动，所以我真的很喜欢会议。我的主要会议在 kdd.org 举行，这是一个有 20 年历史的学术会议，有很强的应用性。他们有来自医药、制造、能源和广告的论文，我发现跟上学术界正在做的事情以及行业中的应用状态是非常有意义的。如果你在寻找更理论化的东西，你会去日本、 ICML 、 ECML 。诸如 O’Reilly’s Strata 和预测分析世界等行业会议对于行业内的网络和实施状态非常重要。有一份名为大数据杂志的新出版物，他们试图在算法和分析的学术界之间架起一座桥梁，同时也着眼于应用方面。机器学习期刊，机器学习研究， KDNuggets 和数据科学中心都有一些有趣的文章。

除了 Dstillery，你最近还参与了哪些有趣的项目？

我通过在 NYU 大学授课以及在许多行业会议上发言来分享我的知识。去年，我在纽约市组织了一次数据挖掘会议，有 2500 人参加，主题是数据科学对社会有益。我们把非营利组织聚集在一起，让他们与具备分析型技能的个人配对，帮助他们解决重要问题。如你所知，对拥有数据科学技能的人有很大的需求，而且供应越来越多。然而，最大的问题实际上是将这些技能与正确的工作相匹配。只要“数据科学家”这个头衔没有明确定义，招聘经理就不确定他们需要什么样的技能，公司也很难表达他们想要什么。有很多不同的模型。例如，Facebooks 和 Googles 希望工程师了解数据，而在 Dstillery，我们寻找的主要是懂数据并有一定编码能力的人。一个完全不同类型的人可以胜任这两种不同类型的角色。我热衷于将人们连接到合适的职位，在那里他们可以努力工作并发挥影响力。

对于刚开始学习机器的学生或从业者，有什么至理名言吗？

做一些你真正喜欢做的事情。

做某事没有意义，因为它在某个时间流行。尤其是有了数据，你可以专攻的东西多了去了；真正让你兴奋的是什么，这完全取决于你自己。对我来说，这是一个侦探游戏，找出我的数据中发生了什么，以及真正能够建立最佳模型的竞争力。你需要在你真正擅长的和你喜欢做的之间找到一个很好的平衡，这种兴奋正是我在面试中说服我雇佣你的原因。

克劳迪娅——非常感谢你抽出时间！我真的很喜欢了解你在 T2 的工作。

在推特上关注克劳迪娅。

Pinterest 的众包和机器学习

原文：https://www.dominodatalab.com/blog/data-science-interview-james-rubinstein-disco-scientist-at-pinterest

我们最近采访了 Pinterest 的数据科学家詹姆斯·鲁宾斯坦。詹姆斯，首先感谢你接受采访。让我们从你的背景和你如何对数据科学感兴趣开始。

你的 30 秒简历是什么？

我是 Pinterest 探索团队的数据科学家，但我更喜欢被称为“迪斯科科学家”。

在加入 Pinterest 之前，我是苹果地图和 iTunes 团队的统计学家。在此之前，我在易贝担任投标团队和搜索度量的产品经理，在那里我介绍了一个实验赌博市场、一个新的投标层、在线调查和众包搜索质量评估。

在我的空闲时间，我制作音频组件，破解干衣机，酿造啤酒，和家人一起出去玩。

你记得使用的第一个数据集是什么？你用它做了什么？

他们说你永远不会忘记你的第一次，这是完全错误的。然而，我确实记得自己是克莱姆森大学的本科生，在我工作的商店里向学生和顾客分发调查问卷。然后我会手动输入数据，并使用 t 检验或其他一些基本统计数据通过 SPSS 运行数据，以寻找人群中的差异。

你和一些最具创新性的公司合作过。有什么想分享的亮点吗？在你的职业生涯中，到目前为止你最自豪的是什么？

我为我(和我的团队，因为数据科学是一项团队运动)在易贝完成的工作感到非常自豪。我们通过创建调查平台，为易贝团队创造了倾听“客户声音”的新渠道。

我们还通过提供独立于实时网站指标(即相关性)的数据，推动了搜索团队内部的许多决策。这些见解现在仍在推动公司的决策，尽管是以不同的方式，由不同的团队来做。

我也很自豪能成为 Pinterest 团队的一员。我和了不起的人一起工作，把了不起的产品带给世界。我们的团队非常努力地工作，以确保 discovery 团队发布的每一个 algo 都是“Pinterest 质量”，对此我也感到自豪。

你认为众包和数据科学以及 ML 将走向何方？什么是近期然后是远期？

我们使用人工评估做两件事:驾驶和得分。转向是提供输入来告诉你哪里情况不好/好，提供 ML 的输入。评分是衡量事情进展的好坏。在短期内，我认为我们会看到更多的从得分到使用这些人为标记的数据进行指导的转变。

在未来，我们可能会看到人和机器之间更紧密的循环，机器学习、训练数据收集和测量都在近乎实时地进行。在遥远的未来，我们将在训练机器方面做得如此出色，以至于我们都可以在没有任何人工监督的情况下在海滩上放松。

你能告诉我们你在 Pinterest 做的工作吗？

我主要致力于测量各种发现算法的性能。探索团队负责帮助你在 Pinterest 上找到你喜欢的东西，无论是通过搜索、Homefeed 还是推荐。我们通过人工评估和现场实验来衡量这种体验。我还致力于测量引脚质量，试图训练 ML 算法区分什么是好引脚，什么是不太好的引脚。

你想解决的其他机会/问题领域有哪些？

一个很大的挑战是我们如何知道一个 Pin 什么时候是好的。什么使它变得好？为什么人们使用一些图钉而不使用其他图钉？当我们第一次看到一个图钉创作时，我们如何知道这个图钉是否会引起图钉爱好者的兴趣？

我最感兴趣的另一个主题是构建评估管道，以便公司中的任何人都可以轻松获得我们在网站上使用的所有不同算法的度量。

你在实验设计和统计方面有着有趣的背景，你能谈谈你的背景如何给你作为数据科学家的职业生涯带来一些优势吗？

我认为我所做的很多事情都是设计调查。在某种程度上，这和我在大学时在布莱凯特礼堂给学生发调查问卷没什么不同。

你需要适当的调查设计，不会让回答者产生偏见，你需要确保结果是因为数据，而不是因为用户的偏见。

建立适当的实验控制也是至关重要的，这并不总是被认为是实验的关键组成部分。

最后，还有统计数据——理解如何以及何时应用它们是很重要的。不过我承认，有很多人比我更擅长更广泛的统计数据，我很乐意寻求帮助或第二种意见。这是在 Pinterest 工作的好处之一:尽管我们的团队很小，但我们有一些在各自领域非常优秀的人，他们愿意帮我找到正确的答案。

你最喜欢使用的工具/应用程序是什么？

嗯，在众包方面，我们通常使用 Crowdflower 或 Mechanical Turk。不过，我们确实在不同的市场使用不同的供应商。你必须将正确的人群与正确的任务相匹配。

说到数据分析，我和 R 合作过很多事情。在过去的几年里，我学习了更多关于 Python 的知识，这很棒。

对数据科学/机器学习的学生或从业者有什么忠告吗？

是的，学习科学。去和真实的人做些真实的实验。找出混淆因素，询问噪音的来源，尝试从尽可能少的参与者那里获得意义。数据科学不会取代科学，而是补充科学。

在 Twitter 和 Pinterest 上关注詹姆斯。在 Pinterest 工程博客上关注 Pinterest 的数据科学。

数据科学采访:Reonomy 首席数据科学家 Jason Dolatshahi

原文：https://www.dominodatalab.com/blog/data-science-interview-jason-dolatshahi-head-data-scientist-reonomy

我最近采访了 Reonomy 的首席数据科学家 Jason Dolatshahi。

Jason，请告诉我们一些你的背景，以及你是如何对数据科学感兴趣的。

我一直喜欢学习和解决问题，所以我学了数学和物理。我以前从事金融营销，数字广告，现在在 reonomy 工作。我最初来自圣地亚哥，现在我住在布鲁克林。

你是如何对数据科学和机器学习产生兴趣的？

在我人生的某个阶段，我有一份有点无聊的工作。所以我下定决心学习 Python，它帮助我建立了金融模型。我就是这样发现了数据科学和机器学习。这个话题似乎涉及了很多我不太了解的很酷的东西，所以我被吸引住了。

你还记得你是什么时候意识到数据的力量的吗？

第一次了解是在大学的时候；我第一次亲眼看到是在一个交易台上。在此期间，我的工作是自营交易。我从金融市场发现并驱动交易信号，收集数据，并根据这些见解执行策略。

你在经济学院做什么工作？

我们是商业地产市场的初创公司。我们业务的很大一部分是通过智能地使用数据来创造价值。我们的产品是为投资者、贷款人、经纪人和其他市场参与者提供的研究平台。像许多其他小公司一样，数据科学工作包括大量的后端工程。

你发现的最令人惊讶的见解或发展是什么？

到目前为止，我发现的最基本的物理模型是谐振子，例如钟摆的运动。此外，我喜欢套用一位最著名的物理学家的话，他说*“一个年轻的理论物理学家的职业生涯包括以不同的抽象程度对待谐振子”*。但类似地，数据科学家的职业生涯包括了解这样一个事实，即在不同的抽象程度上，“你 80%的问题解决发生在建模之前”。

数据科学家的职业生涯包括了解这样一个事实，即在不同的抽象程度上“80%的问题解决发生在建模之前”

你认为商业地产和数据科学以及 ML 将何去何从？

我认为商业地产将变得更加数据驱动。数据科学的应用领域也将越来越广。例如，我认为这将增加私人和公共部门对具有定量问题解决技能的人的需求。医学是头号关键领域，我希望在这个领域看到一些重要的突破。测序一个基因组的成本在 1000 美元左右，这比第一个人类基因组计划后的 20 年要少得多。

在雇佣数据科学家时，你们有哪些方法(如果有的话)？一个成功的候选人最重要的是什么？

我有兴趣看到候选人运用定量推理。我也喜欢了解他们对自己不知道的事情的态度。但对我来说最重要的技能是统计直觉。为此，我会给你一个实际的任务带回家，稍后展示结果。这将向我展示候选人是否具备所需的数据和沟通技巧。我现在正在应聘经济学的职位，如果你认为你具备所需要的条件，我们就来聊聊吧。

在大会上教授数据科学时，您是如何应用自己的知识的？对实习学生有什么建议吗？

未来的数据科学家应该专注于基本技能和统计推理。他们应该试着用简单的术语理解技术，例如为什么他们以的方式工作。不要被数学符号、编程语言、技术吓倒；这些都只是一种工具，或者达到目的的手段。数据科学是一门实践学科，它是围绕权衡取舍而构建的。你需要在自己的工作中认识到这些权衡，并做出明智的选择。这包括但不限于构建预测模型的环境。最后但同样重要的是:“深入思考简单的事情”——另一句名言。

试着用简单的术语理解技术，例如为什么它们以这样的方式工作。不要被数学符号、编程语言、技术吓倒；这些都只是一种工具，或者达到目的的手段

你有如此丰富的数据科学背景，你能分享一些你做过的最有趣的工作吗？还是一两个有纪念意义/有意义的项目？

我在一家移动广告初创公司建立了一个后端数据处理架构，基于 Python、redis 和亚马逊 EMR。即使在我们被收购后，它仍然存在于生产中。然后，我在一个大型广告合作项目中创建了可视化效果，获得了很大的吸引力，并最终被各种商业用途所采用。最近，我开发了一个功能，让 Reonomy 的用户在使用 ElasticSearch 时，可以获得某个人或公司所有权的整体情况。这是一个大项目，因为你必须建立简化的工作流程，许多市场参与者必须与一群人一起手动进行，这将洞察力的时间减少了几个数量级。

未来的数据科学家应该专注于基本技能和统计推理

到目前为止，在你的职业生涯中有什么让你感到最自豪的事情吗？

关于数据科学，我最喜欢的是学习新事物和创新的机会。我真的很喜欢我所做的工作，它带来了惊人的结果，我也很喜欢教学和帮助人们看透复杂性。

今年你在做什么个人/专业项目，为什么/如何让你感兴趣？

我目前专注于函数式编程或数学家编程。我最喜欢的科目之一是机器学习在生物和医学问题上的应用。

你发现了哪些机器学习方法，或者你认为这些方法最有帮助？你最喜欢使用的工具/应用程序是什么？

一般来说，越简单越好。我日常使用的工具是 Python、pandas、tmux、vim 和 unix。我也用 scikit-learn。

Jason——非常感谢你抽出时间！真的很高兴能更多地了解你在经济学院所取得的成就。

了解更多关于杰森·多拉特沙希或雷诺米的信息。

数据科学访谈:Thoughtworks 的资深数据科学家 Sean McClure

原文：https://www.dominodatalab.com/blog/data-science-interview-sean-mcclure-sr-data-scientist-at-thoughtworks

我们最近采访了 Thoughtworks 的高级数据科学家 Sean McClure 博士。肖恩，首先感谢你的采访。让我们从你的背景和你如何对数据科学感兴趣开始。

你的 30 秒简历是什么？

我的学术背景是科学计算，我曾用高性能计算机和尖端算法来计算基于纳米技术的设备的分子属性。我爱上了使用计算机和数据来解决非常具有挑战性的问题。获得博士学位后，我开始了自己的高级分析业务，利用机器学习和数据库技术帮助企业进行分析竞争。在做了几年之后，我加入了 T2 的 ThoughtWorks，成为了我现在工作的数据科学家。我喜欢写作和公开演讲，也喜欢弹钢琴。

你是如何对数据科学和机器学习产生兴趣的？

在我读博士期间，我接触了各种试图在计算机上解决挑战性问题的方法。我很快认识到机器学习技术在解决我的领域中的问题的力量。我看到，我作为一名科学研究人员的经验可以与机器学习中使用的方法相结合，以解决远远超出学术界的问题。当我毕业时，数据科学刚刚开始升温，我觉得这是我的技能和热情的自然延伸。

当你意识到数据的力量时，有没有一个特定的“啊哈”时刻？

我早期的一个项目是在医疗保健领域，涉及构建一个试图将各种症状与疾病结果关联起来的应用程序。尽管该产品远非完美，但它让我看到了数据的力量，以及它在世界上的变革性。这是我意识到我们可以用数据做很多重要工作的时刻之一，它可以对我们的人性产生积极的转变影响。

数据的力量可以改变世界

你在 ThoughtWorks 做什么工作？

在 TW，我构建自适应应用程序，通过从环境中学习来帮助自动化决策。我与软件开发人员合作，将我构建的模型集成到现实世界应用程序的工作代码库中。在 ThoughtWorks，我们认为这是下一代软件，其中敏捷性的理念超越了设计阶段，贯穿于软件的整个生命周期。

在你的职业生涯中，到目前为止你最自豪的是什么？

这是我第一次从企业领导那里得到证实，我的工作为组织增加了价值。从领域专家那里得到积极的反馈，说你的工作产生了真正的影响，这是一种很棒的感觉。

你发现的最令人惊讶的见解或发展是什么？

我在一家搜索引擎营销(SEM)公司工作，我们试图在大量的市场数据中寻找模式。我们发现，虽然许多营销活动被员工处理得很好，但也有其他一些活动与普遍持有的信念相去甚远。虽然这是我们在那里的核心原因，但在组织的数据中发现新的机会仍然是一个令人愉快的惊喜。

今年你做了哪些个人/专业项目，为什么/如何让你感兴趣？

今年，围绕将自动化构建到现有的业务流程中，充满了许多激动人心的事情。我最近完成了一个应用程序，它吸收了成千上万手动输入的文本描述，并呈现出正在讨论的核心主题。这被用来发现大型系统中出现的主要故障；多年来在文档中手工描述的故障。这是一个很好的例子，教一台机器做一些需要很多人和数千小时才能完成的事情。它使用了自然语言处理中的概念，我认为这将在未来的智能数据产品中扮演非常重要的角色。

数据科学的未来是什么样的？

我们仍处于定义这一领域的早期阶段，以及它对寻求分析竞争的组织意味着什么。我认为，在未来，我们将会看到大量的宣传逐渐消失，并成为一个坚实的学科，将数据转化为有价值的产品。将数据转化为价值的需求只会增加，而实现这一点的方法就是进行高质量的科学研究；导致模型的研究，这些模型捕捉驱动我们感兴趣的领域的潜在模式。我个人正在努力在我的写作和公开演讲中帮助定义这个领域，并确保炒作不会干扰对基于大量数据的伟大科学的需求。

我认为，在未来，我们将会看到大量的炒作逐渐平息，并成为一个坚实的学科，将数据转化为有价值的产品

哪些出版物、网站、博客、会议和/或书籍对你的工作有帮助？

亚塞尔·s·阿布·穆斯塔法的《从数据中学习》是一本值得不时阅读的好书。它很好地解释了制造机器的可行性，这些机器可以学习和描述我们作为数据科学家每天应用的许多核心概念。我也经常使用 DeepDyve ，它允许人们租借学术文章。这样我们就可以了解最新的研究，而不必支付大量的期刊订阅费。

你发现了哪些机器学习方法，或者你认为这些方法最有帮助？你最喜欢使用的工具/应用程序是什么？

机器学习构成了我们作为数据科学家使用的各种算法的核心。我不认为你可以将任何一种方法置于另一种之上，直到你试图解决的特定挑战暴露了它的一些秘密。这些就像在发现过程中突然出现的标志，暗示可能证明可行的方法和途径。但最终，采用多种方法，让大自然告诉你什么可行，什么不可行，这才是关键。对于工具和应用程序，我首先使用 R 和 Python，因为它们拥有最丰富的科学计算库。这种多样性对于允许科学家从各种不同角度探索数据并揭示如何对感兴趣的系统建模的洞察力至关重要。我在使用 R 和 Python 的 IPython 时也使用 R studio。当缩放模型有意义时，我会寻找像 Spark 这样的工具，这些工具在使机器学习可扩展和快速方面取得了很大进展。H20 也开始看起来有希望成为扩展我们科学的工具。除此之外，我还负责 NoSQL 数据库和 Hadoop 生态系统。

对数据科学/机器学习的学生或从业者有什么忠告吗？
关注核心概念，因为它们是永恒的
投入并不断练习。失败是学习的唯一途径。我最近就此写了一篇文章，名为你应该关注的唯一技能。

失败是学习的唯一途径

肖恩-非常感谢你的时间！我非常喜欢深入了解你在 Thoughtworks 取得的成就。

肖恩·麦克卢尔可以在这里在线找到，并访问 Thoughtworks 。

数据科学:唐纳德·特朗普的另一个不同之处

原文：https://www.dominodatalab.com/blog/data-science-just-one-way-donald-trump-different

如果你是一名对政治工作感兴趣的数据科学家，不要费心申请特朗普竞选团队——但希拉里有一份工作给你。

在最近的一次美联社采访中，唐纳德·特朗普对数据作为竞选战略工具的价值大打折扣。他的计划是基于他的人格力量去赢得胜利。这位假定的共和党提名人指出了奥巴马的个性，他说:“奥巴马比他的数据处理机器获得更多的选票，我想我也是如此。”

就像特朗普的许多事情一样，这是一个非传统的观点。大多数政治专业人士认为奥巴马 2008 年的竞选在使用数据方面具有开创性，尤其是在获得选票方面。奥巴马在初选中对右翼战场的关注为他赢得了希拉里·克林顿都没有意识到的代表票。

对于任何重大的政治竞选活动，数据科学都是运作的大脑。数据科学家可以回答的问题涵盖了竞选的每一个方面:在哪里筹款，为哪个州而战，向观众传达什么信息，谁是摇摆选民。在这些问题上，数据每次都胜过意见。

抛开战略问题不谈，政治活动的很大一部分就是直接营销。候选人在广告上花费数千万美元。政治广告与汽车、尿布和啤酒的广告同时出现。数据科学是公司将广告瞄准正确受众的方式。这就是地毯式轰炸和智能炸弹的区别:用少得多的钱产生多得多的影响。

对于特朗普来说，他筹集到的资金还不到希拉里·克林顿的一半，努力扩大他的广告收入是有意义的。相反，到目前为止，他从新闻自由中获益匪浅。他的竞选花费比⅓少，只有希拉里竞选花费的 20%。

但是随着竞选转向大选，新闻自由的优势可能会缩小。此外，他控制媒体的能力很大程度上是因为他愿意发表非常规言论。如果他需要传递有针对性的信息，无法运行数据驱动的广告活动将成为一个巨大的负担。

另一方面，克林顿竞选团队正在寻找更多的数据科学家来帮助他们找出如何最好地分配资源以获得最佳结果。他们的分析师从事筹款、广告、地理分析等工作。目前分析团队有十几个空缺职位，从初学者到有经验的研究员都有。一些有趣的工作包括:

战场州分析师
国家数据和分析总监
(或者你经验少，可以做个副主任
数据科学领导
数据科学家

想要帮助唐纳德·特朗普(Donald Trump)“让美国再次伟大”的数据科学家可能会在他们和他们的候选人之间发现一堵墙。另一方面，克林顿竞选团队准备利用数据做出决策。

**更新:**今天早上在一篇关于 fivethirtyeight.com的文章中，Joshua Darr 探讨了川普限制数据在其竞选中的作用的决定将对共和党产生的影响。不是在影响选举结果方面，而是总统选举提供了一个难得的机会，既可以比其他选举收集更多的数据，又可以培养下一代能够利用这些数据的人才。

数据科学发起了一场全球对抗新冠肺炎的战斗

原文：https://www.dominodatalab.com/blog/data-science-launched-a-global-fight-against-covid-19

By David Bloch, Data Science Evangelist, Domino on April 21, 2020 in Perspective

随着世界以疯狂的速度限制新冠肺炎的影响和传播，数据科学行业呼吁采取行动，帮助加快新预防措施和技术的研究，并就指数增长率和呆在家里限制传播的重要性等主题教育人们。

在前所未有的规模上，卫生、学术和政府组织开始公开数据集，以供分析、探索和研究。这些相同的数据集正被新闻媒体用来讲述新型冠状病毒的故事，并教育公众有关社交距离等措施，这些措施可以降低感染率，并帮助医院应对病毒。

在 Domino，我们响应了这一号召，为希望为新冠肺炎研究做出贡献的数据科学家提供了一个免费的环境。该环境已经预先配置了对数据的访问，许多代码片段使其易于开始。注册后，数据科学家可以请求访问更多计算。

我们希望提供这样的环境能让数据科学家更容易起步，并为新冠肺炎研究做出贡献。我们还在下面提供了一些资源，我们发现这些资源发人深省，对我们自己的探索和实验很有帮助。

如何在 Domino 中访问新冠肺炎项目
参观 try.dominodatalab.com 并报名。
完成入职培训后，查看并注册访问新冠肺炎传染病数据项目。
请遵循项目中提供的 readme.md 文件，以获得有关所包含内容的更多信息。
有关 Domino 环境的更多信息，请查看我们在 docs.dominodatalab.com的入门指南。

Source:Domino Try

数据科学家如何帮助对抗传播

科学研究进程中的一个关键原则是“站在我面前的巨人的肩膀上”的能力。对于研究人员来说，获得以前的假设、实验和其他科学家的知识来帮助建立新的见解是很重要的。

云计算的采用以及笔记本工作解决方案的流行，使得研究人员可以轻松分享他们的工作笔记、算法和数据等底层资源。许多组织寻求利用其内部数据科学能力来帮助加快对该病毒的研究。例如:

阿里巴巴集团的科学研究院开发了一种 T2 新冠肺炎诊断模型 T3，利用计算机断层扫描(CT)和医学成像分类来诊断新冠肺炎病毒。该系统将由经验丰富的医生进行的典型的 10-15 分钟诊断缩短到 20 秒钟，这在医疗资源不堪重负的情况下是至关重要的时间节省。
ka ggle-一个举办一系列比赛的平台，在这些比赛中，企业提供挑战和数据，以找到财务奖励的解决方案-已经创建了资源中心和一系列起点文章，以帮助数据科学界在追求提供新智能的过程中轻松起步。
Zindi-一家总部位于南非的初创公司一直在连接非洲的数据科学家社区，宣布向其 12，000 名数据科学家提供现金奖励，以参与寻找可用于帮助抗击新冠肺炎的新见解和信息。
微软与分享了一系列学术论文，这些论文配有一些工具，可以更容易地使用自然语言处理技术搜索文献，帮助研究人员快速找到相关信息。他们还结合了以前对与新冠肺炎病毒作用方式相似的病毒的研究实例，如 SARS 病毒。
再保险供应商 SCOR 专门成立了一个工作组来分析新冠肺炎疫情危机。该团队由来自其知识社区的专家组成，包括医生、流行病学家、精算师和数据科学家。基于约翰·霍普金斯大学提供的数据存储库，他们在 Domino 中构建并发布了一个公共 web 应用程序，以帮助在基本信息的实际显示中可视化新冠肺炎趋势。最初成立的目的是建立生物特征风险模型，多专业工作组意识到共享数据有助于更好地向公众提供信息。
Bayer 数据科学家 Jeffrey Pobst 在 Domino 中开发了一个应用程序，来分析维基百科中与新冠肺炎相关的跨语言词汇的趋势——查看标题为“冠状病毒”、“疫情”、“新冠肺炎”的页面流量，以及有趣的“卫生纸”、“洗手液”、“抗病毒药物”、“远程办公”等相关页面。该应用程序强调了社会一直在研究什么，好奇心如何在不同的地理区域内上升和下降，并可能围绕这个疫情随着时间的推移在区域内看到的文化影响呈现有趣的相关性。
感受到数据科学和人工智能应用的潜力，白宫敦促技术机构为研究做出贡献。

科学界迫切需要发现有助于抗击病毒传播的新见解来源，这导致了前所未有的跨国和跨国信息共享。大型跨职能虚拟团队聚集在一起，更好地通知人们，提供更好的洞察力和清晰度，最终帮助政府共同努力解决问题。

数据可视化如何更好地传达故事

数据科学家应用数据可视化技术来探索信息，并在形成和测试假设之前获得大量数据集。

随着政府寻求实施各种政策，如社会距离和要求人们自我隔离，数据越来越多、也是最重要的用途之一是向人们提供政策原因以及为什么这些政策会有效的建议。

下面是一系列文章和博客帖子，它们使用数据可视化来创建新的见解，并清晰地传达给读者。它们也可以作为研究者可能寻求的不同方法和假设的思想启动器。

信息是美丽的发表了一系列信息图表，这些图表从大量数据中提炼出受病毒影响者的人口统计信息。

纽约时报为提供了一系列模拟和滑块；让人们能够根据疾病的传染性和死亡率轻松评估影响；按照国家和州提供信息，利用人口统计数据以及我们目前对病毒的了解。

约翰霍普金斯大学的团队已经在 T2 建立了一个惊人的资源中心，里面充满了数据集和信息，其中包括 T4 的一个综合仪表板，记录了与病毒及其在人群中传播有关的关键数据。

《华盛顿邮报》提供了一系列的 T2 动画，展示了疾病传播的不同场景。他们介绍了社会距离背后的目的，并展示了它如何帮助减缓病毒的传播，给卫生部门一个更好的机会来管理其影响。

Source:Information is Beautiful

其他资源

Fast AI 为提供了一系列关于数据科学在解决新冠肺炎问题中的作用的见解和想法，有助于定义和描述手头的问题，并展示了数据科学家如何提供帮助。

走向数据科学发表了一篇博客，帮助人们建立了一个与冠状病毒传播相关的仪表板，以及他们的对 COVID 增长模式的探索性分析，并访问了他们的底层代码库。

约翰·霍普金斯大学通过 GitHub 公开了其数据集，全天发布多次数据更新，供数据科学家和分析师访问。

哈佛大学在其中国数据实验室网站上发布了 T2 的文献、学术案例研究、数据集和代码库。

Source:Towards Data Science

Twitter Facebook Gmail Share

来自英国最大零售银行的数据科学领导者将在 Rev 2 中讨论负责任的人工智能

原文：https://www.dominodatalab.com/blog/data-science-leaders-from-uk-s-largest-retail-bank-will-discuss-responsible-ai-at-rev-2

By Karina Babcock, Director Corporate Marketing, Domino on March 27, 2019 in Perspective

随着组织越来越多地将大量模型部署到生产中，以及这些模型影响重要的商业决策，“可解释的人工智能”已经成为一个热门的讨论话题。至关重要的是，要了解每种模型的基础、演变和使用，以确保它们正在做它们在野外应该做的事情，并且如果它们偏离了轨道，能够有效地识别和排除故障。

Domino 首席数据科学家 Josh Poduska 解释说，“在更广泛的模型管理框架的背景下，我们将允许组织掌握每个模型的活动、成本和影响的支柱称为‘模型治理’。”""

在处理金融服务等受监管行业时，这一概念尤为重要。英国最大的零售银行劳埃德银行集团(Lloyds Banking Group)将于今年 5 月派遣其数据科学负责人汤姆·克罗宁(Tom Cronin)和数据科学家托拉·孙心怡(Tola Schmidt)前往纽约，在数据科学领导者峰会上深入探讨这一主题。

汤姆领导着一个由 50 多名“探路者和开拓者”组成的团队，他们正在向劳埃德的同事和客户提供人工智能产品。作为 Tom 团队的一员，Tola 在最近于伦敦举行的数据科学会议上发表了评价最高的演讲；她的个人使命是通过统计技术在现实生活中的应用来激励年轻一代，从而“将数学带入生活”。

以下是他们 Rev 演讲的简短概要，题为“负责任的人工智能”:

随着统计模型在决策中的使用在生活的各个领域变得越来越普遍，从我们可以获得的抵押贷款到我们可以在 Tinder 上向左滑动的人，我们解释这些模型的能力从未如此重要。在本次会议中，我们将向您介绍可解释人工智能(XAI)的概念，它使我们能够解释机器学习模型做出的决定。

Twitter Facebook Gmail Share

数据科学领袖播客探讨了 MLOps、数据道德、团队结构等

原文：https://www.dominodatalab.com/blog/data-science-leaders-podcast-probes-mlops-data-ethics-team-structures-and-more-key-trends-powering-today-s-model-driven-businesses

今天，我们推出了“数据科学领导者”，这是一个新的播客系列，致力于通过记录和分享真实故事、突破性战略和关键见解，帮助团队扩展蓬勃发展的数据科学实践，构建企业数据科学成功模型，这些都来自那些已经做到的人。

我们最近采访了“数据科学领导者”的主持人戴夫·科尔(Dave Cole)，听听他在采访领导者时学到了什么，并希望听众能够了解，这些领导者的丰富经验推动了数据科学团队所能实现的极限。

你为什么要启动数据科学领袖播客？

科尔:我曾经是首席分析官，当时，我作为一名从业者，在分析和理解数据科学方面有着丰富的经验。但我不太明白的是，成为数据科学领导者意味着什么。这意味着理解诸如你的数据科学家应该在你的组织中的什么位置；如何衡量数据科学团队；团队应该如何与业务用户互动。

我觉得成为数据科学领域的领导者有点与众不同，关于这一领域伟大领导者的报道很少。这个角色有一些非常独特的方面，我只是想解释一下。我们的目标是帮助所有的数据科学领导者以及有抱负的数据科学领导者相互学习。

这一点尤其重要，因为有很多不同的方法可以成功胜任这一角色，而且很多取决于各种变量，比如你所处的行业、你所在的组织类型、你的团队规模等等。这也是我喜欢跨行业跟各种领导对话的格式的原因。这是一种有助于理解不同的方法的格式，这些方法适用于不同的高管构建高绩效团队。

你的第一集是《纽约时报》首席数据科学家克里斯·维金斯。关于那次谈话，你印象最深刻的是什么？

科尔:对于像《纽约时报》首席数据科学家这样的人来说，数据伦理是一个重要的问题，我认为这并不奇怪。但真正有趣的是，意识到——坦率地说，定义——作为一名有道德的数据科学家意味着什么，被视为时报数据科学团队的一项重要职责和一个方面。听到 Chris 谈论他如何处理数据伦理，他们如何写下道路规则，甚至创建角色和委员会来支持数据伦理工作，我很感兴趣。

回答数据伦理问题很有意思。我希望在数据科学家中看到更多。这是一个非常重要的话题，尤其是考虑到像欧盟一般数据保护条例或加州消费者隐私法案这样的法规。我在本期播客中采访了另一位嘉宾，我们就数据的未来进行了一些数据伦理的讨论。虽然《纽约时报》可能对数据伦理有独特的强调，但这应该是每个数据科学家都必须真正思考的问题。

关于克里斯的领导风格，有趣的是他对自己团队的期望。你会听到他在播客中谈到他期望他的数据科学家可能会不时向他们的首席执行官展示。所以，他们必须是伟大的沟通者；他们必须能够回答问题，并将他们所做的工作转化为 CEO 能够接受的清晰而简明的建议。这是我在整个对话中看到的一个主题:理解代码和统计数据还不够好；你也必须能够翻译它。

通过采访这些嘉宾，您对数据科学的当前或未来状态有什么了解？

Cole:首先，我了解到数据科学领导者面临一些共同的挑战。例如，围绕数据科学家是否应该集中或嵌入到各个业务部门，或者是否应该有某种混合模式，有这样的对话。关于这场辩论的利弊，我学到了很多，我们也谈了很多。

此外，数据科学领域的领导者强烈希望宣传数据科学的成功，并衡量不仅仅是 ROI。人们一致认为，成功不仅仅在于我们将这种模式投入生产后节省了多少成本。数据科学的领导者需要考虑一些措施，例如数据科学团队应该进行的适量研究，或者如何为可以获得资金的项目寻找好主意。有趣的是，这些想法不一定总是来自你的传统业务领域。他们可以来自您的数据科学团队本身。对我来说，了解不同的公司是如何做到这一点的真的很有趣。

最后，我惊讶地了解到各行各业的数据科学领导者是如何专注于提供业务影响的。我了解到没有一个行业能引领他们所有人。总的来说，各行业似乎都有一种对数据幻灭的共同情绪，这促使我采访的领导者确保他们将模型放在公司业务的最前沿。我希望这个播客能帮助更多的人做好这件事。

你能告诉我们一些你很想让观众听到的即将播出的剧集中的瞬间吗？

科尔:我和赛默飞世尔公司 DNA 测序部门的生物信息学主管菲奥娜·海兰谈过，在那次谈话之前，我从来不知道什么是生物统计学家，什么是生物信息学。我们包括了一本“生物学 101”入门书，以更好地理解一些基本的人类 DNA 概念如何转化为数据科学，这真是一场令人着迷的对话。我们甚至继续谈论冠状病毒以及数据科学本身是如何被用来制造疫苗的；我们略微谈到了测试，这是一个热门话题，非常有趣。

此外，我和作家约翰·汤姆逊的谈话非常激动人心。他是一名作家，已经写了两本书，他正在写第三本，围绕着数据的未来。我们讨论了他关于如何使用数据、数据隐私以及数据伦理的哲学。

最后，我和 Paylocity 的首席数据科学官 Adam McElhinney 就 people analytics 进行了一次非常有趣的谈话。每个公司都有一个人力资源部门，从他那里听到一些数据科学如何被用来试图限制面试过程中的偏见、公平薪酬和类似性质的事情的使用案例真的很有趣。

你希望在以后的剧集中回答哪些问题？

Cole:每次与新的数据科学领导者交谈时，我都会发现有不同的方法适合他们。虽然我认为有一些方法绝对更好，但我发现每种方法都有利弊。

我得出的结论是，数据科学领导者的风格，以及他们的数据科学团队所处的阶段，可以真正决定他们如何最好地管理和构建他们的团队。而且，那将会进化。因此，在某个时候，退一步谈谈数据科学团队随着规模和责任的增长而发生的演变将是一件好事。

数据科学领导者:你们人太多了

原文：https://www.dominodatalab.com/blog/data-science-leaders-there-are-too-many-of-you

感谢 Cassie Kozyrkov 贡献了这篇博客，它最初发表在 Medium 的迈向数据科学。

数据科学有一个问题。实际上是几个问题，但是从头开始，让我们从一个问题开始:领导力。

今天，我在数据科学(该学科涵盖了、人工智能、统计、数据汇总和可视化)领导者的峰会上发表了演讲。当我看着领导当今数据科学团队的最聪明的开拓者的脸的海洋时，我发现自己在想，“你们太多了。”

FiveThirtyEight 的 Nate Silver 向同样的数据科学领导者听众发表演讲

想想这是怎么一回事！在峰会上问任何人，他们可能会告诉你，如果你认为人才短缺很糟糕，那么数据科学领域的领导人才短缺就更糟糕了。

如今，大多数数据科学领导者都是我喜欢称之为“超越数据科学家”的人那些追求科学、工程或统计方面的正规训练的人，然后，奇迹般地，有一天醒来发现他们更感兴趣的是让数据变得有用，而不是为了追求数学的复杂性。

数据科学的领导者克服了所有的困难。

数据科学领导者:你们有太多的人，因为你们克服了所有的困难而存在。由于几乎没有做任何事情来训练你，你比我们应得的要多。这个幸福的意外是怎么发生的？没有人教你如何做你所做的，所以你的存在是我们的幸运。你的计划是希望在研究了十几年的方程式后，你就能知道如何领导了吗？如何做出好的决策？正如我的 SRE 同事所说，“希望不是策略。”

你应该有更多的人，但是计划是什么？

如果你喜欢定理，这里有一个:你的时间是有限的，所以如果你用它来学习费曼或德费内蒂，你就不会花同样多的时间来培养其他技能。我们不能指望数据科学家超越并立即知道如何成为优秀的领导者和决策者。谁会教他们这些？你不会通过整天写代码或证明定理来学习它。

相反，要成为优秀的领导者和决策者，他们必须谦逊地认识到自己萎缩肌肉中的弱点，并勤奋地掌握第二项技能。有时他们会带着很多伤痕艰难地学习。作为一个一开始就认为概率论是宇宙中最热门的东西的人，我非常清楚这有多痛苦。

有个态度问题。我们真的重视这些技能吗？

如果你的经历和我一样，你可能已经在一个亲数学的亚文化中长大，在这个亚文化中，对任何闻起来像“软”技能的东西表现出蔑视是一种时尚。这都是关于你是多么热衷于熬夜证明一些定理或用你的第六语言编码。当你被夹在那种视角中间时，你可能不会想到你应该重视领导力(或者沟通、商业意识、创造力和同理心等)……如果你变得软弱，你的同学会尊重你吗？

部分解决方案是改变时尚，让这些技能成为像原始数据科学这样有吸引力的领域中不可或缺的一部分。如果昂首阔步必须是年轻人的一部分，那么至少让他们相信，最高的荣誉是同时拥有两种肌肉。毕竟是事实。

说到确保数据科学团队得到有效领导，我们是依靠运气还是训练？

门槛很高，并不是每个有工作的人都能真正做到。

数据科学领域的领导力不仅仅是一学期的数字。是它自己的怪兽。你不仅必须对决策和信息如何推动行动有深刻的理解，还需要敏锐的嗅觉来发现如何有效地影响特定业务领域的细微差别，如果这还不够，你还需要了解各种技能的生态系统，这些技能需要汇集在一起才能使大规模数据科学项目取得成功。而这只是进入这个游戏的最低要求。

这是一个相当高的门槛，并不是每个领导数据科学团队的人都能达到。雇主们，你怎么知道你是否真的在雇佣一个人来领导你的数据团队呢？如果您的团队已经有了糟糕的数据科学领导，该怎么办？你怎么知道的？几乎没有任何关于这个角色的智慧可供你依靠。你能问谁呢？

数据科学是泡沫吗？

当今世界正在产生前所未有的数据。然而，有时我会被问到这样的问题，“数据科学是泡沫吗？“我希望我能坚定地回答，”肯定不会！“事实是，这要看情况。可悲的是，当我与各行各业的人聊天时，我总是听到同样的故事:“我们的数据科学家毫无用处！他们所做的就是坐着发表论文。我们会有足够多的有技能的领导者来防止这种司空见惯的现象吗？如果数据科学家不能证明自己的价值，他们不会在这些岗位上呆很久。

期望一个刚毕业的科学博士知道如何对商业做出有意义的贡献是不公平的。那不是他们花了那么多年学的东西。如果没有了解数据科学所涉及的内容并知道如何将数据与业务联系起来的人的指导，他们将面临重重困难。为了确保数据科学不是泡沫，我们迫切需要专业化的领导。它将从哪里来？

数据科学领导者的培训项目在哪里？希望不是策略。

人们，让我们感激我们是多么幸运！不知何故，优秀的数据科学领导者确实存在，技能也在那里。不是有效获得的技能，因为双重掌握是连续获得的，也许很痛苦，但仍然是正确的技能。我希望你和我一样强烈地感受到这种紧迫性。我们中少数几个经历了艰难困苦的人需要开始用更好的方式训练我们更多的人。

就我而言，我承诺尽我的一份力量。在过去的几年里，我一直在谷歌努力培养一种新的思考者，定位于作为团队的一部分，有效地领导或工作，专注于将数据科学应用于实际问题。为了培养正确的技能，我们从数据科学和工程中汲取了一些想法，并用行为科学和管理科学对其进行了补充。结果只是看起来跨学科，直到你看到共同的核心:决策和驱动它们的信息。这就是为什么我们开始称它为决策智能工程(不过如果你愿意，你也可以把它想成应用数据科学++的原因)。

让我们培养一种新的思考者:拥有让数据科学团队成功的技能的决策者。

我一直认为，数据科学是一项受益于技能多样性的团队运动，所以我设计了我们的培训计划，以鼓励各种背景的人参与进来。事实证明，伟大的数据科学领导者不一定是被超越的数据科学家。

我为我们在谷歌员工中培养这些"
DataCamp 博客中文翻译（十五）,2024-10-31,"原文：DataCamp Blog 
 协议：CC BY-NC-SA 4.0 
 
构建数据文化的途径 
 
 原文：https://web.archive.org/web/20221129054701/https://www.datacamp.com/blog/the-path-to-building-data-cultures 
 
 
https://web.archive.org/web/20220704233716if_/https://open.spotify.com/embed/episode/3oRqTkPmtlOHnuO0dX6LDk 
DataCamp 播客 DataFramed 的主持人 Adel Nehme 最近采访了安联-比荷卢经济联盟的区域首席数据和分析官 Sudaman Thoppan Mohanchandralal。 
介绍苏达曼·托普潘·莫汉钱德拉勒 
Adel Nehme:大家好，我是来自 DataCamp 的 Adel Nehme，欢迎来到 DataFramed，这是一个涵盖所有事物、数据及其对世界各地组织的影响的播客。根据新的 vantage partners、2021 大数据和人工智能高管调查，几乎 100%的组织都报告了对人工智能和数据计划的投资。然而，只有 24%的受访者声称他们的组织是数据驱动的，而 92%的受访者表示缺乏数据文化是实现数据驱动的最大障碍。这就是为什么我很高兴今天的节目能请到苏达曼·托普潘·莫汉钱德拉勒。Sudaman 是安联比荷卢经济联盟的首席区域数据分析官。在他任职期间，他在保险领域部署了许多领先的人工智能解决方案，并且一直是安联比荷卢经济联盟数据文化转型计划的先锋。他拥有计算机科学和商业管理两个硕士学位，专攻商业分析和智能。 
Adel Nehme:在这一集里，Sudaman 和我将讨论构建数据文化的重要性、组织在制定数据转换计划时可以采用的最佳实践、数据转换计划的多方面性质以及由此产生的复杂性、数据技能和数据文化之间的联系等等。如果你喜欢 Sudaman 的这一集，一定要看看他最近在 DataCamp 上的网络研讨会，他在会上更详细地介绍了他如何在安联比荷卢运营数据文化。如果你想查看播客的前几集并显示注释，请务必前往www.datacamp.com/community/podcast。 
阿德尔·内赫梅:苏达曼。很高兴你能上节目。 
Sudaman Thoppan Mohanchandralal:谢谢你，Adel。这种感觉是相互的。 
Adel Nehme:我很高兴能与您讨论构建数据文化的重要性。但是在我们开始之前，您介意给我们简单介绍一下您是如何进入数据领域的背景吗？ 
Sudaman Thoppan Mohanchandralal:我已经在数据领域工作了一段时间。目前负责比利时、荷兰和卢森堡地区安联业务的数据和分析，直接向地区首席执行官汇报。我如何进入数据空间是一个我无法回答的问题，因为我总是以这样或那样的方式在数据空间中。首先是复杂的事件处理，称为银行间资金转移处理器，然后是定价环境分析，为 IBM 提供租赁物流解决方案，然后继续从事数据仓库工作，最后专攻业务分析。你可以看到，在我获得商业分析和智能 MBA 学位后，上下文事件处理、数据仓库和分析本身的整个方面，从定价分析，所有这一切都集中在一个超级专业化中，这清楚地为我在数据和分析领域担任领导角色铺平了道路。因此，从 2012 年到 2013 年，数据和分析方面的大量工作将会开始，等等，一直到现在，我一直试图在数据办公室工作，或者试图建立一个数据办公室。 
CDO 今天的使命是什么？ 
Adel Nehme:太好了。与其他领导角色相比，首席数据官的角色相对较新。你能讨论一下 CDO 的角色在过去几年是如何演变的吗？CDO 今天的使命是什么？ 
苏达曼·托普潘·莫汉钱德拉勒:它已经进化到一定程度，不应该再叫我们“新生”了。当我开始扮演这个角色时，情况确实是这样，但现在我认为它也正在成为一种主流，但还没有到那一步。我会说，我同意，它还没有出现。有什么不同？显然，数据成为资产的方面。而且，如果有的话，将会有一项资产，它总是由企业来管理。作为资产的数据也应由业务部门和所有资产管理原则进行管理，这些原则由特定的 CXO 作为监护人适用。必须有一个监护人和监管人，这个特定的设置[听不清]是主要领导人或官员。 
Sudaman Thoppan Mohanchandralal:因此，企业基于战略，将数据视为一种文化或一种使能器或一种工具，无论如何，无论你如何看待它，它都是推动未来商业战略的主要组成部分之一。因此，从这个角度来看，这是一个必须与董事会分享的角色，以确保业务按照战略要求的方向发展。 
Adel Nehme:太棒了。回顾您到目前为止在安联度过的时光，您能否向我们介绍一下该团队取得的一些成就，以及如何让安联比荷卢经济联盟成为一个更加以数据为导向的组织。 
苏达曼·托普潘·莫汉钱德拉勒:当然。我想说，这一切都始于 2018 年。现在我们是在 2021 年，我们已经取得了相当大的距离。我不会说我们已经到达了目的地，因为它不在那里。基本上，我们今天已经交付了至少 30 多种数据产品，这些产品在业务方面投入了运营，在我看来，这是一个了不起的成就。我们还实现了年复一年，我们创造的运营利润超过了设定的目标。所以累积运营利润由数据办公室创造。在某种程度上，它们在结构上被低估了，我稍后会解释这一点。但就拿这部分来说，我们已经涵盖了商业案例。因此，我们现在创造更多的利润情况，而不是试图覆盖投资。因此，我们的业务案例已经实现。因此，这对于我们和我们的业务来说都是一个巨大的成就。 
苏达曼·托普潘·莫汉钱德拉勒:事实上，我保证会向你解释，什么是结构性低估。我们计算数据办公室创造的价值的方式是对业务收入的归属。因此，每次我们启动一项计划时，我们都会与业务部门讨论我们将赋予数据办公室的价值是什么。这或多或少都不科学。 
Sudaman Thoppan Mohanchandralal:它在未来会变得科学，但它还没有那么科学，但它只是把这归功于数据办公室。因此，从这个角度来看，我仍然认为我们在结构上被低估了，但即使有这种结构上的低估，实现商业案例，是我的团队的一个了不起的成就。我为我的团队感到骄傲。 
构建数据文化 
Adel Nehme:这是相当可观的，看到这些数据工作如此专注于价值令人兴奋。现在，我今天真正想和你深入探讨的一件事，也是你一直直言不讳的一件事，就是建立数据文化的重要性。因此，当回顾您加入安联时最初面临的挑战和数据文化的状态时，您能向我们介绍一下构建数据文化对您从数据科学的规模中提取价值有多重要吗？ 
苏达曼·托帕·莫汉钱德拉勒:我认为，如果没有数据文化，我们就永远无法从数据中创造价值。那么，我们所说的数据文化到底是什么意思呢？我们所说的文化是什么？文化只不过是一种组织习惯的集合。所以，如果一个组织文化是你只表达该组织员工的常规习惯的东西。 
苏达曼·托普潘·莫汉钱德拉勒:举例来说，数据驱动的决策是一种必须成为习惯的惯例。而这肯定要报答一些其他的习惯，更多的是本能的驱使。那么在这种情况下，你基本上是在谈论改变文化。现在，如果数据驱动的决策不存在，那么你真的不需要数据。如果你不根据数据做出决定，那么你所做的任何数据活动都没有价值。因此，从这个角度来看，我认为你将无法从数据中创造任何价值，除非，直到你有了这种处于任何成熟度级别的数据文化，但至少必须有这种数据文化慢慢开始在公司内部蔓延。所以，是的，我希望我回答了你的问题。论点是，如果没有这种数据文化，您将永远无法创造任何数据价值。 
Adel Nehme:我完全同意，我很高兴能和你一起深入探讨这个问题。因此，许多组织目前都在投资，变得由数据驱动，并从数据中提取价值。从文化转型计划、数据基础设施和技术能力，有一系列的活动要做。您是否发现关注组织的技术准备程度与其文化之间存在冲突？如果是，你认为组织应该从哪里开始？ 
苏达曼·托普潘·莫汉钱德拉勒:阿黛尔，真的有四样东西。首先是数据文化。第二件事是数据解决方案。第三件事是数据推动者。第四件事是数据工具吧？因此，在大多数情况下，组织中的数据和分析等计划是启动或 kickstart。他们试图解决它是如何或在哪里开始的。例如，如果它从业务方面开始，这总是好的，考虑到你有其他的选择，四件事情中的一件会变得重要。如果从业务方面开始，数据作为一种解决方案或数据作为一种使能因素将开始变得重要，而数据作为一种工具或数据作为一种文化，我想说，仍然会有一点被采用。同样，如果 it 正在启动 IT，您可以想象数据是一种工具，数据作为一种使能因素变得很重要，但解决方案和文化方面仍然被采用。所以这取决于它实际上从哪里开始。 
Sudaman Thoppan Mohanchandralal:根据我的经验，我会说，不幸的是，平行地开始所有这些事情是你取得进步的唯一途径。原因是，你必须维持你投入的投资，为了维持这些投资，你需要一个基本平衡所有投资的结果。这就是回报。还有一种方法可以看出你所做投资的真正价值。如果你想在一些计划上有所突破或坚持，这个回报是非常重要的。这才是你真正的反馈。要做到这一点，你需要一些数据作为思考的工具。你需要数据作为思维的驱动力。你还需要一个解决方案，因为你想把它放到市场上赚钱。因此，它成为解决方案的唯一途径是当你有在决策中使用数据的文化时。所以你看到这样或那样的方式，不幸的是，你没有奢侈的顺序启动它。你甚至没有开始管内衬的奢侈。你必须平行地做。这就是复杂性所在。 
Adel Nehme:在结合这些计划时，特别是在构建数据文化转型计划时，您发现了哪些最佳实践？ 
Sudaman Thoppan Mohanchandralal:最佳实践完全取决于组织文化。这本身就是最佳实践。了解你的生态系统。是啊。了解你的前景，了解你当前的组织文化，了解你当前的业务专家，因为你需要从什么是线索或触发器的角度来看待它？奖励是什么，这是有套路的，这是到位的。因为几乎总是有一个惯例，那就是在适当的地方。你必须明白这些事情。这是第一个最佳实践。我称之为理解现状。 
Sudaman Thoppan Mohanchandralal:然后你需要清楚地了解目的地。从这个意义上说，目的地并不是你所说的遥远的未来的理想状态。当然，这将是非常好的，但尽可能地，这个特定的目的地应该是一个具体的，可衡量的，可实现的，现实的，有时间限制的方式。 
Sudaman Thoppan Mohanchandralal:所以一旦你有了它，那将是一个到达点。所以从目的地到目的地，你必须非常清楚地定义一种到达目的地的方法。现在我要说的是，你不能只有一条路，但你需要有那四条路:数据作为文化之路，数据作为解决方案之路，数据作为使能之路，数据作为工具之路。由于这些道路基本上屈服于有效性或效率，这也需要跟踪。这意味着，把所有这些事情和结果联系起来，是的。因此，这将是第三或第四个最佳实践。在我看来，第五个最佳实践是对业务有一个真正的承诺原则。因此，您需要启动一项计划，不是从数据端或数据办公室端开始，而是必须是一项涉及数据的业务计划。 
Sudaman Thoppan Mohanchandralal:原因很简单，因为业务的优先级仍然存在，尽管你优先考虑的是让数据驱动。因此，我的最佳实践建议不是在现有的业务优先级列表中添加另一个优先级，而是选择其中一个或一组已被列为业务优先级的计划，并获得他们的认可或承诺，即他们将基本上以数据为驱动。这种承诺延伸到结果归因。这一承诺也延伸到运作方面。所以这些是我能立即想到的最佳实践。 
Adel Nehme:您在这里提到的一件事是需要与业务专家和领导者合作，因为组织的优先事项本质上是业务的优先事项。您能否详细说明当您与业务领导合作时获得认可的性质，以及在采用新的数据科学方法方面获得组织领导的认可时是否有时会有摩擦，以及如何最好地引导在组织内灌输这种思维转变？ 
Sudaman Thoppan Mohanchandralal:从技术上来说，没有理由发生摩擦，因为你基本上是在尝试做同一项业务。你增加了更多的资源，而不是制造障碍。从技术上来说，仅仅是因为当然有它的另一面，就可能有政治在起作用。可能存在其他类型的挑战，甚至有理由认为业务可能会考虑焦点漂移，这将影响他们实现目标的能力，因为他们并不真正了解 it 的数据世界。他们没有把它看作资源，而是把它看作某种障碍。所以这些东西总是存在的，你可以称之为摩擦力。但事实上，这正是 it 或数据教育的数据素养部分发挥作用的地方。你必须花费一些精力来确保他们能够将数据视为一种资源，而不是一个障碍。要实现这种转变，就像你说的那样，这是一种思维方式的转变。这是你能够实现无摩擦讨论的唯一途径。 
苏达曼·托普潘·莫汉钱德拉勒:无论如何，都会有这样或那样的摩擦，这将是健康的，因为要找出问题或障碍或差距，这些都需要填补。所以这就是为什么我会称之为健康，但至少良好的理解水平将使那些所谓的硬摩擦成为可能。 
数据技能和数据文化之间的联系 
Adel Nehme:您也提到了数据素养和数据技能的重要性。您能否向我们介绍一下整个组织中数据技能和数据文化之间的联系，无论是针对高管还是个人贡献者。你如何看待这一组织优先事项？ 
Sudaman Thoppan Mohanchandralal:我将参考一下我们在安联-比荷卢经济联盟的数据学院。我们称之为加速数据学院。可以说，我们将数据技能分为三个级别。第一级是铜牌。我们称之为数据青铜。公司里的每个人，不管他们做什么，在哪里。他们应该具备的数据知识还有第二个层次。这正是这种特殊水平的成就所带来的。所以他们，我们称他们为数据公民。我不想把数据公民这个术语和其他定义混淆。只不过，任何人都需要了解数据伦理方面的最基本知识，以前重视数据、数据的保密性，包括所有其他技术、最近刚刚出现的数据等等。对这些东西的高层次理解，它的分析方面，非常高的层次。 
Sudaman Thoppan Mohanchandralal:实际上，无论你在公司中扮演什么角色，你都是一名承保人，你是一名精算人员，你是一名接待员，你不关心，无论他们在做什么角色，他们都应该知道。那是青铜级。我们称他们为数据公民。 
苏达曼·托普潘·莫汉钱德拉勒:当然，下一个级别是白银。当你穿银色时，你会穿得更深一点。然后是溪流。当然，如果你是一名高管，你基本上能够理解数据价值提取过程。我会这么说。您还可以了解数据可以增加价值的业务计划。我是说，如果你是个主管，你就能认出他们。第二类当然是业务经理等，他们会更深入地学习如何构建数据增值的业务案例。 
Sudaman Thoppan Mohanchandralal:当然，第三个层次是技术人员，他们基本上是利用数据等建立模型。在这个层面上，我们称他们为数据大师，因为他们能够真正从数据中创造价值。 
苏达曼·托普潘·莫汉钱德拉勒:第三层是黄金层。这基本上在一定程度上实现了某种成就，即在给定的当前环境下，例如在数据和分析方面，或者在数据和分析密集型的计划和业务计划方面，你实际上对未来有一种观点或看法。所以这一个是在你能够想象什么样的中断会产生更好或改善或增加结果的水平上。此外，你还能够用事实来表达你的观点，你能够推销并参与公司的战略讨论。 
Sudaman Thoppan Mohanchandralal:所以我们称他们为数据大使。因此，在我看来，直接的答案应该是，这取决于你所谈论的组织和他们正在开展的业务，但不要听起来太笼统，而是试图给你一个答案，我认为企业应该就如何使用数据做出明确的决定。他们是真的想把它作为一种竞争优势，还是真的想把它作为一种防御机制，无论他们做出什么决定，他们都可以相应地使用我提到的水平，这就是他们必须要做的战略性劳动力规划。 
Adel Nehme:这是非常有见地的，尤其是在将业务目标与学习目标联系起来的时候。现在，您提到了数据文化是如何融入整个业务的，当谈到创建数据文化并激发它时，您认为谁应对这一文化转型计划负责？ 
Sudaman Thoppan Mohanchandralal:董事会对整个组织的文化负责。因此，他们仍然对数据文化负责。现在，更准确地说是首席执行官，因为首席执行官总是文化的负责人。原因是，如果有一个战略，而这是首席执行官负责的事情，文化就是将战略带到目的地的工具。现在你不仅可以拥有包装好的随时可以移动的东西，这就是战略。但是你也应该建造一辆车，它是你在正确的时间把它带到目的地所必需的，因此你可以看到什么仍然是超级重要的。当然，首席执行官将有一个 CDO 来执行它，甚至会帮助构建这个工具本身，以便将战略推进到其目的地。但我仍然认为，在让数据文化在一个组织中成为现实的过程中，首席执行官的角色，不是别人，正是首席执行官本人，是至关重要的。 
Adel Nehme:您认为职能和业务领导也应该负责激发数据文化，并为组织的其余部分传递这一信息吗？ 
苏达曼·托普潘·莫汉钱德拉勒:当然。CDO 和首席执行官必须确保他们能够让其他首席执行官和他们各自的下属承担起这种文化的责任。因为你应该看到，CDO 在这里的角色是一个催化剂或推动者，但不是真正的角色。实际上，行动总是与其他经营业务的首席执行官一起进行。但是 CDO 的工作是创造所有能创造生态系统的东西，以及足够适合它们生存的环境。但是真正的责任和义务在于其他人。首席执行官也要求这样做。因此，我们不应该在 CDO 下建造一座象牙塔。这不是应该走的路。在我看来，行不通。因此，就像你正确地说的那样，确保文化基本上适应数据的责任也是其他业务领导者的责任。 
D3M 框架 
Adel Nehme:太好了。你最近在 data camp 网络研讨会上谈到的一件事是运营一个数据驱动的框架来发展数据文化，或者你可以称之为 D3M 框架。你介意简单说一下你是怎么做的吗？ 
苏达曼·托普潘·莫汉钱德拉勒:看，我们并没有马上意识到应该这样做。这是一段时间以来，我们基本上不断微调，以达到我们今天的水平。我很自豪能和我的团队一起取得某些成就。就像我说的，我也为我的团队感到骄傲。 
Sudaman Thoppan Mohanchandralal:回到围绕数据文化的讨论，我们首先想了解，当前现有的惯例正在帮助我们的业务实现的所有线索和所有回报是什么。当然，你可以在整个安联比荷卢经济联盟中以非常通用的术语来思考这个问题，但后来我们认为这太过了，这是一个令人热血沸腾的故事。所以让我们只去一个地区，从那里开始。我们从销售开始。我们问他们的问题是，好吧，你们做的所有不同的决定是什么？ 
Sudaman Thoppan Mohanchandralal:我们根据他们创造的商业影响列出了他们。然后我们说对于这些决定，有一个奖励。所以我们称这个决定为线索和奖励，我们已经开始列举了。然后我们开始查看例程:它们是否是数据驱动的。如果例程是数据驱动的，这是个好消息。但是如果 it 例程不是数据驱动的，那就是我们开始行动的地方。所以你看，我们开始让它变得高度相关，对吧？举个例子，我们讨论了一个叫做代理转向的特定线索。我们说过，对经纪人来说，要实现回报，基本上只是更多的潜在客户转化。这就是奖励。例行公事更加本能。例行公事更依赖于舒适，更多的是在销售人员的头脑中。即使有数据报告可用，但他们很少使用它。 
苏达曼·托普潘·莫汉钱德拉勒:这给了我们一个很好的暗示，一个很好的起跑位置。从那时起，我们开始观察，我们做调查，我们做采访，我们学习，然后我们意识到，好吧，如果数据素养提高，如果提供数据教育，数据质量也提高，也有可能改变常规，从今天的一切，到数据驱动的决策，我们将看到经纪人转向数据驱动，当然，影响已经存在，我们开始管理。 
Sudaman Thoppan Mohanchandralal:所以你看，我们现在甚至能够计算出领先一代在我们通过数据文化造成的中断中所占的百分比。这就是我们如何操作它。现在，您可以想象这可以扩展到业务线和其他领域的其他站点。现在，这是一种非常侵入性的，但随后又是一种非常相关和可量化的向组织交付数据文化的方法。这就是我们如何操作它。 
Adel Nehme:太棒了。您还提到了在考虑运营数据文化时使用调查和关键领域进行评估。您能否向我们介绍一下，测试组织何时应该开始评估和制定数据转换计划的主要领域是什么？ 
Sudaman Thoppan Mohanchandralal:还是那句话，完全取决于组织本身。对我们来说，感觉更好的是围绕数据的领域。我的意思是，在数据收集中，业务流程基本上引入了数据以及存储和计算数据的基础架构区域。我们也考虑了这些东西的可及性、可发现性、可信赖性和可理解性。这意味着文档、治理机制、管理方面以及组织的日常事务，对吗？我是说，这些决定是怎么做出来的。在我之前的回答中，我指的是基于线索的套路，并期望得到回报，套路被贯彻到底。所以这些是我们选择的不同维度。回答你的问题，Adel，这肯定是每个组织都有很大不同的地方。通用方法肯定是不存在的，因为我们谈论的是一种真正的文化。 
行动呼吁 
Adel Nehme:我非常感谢您在文化的独特性以及组织如何不同方面的投入。但是在我们结束之前，您是否有任何其他行动要求或最佳实践想与其他希望建立数据文化计划的数据领导者分享？ 
苏达曼·托普潘·莫汉钱德拉勒:我看到的真正有效的最佳实践是自上而下的工作。它不会帮助您建立数据素养和数据教育，就像所有其他素养和教育行动在整个员工群中执行一样。但我认为对我有效的最佳实践最有潜力，从顶层开始工作。也就是说从你组织的最高层开始。但同时，别忘了中间。此外，在整个组织中，对交付和您所做的内容必须有明确的区分。所以，当你从高层开始工作时，不要只和高层一起工作，当你和高层一起工作时，也要把中层带到同一个房间里。这对于确保高层和中层之间的对话至关重要。 
苏达曼·托普潘·莫汉钱德拉勒:我的意思是，当我和首席财务官一起工作时，我也把所有向首席财务官汇报的人召集在一起，一起工作。这一点非常重要，因为我们是推动者。所以我们应该促成对话。数据驱动的决策对话必须找到自己的位置。它只会从这两者之间开始，因为它必须在那里发生。 
苏达曼·托普潘·莫汉钱德拉勒:这就是我的建议。一个强有力的最佳实践是，是的，从顶层开始工作。不要只考虑普通的大民主教育，它适用于每个人。不管怎样，那是必须要做的。但是要从高层开始工作，也要和中层一起工作。但请记住，成功在于你把顶端和中间的部分结合在一起，因为有一个对话真的非常关键。对话也反馈到你如何设置下一级教育的内容和交付。也有助于你向那些在底层工作的人解释上层和中层的观点。所以这是我最好的练习，阿黛尔。 
Adel Nehme:好的。太棒了，苏达曼。非常感谢你来到这个节目并分享你的见解。真的很感谢。 
苏达曼·托普潘·莫汉钱德拉勒:不客气。谢谢你给我这个机会。 
Adel Nehme:今天的 DataFramed 节目就到这里了。谢谢你和我们在一起。我非常欣赏 Sudaman 关于构建数据文化的见解，以及文化转型项目的独特性质。如果你喜欢今天的节目，记得在 iTunes 上给我们评分。我们的下一集将与 Y 实验室的首席执行官 Alyssa Visnic 讨论 ML ops 以及它为什么如此重要。我希望你会发现它很有用，我们希望下次能在 DataFramed 上看到你。 
人工智能在金融和算法交易中的力量 
 
 原文：https://web.archive.org/web/20230101103301/https://www.datacamp.com/blog/the-power-of-ai-in-finance-and-algorithmic-trading 
 
 
 
Recently, Dr. Hugo Bowne-Anderson (DataCamp) and Dr. Yves Hilpisch (The Python Quants) discussed how AI can be leveraged in finance, how it compares to more traditional approaches, and why data scientists are now key players in finance. They also discussed uncertainty and risk, and how useful AI and machine learning can be in market crashes, during black swan events, and in volatile markets. Watch the webinar on-demand or read on for some key takeaways. ## What is AI and what are the implications? Yves defines artificial intelligence as the whole range of fields that try to emulate and improve what people can do. In that sense, AI in chess tries to play chess better than humans, and AI in finance can mean building and training bots that can trade better than humans do. > AI is where software achieves something that we would consider an intelligent act. And intelligence can be defined as the ability to achieve a certain goal.—Dr. Yves Hilpisch Machines can follow the scientific method much more efficiently than humans. They are able to look at the data and find out revealed preferences by tracking what people actually do. And from that, they derive insights. They leverage millions of data points to form these insights, which end up being far more accurate than theoretical models. > So what is the scientific method?..You set up an experiment, you collect data, and then you try to either find support for the hypothesis, or maybe you find data that contradicts the hypothesis. Then the next step will be to adjust your hypothesis, your assumptions, or maybe retract the hypothesis altogether. This process of the scientific method can be replicated by machines a million times in seconds.—Dr. Yves Hilpisch ## AI has provided a major competitive advantage in the past 20 years AI serves the same purpose in finance as it does for any other field: to gain a competitive advantage. In finance, it got its start in the 1950s, when Markowitz led the way with the first quantitative finance model. It was the first model in variance portfolio theory that gained widespread acceptance. Today, it’s still used to manage trillions of dollars. But it uses normative theory which is based on analyzing various possible portfolios of securities—it’s not prescriptive because it doesn’t suggest a possible course of action. So it’s not based on data, but rather behavioral data or market data. In the past 20 years, data has become more readily available than ever before—the key is to use it appropriately. Gregory Zuckerman’s The Man Who Solved the Market details how the hedge fund Renaissance Technologies led the way in amassing data for financial gain. Today, almost every industry has embraced their strategy of creating mathematical models and crunching data—and are able to make more optimal decisions as a result. ## Modern use cases and best practices for quantitative finance There has been a dramatic evolution of AI in financial markets. For example, using the mountains of data available today, supervised learning models are able to predict the behavior of creditors or consumers with a high degree of accuracy. Algorithmic trading also leverages reinforcement learning to reward and punish trading bots based on how much money they make or lose. > We have now millions and millions of data points that we can use to analyze the behavior of people. And with the new technologies that we have, banks and institutions [such as] fintech startups are ten times, a hundred times better at predicting consumer behavior, creditor behavior, etc, than every theory that ever has been devised by financial professors.—Dr. Yves Hilpisch But in times like these where there is high market volatility, it’s clear that AI in finance requires safety precautions. Supervised learning relies on historical data and assumes that what has happened in the past is representative of what will happen in the future. This is faulty logic in times of market volatility. That’s why you must have rules in place to prevent bad trading decisions, like putting a stop loss—which specifies a price that a security or commodity will be sold—on the algorithms. This logic limits losses that are too great to bear and is akin to putting security mechanisms on a self-driving car to prevent it from hitting pedestrians on a sidewalk. AI is useful for reducing risk resulting from current events and its impact on the market. In black swan events, supervised learning tactics must be carefully monitored, and reinforcement learning can be used to retrain models based on new market conditions. Human intervention and careful monitoring of deployed algorithms will still be required for the foreseeable future. ## The need for simple, replicable, scalable tools In computational finance, the Monte Carlo simulation has a forward-looking approach that starts with a certain point in time and simulates outward into the future. Python is a great tool for this. The Python data stack today is a set of interoperable packages that allow you to ingest data, transform it with pandas, conduct statistical modeling, and create machine learning algorithms. Python allows all this to happen at scale. Today, algorithmic trading depends on having the proper tooling. And getting a job in this industry requires a certain degree of proficiency in these tools—namely, Python and R. The Python package pandas and frameworks like TensorFlow and Keras have allowed many people to be able to do what was previously impossible. ## Lowering the barriers to entry Learning how to use these tools is a big investment, and more companies and individuals are making this investment than ever before. Educational offerings like DataCamp and The Python Quants can democratize the learning process. Learners become familiar with data science and machine learning concepts and get comfortable with building algorithms independently in a coding environment. They’re also able to get familiar with how to set up a proper environment and tool chain to work on the server—empowering them to deploy algorithms in the cloud. 
 
朱莉娅的崛起——2022 年值得学习吗？ 
 
 原文：https://web.archive.org/web/20221129041532/https://www.datacamp.com/blog/the-rise-of-julia-is-it-worth-learning-in-2022 
 
当考虑数据科学的编程语言时，Python 占据了至高无上的地位。虽然 Python 在大多数领域都超过了 R，但 R 在跟踪编程语言受欢迎程度的 5 月 TIOBE 指数中保持了其在前 15 名编程语言中的位置。事实上，在过去的一年中，R 使用的百分比增长远远高于像 JavaScript、Java、C 和 PHP 这样的泰坦语言。 
Python 和 R 一起统治了数据世界，给类似的语言投下了阴影。 
但是有一种语言已经脱颖而出，慢慢走向光明。那种语言就是朱莉娅。尽管它还很年轻，Julia 爱好者已经称它为数据科学和人工智能的“未来语言”。为了验证他们是否正确，让我们来看看 Julia 采用的一些指标，这些指标摘自 Julia 关于增长统计的年度简讯。 
截至 2022 年 1 月 1 日，朱莉娅已被下载 3500 万次，2021 年的下载次数是过去三年总和的三倍。核心语言及其注册包已经在 Github 上聚集了总共 250，000 名明星，是六年前总和的 13 倍。 Julia 的 GitHub repo 一家独大，39.1K 星，4.9K 叉，3.3K 发行。 
朱莉娅登记的包裹数量在 4 月份达到了 7400 多个，比 6 年前多了 10 倍。至于它在所有其他语言中的排名，5 月 TIOBE 指数将其置于第 25 位，比去年从第 47 位跃升的排名下降了三位。 
 
5 月 TIOBE 指数作者截图 
Julia 的实际 TIOBE 指数为 0.64%，也就是说在搜索引擎上所有与编程语言相关的查询中，有 0.64%的时候会弹出 Julia 的名字。换句话说，大约每 200 个编程相关的问题都是关于 Julia 的。 
考虑到朱莉娅最早的稳定发布是在 2018 年，这些数字令人印象深刻。但是，这些数字是否意味着在可预见的未来，Julia 的受欢迎程度将只有 Python 或 R 的一半呢？现在还值得学习吗？在我们回答这些问题之前，让我们更仔细地看看语言本身。 
朱莉娅是什么？ 
Julia 是一种科学计算语言，于 2012 年首次发布。编写它的四位创始人，Jeff Bezanson、Stefan Karpinski、Alan Edelman 和 Viral B. Shah，都有不同的背景，但都欣赏所有其他编程语言的集体力量。 
为了让 Julia 成为最强大的语言，创始人为这种语言设计了一个特定的愿景。在一篇解释他们为什么创造朱莉娅的博客文章中，他们说朱莉娅应该是: 
开源和 C 一样快像 Ruby 一样充满活力同形符号，或把代码当作数据数学符号中的类 Matlab在宏中类似 Lisp像 Python 一样通用像 R 一样易于统计像 Perl 一样自然地进行字符串处理像 Matlab 一样强大的线性代数类似 Hadoop 的分布 
这些超凡脱俗的雄心壮志的确雄心勃勃。然而，当 Julia 1.0.0 在 2018 年发布时，他们已经兑现了六年前做出的 90%的承诺。 
Julia 擅长编程的某些方面，这是任何普通语言都做不到的。例如，Python 为了灵活和动态类型化而牺牲了速度，而像 C、C++和 Java 这样的语言为了在运行时更快而严格静态和严格。至于 Julia，这种语言读起来像英语，完全是动态的，并且仍然是历史上最快的语言之一。 
这些好处已经导致了这门语言引人入胜的应用。例如，巴西国家空间研究所(INPE)使用 Julia 来规划太空任务，而联邦航空管理局(FAA)正在使用该语言开发下一代防撞系统。 
使用 Julia 的数据团队 
不仅仅是航空航天工程，制药公司也在和 Julia 一起发展。辉瑞公司使用在 Julia 运行的模型，将心力衰竭治疗的模拟加速了 175 倍。阿斯利康使用带有 Flux.jl 和 Turing.jl Julia 软件包的贝叶斯神经网络来预测药物毒性。你可以在另一篇文章中了解更多关于【Julia 用于的信息。 
朱莉娅在全球一些最负盛名的机构任教，如麻省理工学院、斯坦福大学、康奈尔大学、加州大学伯克利分校、布朗大学、东京城市大学等。以下是一些在朱莉娅发展的全球知名公司。 
美国英特尔公司(财富 500 强公司之一ˌ以生产 CPU 芯片著称)迪斯尼亚马孙大写一毕马威会计事务所谷歌微软国家航空与航天局国际商用机器公司 
 
如今使用 Julia 的公司 
是什么让朱莉娅与众不同？ 
朱莉娅的许多品质使其成为独一无二的语言。这里有一个让学习 Julia 引人注目的非详尽特征列表。 
1.速度 
我已经多次接触到它惊人的速度。但是，我们真正交谈的速度有多快呢？ 
1998 年，阿帕奇角天文台的研究人员开始对 35%天空范围内的所有可见物体进行成像。他们称该项目为 Celeste，并获得了 5 亿颗恒星和星系的数据集。16 年来，对这些图像进行编目是一项繁琐而费力的工作。 
朱莉娅上场了。 
2014 年，一个科学家和程序员团队聚集了 178TB 的图像数据，并在仅仅 14.6 分钟内对 1.88 亿颗恒星和星系进行了点估计。 
这一科学和编程壮举是通过在 Cori 超级计算机上运行 Julia 实现的，在 9300 个节点上跨 130 万个线程实现了 1.54 petaflops(每秒 10 次⁵运算)的峰值性能。因此，Julia 成为继 C、C++和 Fortran 之后第四种达到 petaflops 性能的语言。 
2.句法 
Julia 的另一个亮点是它的语法，它以一种曾经被认为不可能的方式表现了语言。在 Julia 发布之前，科学计算主要是用 Python 或 r 等其他动态类型语言完成的。由于计算是由物理学家、生物学家和金融专家完成的，他们不是经验丰富的开发人员，所以这些专家更喜欢更简单的语法，即使这是以失去计算速度为代价的。 
Julia 也是动态类型的，使得学习和编写非常简单，但是作为一种编译语言，它可以和静态类型的语言如 C 或 Fortran 一样快。 
3.多重调度 
另一个优点是多重分派，尽管这对于编程来说并不陌生。不要太专业，多重分派指的是一个函数根据其参数类型做出不同行为的能力。这里有一个例子可以说明这一点是如何有用的: 
function _add_str(str1, str2)
 str1 * str2
end

function _add_int(int1, int2)
 int1 + int2
end

add(item1::String, item2::String) = _add_str(item1, item2)
add(item1::Int64, item2::Int64) = _add_int(item1, item2)
12345678910 
[OUT]: add (generic function with 2 methods)
1 
add(4, 5)
1 
[OUT]: 9
1 
add(""Julia "", ""is awesome!"")
1 
[OUT]: ""Julia is awesome!""
1 
如果你已经注意到，我刚刚在 Python 中实现了+ plus 操作符，它随着输入类型的改变而表现不同。上面这个例子的美妙之处在于，您可以在几乎任何您想要的函数上启用 Julia 的多调度特性，并使它在不同的输入下表现不同。 
这些是关于朱莉娅的一些美好的事情。现在，让我们把它与 Python 进行比较。 
Julia vs. Python，详细对比 
在这一节中，我将尝试概述 Julia 和 Python 之间的差异。虽然比较将主要在 Julia 和 Python 之间进行，但是它们也适用于 R，因为 Python 在许多方面优于 R 或与 R 表现相似。 
1.速度 
我们先来看看语速差异，因为朱莉娅爱好者不断吹嘘自己语言的速度。我们将测量找到 Julia 的Flux.jl包中的函数的导数所需的时间，并与 Python 中的Tensorflow的GradientTape进行比较。 
我们将从张量流开始: 
import time

import tensorflow as tf
start = time.time()
x = tf.Variable(5.0)

with tf.GradientTape() as tape:
 y = 3 * x ** 3 + 4 * x ** 2 + 6 * x + 5

 # dy_dx = 9x^2 + 8x + 6
 dy_dx = tape.gradient(y, x)

print(time.time() - start)
12345678910111213 
[OUT]: 0.003016233444213867
1 
求x的导数大约需要 0.003 秒。让我们在朱丽亚身上看到同样的操作: 
 
两个例子都在本地机器上运行(AMD 锐龙 9 3900x 12 核处理器，英伟达 RTX 2060 SUPER 8GB VRAM) 
我们第一次运行渐变，Julia 用了~0.002 秒编译，已经比 Tensorflow 快了。下一次运行时，评估速度提高了约 450 倍。速度测试—检查！ 
2.句法 
现在，我们来谈谈语法。许多人声称朱莉娅代码读起来更像英语；它比 Python 更有表现力，更干净，尤其是对于科学计算。让我们来看看实际情况。 
第一个例子是写数学表达式——你可以像在 MATLAB 中一样写多项式: 
x = 5
3x^2 + 5x + 4
12 
[OUT]: 104
1 
(x + 5)x
1 
[OUT]: 50 
1 
还有一些大大简化条件句的短路表达式，来自于此: 
x = 73; y = 37

if x > y
 println(""$x is higher than $y"")
end
12345 
[OUT]: 73 is higher than 37
1 
对于这个精简的例子: 
x > y && println(""$x is higher than $y"") 
1 
[OUT]: 73 is higher than 37
1 
本地 Python 和本地 Julia 之间还有其他有益的区别。例如，默认情况下，Julia 中的数组被认为是向量和矩阵，任何在其上运行的函数都将被矢量化，从而消除了不必要的循环。尽管如此，对于数据科学来说，Python 仍然是最用户友好的编程语言之一。 
3.流行度和包装生态系统 
然而，并不是每个人都准备好跳上朱莉娅的列车。我已经在介绍中谈到了朱莉娅的成长统计数据。就其本身而言，它们看起来相当令人印象深刻，但与 Python 相比，还有很长的路要走。 
Python 以 12.74%的指数位居 TIOBE 排行榜之首，而 Julia 的指数为 0.64%。作为一门语言，有像 TensorFlow 或 PyTorch 这样的完整 Python 包比 Julia 更有吸引力。 
Python 的数据科学和机器学习生态系统更加广泛和成熟。Julia 有超过 7k 的注册包，而 Python 有超过 110k。这些数字意味着 Julia 要成为最好的通用语言还有很长的路要走，尽管它可能在许多方面比原生 Python 有明显的优势。 
虽然 Python 在规模上令其相形见绌，但对 Julia 程序员来说，一个优势是他们有更大的机会来改进语言，并在 Julia 生态系统中的许多不断发展的学科中留下自己的印记。此外，要开始为朱莉娅做贡献，不需要有多年的专业知识或精通多种语言。用 Julia 编写的库完全是用 Julia 编写的，没有其他语言的贡献。下面是一个Flux.jl存储库的例子: 
 
相比之下，你不仅要掌握 Python，还要掌握 C++、C、GO 等其他语言。，以及它们是如何相互作用来开始理解关键 Python 包的代码的。 
 
如果你确实想在 Julia 中调用 Python 等其他语言，像Pycall这样的包可以让它变得简单。这是一个正在运行的包: 
using Pkg

Pkg.add(PackageSpec(name=""PyCall"", rev=""master""))
Pkg.build(""PyCall"")
using PyCall

plt = pyimport(""matplotlib.pyplot"")

x = range(0; stop=2*pi, length=1000)
y = sin.(3*x + 4*cos.(2*x));

plt.plot(x, y, color=""red"", linewidth=2.0, linestyle=""--"")
plt.show()
12345678910111213 
 
有类似的 Julia 包可用于调用其他语言，如RCall、MATLAB.jl等等。 
4.学习资源 
不幸的是，虽然 Python 和 R 有丰富的初学者友好的学习材料，但缺乏学习 Julia 的类似资源。缺乏免费和付费的关于 Julia 的指南、课程和书籍是它仍然没有被大量采用的原因之一，因为开发人员讨厌通过阅读文档来学习一门新语言。 
由于给定主题的教育资源的数量通常与其搜索需求相关，我们可以查看 Google Trends 获取这方面的数据: 
 
这个图显示 Python 的受欢迎程度明显高于 Julia。 
对 Python 和 R 的社区支持也比 Julia 多得多。我们可以通过查看 StackOverflow 上每种语言的标记问题数量来确认这一点: 
 
 
 
Python 无疑领先于这两种语言，但是随着它的采用，Julia 的培训材料和社区支持的可访问性也在增加。 
朱莉娅值得学习吗？ 
那么，是时候回答这个百万美元的问题了——朱莉娅值得学习吗？ 
权衡这种语言的利弊，答案倾向于“是！”。这只是时间问题。 
大多数数据科学家会说，在数据科学和机器学习之旅的早期阶段，你不应该学习 Julia。几乎 100%的数据角色都将 Python 或 R 列为要求，在职业生涯初期选择学习 Julia 而不是它们可能是一个严重的错误。如今，数据工作所需的技能和工具已经非常庞大，以至于要花相当多的时间投资才能让自己成为某个角色的资产。用朱莉娅分散你的注意力会让你远离一份有报酬的工作，并减缓你的职业发展。 
开始学习 Julia 的最佳时机是当你觉得你已经掌握了 R 或 Python，并且可以用它们的库解决业务问题的时候。在那个阶段，Julia 将是您工具箱中的一个很好的补充，因为您可以充分利用该语言带来的所有优势和能力。 
然而，一些专家对学习朱莉娅有完全激进的看法。正如 Ari Joury 在他们的文章中声称的那样，如果你早学并且学得好，Julia 可能是你通向未来的黄金门票。作者引用了人工智能本身的例子，因为它在 30-40 年前是一个非常小众的领域，但在 90 年代采用它的人现在非常受欢迎。 
时间会证明这是否同样适用于朱莉娅，因为朱莉娅可能是你可以放在简历上的一件事，以真正从皮托尼斯塔中脱颖而出。决定权在你。你可以在我们单独的文章中找到更多关于机器学习的信息，朱莉娅和从 MATLAB 进展到朱莉娅。 
每周综述|第 4 集| PyScript 和 AI 音频编辑 
 
 原文：https://web.archive.org/web/20220627024316/https://www.datacamp.com/blog/the-weekly-round-up-or-episode-4-or-pyscript-and-ai-audio-editing 
 
Anaconda 发布 PyScript 
我们今天的第一个故事是关于 Anaconda 的一个激动人心的新版本。4 月 30 日， Anaconda 发布了 PyScript ，这是一个框架，允许用户在他们的 web 浏览器中完全使用 Python，包括嵌入内容、外部文件托管、访问 Python 生态系统等功能，所有这些都不依赖于服务器端配置。PyScript 背后的目标是允许任何具有少量 Python 知识的人将应用程序发布到任何硬件和软件平台上。这就是 Anaconda 的团队选择网络浏览器的原因。浏览器安全、稳定、强大，几乎可以在任何地方工作。 
用 Anaconda 团队的话来说，PyScript 只是 HTML，只是功能强大得多，这要归功于 Python 库的丰富且可访问的生态系统。他们喜欢将 PyScript 视为“软件开发的《我的世界》”:用户制作他们自己的世界(应用程序)或新的模块和组件(组件和小部件)，并与世界共享。 
PyScript 的主要特性是: 
它允许我们在浏览器中使用 Python 及其库生态系统(包括 numpy、pandas 和 scikit-learn)。通过使用环境管理，用户可以决定在运行页面代码时哪些包和文件是可用的。我们可以使用一些现有的 UI 元素(如按钮、容器和文本框)来可视化地开发应用程序。我们不必担心部署，因为有了 PyScript，一切都会在我们的 web 浏览器中发生。作为数据科学家，我们可以与我们的利益相关者共享包含仪表板和/或分析的 HTML 文件，他们将能够立即在浏览器中访问它们，而无需任何复杂的设置。 
PyScript 的到来意味着，现在用 Python 创建有趣的数据科学 web 应用程序的障碍比以往任何时候都要低。任何拥有 Python 数据科学技能的人都将能够直接与他们的利益相关者共享数据应用程序，而无需了解 Javascript 或自行解决部署问题。我们已经看到相当多的数据科学家在野外试用 PyScript，我们确保链接到下面的这些例子！"" 
PyScript 目前只支持 Python，但长期目标是在未来提供对其他语言的支持，让 web 成为一个对每个人都更友好的地方。 
自动驾驶汽车的世界速度记录 
接下来的故事让我们来看看自动驾驶汽车的进展。百度最近不仅获得了有史以来第一张 robotaxi 牌照，希望在中国道路上部署无人驾驶出租车，而且还创造了自动驾驶汽车的世界速度纪录。 
这辆自动驾驶汽车由意大利米兰理工大学-波利马沃的一组研究人员制造，成功达到 309 公里/小时(或 192.2 英里/小时)，突破了 300 公里/小时的历史性“墙”。该团队在美国宇航局的肯尼迪航天中心跑道上创下了记录，希望在亚特兰大赛道上重现这一结果，因为亚特兰大赛道显然不是直的。 
该团队表示，试运行令人振奋，但他们也对试运行产生的数据将如何公之于众感到非常兴奋，这将使整个行业从他们的工作和学习中受益。 
Airbnb 的远程工作 
Airbnb 加入了众多引入远程工作措施的公司的行列，为员工提供更多灵活性。在封锁限制期间，该公司的生产率大幅提高，并决定不仅允许员工远程工作，还致力于实现薪酬标准化，无论 AirBnb 的位置如何，以区别于其他跨国科技公司。随着公司在疫情期间学会适应远程工作，许多人发现他们实际上更喜欢远程工作，而不是每天去办公室。越来越多的人表达了对灵活性的渴望，苹果公司的一些员工甚至威胁说，如果他们不能完全做到远程办公，他们就要辞职。 
如今，向这种远程工作方式的转变越来越成为许多雇主和数据团队的竞争优势，因为雇主可以更广泛地接触到各种人才，员工也享受到更多的灵活性和自由。 
虽然许多公司都选择了混合模式，但员工的抵触情绪明显存在，74%的员工表示，远程工作的选择会降低他们离开公司的可能性。 
用于音频合成的机器学习 
在我们的下一个故事中，我们将看看音乐制作的领域。抖音的崛起引起了人们对音乐和音频内容在社交媒体中所扮演角色的关注，这促使支持抖音的 Bytedance 公司开发了一种令人兴奋的音乐制作工具，该工具使用机器学习将音频信号转变为真实乐器的模拟。 
这个名为“Mawf”的新插件目前正在测试中，它有三种乐器的模型:萨克斯管、小号和泰国 khlui。Mawf 首先使用机器学习来分析专业音乐家的录音，提取与音高和振幅变化相关的乐器声音的详细变化。 
然后，系统根据用户的数字输入(如 MIDI 或合成器音轨)来近似这些乐器的声音。 
鉴于这些创新，我们已经在数字内容创作的其他方面看到了，例如 DALL-E 2 生成图像，以及人工智能驱动的视频编辑软件，难怪音频制作也正在通过机器学习得到增强。未来的内容创作前景看起来肯定很有趣——我们正在向这样一个世界迈进，在这个世界里，我们可能会说“给我一个斑马戴着帽子跳抖音舞的视频，听起来像是”没人会知道…“人工智能也许能给我们这种感觉。 
了解最新的数据新闻 
要了解最新的数据新闻，请务必订阅 DataCamp 的 Youtube 频道，获取每周一次的数据新闻综述。 
每周综述|第 5 集|肯德里克·拉马尔在新音乐视频中使用 DeepFakes 
 
 原文：https://web.archive.org/web/20221129041532/https://www.datacamp.com/blog/the-weekly-round-up-or-episode-5-or-kendrick-lamar-uses-deepfakes-in-new-music-video 
 
Cookie Enforcer 用人工智能对抗烦人的 Cookie 
本周的第一个主要故事是关于一组研究人员如何使用人工智能打击烦人的 cookie 弹出窗口。 
如今，几乎每个网站都有一个要求 cookie 同意的弹出窗口，其中一些甚至被设计为难以拒绝 cookie，以迫使用户做出对网站友好的选择，从而将他们的隐私置于风险之中。来自谷歌和威斯康星大学麦迪逊分校的研究人员创建了一个名为“Cookie Enforcer”的系统来打击这些无孔不入的弹出设计。 
该系统通过扫描网站 HTML 元素的呈现模式来检测 cookie 通知将如何显示。然后，CookieEnforcer 分析这些通知，并预测哪些操作将禁用所有不必要的 cookies。最后，机器学习模型选择选定的设置并关闭弹出窗口，使网站用户不必自己破译和点击通知。 
研究小组发现，该系统在自动禁用 500 多个网站上的非必要 cookies 方面有 91%的效率。 
正式的公开发布日期尚未宣布，但该团队已经明确了他们的意图，即让 Cookie Enforcer 作为浏览器扩展广泛可用，并希望既能保护用户免受恶意或不透明设计的影响，又能节省他们的时间。 
IBM 发布欧洲人工智能技能差距研究 
接下来，IBM 最近关于欧洲人工智能技能缺口的报告提供了对人工智能就业市场状况的有用见解。德国、西班牙和英国的员工、招聘人员和申请人接受了调查，以评估人工智能职位空缺时是否存在技能短缺。 
该报告发现，在人工智能就业市场上，技术和非技术技能都存在明显的短缺。雇主指出，他们很难找到具备工作所需人工智能知识和经验的候选人。除此之外，大约四分之一的科技招聘人员提到，他们很难找到兼具技术和软技能的应聘者。 
随着人工智能在每个行业变得越来越普遍，专业技术人员与业务经理的合作比以往任何时候都更加密切。因此，能够展示人际沟通、战略问题解决和批判性思维的软技能，可以极大地提高人工智能的就业能力和职业发展。 
该报告还指出，提高技能和重新获得技能是解决组织技能缺口问题的潜在方法。在西班牙和德国的所有受访者中，42%的员工正在通过包括编程语言、数据工程/分析和软件工程等主题的培训来提高技能。英国落在后面，只有 32%的员工接受过此类培训。 
Meta 的大语言模型 
Meta 已经公开发布了一个大型语言模型，试图让这种模型更容易理解。在 5 月 3 日的一篇博客文章中，该公司宣布了其开放式预训练转换器(也称为 OPT-175B)，这是一个拥有 1750 亿个参数的自然语言处理系统。 
自然语言处理现在很热门，像 GPT-3 和 PaLM 这样的系统在人工智能领域掀起了风暴，并通过令人兴奋的用例，如语言生成、代码完成等，展示了这些模型的强大功能。Meta AI 表示，这些大型语言模型的潜力是显而易见的，唯一的限制是它们在很大程度上仍然无法被更广泛的研究社区所访问。 
除了庞大的语言模型，Meta 还公开发布了构建 OPT-175B 的所有开发说明、决策基本原理和幕后额外文档。该公司还高度重视该模型的生态足迹，并成功创建了一个与 GPT-3 规模相当的系统，但其碳足迹仅为 1/7。 
肯德里克·拉马尔在他最新的音乐视频中使用了 deepfakes 
最后，让我们谈谈在 Kendrick Lamar 备受期待的新版本中 DeepFakes 的使用。《The Heart Part 5》不仅是这位艺术家自 2017 年热门专辑《Damn》以来的第一张专辑，而且其音乐视频还在这位说唱歌手的脸上应用了一系列 deepfakes。 
辛普森，坎耶，威尔·史密斯和科比都出现了，拉马尔的脸在整个视频过程中无缝地融入了深度假动作。视频背后的技术公司“深度巫毒教”是由《南方公园》的创作者崔·帕克和马特·斯通创建的，以前曾使用 deepfake 技术创建过有争议的病毒视频。 
通常，deepfake 视频使用两种机器学习模型。一个模型根据一组样本视频创建新的面部数据，而另一个模型则试图检测视频是否确实是假的。当第二个模型不再能够辨别视频是否是伪造的时，那么 deepfake 对于人类观众来说也被认为是足够可信的。这个过程是一种被称为生成式对抗网络的机器学习算法，它通常与自动编码器一起工作，以操纵或生成视频和音频内容。 
《心脏第五部分》表明，尽管围绕 deepfakes 的道德使用存在复杂的争论，但这项技术仍然能够被编织到艺术叙事中，以在积极的背景下创造意义。 
获取数据科学的最新消息 
要了解更多关于数据科学的最新信息，请务必订阅 DataCamp 的 Youtube 页面以获取最新的每周综述。 
这个新的人工智能孵化器让社会影响初创公司能够接触到 1，700 名数据科学家 
 
 原文：https://web.archive.org/web/20221129033105/https://www.datacamp.com/blog/this-new-ai-incubator-gives-social-impact-startups-access-to-1700-data-scientists 
 
社会影响初创公司面临着筹集资金和创建可扩展商业模式的艰苦斗争，通常吸引不到风险基金的关注。但社会影响初创公司对社会的价值远远超过其资本估值。Omdena 是人工智能合作开发的领导者，它优先考虑社会影响，而不是对投资者的价值。通过将尖端数据科学放到最需要它们的人手中，Omdena 正在使初创公司能够在他们的业务中利用人工智能的力量，无论是在哪个行业或问题的大小。 
2021 年 50 家有影响力的创业公司 
自 18 个月前 Omdena 成立以来，来自 88 个国家的 1700 多名人工智能工程师已经合作为联合国难民署、WRI 等世界知名组织以及 Y-Combinator 和鹿角等集团支持的影响驱动型初创公司开发解决方案。 
2021 年，Omdena 将给予 50 家影响力创业公司加入我们独特的孵化器项目的机会。通过孵化器，创业公司将获得广泛的资源，包括从数千名申请人中选出的多达 50 名工程师的专门和多样化的数据科学团队，专有知识库，高级 ML 工程师导师和尖端技术。 
Omdena 的平台提供了与合适的人和环境的即时接触，在这里，最佳创意通过自下而上的协作而胜出。 
 
资料来源:omna 
Omdena 社区将世界各地的人们联系在一起，就有意义的问题进行协作，并构建现实世界的解决方案。我们的合作者使用 DataCamp 课程来构建现实世界中数据科学和机器学习的基本技能。 
八周从问题到复杂的人工智能模型 
 
资料来源:omna 
最近的一个项目是与总部位于新加坡的初创公司 Solar AI 合作完成的，该公司旨在改善太阳能电池板的销售和部署方式。评估太阳能电池板在任何特定建筑上的应用潜力的过程既昂贵又耗时。太阳能人工智能使用卫星图像和人工智能来大幅降低这一成本，并缩短评估建筑物所需的时间。 
在短短八周内，Omdena 的合作者提供了一个计算机视觉模型，可以检测图像中的屋顶，并估计屋顶上适合太阳能电池板使用的空间大小。该模型甚至能够检测到障碍物、分离器和屋顶上不同高度的部分。 
你是一家旨在解决世界上最有意义的问题的早期影响力创业公司吗？了解更多关于 Omdena AI 孵化器的信息。 
构建数据科学家简历的技巧 
 
 原文：https://web.archive.org/web/20221129044127/https://www.datacamp.com/blog/tips-to-build-your-data-scientist-resume 
 
数据科学是现代就业市场上最具竞争力的领域之一，吸引着来自世界各地的有志专家。成为一名数据科学家意味着进入拥有诱人职业前景和高薪的特权圈子。对这一领域的兴趣不断增长，对数据专业人员的需求也在不断增加，但候选人需要满足的要求也相当苛刻。 
对于数据科学求职者来说，重要的第一步是写一份有效的简历，能够给招聘经理/招聘人员留下足够深刻的印象，从而让你获得面试机会。对于那些已经在该领域拥有大量工作经验的专家来说，这是一项相对容易的任务，而对于入门级的数据科学家来说，这可能相当具有挑战性，并引发许多问题:我如何才能弥补我工作经验的不足？如果与数据科学不完全相关，描述以前的工作经历和/或教育是个好主意吗？哪个部分应该先走:教育还是经验？我需要包括什么样的项目，有多少个？我应该列出我的软技能吗？该不该给简历增色？总而言之，我如何才能脱颖而出，更接近被聘用为数据科学家的诱人目标？ 
在这篇文章中，我们将讨论一些基本的技巧和诀窍，如何制作一份引人注目、专业、易读的数据科学家简历，以吸引任何招聘经理或招聘人员的注意。 
如何有效地组织你的简历布局和格式 
 
把你的简历放在一页纸上 
如果你打算在简历中加入大量信息，遵循这个建议可能会很痛苦。这甚至看起来违反直觉；为了给潜在雇主留下深刻印象，尽可能详细地向他们描述你的经历、教育、成就和技能不是更好吗？如果是这样，如果它占用了一页以上的空间呢？不管怎样，强烈建议你的简历总是保持一页长。 
能够以浓缩的形式展示你的经历和成就是展示你沟通技巧的一个很好的方式。 
选择简历模板 
虽然你可以从头开始创建你的简历，并根据你的喜好组织它，但你可以通过使用在线简历生成器并从各种预先存在的简历模板中进行选择来节省时间。或者，可以从 Google Docs 或 MS Word collections 中选择一个合适的免费简历模板。这种模板通常清晰、视觉上吸引人、格式良好、易读、易于使用，并且在设计上可以帮助您在一个页面上容纳大量信息，而不会显得拥挤不堪。 
这里有一些简历生成器，你可能会觉得有用: 
简历Zety简历生成器坎瓦蛋糕简历VisualCVResumeCoach 
许多简历制作商，包括上面列出的那些，都提供了大量功能齐全的免费模板，以及更高级的付费版本。在为你的数据科学家简历选择模板时，优先选择除了黑白以外只有一两种颜色的简单模板，而不是花哨多彩的模板。后者更适合艺术或设计等其他领域，在这些领域，看起来有创意的简历会给雇主留下深刻印象。在我们的情况下，最好避免不必要的装饰。 
考虑选择两栏布局 
选择一个有两栏的简历模板(额外可以有一些窄栏)有助于更有效地利用页面的垂直空间。这样，你可以在简历中放入更多的信息，并且更好地组织不同的部分。 
使用有效的格式 
如果你决定从头开始创建你的简历，或者对已有的模板进行修改，这一组提示将会很有用: 
使用优雅的文本字体 
好例子:Calibri，Times New Roman，Arial，Verdana，Cambria，Tahoma，Georgia不好的例子:漫画无 MS，MS 哥特，墨水免费，代理 FB，OCR A 扩展 使用足够大的字体:文本使用 11-12pt，章节标题和页眉使用 14-16pt将章节标题和页眉加粗避免使用过多的文本样式，如粗体、斜体、粗斜体和下划线使用 1-1.15 倍行距避免枯燥的黑白简历——加点颜色让简历突出，但不要夸张，1-2 种颜色就够了避免视觉效果、装饰或不必要的图标使用项目符号让你的简历看起来干净、有条理、容易理解 
保持格式一致 
这是指各种文体特征。例如，如果你在简历的不同部分使用项目符号，你应该使用统一的风格，例如，空方块。缩进、字体样式、日期格式等也是如此。幸运的是，如果你在一个在线简历生成器中创建简历，所有这些元素和格式通常都会为你预先定义好。 
关于数据科学家简历内容的一般提示 
 
创建主简历 
在开始申请数据科学工作之前，首先创建一份“主简历”是有意义的。这是一个巨大的、非常详细的简历版本，可以有 2-3 页甚至更多，你可以在其中包括你所有的工作经历、学习、项目、技术和软技能以及其他成就。 
如果你是一个试图从另一个领域进入数据科学的职业改变者，考虑在你的主简历中包括你以前工作的经验；你可能会比你意识到的拥有更多的可转移技能。毫不奇怪，你的主简历的每一部分都有很多要点。完成后，您可以轻松地将其作为各种数据科学工作职位的申请基础；只需从主简历中删除多余的细节、章节和要点，并根据每次提交的内容进行调整。 
根据每个职位描述定制您的数据科学家简历 
这一步在你的求职过程中至关重要。为了增加你的简历脱颖而出的机会，你必须为你申请的每个职位定制简历。招聘经理通常每天都会收到大量的申请，所以可能只有几秒钟的时间浏览每一份申请，然后再决定它们是否值得更详细的阅读。因此，你的简历吸引他们的眼球是至关重要的。 
这里最好的方法是仔细阅读职位描述，弄清楚整个“愿望清单”和公司对候选人的要求。基于这些事实，在你的简历中加入并突出这些想要的技能。在工作描述中，只要有可能，就要使用相关的关键词；这将有助于你的简历通过公司可能使用的任何求职者跟踪系统(ATS ),并到达人们的眼前以供进一步考虑。 
除了仔细阅读职位描述，浏览一些关于公司本身的信息也是一个不错的主意。探索该公司的网站，它的使命，价值观，社交媒体，产品和服务，这样你就可以更好地了解这个雇主在一个成功的候选人身上寻找什么。让你的简历显示出你是这家公司的最佳人选，拥有帮助公司发展的所有潜力。每个公司寻找的不是想要任何工作的专家，而是想要这份工作的人。 
简洁但信息丰富 
现在，您已经确切地知道了应该在数据科学家简历中包含哪些内容，您应该重新加工和调整这些信息，使其简洁、精确、相关且表达清晰。不要包括任何可能不会给这个角色增加额外价值的东西，也不要包括工作描述中没有提到或暗示的东西。保持你的简历简洁、有目的、信息丰富。任何项目符号语句都应该简短，不要跨越几行。 
遵循逆时间顺序 
教育和工作经历部分最常见和首选的格式是倒序排列。最近的经历应该放在最上面，然后是倒数第二个，依此类推。这让招聘经理可以快速跟踪你的职业发展和最新成就。 
使用简单但有效的语言 
使用简单明了的语言是展示你沟通技巧的另一种方式。为了有效地做到这一点，请记住以下建议: 
不要让你的数据科学家简历充斥着技术术语。虽然你申请的工作岗位可能需要高水平的技术技能和知识，但请记住，第一个看你简历的人将是招聘经理或招聘人员。取决于谁是公司的招聘主管，他们可能有不同的背景，因此可能会发现太多的技术术语令人困惑。因此，确保你写的东西让每个人都能理解你对公司的潜在价值。以工作描述为指导；如果它充满了行话，那么你可以听起来更专业一点。否则，尽量只包括广告中提到的主要算法和技术。用简洁的句子写。保持简历的时态一致。避免夸大其词。用帮助代替便利，用使用代替利用，用管理代替管理，用复杂代替复杂，等等。当描述你的工作经历、项目和成就时，使用有意义的动词。例如，用构建的、自动化的、优化的等来代替诸如工作的、制造的或参与的动词。我们将在本文后面更详细地讨论这个技巧。替换最高级(最高、最好、最重要等。)和看似强大但实际上信息不丰富的形容词，如有经验的、强大的、可观的、高效的，以及具体的指标和结果。使用这些强有力的词语(有经验的、深刻的、经过验证的等等。)在你简历的总结部分保留。请一位非技术背景的朋友阅读你的数据科学家简历。请他们给你反馈:是否足够容易理解？语言是否清晰简单？你成功传达了你的主要职业成就了吗？ 
检查错误和打字错误 
显然，较小的错误、愚蠢的错误和打字错误会给招聘人员留下负面印象，导致他们拒绝你的简历。使用专门的在线服务(语法或类似服务)仔细检查你的语法和拼写，并请人检查你简历中的任何打字错误或错误。 
将您的数据科学家简历发送给一个真实的人 
在进入招聘人员手中之前，简历通常会经过一个名为申请人跟踪系统(ATS)的机器学习程序，该程序会决定它是否值得传递给人类或应该被拒绝。要绕过这一步，试着把你的简历直接寄给招聘经理，甚至是负责该职位部门的技术经理。 
如何安排你的数据科学家简历的整体结构 
 
简历中应该包括哪些部分 
按照从上到下的推荐顺序，数据科学家简历的典型部分包括: 
联系信息总结(或目标)工作经验(或者仅仅是经验)项目技能教育附加部分 
这些标题的顺序可以相对灵活，并且应该反映每个部分的重要性，所以第一个标题应该比最后一个标题占据更多的空间。在两栏简历设计的情况下(首选)，记住招聘人员通常从左上角到右下角浏览简历。所以，确保你把最相关的信息放在页面的左上角。 
如何确定各部分的最佳顺序 
各部分的最佳顺序，尤其是工作经验、项目、技能和教育，取决于两个因素: 
你的真实经历。在这里我们可以找到不同的场景: 
如果你有很长的相关工作经验记录，优先考虑简历中相应的部分，大幅减少关于你的教育信息，甚至可能排除你的额外项目。如果你是一名经验有限的应届毕业生，把教育部分放在第一位，添加关于你的最终投票的信息(当然，只有当他们有利高的时候)，并更详细地描述你的课程工作或论文。如果你在数据科学方面的经验有限，并且主要从事自由职业者的项目，交换项目和工作经验(或者甚至将它们合并成一个部分的经验)，包括你有过的任何与数据相关的实习经历(如果有的话)，并详细说明你的技能。如果你是从另一个领域进入数据科学的职业改变者，专注于项目和技能，少写你的工作经历和教育，尽管不完全省略这些部分是明智的。 公司简介和职位描述。很符合逻辑的是，如果你在学术环境中申请工作，更多地扩展你的教育、证书、额外的课程和科学出版物。否则，多暴露一些商业导向的经验和成就。特别是对于初创项目，潜在候选人的宝贵品质是独立工作的能力、积极主动和具有创新思维。在这种情况下，展示更多的单个项目可能是一种优势。 
你的简历中还包括哪些内容 
对于您的数据科学家简历，请考虑以下附加部分: 
证书出版物会议黑客马拉松奖金；奖品竞争自愿参军语言 
如果每一部分至少有两个要点，你可以分别介绍其中的任何一个。例如，如果您完成了三门数据科学课程，并在两个与数据相关的会议上发表了演讲，那么添加认证和会议部分是有意义的。否则，如果你对某些类别只有一个要点(例如，一个会议、一个出版物和一个黑客马拉松)，最好将这些成就合并到一个叫做 Extras 的部分。 
避免冗长的副标题 
命名副标题时，尽可能简洁: 
相关工作经验工作经验(或仅仅是经验)职业经历工作经历(或经历)著名项目项目技能和知识技能技能和工具技能执照和证书证书附加信息附加信息 
如何正确填写您的联系方式 
 
你的联系信息最适合放在简历的顶部，尽管有些在线模板把它放在页面的底部(如果你选择的模板是这种情况，就手动把它拖到顶部)。这一部分最关键的要求是信息的准确性。 
以下是您需要在联系信息中包含的内容: 
你的全名。你的职位。在你的全名下面，写上你申请的职位名称，而不是你现在的职位名称。您可以从确切的职位描述中复制职位名称，或"
KDNuggets 博客中文翻译（九十五）,2024-10-28,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
50+ 个数据科学、机器学习备忘单，已更新 
 
 原文：www.kdnuggets.com/2016/12/data-science-machine-learning-cheat-sheets-updated.html 
 
 评论 
本文更新了 Bhavya Geethika 的一篇非常受欢迎的文章 50+ 个数据科学、机器学习备忘单。如果我们遗漏了一些受欢迎的备忘单，请在下方评论中添加。 
 
Python、R 及 Numpy、Scipy、Pandas 的备忘单 
 
我们的前三名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析能力 
 3. Google IT 支持专业证书 - 支持你的组织 IT 需求 
 
数据科学 是一个多学科领域。因此，在数据科学世界中有成千上万的包和数百个编程函数！有志的数据爱好者无需了解所有内容。备忘单或参考卡是一个汇总了最常用命令的工具，帮助你更快地学习该语言的语法。以下是一些经过深思熟虑并总结在几个紧凑页面中的最重要备忘单。 
精通数据科学涉及统计学、数学、编程知识，尤其是 R、Python 和 SQL，然后将这些结合起来，利用业务理解和人类直觉——这是驱动决策的终极因素。 
以下是按类别划分的备忘单： 
Python 备忘单： 
Python 是初学者的热门选择，但仍然足够强大，可以支持一些世界上最受欢迎的产品和应用程序。其设计使编程体验几乎像用英语书写一样自然。Python 基础或 Python 调试器备忘单覆盖了入门所需的重要语法。社区提供的库，如 numpy、scipy、sci-kit 和 pandas，广泛依赖，而 NumPy/SciPy/Pandas 备忘单提供了这些库的快速复习。 
 Python 2.7 快速参考表DaveChild 的 Python 备忘单 来自 cheatography.comPython 基础参考表 来源：cogsci.rpi.eduPython 调试器备忘单NumPy / SciPy / Pandas 备忘单OverAPI.com Python 备忘单Python 3 备忘单 作者：Laurent PointalPython 语言与语法备忘单DataCamp 的 Python Pandas 备忘单
R 的备忘单： 
R 的生态系统扩展如此之大，以至于需要大量的参考。R 参考卡涵盖了 R 世界的大部分内容。Rstudio 还发布了一系列备忘单，以便为 R 社区提供便利。ggplot2 的数据可视化似乎是一个受欢迎的选择，因为它在你创建结果图表时非常有用。 
在 cran.r-project.org 上： 
 R 回归分析函数R 简短参考卡R 时间序列分析函数数据挖掘 R 参考卡
在 Rstudio.com 上： 
 互动网页应用备忘单使用 ggplot2 的数据可视化备忘单使用 devtools 进行包开发的备忘单数据整理备忘单R markdown 备忘单，第二部分
其他： 
 R 数据管理备忘单R 图形参数的备忘单R 备忘单DataCamp 的 数据分析（data.table 方法）
MySQL 和 SQL 的备忘单： 
对于数据科学家来说，SQL 的基础知识与其他任何语言一样重要。PIG 和 Hive 查询语言与 SQL——原始结构化查询语言密切相关。SQL 备忘单提供了一个 5 分钟的快速指南，然后你可以深入探索 Hive 和 MySQL！ 
 Dave Child 的 MySQL 备忘单SQL 备忘单一页 SQLMySQL 参考指南可视化 SQL 连接SQL 初学者指南 备忘单
Spark、Scala、Java 备忘单： 
Apache Spark 是一个用于大规模数据处理的引擎。对于某些应用，如迭代机器学习，Spark 可以比 Hadoop（使用 MapReduce）快多达 100 倍。Apache Spark 备忘单解释了它在大数据生态系统中的位置，讲解了基本的 Spark 应用程序的设置和创建，并解释了常用的操作和动作。 
 Dzone.com 的 Apache Spark 参考卡Scala 备忘单 1Scala 备忘单 2DZone.com 的 Scala 参考卡GitHub 上的 Spark 备忘单Openkd.info 的 Scala on Spark 备忘单Java 备忘单 由 MIT.edu 提供Java 备忘单 由 Princeton.edu 提供
Hadoop 和 Hive 备忘单： 
Hadoop 作为一种非传统工具，解决了被认为无法解决的问题，通过提供一个开源软件框架来进行大规模数据的并行处理。探索 Hadoop 备忘单以发现使用 Hadoop 命令行时的有用命令。SQL 和 Hive 函数的组合是另一个值得关注的内容。 
 Hadoop 初学者指南备忘单入门 Apache Hadoop 参考卡Hadoop 命令行备忘单从命令行使用 HDFS - Hadoop 备忘单Hive 函数备忘单SQL 转 Hive 备忘单
Django Web 应用框架备忘单： 
Django 是一个免费的开源 Web 应用框架，使用 Python 编写。如果你是 Django 新手，可以查看这些备忘单，快速了解概念，并深入学习每一个。 
 Django 备忘单 v.1Django 备忘单第一部分，第二部分， 第三部分，第四部分Django 参考备忘单Django 快速入门指南及备忘单
机器学习备忘单： 
我们经常花时间思考哪种算法最好？然后再回到我们的厚厚的书本中查找参考！这些备忘单提供了关于数据的性质和你正在解决的问题的思路，然后建议你尝试某种算法。 
 机器学习备忘单 于 scikit-learn.orgScikit-Learn 备忘单：Python 机器学习 来自 yhat（由 GP 添加）预测学习模式备忘单 于 Dzone.com微软 Azure 的机器学习算法备忘单方程式和技巧 机器学习备忘单在 Github.com监督学习迷信 Github.com 的备忘单R 语言的十种机器学习算法备忘单
Matlab/Octave 备忘单 
MATLAB（MATrix LABoratory）由MathWorks于 1984 年开发。Matlab d 一直是学术界用于数值计算的最流行语言。它适用于处理几乎所有可能的科学和工程任务，配备了多个高度优化的工具箱。MATLAB 不是开源工具，但有一个免费的GNU Octave 重新实现版，它遵循相同的语法规则，使大多数代码与 MATLAB 兼容。 
 麻省理工学院 2007 基本函数 Matlab 备忘单统计学和机器学习 Matlab 备忘单
跨语言参考备忘单 
矩阵函数 MATLAB/Octave Python NumPy, R, Julia 
相关： 
 50+ 数据科学与机器学习备忘单数据科学备忘单指南按受欢迎程度排名的前 20 个 R 包
更多相关话题 
 停止学习数据科学以寻找目标，寻找目标去…学习数据科学统计的顶级资源建立一个稳固的数据团队成功数据科学家的 5 个特征成为优秀数据科学家所需的 5 项关键技能每个初学者数据科学家都应该掌握的 6 种预测模型
数据科学与机器学习：免费电子书 
 
 原文：www.kdnuggets.com/2020/12/data-science-machine-learning-free-ebook.html 
 
评论 
我们已经有一段时间没有与读者分享免费电子书了，但本周我们发现了另一部值得关注的作品，并希望在假期学习季节（这绝对是一个存在的事情）来临之前与大家分享。 
今天我们分享**数据科学与机器学习：数学与统计方法**，由 D.P. Kroese、Z.I. Botev、T. Taimre 和 R. Vaisman 合著。该书去年出版，除了可以免费下载 PDF 外，还可以购买印刷版（和 Kindle 版）。 
 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
数据科学与机器学习：数学与统计方法是一本以实践为导向的教材，重点介绍使用 Python 进行数据科学和实施机器学习模型。它很好地解释了相关理论，并根据需要介绍了必要的数学，结果使得这本实用书的节奏非常适中。 
根据其网站，这本书的raison d’être 实际上与我的看法有些不符： 
 
 本书的目的是提供一本易于理解但内容全面的教科书，旨在帮助学生更好地理解数据科学中丰富的思想和机器学习算法背后的数学和统计学。 
 
我相信这是同一枚硬币的另一面：当我看到这本书的优势在于教授实用技巧并用必要的理论和基础数学加以强化时，可以明确地说，它实际上专注于理论和基础数学，并通过实际应用来强化这些内容。 
我会说可能平分秋色。 
无论你支持哪种方法，这本书的目录如下： 
 导入、总结和可视化数据统计学习蒙特卡洛方法无监督学习回归分析正则化和核方法分类决策树和集成方法深度学习
这里涵盖了许多相关主题，并且逻辑顺序合理。我特别喜欢从蒙特卡罗方法过渡到无监督学习的过程，以及这种过渡发生在引入监督学习概念之前。尽管分类在长远来看可能更有用（至少目前似乎如此），但我第一次接触机器学习时，分类的影响远远不如聚类，因此在我看来，其先行引入可能对其他新学习者同样具有吸引力。 
为确保本书对即使是最新的数据科学和机器学习学生也足够自给自足，本书包括了足够且有用的附录： 
 线性代数与泛函分析多变量微分与优化概率与统计Python 基础
通过阅读这本书，你不会成为一个完整的数据科学专家，但这并不是它的目标。通过学习《数据科学与机器学习：数学与统计方法》，你将获得该领域基础知识的坚实基础，在此基础上可以添加更多前沿的方法和算法。 
我最喜欢的机器学习书籍之一，是我首次接触该主题时使用的书籍，名为《数据挖掘：实用机器学习工具与技术》，也称为 Weka 书。我作为初学者非常喜欢它如何将实践和理论结合起来，根据需要介绍和解释数学，以学习当时呈现的实际实施。我发现这本书有类似的格式，具有使用 Python 代替 Weka 工具包的优势，至少在今天，Python 的实现路径要更为相关。 
我推荐这本书给任何学习数据科学和机器学习基础知识的人，并且希望按照所描述的展示格式进行学习。 
相关： 
 理解机器学习：免费的电子书统计学习导论：免费的电子书数据挖掘与机器学习：基本概念与算法：免费的电子书
更多相关主题 
 机器学习数学：免费的电子书通过这本免费的电子书学习数据清理和预处理超级学习指南：免费算法与数据结构电子书命令行下的数据科学：免费的电子书掌握生成式人工智能和提示工程：免费电子书数据科学家的认知偏见指南：免费电子书
数据科学，机器学习：2017 年的主要发展及 2018 年的关键趋势 
 
 原文：www.kdnuggets.com/2017/12/data-science-machine-learning-main-developments-trends.html 
 
 评论今年我们再次询问了该领域的杰出领导小组 
 
 数据科学、机器学习、预测分析的主要发展是什么？你期望 2018 年有哪些关键趋势？ 
 
主要主题包括人工智能和深度学习——这两者既有实际进展也有炒作，机器学习，安全，量子计算，AlphaGo Zero，等等。阅读 Kirk D. Borne、Tom Davenport、Jill Dyche、Bob E. Hayes、Carla Gentry、Gregory Piatetsky-Shapiro、GP Pulipaka、Rexer Analytics 团队（Paul Gearan、Heather Allen 和 Karl Rexer）、Eric Siegel、Jeff Ullman 和 Jen Underwood 的观点。 
还可以阅读大数据：2017 年的主要发展及 2018 年的关键趋势和去年的预测。 
有不同的观点？在文章末尾评论。 
 
 
Kirk D. Borne， @KirkDBorne，BoozAllen 首席数据科学家，博士天体物理学家。顶级数据科学和大数据影响者。 
2017 年我们看到大数据让位于人工智能成为技术炒作周期的中心。这种对人工智能的过度媒体和从业者关注包括了正面消息（越来越强大的机器学习算法和人工智能应用于众多行业，包括汽车、医学影像、安全、客户服务、娱乐、金融服务）和负面消息（机器威胁到我们的工作和接管我们的世界）。我们还见证了围绕数据的价值创造创新增长，包括更大规模的 API 使用、即服务产品、数据科学平台、深度学习以及主要供应商的云机器学习服务。数据、机器学习和人工智能的专业应用包括机器智能、规范分析、旅程科学、行为分析和物联网。 
在 2018 年，我们应看到超越人工智能炒作的趋势。将是验证人工智能价值、衡量其投资回报率并使其可操作的时候。我们将看到这些发展领域与 2017 年的重点领域没有太大不同，包括流程自动化、机器智能、客户服务、超个性化和劳动力转型。我们还将见证物联网的成熟度增长，包括更高的安全特性、模块化平台、访问传感器数据流的 API 和边缘分析接口。同时，我们可能会看到数字双胞胎在制造、公共事业、工程和建筑行业变得更加主流。我还相信，在 2018 年，将有更多从业者迎接挑战，向怀疑的公众传达人工智能的积极益处。 
 
汤姆·达文波特，是巴布森学院的 IT 和管理学杰出教授，国际分析学会的共同创始人，麻省理工学院数字经济倡议的研究员，以及德勤分析的高级顾问。 
2017 年的主要发展： 
 企业人工智能进入主流： 许多大型、成熟的公司正在进行人工智能或机器学习项目。一些公司有超过 50 个使用各种技术的项目。这些大多是“低垂的果实”项目，目标相对有限。趋势是远离拥有“变革性”产品的大型供应商，转向开源、自己动手类型的项目。当然，这意味着公司必须招聘或培养高水平的数据科学技能。机器学习应用于数据集成： 数据分析和管理中最古老的挑战现在正通过机器学习得到解决。集成和整理数据的劳动密集型方法正被“概率匹配”技术所取代——或至少被增强——通过对不同数据库中的相似数据元素进行匹配。这个工具的使用——通过工作流和专家众包来增强——可以将数据集成的时间减少十倍。 保守型公司接受开源： 传统上保守的公司，如银行、保险和医疗行业的公司，如今正积极接受开源分析、人工智能和数据管理软件。一些公司积极鼓励员工避免使用专有工具，而其他公司则让个人自由选择。成本是转变的一个原因，但性能的提升和吸引最近大学毕业生更为普遍。
2018 年的发展： 
我们已经进入了“后算法”时代：历史上，分析师和数据科学家需要对使用何种算法有相当的了解。但分析和机器学习过程的自动化使得可以考虑一百种或更多不同的算法。重要的是模型或模型集合的表现如何。这当然促进了“公民数据科学家”的崛起。尽管这种发展可能最终会有一些令人担忧的故事，但目前还没有出现。独立 AI 初创公司的吸引力开始减弱： 在风险资本资助的刺激下，过去几年成立了数百家 AI 初创公司。大多数解决相对狭窄的问题。然而，即使它们有效，集成到现有的流程和系统中仍然是大多数组织的主要挑战。因此，成熟的公司更倾向于开发自己相对易于集成的 AI“微服务”，或从那些将 AI 内容嵌入其交易系统的供应商那里购买。
 
吉尔·迪奇、@jilldyche，SASBestPractice 的副总裁。畅销商业书籍作者。 
现在几乎每个人和他们的兄弟都有人工智能或机器学习的产品。2017 年证明了亮眼的新物品被重新打磨，许多供应商——其中许多甚至不是我所谓的“AI 邻近”——准备好擦亮和展示他们的产品集。讽刺的是，由于其新颖性，许多这些供应商可能会超越已有的产品。 
2018 年将会看到更多关于 AI/ML 的商业对话和使用案例。为什么？因为管理者，大多数是有问题要解决的商业人士，不在乎神经网络是否在稀疏数据上挣扎。他们对自然语言处理中的词汇推理挑战不感兴趣。他们想要加速供应链，知道客户将会做什么/购买什么/说什么，并且仅仅告诉计算机他们想知道什么。这是大规模的处方分析，能够以最小的采用摩擦来提供这些服务的供应商可以主宰世界。 
 
卡拉·根特里，Analytical Solution 的数据科学家， @data_nerd。 
 2017 年是大家开始讨论机器学习、人工智能和预测分析的年份，不幸的是，许多这些公司/供应商只是“追逐流行词”，并没有真正的背景去实现他们所宣传的……如果你查看任何上述提到的“热门话题”在 Twitter 上的讨论，你会发现一堆来自去年谈论社交媒体营销的同一群人的帖子！这些领域的经验需要时间和才能，而不仅仅是“行动号召”和流行词……如常所说，经验确实重要！ 
我认为 2018 年是我们关注数据科学和预测分析领域领导者的一年，这不仅仅是因为它很流行，而是因为它能对你的业务产生巨大的影响。预测招聘可以节省数百万的人员流动成本；人工智能和机器学习可以在几秒钟内完成你以前需要几天的工作！我们同意技术可以带我们达到新的高度，但也要记得做一个有良知的人。作为数据科学家或算法编写者，你有责任避免造成伤害，无论是法律上的，还是在人性和伦理实践上的，要保持透明和公正。 
 
Bob E. Hayes @bobehayes，研究员和作家，《Business Over Broadway》的出版者，拥有工业-组织心理学博士学位。 
数据科学和机器学习能力的实践正被越来越多的行业和应用所采纳。 
在 2017 年，我们见证了人工智能能力的重大进展。虽然之前的深度学习模型需要大量数据来训练算法，但神经网络和强化学习的使用表明，创建高性能算法不再需要数据集。DeepMind 运用了这些技术，并通过自我对弈创建了 Alpha Go Zero，这一算法超越了之前的算法。 
随着人工智能在刑事司法、金融、教育和工作场所等领域的持续增长，我们需要为算法建立标准，以评估它们的准确性和偏差。对人工智能社会影响的关注将继续增长（见这里和这里），包括制定人工智能可以使用的规则（例如，避免“黑箱”决策）以及理解深度学习算法如何做出决策。 
 即使在互联网时代诞生的公司（例如 imgur、Uber）中，安全漏洞也将继续上升。因此，我们将看到对安全方法进行彻底改革的努力，增加区块链（一个虚拟账本）的可见性，作为改善公司如何保护其客户数据的可行方式。 
 
Gregory Piatetsky-Shapiro，KDnuggets 的总裁，KDnuggets的创始人，数据科学家，KDD 会议和 SIGKDD 的联合创始人，知识发现和数据挖掘的专业组织。 
2017 年的主要发展： 
AlphaGo Zero 可能是 2017 年人工智能领域最重要的研究突破数据科学的自动化程度不断提高，更多工具提供自动化机器学习平台。人工智能的炒作和期望的增长速度甚至超过了人工智能和深度学习的成功增长。
2018 年的关键趋势 
 GDPR（欧洲通用数据保护条例）于 2018 年 5 月 25 日生效，将对数据科学产生重大影响，其要求包括解释权（你的深度学习方法能否解释为什么这个人被拒绝了信用？）以及防止偏见和歧视。谷歌 DeepMind 团队将跟进 AlphaGo Zero 的惊人成果，并在一个几年前许多人认为计算机无法完成的任务上取得另一个超人类表现。 （注意：下一个 DeepMind 的突破发生在 2017 年 12 月，AlphaZero在仅 4 小时内掌握了国际象棋，该自我学习程序在国际象棋、围棋和将棋中达到了超人类表现。）我们将看到更多的自动驾驶汽车（和卡车）进展，包括首次出现的问题（如拉斯维加斯的自动驾驶班车不知道要让路）被解决（下次它会让路）。人工智能泡沫将继续，但我们将看到震荡和整合的迹象。
 
Dr. GP (Ganapathi) Pulipaka， @gp_pulipaka 是 DeepSingularity LLC 的首席执行官兼首席数据科学家。 
机器学习、深度学习、数据科学发展，2017 年 
 AlphaGo Zero 引入了一种新的强化学习形式，其中 AlphaGo 成为自己的教师，无需人工干预和历史数据集。 Python（1.65 M GitHub 推送）、Java（2.32 M GitHub 推送）和 R（163,807 GitHub 推送）是 2017 年最受欢迎的编程语言。在 CPU 上处理大规模大数据以执行神经网络功能可能会在长期内带来巨大的能源成本。谷歌发布了第二代 TPU。TPU 的精密设计工程包括将协处理器附加到常见的 PCIe 总线，并附有处理流量的指令，使用乘法累加器（MACs）重用寄存器中的值进行数学计算，并节省数十亿美元的能源。Nvidia 发布了基于 Volta 架构的 Tesla GPU，用于超级充电深度学习和机器学习，每个 GPU 的峰值性能达到 120 万亿次浮点运算。从 D-Wave 量子退火计算机的炒作中脱身，转向具有 20 量子比特能力的量子计算机，使用 QISKit 量子编程栈在 Python 中进行编程。
机器学习、深度学习、数据科学趋势 - 2018 年。 
 McAfree Labs 2018 年的威胁研究报告显示，对抗性机器学习将用于网络入侵检测、欺诈检测、垃圾邮件检测和恶意软件检测，在无服务器环境中以极高的机器速度进行。HPE 将开发点积引擎，并推出自己的神经网络芯片，用于高性能计算，支持来自深度神经网络、卷积神经网络和递归神经网络的推理。 量子机器学习的未来依赖于具有 10 个或更多状态和 100 多个维度的量子比特，而不是只能采用两种可能状态的量子比特。将制造出大量带有量子比特的微芯片，从而创造出极其强大的量子计算机。2018 年物联网和边缘计算与机器学习的障碍将降低。地理空间智能将在应用于手机、RFID 传感器、无人机、无人机和卫星的突破性算法的推动下跃升。自我监督学习和自主学习将为机器人提供动力，采用新颖的深度学习技术进行控制任务，使机器人能够与其周围的地面环境和水下环境进行交互。
 
Paul Gearan、Heather Allen 和 Karl Rexer，Rexer Analytics的负责人，该公司是一家领先的数据挖掘和高级分析咨询公司。 
尽管商业智能软件的普及和有效使用仍面临许多障碍，但对于没有研究或分析背景的人来说，这一承诺仍面临许多困难。虽然像 Tableau、IBM 的 Watson、Microsoft Power BI 等软件已经取得了一些进展，但根据 Rexer Analytics 在 2017 年收集的数据，仅有略超过一半的受访者表示自助工具正在被数据科学团队之外的人使用。当这些工具被使用时，大约 60%的时间会报告遇到挑战，最常见的主题是对分析过程的理解失败和结果的误解。 
对于 2018 年，实现这些“公民数据科学”工具的承诺，以扩展分析的使用和力量，产生有效且有意义的结果至关重要。正如我们经验所示，综合多学科团队的方法仍然是最佳选择：为没有数据分析训练的员工和高管提供探索和可视化假设的工具很重要。但同样重要的是，团队要与经过专业训练的数据科学专家一起开发模型和解释结果，以理解特定分析技术的应用和局限性。 
 
Eric Siegel, @predictanalytic，预测分析世界会议系列创始人。 
在 2017 年，机器学习中的三个高速趋势持续全速前进，我预计它们在 2018 年也会如此。其中两个是积极的，另一个则是玫瑰的不可避免的刺： 
1) 机器学习的商业应用在各个行业中的采纳范围持续扩大 —— 例如在市场营销、金融风险、欺诈检测、员工优化、制造业和医疗保健等领域。要一目了然地了解这些广泛的活动范围以及哪些领先公司正在实现价值，请查看预测分析世界 2018 年 6 月拉斯维加斯会议，这是首个“Mega-PAW”以及全美唯一的 PAW 商业会议。 
2) 深度学习蓬勃发展，无论是热度还是实际价值。 这套相对新的先进神经网络方法将机器学习提升到一个新的潜力层次——即在大信号输入问题上实现高性能，如图像分类（自动驾驶汽车、医学图像）、声音（语音识别、说话人识别）、文本（文档分类），甚至是“标准”业务问题，例如，通过处理高维点击流。为了推动其在各行业领域的商业化部署，我们将推出深度学习世界，配合 PAW Vegas 2018。 
3）不幸的是，人工智能仍然被过度宣传和“过度神化”（这个双关语归功于模型机构的埃里克·金 😃。虽然专家从业者有时使用术语 AI 特指机器学习，但分析供应商和记者更常用它来暗示那些明显不切实际的能力，并培养更多幻想而非现实的期望。正如亚瑟·C·克拉克曾经著名地提出，“任何足够先进的技术都与魔法无异”，但这并不意味着我们想象或在科幻小说中包含的任何“魔法”都可以或将被技术实现。你的逻辑是倒置的。人工智能将拥有自己的意志，可能会恶意或鲁莽地对人类构成生存威胁，这是一种鬼故事——这种鬼故事进一步推动了机器的拟人化（甚至是神化），许多供应商似乎希望这能提高销售。朋友们、同事们和国人们，我敦促你们减少对“人工智能”的关注。这只会增加噪音和混乱，最终会引发反弹，就像所有“虚幻软件”销售一样。 
 
杰夫·乌尔曼，斯坦福大学 W·阿施曼计算机科学荣誉教授。他的研究兴趣包括数据库理论、数据库集成、数据挖掘以及利用信息基础设施的教育。 
最近我参加了一次会议，与我的两位最老的同事约翰·霍普克罗夫特和阿尔·阿霍重聚。（编辑：见经典教科书数据结构与算法，作者：阿霍、霍普克罗夫特和乌尔曼）。在我的演讲中，我没有什么新内容，但阿尔和约翰都在关注一些可能对 KDnuggets 读者真正感兴趣的事情。 
约翰（霍普克罗夫特）谈到了深度学习算法的分析。他做了一些实验，观察了在不同顺序下对相同数据进行系列训练时网络节点的行为（或类似实验）。他发现了在每个生成的网络中，某些节点基本上做着相同的事情的情况。还有其他情况，其中无法将节点与节点映射，但一个网络中的小组节点与另一个网络中的另一个节点组产生相同的效果。这项工作还处于起步阶段，但我认为可能会在明年继续保持的一个预测是： 
 
 对深度学习网络的细致分析将推进对深度学习如何真正工作的理解，以及其用途和陷阱。 
 
然后，Al Aho 谈到了量子计算。许多全球最大的公司，如 IBM、微软、谷歌，都在投入大量资金建设量子计算机。对于这些设备有许多不同的方法，但 Al 兴奋的是他的前学生在微软的工作，他正在开发一套编译器和模拟器，用于设计量子算法并进行测试，虽然不是在一个真正存在的机器上，而是在模拟器上。这让我想起了 1980 年代的集成电路设计工作。当时我们也有从高级语言到电路的编译器，这些电路先是被模拟，而不是制造（至少在最初是这样）。这种方法的优势在于你可以尝试不同的算法，而不需要花费巨大的物理电路制造费用。当然，在量子世界中，这不仅仅是“缓慢且昂贵”，而且可能“根本不可能”，我们目前还不知道。Al 实际上和我一样对量子计算是否很快实现持怀疑态度，但毫无疑问资金将会投入，算法将会设计。例如，Al 指出去年在更高效的量子算法用于线性代数方面取得了有趣的进展，如果实现，这肯定会引起数据科学家的兴趣。所以这是另一个预测： 
 
 量子计算，包括数据科学中的算法，将在未来几年受到更多关注，即使真正能够在足够大规模下运行的量子计算机从未实现或还需要几十年。 
 
我会再添加一个我自己更平凡的预测： 
从 Hadoop 迁移到 Spark 的趋势将继续，最终导致人们几乎忘记 Hadoop。 
 
Jen Underwood、@idigdata、Impact Analytix, LLC 的创始人，是一位公认的分析行业专家，拥有产品管理、设计和超过 20 年的“动手”数据仓库、报告、可视化及高级分析解决方案开发经验。 
当我回顾 2017 年时，我会温馨地记住那一年智能分析平台的出现。从分析机器人到自动化机器学习，数据科学的各个方面都出现了大量复杂的智能自动化能力。数据集成和数据准备平台已经变得足够智能，可以即插即用数据源，在数据管道出现错误时自行修复，甚至根据从人类互动中学到的知识自行管理维护或数据质量任务。增强分析产品开始兑现将机器学习普及的承诺。最后，拥有预打包的最佳实践算法设计蓝图和部分自动化特征工程能力的自动化机器学习平台在数字时代的分析武器库中迅速成为游戏规则改变者。 
明年我预计自动化人工智能将在更多的分析和决策过程中无缝整合。随着组织的适应，我预计会出现大量关于如何做出自动化决策的问题，以及如何在我们不完美的世界中负责任地引导这些系统。即将到来的欧盟通用数据保护条例合规截止日期将进一步提高我们需要打开分析黑箱、确保正确使用并尽职管理个人数据的需求。 
相关内容： 
 大数据：2017 年的主要发展和 2018 年的关键趋势数据科学、预测分析在 2016 年的主要发展及 2017 年的关键趋势2017 年预测
更多相关话题 
 AI、分析、机器学习、数据科学、深度学习……2021 年的主要发展和 2022 年 AI、数据科学等的关键趋势数据科学与分析行业在 2021 年的主要发展和关键…成为优秀数据科学家所需的 5 项关键技能每个初学者数据科学家应掌握的 6 种预测模型2021 年最佳 ETL 工具
企业的数据科学与机器学习平台 
 
 原文：www.kdnuggets.com/2017/05/data-science-machine-learning-platforms-enterprise.html 
 
评论 
作者：Ahmad AlNaimi，Algorithmia。 
 
TL;DR 一个具有弹性的 数据科学平台 对于大型公司中的集中式数据科学团队是必需的。它帮助他们在 PB 规模下集中、重用和生产化他们的模型。我们为此目的构建了Algorithmia Enterprise。 
 
你已经构建了那个 R/Python/Java 模型。它运行良好。接下来呢？ 
 
 “一切始于你的 CEO 听说了机器学习和数据是新石油的说法。数据仓库团队有人刚刚提交了 1PB Teradata 系统的预算，而 CIO 听说 Facebook 使用便宜的 Hadoop 商品存储，超级便宜。一场完美的风暴爆发了，现在你被要求建立一个数据优先的创新团队。你雇佣了一组数据科学家，大家都很兴奋，开始找你寻求一些数字魔法来‘Googlify’他们的业务。你的数据科学家没有任何基础设施，花费所有时间为高管构建仪表板，但投资回报是负面的，大家都怪你没有在他们的盈亏表上倾注足够的独角兽血。” – Vish Nandlall (source) 
 
在 PB 规模上共享、重用和运行模型不是数据科学家的工作流程的一部分。这种低效在企业环境中被放大，因为数据科学家需要与 IT 协调每一步，持续部署是一团糟（如果不是不可能的话），重用性低，随着公司不同部门开始“Googlify”他们的业务，问题越来越严重。 
数据科学与机器学习平台旨在满足这种需求。它作为一个基础层，供三个内部利益相关者协作：产品数据科学家、中央数据科学家和 IT 基础设施。 
 
图 1：一个数据科学平台服务于三个利益相关者：产品、中央和基础设施。它对于大型公司来说是必需的，这些公司对机器学习有复杂且不断增长的依赖。 
在这篇文章中，我们将涵盖： 
 谁需要 数据科学与机器学习 (DS & ML) 平台？什么是数据科学与机器学习平台？如何区分平台？平台的例子
你需要一个数据科学平台吗？ 
这并不适合每个人。拥有一个或两个用例的小团队最好是围绕共享和扩展自己开发解决方案（或使用私有托管解决方案）。如果你是一个有许多内部客户的中央团队，你很可能正在遭受以下一种或多种症状： 
症状 #1 你在拆分代码库 
你的数据科学家创建了一个模型（假设是用 R 或 Python 编写的），并希望将其接入生产环境，以便作为 Web 或移动应用的一部分使用。你的后端工程师们，使用 Java 或 .NET 构建了他们的基础设施，最终会在他们选择的技术栈中重新编写这个模型。现在你有两个代码库需要调试和同步。随着时间的推移，当你构建更多模型时，这种低效率会加剧。 
症状 #2 你在重新发明轮子 
无论是像预处理函数这样的小功能，还是像完整训练模型这样的大功能。你的团队产出的越多，当前团队成员、过去团队成员，特别是项目之间系统性重复工作的可能性就越大。 
症状 #3 你在努力招募最佳人才 
你公司每个角落都有数据科学或机器学习的想法，以保持领先，但你只有少数几个天才专家，他们一次只能处理一个挑战。你会雇佣更多的人，但数据科学和机器学习人才稀缺，顶尖人才像顶级 NFL 四分卫一样昂贵。 
症状 #4 你的云账单激增（P2 实例太多了！） 
你已经将模型部署在一个网络服务器后面。在深度学习的世界里，你可能会需要一个支持 GPU 的机器，比如 AWS EC2 上的 P2 实例（或 Azure N-Series 虚拟机）。为每一个生产化的深度学习模型运行这些机器可能会迅速变得昂贵，特别是对于负载波动较大或难以预测的模式。 
什么是数据科学与机器学习平台？ 
这涉及到除训练之外的一切。数据科学与机器学习平台关注的是模型在训练阶段之后的生命周期。这包括：模型的注册，显示模型从一个版本到下一个版本的传承，将它们集中起来以便其他用户可以找到它们，并将它们作为自包含的工件提供，这些工件可以随时插入任何数据管道中。 
 
库 vs. 注册表 
像 scikit-learn 和 Spark MLlib 这样的工具包含了一系列独特的算法。这就是一个库。数据科学与机器学习平台则是一个注册表。它包含来自不同来源的多个算法实现，每个算法都有自己的版本（或传承），这些实现同样是可发现和可访问的。注册表的用户将能够轻松找到并比较不同算法实现的输出。 
训练与推理 
数据科学家会使用适合问题的工具。有时这些工具是 scikit-learn 和 Keras 的组合，是 Cafe 和 Tensorflow 模型的集成，或者是用 R 编写的 H2O 脚本。一个平台不会指定工具，但能够注册和操作这些模型，无论它们是如何训练或组合的。 
手动与自动部署 
将模型部署到生产环境中有多种方式，最终结果大多是一个 REST API。不同的方法引入了许多风险，包括不一致的 API 接口设计、不一致的认证和日志记录，以及消耗开发运维资源。一个平台应该能够通过最小的步骤自动化这些工作，通过一致的 API 和认证暴露模型，并减少对开发运维的操作负担。 
如何区分数据科学与机器学习平台？ 
从表面上看，所有的数据科学平台听起来都一样，但关键在于细节。以下是一些比较的数据点： 
支持的语言 
R 和 Python 是大多数数据科学和机器学习项目的必备语言。Java 也很重要，因为像deeplearning4j和H2O 的 POJO模型提取器等库。C++在科学计算或HPC中尤其相关。其他运行时是可选的，取决于你的使用场景和非数据科学同事使用的主要技术栈，如 NodeJS/Ruby/.NET。 
CPU 与 GPU（深度学习） 
随着深度学习在数据科学和机器学习领域的日益突出，随着该领域的成熟和模型库的增长，这种趋势只会增加。尽管 TensorFlow 很受欢迎，但它并不总是向后兼容，Caffe 可能需要特殊的编译标志，而 cuDNN 实际上是另一个需要管理的 GPU 集群复杂层。如果能够完全容器化和生产化异构模型（无论是代码、节点权重、框架还是底层驱动程序）并在 GPU 架构上运行，这将是平台的一个强大差异化因素，甚至是强制性要求。 
单版本与多版本控制 
版本控制是能够列出模型随时间变化的历史，并独立访问每个版本。当模型被版本化时，数据科学家可以衡量模型的漂移。单版本架构为该模型（当前稳定版本）暴露一个 REST API 端点，只有作者能够从其控制面板“切换”模型。多版本架构除了为稳定版本暴露一个 REST API 端点外，还暴露每个先前版本，使它们同时可用，这消除了向后兼容性挑战，并使后端工程师能够实施部分推出或实时 A/B 测试。 
垂直与水平扩展 
将模型作为 REST API 提供是不够的。垂直扩展是将模型部署到更大的机器上。水平扩展是将模型部署到多台机器上。无服务器扩展，如Algorithmia 企业版实现的，是通过将模型封装在专用容器中，根据需求在计算集群中即时部署该容器，并在执行后立即销毁容器以释放资源。无服务器计算带来了扩展和经济 效益。 
单租户 vs. 多租户 
处理敏感或机密模型时，共享硬件资源可能会成为挑战。单租户平台会在相同资源（机器实例、虚拟内存等）中运行所有生产模型。多租户平台则将模型作为虚拟隔离的系统（通过为每个模型使用容器或虚拟机）进行部署，并可能提供额外的安全措施，如防火墙规则和审计跟踪。 
固定 vs. 可互换数据源 
数据科学家可能需要在模型上运行来自 S3 的离线数据，而后端工程师则同时在同一模型上运行来自 HDFS 的生产数据。固定数据源平台要求模型作者实现两个数据连接器：HDFS 和 S3。可互换数据源平台则要求作者实现一个通用数据连接器，作为多个数据源的适配器，并为未来可能出现的数据源做好模型兼容性准备。在 Algorithmia 企业版中，这被称为 Data API。 
示例数据科学与机器学习平台 
这绝非详尽无遗的列表。如果你有建议，请随时给我们留言或发送通知。 
 Algorithmia 企业版Domino Data LabyHatDataikuCloudera WorkbenchAlteryxRapidMiner
原始文章。经许可转载。 
个人简介：Ahmad AlNaimi 是一位具有行动偏向的跨学科问题解决者。喜欢将创意从概念阶段推进到原型阶段再到早期应用。软件工程师和早期业务开发专家。 
相关： 
 DataScience.com 新更新旨在成为行业领先的数据科学平台Gartner 数据科学平台 – 深入分析Forrester 与 Gartner 对数据科学平台和机器学习解决方案的比较
 
我们的 3 个推荐课程 
 1. Google 网络安全证书 - 快速进入网络安全职业的快车道 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 为你的组织提供 IT 支持 
 
更多相关主题 
 企业中的机器学习：用例与挑战成为数据驱动型企业的好处前 5 名云端机器学习平台和工具构建强大数据科学作品集的 7 个免费平台练习关键数据科学技能的 9 个顶级平台构建强大数据科学作品集的 5 个免费平台
数据科学机器，或‘如何进行特征工程’ 
 
 原文：www.kdnuggets.com/2015/10/data-science-machine.html 
 
 麻省理工学院硕士生马克斯·坎特的最新研究实现了他所称之为“数据科学机器”的技术。坎特和他在CSAIL的论文导师卡利扬·维拉马查内尼合作撰写了一篇关于数据科学机器（DSM）及其基础创新——深度特征合成算法的论文，该论文将于下周在 IEEE 国际数据科学与高级分析会议上发布。他们的论文《深度特征合成：自动化数据科学工作》现已在线发布。坎特和维拉马查内尼简明地描述 DSM 为“一个从原始数据中生成预测模型的自动化系统”，它将作者们创新的特征工程方法与端到端数据科学管道相结合。迄今为止，DSM 在其参加的数据科学竞赛中击败了 68.9%的团队。也许最值得注意的是，取得这一成功率的提交通常在 12 小时内完成，而不是需要几个月的人工工作。DSM 的基础是观察到数据科学竞赛问题通常具有以下共同特性：它们是结构化和关系型的，它们建模人类与复杂系统的互动，并且尝试预测某些人类方面的内容。深度特征合成 与任何数据科学问题一样，特征必须首先从现有变量中识别出来，或从现有变量中创建出来。虽然特征工程在非关系数据（如文本和图像）领域取得了显著进展，但坎特和维拉马查内尼指出，这一任务仍然是数据科学管道中最依赖人工干预的任务，即使对于经验丰富的数据科学家来说也可能是困难且耗时的。如果要真正实现自动化，这一任务也必须最接近人类的效率。DSM 的特征工程算法深度特征选择（DFS）专门用于关系型数据集，用于自动化识别和生成洞察性特征。DFS 以关系表为输入，能够处理这种数据结构中包含的各种数据类型。成功的 DFS 算法旨在像数据科学家一样思考，将洞察性问题转化为输入特征。DFS 算法遍历关系，并在此过程中应用特征选择函数，逐步创建最终特征。DFS 在执行这一遍历时，将数学函数的计算叠加到特定深度，这也是 DFS 名称的由来。根据输入数据类型，DSM 在两个不同层次上应用多个数学函数：实体和关系。实体层特征关注于转换和翻译函数，例如更改数据表示、四舍五入数字，以及将现有的广义属性提取为更多且更简洁的属性。关系层特征关注于表中实体之间的关系（比如你的主键和外键）。这些特征函数能够从其他表中提取相关数据以与给定特征关联（例如，找到与订单相关的最大项目价格或项目数量），这些数据可能作为有用特征输入到模型中。机器学习路径 为启动 DSM 的机器学习路径，首先选择一个输入特征作为目标值，用于形成预测问题。通过元数据选择适当的特征，称为预测变量，以帮助预测过程。DSM 然后创建一个数据预处理、特征选择、降维、建模和评估的路径，所有这些都被参数化并在必要时可供重用。参数优化通过Copula 过程完成，并通过观察相关性来减少特征数量。减少后的特征集在样本数据上进行测试，通过不同的组合来优化预测准确性。通过使用自动调节功能（作者认为这是其性能绝对关键的因素），DSM 在所有三个竞赛中提高了其得分。讨论 所有这些似乎基本上表明：DSM 利用智能关系型数据库的关系遍历来帮助构建和建立候选特征，通过寻找相关值来缩小特征集，并在等同于蛮力特征工程的组合中，应用迭代特征子集到样本数据中，同时重新组合以优化，直到找到最佳解决方案。为了测量 DSM 的性能，它被投入到 2014 年 KDD Cup、IJCAI 和 2015 年 KDD Cup 等竞赛中，如前所述，它击败了超过 2/3 的人类竞争者。坎特和维拉马查内尼声称，即使在表现最差的时候（IJCAI），DSM 仍能以类似于人类竞争者的方式构建预测问题，证据是它在任务中沿用类似的数据建模途径。在同一竞赛中，它与获胜者的 AUC 差距约为 0.04，表明 DSM 捕捉到了竞赛数据集中的主要方面。坎特和维拉马查内尼认为，虽然它目前不能与最高表现的人类科学家竞争，但 DSM 仍然有其与他们并肩的作用。尽管在每场比赛中有许多人战胜了 DSM，但它能够以相当少的努力（在某些情况下少于 12 小时，而不是几个月）击败大多数人。他们建议，考虑到这一点，它可以用来设定基准以及激发创造力。通过前置特征工程和生成潜在顶级特征集，能够让人类在数小时内重新思考问题，有效地从 DSM 解决方案出发，继续前进。需要注意的是，虽然 DSM 令人印象深刻，但它并不是第一个旨在自动化机器学习的系统。其他例子包括许多自动构建广告竞标模型的系统，或KXEN Model Factory（现在是 SAP 的一部分），它在 2010 年就已经提供了自动化模型构建。此外，DSM 显然并不适用于所有类型的数据，它是一个专注于关系型数据集的系统。尚未证明它在不符合先前识别的数据科学竞赛问题模式的关系型数据集上有效。DSM 已经衍生出一个名为FeatureLab的初创公司，称其为“界面上的洞察”，坎特担任首席执行官。该网站声明“利用你的数据，无需更多数据科学家”，并声称它是“寻求增加数据科学资源的公司的最佳解决方案”。这些都是大胆的声明，尤其是考虑到 DSM 的个别部分并不能真正算作突破。FeatureLab 很可能会在“商业智能”服务平台的云雾中迷失。但大数据依然存在，特征工程在过去 12 个月里一直是机器学习的热门话题。DSM 可能正是其特定技术组合在恰当的时间点上，导致了对数据科学的新思考方式。哈佛计算机科学教授玛戈·塞尔茨（Margo Seltzer）在谈到 DSM 时表示：“我认为他们做的事情将迅速成为标准——非常迅速。”如果情况确实如此，FeatureLabs 可能会处于良好的位置。你可以在这里阅读更多关于坎特和维拉马查内尼的数据科学机器的信息。简介：马修·梅奥 是一名计算机科学研究生，目前正在从事机器学习算法的并行化研究。他还是数据挖掘学生、数据爱好者和有志于成为机器学习科学家的人员。相关链接： 
 数据科学中书本上没有的 3 件事数据维度减少的七种技术2015 年 8 月的分析、大数据、数据挖掘、数据科学收购和初创公司汇总
更多相关话题 
 2022 年特征存储峰会：关于特征工程的免费会议机器学习中的特征工程实用方法为多变量时间序列构建可处理的特征工程管道使用 RAPIDS cuDF 在特征工程中利用 GPU特征工程初学者指南通过声明式机器学习从工程师转型为 ML 工程师
利用数据科学使清洁能源更加公平 
 
 原文：www.kdnuggets.com/2022/03/data-science-make-clean-energy-equitable.html 
 
 
图片由 Pedro Henrique Santos 提供，来源于 Unsplash 
数据科学在应对气候变化的斗争中扮演着越来越重要的角色。它可以用于预测清洁能源系统的供需情况，以及 确定 哪些建筑物需要进行能源效率升级。当我们运用数据科学开发应对气候变化的创新解决方案时，我们应该小心不要复制或加剧现有的不平等，或创造新的不平等。 
 
我们的前三推荐课程 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你所在组织的 IT 
 
气候变化的影响，从极端天气到严重干旱，将在最贫困、最缺乏应对能力的国家和社区中造成更大冲击。不幸的是，我们正在开发的许多解决方案也存在不公平性。仍使用煤炭和煤油烹饪的贫困家庭无法负担太阳能屋顶和节能家电。低收入父母在工作和育儿之间疲于奔命，几乎没有时间去充电站充电，且可能没有车库或车道在家充电。 
当我们使用数据科学来帮助应对气候变化时，我们应该寻求既环保又公平的解决方案。例如，许多社区没有可靠的公共交通。根据蒂埃拉·比尔斯博士，加州大学洛杉矶分校土木与环境工程助理教授，关于交通需求的数据通常不可用或不准确，尤其是对老年人或残疾人等面临出行问题的群体。新的地理定位数据源和直接从用户处收集出行偏好的方法可以提供更全面的交通需求图景。数据科学可以帮助我们优化路线规划，并为服务不足的地区创建按需公交线路和拼车匹配服务。 
数据科学支持公平解决方案的另一个领域是清洁、可靠能源的获取。全球有高达 7.6 亿人缺乏电力，还有更多人经历频繁的中断。根据能源专家马新博士，TotalEnergies Ventures 亚太平台总监，数据科学的进步可以帮助我们扩展和民主化能源获取。例如，微电网和小型电网可以通过实现小规模的本地发电，为偏远和服务不足的社区提供能源，而不依赖于传统电网。数据科学用于基于天气模式预测能源供应，并监测和优化太阳能电池板和电池的状态。近年来，微电网和小型电网已经为全球超过 1100 万人提供了电力。 
这些例子来源于我主持的去年秋季的C3E 会议，展示了数据科学的深思熟虑的使用如何支持公平的清洁能源解决方案。3 月 7 日，讨论将继续由我的同事珍妮·萨卡尔博士在全球女性数据科学会议上主持，该会议将在斯坦福大学和在线进行。以下是这些小组讨论启发的一些教训，关于数据科学家如何帮助将公平付诸实践。 
1) 考虑公平的多个维度。 过去，推动公平的努力通常集中在为人们提供平等的资源和机会。然而，当人们起点不平等时，平等的访问权很少能带来公平的结果。公平是一个更广泛的框架，部分关注利益和负担如何在不同群体（如种族、收入、性别等）以及地理区域（如城乡差异、全球北方与南方）之间分配。它还考虑了人们的行动能力，提出诸如：在决策时谁被代表？人们是否有权力改变他们的情况和塑造他们的社区？在用公平的视角查看数据集时，询问被测量的内容、由谁测量以及目的是什么，以及数据中包含了谁而不包含谁。同样，边缘化群体在数据科学和数据驱动决策中的代表性也至关重要，以确保他们参与会影响他们社区的决策。 
2) 理解你想解决的问题的人性方面。 在任何数据科学项目开始时，明确表达你的目标很重要，而不仅仅是收集一堆数据并套用算法。如果你不了解你试图解决的问题的人类影响，你就无法制定公平的响应。例如，劳伦斯伯克利国家实验室的研究人员正在处理屋顶太阳能安装的数据。只有将这个数据集与人口统计数据结合，他们才得以洞察公平问题，例如谁在（或不在）安装太阳能，原因是什么，以及哪些障碍阻碍了在太阳能渗透较少的群体中的采用。 
3) 留意“数据沙漠”并努力纠正它们。 如果人们在我们关于某个问题的数据中没有被代表，他们在解决方案设计时将会是隐形的。例如，如果你正在开发清洁能源系统的供需模型，但没有关于偏远农村地区的电力需求的数据，结果系统可能不适合这些地区。有一些有效的技术方法来处理稀疏或混乱的数据，但我们不应该仅仅通过统计技术来修补数据沙漠。相反，我们需要确保数据收集代表所有相关的人和地方。我们需要广泛思考如何获取更多、更好的数据，并推动私营、研究和政府部门的数据开放。这通常需要精心设计的利益相关者和社区参与。 
4) 认识到现有数据的局限性。 数据展示了人们在现有资源和限制下的行为，而不是他们理想中想要或需要的东西，或是在不同激励下会做的事情。例如，电力供应不稳定的地区可能因缺乏需求而表现出低能源使用，而不是因为人们不安装电器或参与其他活动，如果他们能保证获得稳定的电力供应，他们可能会这样做。我们不应该假设人们现在的行为就是他们在一个更公平的世界中希望采取的方式。 
5) 小心不要通过大数据掩盖不平等现象。 尽管大数据现在非常流行，但它并不是我们唯一应该关注的尺度。不平等问题往往是局部的、细节性的。这些微小的信号可能在庞大的数据集中被忽略。例如，通勤者的平均旅行时间可能掩盖了那些唯一的出行选择是两小时多阶段旅程的异常情况。你可以通过使用分解数据的技术来避免掩盖不平等现象，从而尽可能地精细化数据。例如，BEAM 模型等基于代理的模型可以让你在个体层面上查看人们的行为和偏好。 
6) 寻找创新的方法来收集社区需求的数据。 边缘化群体通常在现有数据来源中代表性不足。寻找创造性的方法来收集这些社区的定量和定性数据，并收集他们对数据收集和分析过程的意见。新的数据收集工具如移动应用和在线调查可以提供帮助，但要小心技术工具不会进一步忽视那些接入有限的人群。直接与边缘化群体接触，与关键利益相关者和社区成员合作，以了解他们对要问的问题、要收集的数据和要设计的解决方案的看法，这是至关重要的。 
这些只是我们可以利用数据科学以更公平的方式应对气候变化的一些方法。目前，存在大量的“公平洗牌”现象。组织声称考虑公平性，但实际上并未真正挑战现状。相反，公平性需要在整个数据科学过程中占据核心地位，从数据收集和问题分析到解决方案设计和实施。这是我们如何建设一个既更环保又更公正的世界的方法。 
Margot Gerritsen 博士 是斯坦福大学能源资源工程系的教授。她是数据科学中的女性（WiDS）的共同创始人和联合主任，该计划旨在激励和教育全球的数据科学家，无论性别如何，并支持该领域的女性。如果你想深入了解如何利用数据科学以公平的方式应对气候变化等关键问题，WiDS 邀请你参加他们 2022 年 3 月 7 日在斯坦福大学及全球线上举办的年度会议。 
更多主题 
 使用管道编写清晰的 Python 代码利用人工智能设计公平和公正的电动汽车充电网络绿色人工智能：使应用程序更具可持续性的 7 种策略如何让大型语言模型与你的软件和谐相处…为什么越来越多的开发者使用 Python 进行机器学习项目？我如何通过数据科学每月在线赚取$3,500
数据科学与 DevOps 相遇：使用 Jupyter、Git 和 Kubernetes 的 MLOps 
 
 原文：www.kdnuggets.com/2020/08/data-science-meets-devops-mlops-jupyter-git-kubernetes.html 
 
评论 
作者：Jeremy Lewi，谷歌软件工程师 & Hamel Husain，GitHub 高级机器学习工程师 
问题 
 
我们的前三名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升您的数据分析技能 
 3. Google IT 支持专业证书 - 支持您的组织的 IT 工作 
 
Kubeflow 是一个快速发展的开源项目，使在 Kubernetes 上部署和管理机器学习变得简单。 
由于 Kubeflow 的爆炸性流行，我们接收了大量的 GitHub 问题，这些问题必须被分类并转交给合适的主题专家。下图展示了过去一年中新开启的问题数量： 
 
图 1: Kubeflow 问题数量 
为了应对这种流量，我们开始投资一个叫做 Issue Label Bot 的 GitHub 应用程序，它使用机器学习来自动标记问题。我们的 第一个模型 是通过使用 GitHub 上流行的公共代码库集合来训练的，只预测了通用标签。随后，我们开始使用 Google AutoML 来训练一个特定于 Kubeflow 的模型。新模型能够以 72% 的平均精度和 50% 的平均召回率预测 Kubeflow 特定标签。这大大减少了与 Kubeflow 维护相关的问题管理的繁琐工作。下表包含了对 Kubeflow 特定标签在保留集上的评估指标。以下的 精度和召回率 与我们为适应需求而调整的预测阈值一致。 
标签精度召回率area-backend0.60.4area-bootstrap0.30.1area-centraldashboard0.60.6area-components0.50.3area-docs0.80.7area-engprod0.80.5area-front-end0.70.5area-frontend0.70.4area-inference0.90.5area-jupyter0.90.7area-katib0.81.0area-kfctl0.80.7area-kustomize0.30.1area-operator0.80.7area-pipelines0.70.4area-samples0.50.5area-sdk0.70.4area-sdk-dsl0.60.4area-sdk-dsl-compiler0.60.4area-testing0.70.7area-tfjob0.40.4platform-aws0.80.5platform-gcp0.80.6 
表 1: 各种 Kubeflow 标签的评估指标。 
鉴于新问题的到来速度，定期重新训练我们的模型变成了一个优先事项。我们认为，持续重新训练和部署我们的模型以利用这些新数据对于保持模型的有效性至关重要。 
我们的解决方案 
我们的 CI/CD 解决方案如 图 2 所示。我们不显式地创建一个有向无环图（DAG）来连接 ML 工作流中的各个步骤（例如预处理、训练、验证、部署等）。相反，我们使用一组独立的控制器。每个控制器声明性地描述世界的期望状态，并采取必要的行动使实际状态与之匹配。这种独立性使我们可以使用最适合每个步骤的工具。更具体地说，我们使用 
 用于开发模型的 Jupyter 笔记本。GitOps 用于持续集成和部署。Kubernetes 和托管云服务用于基础设施。
 
图 2: 展示了我们如何进行 CI/CD。我们目前的管道由两个独立操作的控制器组成。我们通过描述我们希望存在的模型（即我们的模型“新鲜”的含义）来配置 Trainer（左侧）。Trainer 定期检查训练模型是否足够新鲜，如果不够新鲜则训练一个新模型。同样地，我们配置 Deployer（右侧）来定义部署模型与训练模型集合同步的含义。如果未部署正确的模型，它将部署一个新模型。 
有关模型训练和部署的更多细节，请参考 下面的驱动部分。 
背景 
使用协调器构建弹性系统 
协调器是一种被证明对构建弹性系统极其有用的控制模式。协调模式 是 Kubernetes 工作原理的核心。图 3 展示了协调器的工作原理。协调器通过首先观察世界的状态来工作；例如，当前部署了什么模型。然后协调器将其与世界的期望状态进行比较并计算差异；例如，标签为“version=20200724”的模型应该被部署，但当前部署的模型标签为“version=20200700”。协调器随后采取必要的行动将世界驱动到期望状态；例如，打开一个拉取请求以更改部署的模型。 
 
图 3. 我们的部署器应用的 Reconciler 模式示意图。 
Reconciler 已被证明在构建弹性系统方面极为有用，因为一个良好实现的 Reconciler 提供了高度的信心，无论系统如何扰动，它最终都会恢复到期望的状态。 
无 DAG 
控制器的声明性特征意味着数据可以通过一系列控制器流动，而无需显式创建 DAG。与其使用 DAG，不如将一系列数据处理步骤表示为一组期望的状态，如下图 4 所示： 
 
图 4: 说明了管道如何从独立的控制器中产生，而无需明确编码一个有向无环图（DAG）。这里我们有两个完全独立的控制器。第一个控制器确保每个元素 a[i] 都应该有一个元素 b[i]。第二个控制器确保每个元素 b[i] 都应该有一个元素 c[i]。 
与许多传统的 DAG 基础工作流相比，这种基于 Reconciler 的范式提供了以下优势： 
 抗故障能力：系统持续寻求实现并维持期望的状态。工程团队自主性的增加： 每个团队可以自由选择适合其需求的工具和基础设施。Reconciler 框架只要求控制器之间有最少的耦合，同时仍允许编写表达力强的工作流。经过实战检验的模式和工具：这个基于 Reconciler 的框架并没有发明新的东西。Kubernetes 拥有丰富的工具生态系统，旨在简化控制器的构建。Kubernetes 的流行意味着有一个庞大且不断增长的社区熟悉这种模式及其支持工具。
GitOps：通过拉取请求进行操作 
GitOps，如图 5 所示，是一种管理基础设施的模式。GitOps 的核心思想是源代码控制（不一定是 git）应当是描述基础设施配置文件的真实来源。然后，控制器可以监视源代码控制并在配置更改时自动更新你的基础设施。这意味着要进行更改（或撤销更改），你只需打开一个拉取请求。 
 
图 5: 要推送 Label Bot 的新模型，我们创建一个 PR 更新配置映射，以存储我们想要使用的 Auto ML 模型的 ID。当 PR 被合并后， Anthos Config Management(ACM) 会自动将这些更改部署到我们的 GKE 集群。因此，后续的预测将使用新模型。 （图片由 Weaveworks 提供） 
综合起来：Reconciler + GitOps = 机器学习的 CI/CD 
有了这些背景知识，我们接下来深入探讨如何通过结合 Reconciler 和 GitOps 模式来构建 CI/CD 以支持机器学习。 
我们需要解决三个问题： 
 我们如何计算世界的期望状态与实际状态之间的差异？我们如何进行必要的更改，以使实际状态与期望状态匹配？我们如何构建一个控制循环来持续运行 1 和 2？
计算差异 
为了计算差异，我们只需编写执行我们需要的操作的 lambda 函数。因此，在这种情况下，我们编写了两个 lambda 函数： 
 第一个 lambda根据最近模型的年龄来确定是否需要重新训练。第二个 lambda通过将最近训练的模型与在源代码控制中检查的配置映射中的模型进行比较来确定模型是否需要更新。
我们将这些 lambda 函数封装在一个简单的 Web 服务器中并部署到 Kubernetes 上。我们选择这种方法的原因之一是我们希望依赖 Kubernetes 的git-sync将我们的仓库镜像到 pod 卷中。这使得我们的 lambda 函数变得非常简单，因为所有的 git 管理都由一个运行git-sync的 side-car 处理。 
执行 
为了应用必要的更改，我们使用 Tekton 将我们用来执行各个步骤的不同 CLI 连接在一起。 
模型训练 
为了训练我们的模型，我们有一个Tekton 任务： 
 使用papermill 运行我们的笔记本。使用nbconvert将笔记本转换为 HTML。使用gsutil将 .ipynb 和 .html 文件上传到 GCS。
该笔记本从 BigQuery 中提取 GitHub Issues 数据，从 BigQuery 并生成适合导入到Google AutoML的 GCS 上的 CSV 文件。然后，笔记本启动一个AutoML任务来训练模型。 
我们选择 AutoML 是因为我们希望专注于构建一个完整的端到端解决方案，而不是在模型上进行迭代。AutoML 提供了一个竞争性的基准，我们可以在未来尝试改进。 
为了轻松查看执行的笔记本，我们将其转换为 HTML 并上传到GCS，这使得公开提供静态内容变得容易。这允许我们使用笔记本生成丰富的可视化来评估我们的模型。 
模型部署 
要部署我们的模型，我们有一个 Tekton 任务： 
 使用 kpt 更新我们的 configmap 为所需值。运行 git 将我们的更改推送到一个分支。使用一个包装器来操作 GitHub CLI (gh) 来创建 PR。
控制器确保每次只有一个 Tekton 管道在运行。我们配置我们的管道始终推送到相同的分支。这确保我们只会打开一个 PR 来更新模型，因为 GitHub 不允许从同一分支创建多个 PR。 
一旦 PR 合并，Anthos Config Mesh 会自动将 Kubernetes 清单应用到我们的 Kubernetes 集群。 
为什么选择 Tekton 
我们选择 Tekton 是因为我们面临的主要挑战是顺序运行一系列 CLIs 在不同的容器中。Tekton 对此非常合适。重要的是，Tekton 任务中的所有步骤都在同一个 Pod 上运行，这允许使用 Pod 卷在步骤之间共享数据。 
此外，由于 Tekton 资源是 Kubernetes 资源，我们可以采用相同的 GitOps 模式和工具来更新我们的管道定义。 
控制循环 
最后，我们需要构建一个控制循环，定期调用我们的 lambdas 并根据需要启动 Tekton 管道。我们使用 kubebuilder 创建了一个 简单的自定义控制器。我们控制器的协调循环将调用我们的 lambda 来确定是否需要同步，如果需要的话，使用哪些参数。如果需要同步，控制器会触发 Tekton 管道来执行实际更新。下面是我们的 自定义资源 的示例： 
apiVersion: automl.cloudai.kubeflow.org/v1alpha1
kind: ModelSync
metadata:
name: modelsync-sample
namespace: label-bot-prod
spec:
failedPipelineRunsHistoryLimit: 10
needsSyncUrl: http://labelbot-diff.label-bot-prod/needsSync
parameters:
- needsSyncName: name
pipelineName: automl-model
pipelineRunTemplate:
spec:
params:
- name: automl-model
value: notavlidmodel
- name: branchName
value: auto-update
- name: fork
value: git@github.com:kubeflow/code-intelligence.git
- name: forkName
value: fork
pipelineRef:
name: update-model-pr
resources:
- name: repo
resourceSpec:
params:
- name: url
value: https://github.com/kubeflow/code-intelligence.git
- name: revision
value: master
type: git
serviceAccountName: auto-update
successfulPipelineRunsHistoryLimit: 10 
1234567891011121314151617181920212223242526272829303132333435 
自定义资源指定了端点needsSyncUrl，用于计算是否需要同步的 lambda 和一个 Tekton PipelineRunpipelineRunTemplate，描述了在需要同步时创建的管道运行。控制器处理详细信息；例如，确保每个资源每次只有一个管道在运行，垃圾回收旧的运行等……所有繁重的工作都由 Kubernetes 和 kubebuilder 为我们处理。 
注意，出于历史原因，kind ModelSync 和 apiVersion automl.cloudai.kubeflow.org 与控制器实际执行的内容不完全一致。我们计划在未来修复这个问题。 
构建您自己的 CI/CD 管道 
我们的代码库距离成熟的、易于重用的工具还很遥远。尽管如此，它是公开的，可以作为构建自己管道的有用起点。 
以下是一些入门指引： 
 使用 Dockerfile 来构建您自己的 ModelSync 控制器修改 kustomize 包 以使用你的镜像并部署控制器根据你的用例定义一个或多个 lambda 函数 
 你可以参考我们的 Lambda 服务器 作为示例我们用 Go 编写了我们的代码，但你可以使用任何你喜欢的语言和 web 框架（例如 flask） 定义适合你的用例的 Tekton 流水线；我们的流水线（见下文链接）可能是一个有用的起点 
 Notebook Tekton 任务 - 使用 papermill 运行 notebook 并上传到 GCSPR Tekton 任务 - Tekton 任务用于打开 GitHub PR 根据你的用例定义 ModelSync 资源；你可以参考我们的资源作为示例 
 ModelSync 部署规范 - YAML 用于持续部署标签机器人ModelSync 训练规范 - YAML 用于持续训练我们的模型 
如果你希望我们清理这些内容并在未来的 Kubeflow 版本中包含，请在问题 kubeflow/kubeflow#5167 中发言。 
下一步 
血统跟踪 
由于我们没有明确的 DAG 表示 CI/CD 流水线中的步骤顺序，因此理解我们的模型的血统可能会很具挑战性。幸运的是，Kubeflow Metadata 通过使每个步骤记录其生成的输出所使用的代码和输入的信息变得容易，解决了这个问题。Kubeflow 元数据可以轻松恢复和绘制血统图。下图展示了我们 xgboost 示例 的血统图示例。 
 
图 6： 我们 xgboost 示例 的血统跟踪 UI 截图。 
我们的计划是让我们的控制器自动将血统跟踪信息写入元数据服务器，以便我们可以轻松了解生产中的内容的血统。 
结论 
 
构建 ML 产品是团队合作的成果。为了将模型从概念验证阶段转变为已发布的产品，数据科学家和 DevOps 工程师需要协作。为了促进这种协作，我们认为允许数据科学家和 DevOps 工程师使用他们首选的工具非常重要。具体来说，我们希望支持以下工具：数据科学家、DevOps 工程师和 SRE。 
 Jupyter 笔记本用于模型开发。GitOps 用于持续集成和部署。Kubernetes 和托管云服务作为基础设施。
为了最大限度地提高每个团队的自主性并减少对工具的依赖，我们的 CI/CD 过程采用了去中心化的方法。我们的方法不是明确地定义一个连接步骤的 DAG，而是依赖于一系列可以独立定义和管理的控制器。我们认为这自然适用于责任可能在团队之间分配的企业；数据工程团队可能负责将网络日志转换为特征，建模团队可能负责从特征中生成模型，而部署团队可能负责将这些模型推向生产环境。 
深入阅读 
如果你想了解更多关于 GitOps 的内容，我们建议你查看 Weaveworks 的这个指南。 
要学习如何构建自己的 Kubernetes 控制器， kubebuilder 书籍 提供了一个端到端的示例。 
Jeremy Lewi 是谷歌的软件工程师。 
Hamel Husain 是 GitHub 的高级机器学习工程师。 
原文。已获得许可转载。 
相关： 
 我从 200 种机器学习工具中学到的东西在边缘设备上实施 MLOps端到端机器学习平台的概览
更多相关内容 
 Kubernetes 实战：第二版Kubernetes 中的高可用 SQL Server Docker 容器数据科学中的 Git 速查表通过这个免费的 DevOps 速成课程释放你的潜力每个初学者应该学习的 10 个必备 DevOps 工具数据科学家必备的 14 条 Git 命令
数据科学对我们心理发展的作用 
 
 原文：www.kdnuggets.com/2019/02/data-science-mental-development.html 
 
 评论 
由 Syed Sadat Nazrul，分析科学家 
 
 
我们的前三课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析能力。 
 3. Google IT 支持专业证书 - 支持你所在组织的 IT 需求。 
 
情感是人类社会的基本元素。如果你仔细考虑，任何值得分析的事物都受到人类行为的影响。网络攻击受到不满员工的高度影响，他们可能会忽视应有的谨慎，或从事 内部滥用。股市依赖于经济气候的影响，而经济气候本身取决于大众的整体行为。在沟通领域，众所周知 我们所说的只占信息的 7%，其余 93%则编码在面部表情和其他非言语线索中。整个心理学和行为经济学领域都致力于这一领域。尽管如此，有效测量和分析情感的能力将使我们以显著的方式改善社会。例如，加州大学旧金山分校的心理学教授 保罗·艾克曼 在他的书中，撒谎：市场、政治和婚姻中的欺骗线索 描述了如何通过面部表情来帮助心理学家发现潜在的自杀意图，即使患者在隐瞒这种意图。这听起来像是面部识别模型的工作？神经映射呢？我们能否有效地从神经冲动中映射情感状态？改善认知能力又如何？甚至是情感智力和有效沟通？利用我们拥有的大量非结构化数据来解决世界上许多问题是大有作为的。 
尽管如此，像所有数据科学问题一样，我们需要深入探讨建模情感的核心挑战： 
 如何框定问题？我们的模型类别应是什么？我们优化的目标是什么？我们应该收集哪些数据？我们在寻找哪些相关性？哪些方面需要更深入地探讨？获取此类数据是否存在任何问题？社会和文化对获取情感数据的看法是什么？需要遵守哪些隐私法规？数据安全又如何保障？
想了解如何有效设计 AI 产品，请阅读我的数据科学家和 AI 产品 UX 设计指南。在这篇博客中，我旨在概述 AI 如何帮助我们未来的心理发展，并讨论一些当前的解决方案。 
医疗保健 
患者对医生撒谎并不少见。男性和女性的信任障碍源于尴尬和与医生接触时间太少。一项数字健康平台的研究， ZocDoc，揭示了近一半（46%）的美国人因为尴尬或害怕被评判而避免告诉医生健康问题。约三分之一的人表示，他们因无法找到合适的机会或在预约期间时间不足（27%），或因医生没有提问或特别询问是否有困扰他们的事（32%）而隐瞒了细节。这对自杀领域产生了重大影响。根据世界卫生组织（WHO）的数据，每年多达 80 万人死于自杀，60%的人面临严重的抑郁症。尽管抑郁症使患者更容易进行自杀行为，但区分自杀性抑郁症和普通抑郁症并不容易。 
Deena Zaidi 在她的博客中，机器学习利用面部表情区分抑郁症和自杀行为*, *描述了一位自杀专家如何通过对风险因素的深入评估，准确预测患者未来的自杀想法和行为，与不了解患者的人的准确性相当。这与根据抛硬币做决定没有区别。虽然使用监督学习模型读取面部表情的技术仍在开发中，但这一领域已显示出很大的潜力。 
 
从 SVM 结果分析的杜氏（上）与非杜氏（下）笑容有助于检测自杀风险（来源： Laksana 等人研究自杀意念的面部行为指标）。 
一份报告，与南加州大学、卡内基梅隆大学和辛辛那提儿童医院医疗中心的科学家合作撰写，研究了非语言面部行为以检测自杀风险，并声称发现了一种区分抑郁症和自杀患者的模式。使用 SVM，他们发现面部行为描述符如涉及眼轮匝肌收缩的微笑百分比（杜氏笑容）在自杀和非自杀群体之间具有统计学意义。 
认知能力 
认知能力是我们进行从最简单到最复杂任务所需的脑基技能。它们与我们如何学习、记忆、解决问题和集中注意力的机制关系更大，而不是与任何实际知识有关。人们都希望提高认知能力。谁不希望更好地记住名字和面孔，更快地掌握困难的抽象概念，并且能更好地“看到联系”呢？ 
 
Elevate 应用程序在 Apple Store 
目前，有一些应用程序可以帮助我们训练认知能力。其中一个例子是 Elevate，它由脑力游戏组成，用户可以在合适的难度级别上玩，以提高心理数学、阅读和批判性思维能力。 最佳认知功能 的价值是如此显而易见，以至于详细说明可能是多余的。我们不断推陈出新，以超越我们的五感来更深刻地理解周围的世界。例如，在图像识别领域，AI 已经能够 “看”得比我们更清楚，通过观察远超 RGB 光谱的变量，从而帮助我们突破自身的视觉限制。然而，当我们可以进入虚拟世界时，为什么要局限于二维屏幕来可视化三维物体呢？ 
 
Nanome.AI 开发了用于分析抽象分子结构的增强现实技术 
增强现实让我们感觉仿佛已经传送到了另一个世界。当我想到这个问题时，计算材料科学和生物学是我立即想到的领域。作为过去的一名计算材料科学家，我知道可视化复杂的分子结构对于许多研究人员来说是一项挑战。Nanome.AI帮助在增强现实中可视化这些复杂的结构。更进一步，已经有很多初创公司在解剖学领域使用增强现实培训外科医生。 
 
平行坐标图可视化 7 维空间 
数据可视化和降维算法的新术语不断出现，帮助我们更好地体验周围的世界。例如，我们有了平行坐标，它允许我们在高维空间中进行可视化和过滤，而t-SNE则因能够将复杂空间降维到二维或三维空间而广受欢迎。 
情商 
情商是意识到、控制和表达自身情绪的能力，以及审慎和富有同理心地处理人际关系的能力。每个人都会经历情绪，但只有少数人能够准确识别它们的发生。这可能是因为缺乏自我意识，或者只是我们有限的情感词汇。很多时候，我们甚至不知道自己想要什么。我们努力以特定的方式与周围的人建立联系，或消费某种产品，只是为了体验一种非常独特的情感，而我们无法描述它。我们的感受远不止于幸福、悲伤、愤怒、焦虑或恐惧。我们的情绪是上述所有情绪的复杂组合。理解自身情绪以及他人情绪的能力对情感健康和维持积极关系至关重要。 
 
大脑活动的分布模式预测了使用 fMRI 扫描检测到的离散情绪的体验，顶部显示活动模式，底部显示灵敏度范围（来源：解码人脑中的自发情绪状态，Kragal 等） 
随着神经映射的创新，我们将更好地理解作为人类的自我以及我们能够达到的各种情感状态。监督学习已经提供了一些常见情感的识别。通过对脑波进行无监督学习，我们可能会更好地理解复杂的情感模式。例如，一个简单的异常检测算法或许可以揭示新的情感模式或需要关注的显著情感压力点。这些研究有可能揭示改进我们情感智力的创新方法。 
 
准确解读微表情如何有助于在商业交易中进行谈判的演示（来源：TED 演讲：如何通过身体语言和微表情预测成功 — Patryk & Kasia Wezowski） 
即使是帮助预防自杀的监督图像识别模型也可以让个人读取与他们交谈者的情感。例如，一个关于微表情的 TED 演讲展示了如何通过注意某些面部表情来估算商业谈判中的理想价格点。该 TED 演讲还提到，能够更好地解读微表情的销售人员比无法做到这一点的销售人员多销售 20%。因此，这 20%的优势可能通过投资于能够揭示谈话者情感的眼镜来实现。 
情感智力包括理解我们自己的情感以及对周围人的情感更为敏"
KDNuggets 博客中文翻译（三十四）,2024-10-28,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
6 个对数据科学家有利的商业趋势 
 
 原文：www.kdnuggets.com/2021/05/6-business-trends-data-scientists.html 
 
评论 
各行业的公司领导越来越关注可能影响利润的商业趋势。他们往往会参与更多需要数据科学家专业知识的活动。以下是六个让数据科学家更加抢手的趋势。 
 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业的快车道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你在 IT 领域的组织 
 
1. 数据质量管理 
数据质量管理结合了专业知识和专门工具，以不断提高公司信息的准确性和可靠性。其目的是开发和实施一套流程，以提升企业未来的数据质量并消除当前存在错误的信息。 
一项 2021 年的调查显示，2200 多名商业智能专业人士选择数据质量管理作为年度最重要的趋势。研究还表明，该主题在前几年也曾获得过第一的位置。 
数据科学家可以帮助商业领袖采纳这一趋势。取得进展需要了解公司目前如何使用数据以及存在的弱点。一旦数据专业人士掌握了这些见解，他们可以帮助公司领导最大限度地利用现有的信息并监控其质量。 
2. 数据故事讲述 
当商业领袖投资于数据故事讲述时，他们将数据和叙事融合在一起，把复杂的统计数据拆解成易于管理的格式，以便他们的受众理解。例如，在公司年报中添加条形图，使得利益相关者更容易理解和欣赏数据，同时看到数据的背景。 
数据故事讲述并不总是直接为客户带来好处。一项研究显示，56%的营销人员为他们的团队成员创建数据可视化。而 52%的受访者则为销售人员制作这些资源。 
在使用数据故事讲述时，营销人员可能会提供持续的反馈。然而，他们仍然可能需要数据科学家来帮助获取、清理和组织信息。 
数据科学家还具备展示材料不误导人们的欢迎专业知识。仅仅以视觉上吸引人的方式呈现内容是不够的。观众还必须正确解读数据，即使他们只是简短地查看它。 
3. 数据驱动的定制 
向人们展示针对大众的内容已不再有效。他们希望更多的个性化——即使是在看到在线产品广告时。在一项 2020 年的研究中，31%的人对那些向他们分发个性化广告的品牌表现出更高的忠诚度。同样，30%的受访者表示，他们更有可能因个性化广告而购买某物。 
尽管许多消费者喜欢更多的个性化，但一些人不希望品牌了解他们过多的信息。然而，一个有效的方式是展示公司如何使用数据及其需求的透明度，以帮助人们感到更加舒适。 
此外，数据科学家可以确保公司领导遵循最佳实践，并在收集、存储和使用信息时遵守所有适用的法规。由于加州已经有了数据隐私法，而州立法者考虑在其他地方引入类似法律，商业代表必须了解他们的义务。 
4. 实时数据 
实时数据在收集后立即发送进行分析，而不是存储起来，等待几天、几周甚至几个月后再进行检查。如果公司领导从事的行业中，延迟决策可能会导致过多的不满客户或利润损失，他们特别希望收到及时的信息。 
在沃尔玛的测试店中，智能摄像头和实时分析监控库存水平和购买模式。然后，零售助理会收到补充货架的警报，以便在货架空置之前进行补充。 
实时数据在制造业中也至关重要，尤其是因为意外停机的成本极其高昂。许多品牌使用预测分析工具来获取机器性能和可能需要解决的问题的实时数据。数据科学家可以帮助公司领导确定利用实时数据实现业务目标的最佳方法。 
5. 电子邮件自动化 
一些市场营销分析师认为电子邮件自动化是与客户互动的最佳方式之一。这涉及在某些行动发生时向人们发送内容。例如，一旦一个人注册了电子邮件列表，他们可能会自动收到一条包含特别优惠的欢迎信息。 
数据科学家拥有深入挖掘信息集合并确认公司领导者如何通过自动化电子邮件实现业务改进的必要专业知识。 
也许统计数据显示，一家电子商务公司的购物车放弃率很高。如果是这样，自动发送电子邮件提醒购物者他们没有完成购买可能会促成交易的完成。 
商业领袖还可以依赖数据科学家找到显示哪些电子邮件技巧最有效的统计数据。例如，一个人在邮件主题行中包含他们的名字更可能打开邮件吗？在主题行中注明特定的折扣百分比对客户的购买兴趣有何影响？ 
6. 数据作为核心业务功能 
许多公司领导者不再将数据视为可选的业务部分。他们意识到持续获得准确的信息将有助于决策，并增强业务。数据科学家可以评估公司如何更依赖数据并扩展这些努力的最佳方式。 
Gartner 的分析师提到数据作为核心业务功能被列入公司 2021 年的数据和分析趋势前十名名单中。他们澄清了，数据驱动的活动曾经是主要业务操作的辅助部分，通常由一个独立的团队处理。但这种情况正在发生变化，数据正成为企业的基础元素。 
然而，他们警告说，商业领袖往往被数据淹没而错过相关的机会。Gartner 的专家建议涉及首席数据官来设定目标和策略。他们表示，这样做可能会导致业务价值的持续增长，增长倍数为 2.6 倍。 
数据科学家至关重要 
数据可以帮助各行业的商业领袖发现他们本来无法获得的洞察。然而，解读这些信息并不总是简单的。数据科学家拥有帮助公司决策者利用手头信息并以此取得成功的知识和技能。 
Bio: Devin Partida 是一位大数据和技术作家，同时也是ReHack.com的主编。 
相关内容： 
 使用数据科学预测和防止现实世界问题如何成为数据科学家如何成功成为自由职业数据科学家
更多相关话题 
 5 个关键的数据科学趋势和分析趋势数据革命导航：探索数据科学和机器学习中的热门趋势人工智能、分析、机器学习、数据科学、深度学习等2021 年数据科学与分析行业主要发展及 2022 年关键预测2021 年主要发展与 2022 年关键趋势：人工智能、数据科学等2022 年及以后 10 大人工智能与数据分析趋势
6 个令人惊叹的 ChatGPT 扩展，用于任何地方 
 
 原文：www.kdnuggets.com/2023/04/6-chatgpt-mindblowing-extensions-anywhere.html 
 
 
由 Dall-E 生成的图像。AI 驱动的图像生成器 
今天，我想揭开 ChatGPT 的神秘面纱——这是一个令人着迷的新 AI 应用，最近发布并引起了很多关注。它是由 OpenAI 开发的 AI 聊天机器人，专注于对话，其主要目标是使 AI 系统的互动更加自然——它真的无所不知！ 
我很确定你已经尝试过了……对吧？ 
不过，今天我想讨论如何增强我们与这个全新工具的互动方式。 
互联网已经充斥着由这个新发布的服务驱动的新工具和扩展，这些工具可以使我们的日常任务变得更轻松——并且提高我们的最终输出。 
这就是为什么我在这里总结了 6 个工具，可以让 ChatGPT 成为你的日常助手，甚至超越这个功能！ 
#1. 在任何地方使用 ChatGPT — Google Chrome 扩展 
你想要轻松地在任何地方使用 ChatGPT 吗？今天是你的幸运日，有一个很棒的 Chrome 扩展** 你可以用来写推文、检查电子邮件、找出代码漏洞……**字面上的任何你能想象的事情！ 
 
Chrome 扩展所有者截图 
#2. 将 ChatGPT 与搜索引擎结合 
**如果你更愿意将 ChatGPT 集成到你平常使用的搜索引擎中，以便你可以直接获得答案而无需使用其自身界面，**你也可以做到！ 
你只需为 Chrome 和 Firefox 添加这个扩展，就能在 Google 搜索中直接获取 ChatGPT 的回应。 
 
来自扩展的 GitHub 截图 
如果你更愿意访问一个预集成的搜索引擎，你可以查看 这个搜索引擎，它将 OpenAI ChatGPT 和 Bing 结合在一起，直接回答你的问题。 
 
Perplexity 网站截图 
#3. 使用语音命令与 ChatGPT 
你是 Alexa 或 Siri 的粉丝吗？那我敢打赌你通常喜欢大声说出你的问题和需求。已经有一个扩展可以让你直接通过 Chrome 与 ChatGPT 对话。你可以在下面的视频中查看它的工作原理。 
#4. 在 Telegram 和 WhatsApp 中集成 ChatGPT 
你可以按照这些 GitHub 说明在 Telegram 中创建一个由 ChatGPT 驱动的机器人，并与它对话——或者我应该说是他或她？？？？ 
 
Telegram 机器人的截图由 ChatGPTTelegramBot 提供。 
你更喜欢 WhatsApp 吗？好消息！！你也可以在 WhatsApp 中集成 ChatGPT。你可以关注这个 GitHub 来实现。 
#5. 在 Google Docs 或 Microsoft Word 中集成 ChatGPT 
你可以在 Google Docs 和 Microsoft Word 中集成 ChatGPT，以便在你喜欢的文本编辑器中使用它的所有功能，使用以下 GitHub。 
 
ChatGPT 集成在 Google 文档中的截图。图片由 CesarHuret 提供。 
#6. 保存你在 ChatGPT 中生成的所有内容 
你是否与 ChatGPT 有过深入且有趣的对话，并希望将它们保存起来以供重读——或者也许是写一本包含所有知识的书？ 
然后你可以使用 以下扩展 将所有对话保存为 PDF、PNG 或 HTML 链接，适用于 Chrome、Edge 或 Firefox。 
 
图片由 liady 提供。 
#7. 附加功能 — Twitter ChatGPT 账户. 
Twitter 上充斥着允许你在提及它们时向 ChatGPT 提问的机器人，而不是直接在 openAI 网页上提问。 
一些示例包括： 
mobile.twitter.com/chatwithgpt 
twitter.com/ChatGPTBot/with_replies 
希望你觉得这些 ChatGPT 扩展有用！ 😃 
Josep Ferrer 是来自巴塞罗那的分析工程师。他拥有物理工程学位，目前在应用于人类流动性的数据科学领域工作。他还是一名兼职内容创作者，专注于数据科学和技术。 
原文。经许可转载。 
更多相关主题 
 数据科学的 12 个必备 VSCode 插件AI Chrome 扩展程序供数据科学家使用备忘单KDnuggets 新闻，7 月 6 日：12 个必备数据科学 VSCode 插件…KDnuggets 新闻，7 月 5 日：一个糟糕的数据科学项目 • 10 个 AI…Visual ChatGPT：微软将 ChatGPT 与 VFMs 结合ChatGPT CLI：将您的命令行界面转换为 ChatGPT
6 个 ChatGPT 提示，提升工作生产力 
 
 原文：www.kdnuggets.com/6-chatgpt-prompts-to-enhance-your-productivity-at-work 
 
 
图片来源：Freepik 
时间是我们在当前职业环境中最宝贵但却有限的资源之一。找到提高个人生产力的解决方案，在任务和待办事项不断涌入的情况下，已经成为一个优先事项。人工智能工具，特别是 ChatGPT，最近在许多组织中获得了重要地位，成为实现这一目标的宝贵资产。这篇文章列出了六种情况，其中 ChatGPT 可以帮助你在各种工作相关场景中提升生产力。只需制定准确有效的提示，就能引导 ChatGPT生成满足你需求的正确回应。 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织的 IT 
 
注意： 以下六个示例中的提示响应是使用 ChatGPT 免费在线版本 3.5 生成的。 
1. 一个美好的开始：规划与优先排序 
开始一天时面对一大堆待办事项，不知道从何开始？通过让 ChatGPT 帮助你整理和优先排序你的日程，可以很快解决这个问题。以下是一个示例提示和生成的回应： 
提示： 
 
 “ChatGPT，帮助我根据以下任务创建一个今天的待办事项列表：完成季度报告，回应客户邮件，为团队会议做准备，并审查项目提案。季度报告截止时间是今天结束前，团队会议在下午 2 点，昨天有大约 15 封客户邮件待处理。项目提案是下周战略会议的内容。” 
 
回应： 
 
 ""当然！这是今天的优先任务列表： 
 *完成季度报告（高优先级，截止时间为今天结束前） 
 截止日期：今天 
 理由：这是时间敏感的任务，需要立即处理，以确保按时提交。* 
 *为团队会议做准备（高优先级，会议在下午 2 点） 
 截止日期：今天下午 2 点 
 理由：为会议做准备至关重要，因为它直接影响你在会议中的表现和贡献。* 
 *回应客户邮件（中等优先级，确保所有问题得到解答） 
 截止日期：今天结束前，但在处理更高优先级任务后开始 
 原因：回复客户电子邮件对维护良好的客户关系很重要，但可以安排在更紧急的任务之后进行。* 
 *审查项目提案（优先级低，可以在下午如果时间允许的话完成） 
 截止日期：下周的战略会议 
 原因：虽然这些提案很重要，但不紧急，可以在更紧迫的任务完成后再审阅。"" 
 
2. 高效的电子邮件管理和回复技巧 
以下示例提示展示了如何高效处理未读电子邮件的过载（特别是在您从假期归来后的第一天！），通过获得帮助来撰写回复，特别是对于那些可能需要仔细思考的挑战性案例： 
提示： 
 
 “ChatGPT，我应该如何回应询问订单状态并对延迟感到非常不满的客户？” 
 
回复： 
 
 ""*亲爱的[客户的名字]， 
 我对处理您的订单延迟表示真诚的歉意。我理解这对您来说可能很令人沮丧。您的订单目前正在加快处理，我们预计将在[具体日期]之前发货。感谢您的耐心和理解。如果您有任何其他问题或疑虑，请随时联系我。 
 *此致， 
 [你的名字]*"" 
 
3 - 在会议上掌握高管演示的关键要点和行动项目 
您的直线经理要求您创建两张幻灯片，总结高管会议的要点和行动项目，而整个对话记录已经被记录下来。那么为什么不请求 ChatGPT 的帮助，以提升您的演示技巧并留下深刻印象呢？ 
提示： 
 
 “ChatGPT，总结一下我们上次高管会议的要点和行动项目。” 
 附件： 
 [Meeting_Minutes_2024_06_23.docx] 
 
备注： 如上所示，这种情况下的提示通常会与详细的会议资料一起提供，例如会议记录或录音记录，这些资料可以作为文本文件附加，或如果文本不长，也可以粘贴为提示的一部分。 
回复： 
 
 ""上次高管会议的要点： 
 回顾第二季度财务业绩：超出目标 10%讨论产品发布的新营销策略评估了正在进行的并购谈判的进展 
 行动项目： 
 财务团队需在周五之前准备好第二季度业绩的详细报告。营销团队需在下周一之前制定并呈现新的活动策略。法务团队需完成并购协议条款并安排下周的跟进会议。"" 
 
4. 更好地理解复杂信息 
想象一下，你必须向组织中的非技术部门讲解一个高度技术性的概念或技术，以获得提交财务支持申请的批准。像这样的简单提示可能在将复杂的概念转化为易于理解的见解方面非常有效。 
提示： 
 
 “ChatGPT，你能用简单的术语解释区块链的概念吗？” 
 
回应： 
 
 “当然！区块链是一种数字账本，记录了许多计算机上的交易，以确保数据无法被事后更改。它就像一串区块，每个区块包含一系列交易。这项技术安全且透明，使其非常适合用于加密货币交易和供应链管理。” 
 
5. 将头脑风暴的想法转化为创意内容 
团队头脑风暴会议是促进新想法和倡议的流行且有趣的方式。在收集了团队提供的所有想法后，使用此提示将你凌乱的“便签”转化为连贯的内容。 
提示： 
 
 ""ChatGPT，这是一个关于远程工作好处的头脑风暴想法列表。你能帮我将这些转化为一个组织良好且吸引人的博客帖子吗？ 
 工作时间的灵活性无需通勤更好的工作与生活平衡更高的生产力雇主和员工的成本节省访问全球人才库减少环境影响增加工作满意度"" 
 
回应： 
 
 “标题：远程工作的好处：为何越来越多的公司正在采纳它” 
 介绍： 
 远程工作变得越来越受欢迎，为员工和雇主提供了诸多优势。这一转变正在改变传统的工作场所，带来显著的好处。 
 主要观点： 
 1 -工作时间的灵活性： 
 远程工作最吸引人的好处之一是其提供的灵活性。员工可以制定适合个人生活的工作时间表，从而对日常工作有更大的自主权和控制权。 
 (…) 
 8 - 增加工作满意度： 
 远程工作相关的灵活性和自主性有助于提高工作满意度和员工士气，降低离职率，并培养更忠诚的员工队伍。 
 结论： 
 远程工作的好处很明显，随着越来越多的公司采用这种模式，工作的未来变得更加灵活和高效。拥抱远程工作可以带来更快乐、更高效和更环保的员工。"" 
 
注意： 为了简洁起见，上述回答部分内容已被省略。在完整的回应中，所有八个支持远程工作的头脑风暴要点将类似地详细阐述。 
6. 打磨你的写作 
最后，ChatGPT 可以作为一种有用的工具，以精准的方式完善你的写作，提高其清晰度和吸引力，或简单地进行语法和词汇检查。 
提示： 
 
 *""ChatGPT，你能帮我编辑和润色这段文字，以提高其清晰度和简洁性吗？ 
 ‘在当今快节奏的商业环境中，具备迅速做出明智决策的能力至关重要。AI 工具的实施可以通过提供实时数据分析和立即可用的见解来极大增强这一能力。’""* 
 
回应： 
 
 “""在当今快节奏的商业环境中，能够迅速做出明智的决策至关重要。实施 AI 工具可以通过提供实时数据分析和可操作的见解来增强这一能力。” 
 
伊万·帕洛马雷斯·卡拉斯科萨 是 AI、机器学习、深度学习及大型语言模型领域的领军人物、作家、演讲者和顾问。他培训和指导他人在实际应用中利用 AI。 
更多相关主题 
 7 款 AI 驱动的工具提升数据科学家的生产力5 款 ChatGPT 功能提升你的日常工作用 Ruff 提升你的 Python 编码风格ChatGPT 是如何工作的，为什么它有效？忘掉 ChatGPT，这款新的 AI 助手领先一步，将…ChatGPT CLI：将你的命令行界面转换为 ChatGPT
数据科学中的六个常见错误及如何避免 
 
 原文：www.kdnuggets.com/2020/09/6-common-data-science-mistakes.html 
 
评论 
 
照片由 chuttersnap 在 Unsplash 提供。 
介绍 
在数据科学或机器学习中，我们使用数据进行描述性分析，以从数据中得出有意义的结论，或者我们可以将数据用于预测目的，建立可以对未见数据进行预测的模型。任何模型的可靠性取决于数据科学家的专业水平。构建一个机器学习模型是一回事，确保模型最优且质量最高是另一回事。本文将讨论六个常见错误，这些错误可能会对机器学习模型的质量或预测能力产生不利影响，并包含几个案例研究。 
数据科学中的六个常见错误 
在本节中，我们讨论六个常见错误，这些错误可能严重影响数据科学模型的质量。包含了几个实际应用的链接。 
我们经常假设我们的数据集质量良好且可靠 
数据是任何数据科学和机器学习任务的关键。数据有不同的形式，如数值数据、分类数据、文本数据、图像数据、语音数据和视频数据。模型的预测能力取决于用于构建模型的数据质量。因此，在执行任何数据科学任务如探索性数据分析或构建模型之前，检查数据的来源和可靠性是极其重要的，因为即使是看似完美的数据集也可能包含错误。有几个因素可能会降低数据的质量： 
 错误数据缺失数据数据中的离群值数据冗余数据不平衡数据缺乏变异性动态数据数据规模
欲了解更多信息，请参阅以下文章：数据总是不完美的。 
根据我个人在工业数据科学项目中的经验，我的团队花了 3 个月的时间与系统工程师、电气工程师、机械工程师、现场工程师和技术员合作，才理解现有的数据集以及如何利用这些数据提出正确的问题。确保数据无误且质量高将有助于提高模型的准确性和可靠性。 
不要专注于使用整个数据集 
有时作为一个数据科学初学者，当你需要处理一个数据科学项目时，你可能会被诱惑使用提供的整个数据集。然而，如上所述，数据集可能存在若干缺陷，如异常值、缺失值和冗余特征。如果你的数据集中存在缺陷的数据部分非常小，你可以简单地从数据集中删除这些不完美的数据子集。然而，如果不适当数据的比例显著，则可以使用数据插补技术来近似缺失数据。 
在实施机器学习算法之前，有必要只选择训练数据集中相关的特征。将数据集转化为只选择训练所需的相关特征的过程称为降维。特征选择和降维很重要，主要有三个原因： 
a) 防止过拟合：一个具有过多特征的高维数据集有时可能导致过拟合（模型捕捉到真实和随机效应）。 
b) 简洁性：一个特征过多的过于复杂的模型可能难以解释，特别是当特征之间相关时。 
c) 计算效率：在低维数据集上训练的模型计算效率高（算法执行所需的计算时间更少）。 
有关降维技术的更多信息，请参阅以下文章： 
 使用协方差矩阵图进行特征选择和降维机器学习：通过主成分分析进行降维
使用降维技术去除特征之间的不必要相关性可能有助于提高你的机器学习模型的质量和预测能力。 
在使用数据进行模型构建之前对数据进行缩放 
缩放你的特征将有助于提高模型的质量和预测能力。例如，假设你希望建立一个模型来预测目标变量信用度，基于预测变量如收入和信用评分。由于信用评分的范围从 0 到 850，而年收入的范围可能从 25,000 美元到 500,000 美元，如果不对特征进行缩放，模型将会偏向于收入特征。这意味着与收入参数相关的权重因子将非常小，这将导致预测模型仅仅基于收入参数来预测信用度。 
为了将特征缩放到相同的尺度，我们可以选择对特征进行标准化或归一化。通常情况下，我们假设数据呈正态分布，并默认使用标准化，但这并非总是如此。在决定使用标准化还是归一化之前，重要的是先查看特征的统计分布。如果特征趋向于均匀分布，则可以使用归一化（MinMaxScaler）。如果特征大致呈高斯分布，则可以使用标准化（StandardScaler）。再次注意，无论你使用归一化还是标准化，这些方法都是近似的，并且会对模型的总体误差产生影响。 
调整模型中的超参数 
在模型中使用错误的超参数值可能导致模型非最优且质量较低。重要的是你需要对所有超参数进行训练，以确定性能最佳的模型。模型的预测能力如何依赖于超参数的一个很好的示例可以在下面的图中找到（来源：差劲与优秀的回归分析）。 
 
图 1。使用不同学习率参数进行回归分析。来源：差劲与优秀的回归分析，发布于 Towards AI，2019 年 2 月，由 Benjamin O. Tayo 编写。 
请记住，使用默认超参数并不总能得到最佳模型。有关超参数的更多信息，请参见本文：机器学习中的模型参数和超参数 — 有什么区别。 
比较不同算法 
在选择最终模型之前，比较几个不同算法的预测能力是很重要的。例如，如果你正在构建一个分类模型，你可以尝试以下算法： 
 逻辑回归分类器支持向量机（SVM）决策树分类器K-最近邻分类器朴素贝叶斯分类器
如果你正在构建一个线性回归模型，你可以比较以下算法： 
 线性回归K-邻近回归（KNR）支持向量回归（SVR）
有关比较不同算法的更多信息，请参阅以下文章： 
 线性回归与 KNN 回归的比较研究机器学习过程教程
量化模型中的随机误差和不确定性 
每个机器学习模型都有固有的随机误差。这种误差源于数据集的固有随机特性；源于在模型构建过程中数据集被划分为训练集和测试集的随机方式；或者源于目标列的随机化（用于检测过拟合的方法）。始终量化随机误差对模型预测能力的影响是非常重要的。这将有助于提高模型的可靠性和质量。有关随机误差量化的更多信息，请参见以下文章：机器学习中的随机误差量化。 
总结 
总结来说，我们讨论了六个常见的错误，这些错误可能影响机器学习模型的质量或预测能力。始终确保你的模型是最佳的且质量最高是非常有用的。避免上述讨论的错误可以帮助数据科学爱好者构建可靠和值得信赖的模型。 
相关： 
 数据质量评估并非全是美好。你应该意识到哪些挑战？必须知道：大数据常见的数据质量问题及其处理方法数据科学家认为数据是他们的头号问题。这就是他们错在哪里。
更多相关话题 
 5 个常见的数据科学错误及如何避免避免这些每个 AI 初学者都会犯的 5 个常见错误5 个常见的 Python 问题（及如何避免）新手数据科学家应避免的错误对话式 AI 开发中的 3 个关键挑战及如何避免10 个最常见的数据质量问题及其解决方法
我最近发现的 6 个酷炫的 Python 库 
 
 原文：www.kdnuggets.com/2021/09/6-cool-python-libraries-recently.html 
 
评论 
作者 Dhilip Subramanian，数据科学家和 AI 爱好者 
 
 
我们的前三个课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业的快车道。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你在 IT 方面的组织 
 
Python 是机器学习的核心组成部分，库使我们的生活更简单。最近，我在进行 ML 项目时发现了 6 个很棒的库。它们帮助我节省了大量时间，我将在这篇博客中讨论它们。 
1. clean-text 
一个真正不可思议的库，clean-text 应该是你处理抓取或社交媒体数据时的首选。最酷的地方在于，它不需要任何长的复杂代码或正则表达式来清理我们的数据。让我们看看一些示例： 
安装 
!pip install cleantext
1 
示例 
#Importing the clean text library
from cleantext import clean# Sample texttext = """""" Zürich, largest city of Switzerland and capital of the canton of 633Zürich. Located in an Al\u017eupine. ([`google.com`](https://google.com/)). Currency is not ₹""""""# Cleaning the ""text"" with clean textclean(text, 
fix_unicode=True, 
to_ascii=True, 
lower=True, 
no_urls=True, 
no_numbers=True, 
no_digits=True, 
no_currency_symbols=True, 
no_punct=True, 
replace_with_punct="" "", 
replace_with_url="""", 
replace_with_number="""", 
replace_with_digit="" "", 
replace_with_currency_symbol=""Rupees"")
123456789101112131415 
输出 
 
从上面可以看到，它包含了“Zurich”中的 Unicode（字母‘u’已编码）、ASCII 字符（在 Al\u017eupine.）、货币符号（卢比）、HTML 链接和标点符号。 
你只需要在 clean 函数中提及所需的 ASCII、Unicode、URLs、数字、货币和标点符号。或者，它们可以在上述函数中用 replace 参数替换。例如，我将卢比符号更改为 Rupees。 
完全没有必要使用正则表达式或长代码。这个库非常方便，尤其是当你需要清理抓取或社交媒体数据的文本时。根据你的需求，你也可以单独传递参数，而不是将它们全部组合在一起。 
欲了解更多详细信息，请查看这个GitHub 仓库。 
2. drawdata 
Drawdata 是我发现的另一个酷炫的 Python 库。你有多少次遇到需要向团队解释 ML 概念的情况？这肯定很常见，因为数据科学完全是团队合作的事。这个库可以帮助你在 Jupyter Notebook 中绘制数据集。 
个人来说，当我向我的团队解释 ML 概念时，我非常喜欢使用这个库。对创建这个库的开发者致以敬意！ 
Drawdata 仅适用于具有四个类别的分类问题。 
安装 
!pip install drawdata
1 
示例 
# Importing the drawdata 
from drawdata import draw_scatterdraw_scatter()
12 
输出 
 
图片来源：作者 
执行 draw_Scatter() 后，上述绘图窗口将打开。显然，有四个类别，即 A、B、C 和 D。你可以点击任何类别并绘制你想要的点。每个类别在绘图中代表不同的颜色。你还可以选择将数据下载为 CSV 或 JSON 文件。此外，数据可以复制到剪贴板，并从以下代码中读取。 
#Reading the clipboardimport pandas as pd 
df = pd.read_clipboard(sep="","")
df
123 
这个库的一个局限性是它只提供两个数据点和四个类别。但除此之外，它绝对值得尝试。有关更多细节，请查看这个GitHub 链接。 
3. Autoviz 
我永远不会忘记我使用 matplotlib 进行探索性数据分析的时光。有很多简单的可视化库。然而，我最近发现了 Autoviz，它可以用一行代码自动可视化任何数据集。 
安装 
!pip install autoviz
1 
示例 
我在这个示例中使用了 IRIS 数据集。 
# Importing Autoviz class from the autoviz library
from autoviz.AutoViz_Class import AutoViz_Class#Initialize the Autoviz class in a object called df
df = AutoViz_Class()# Using Iris Dataset and passing to the default parametersfilename = ""Iris.csv""
sep = "",""graph = df.AutoViz(
filename,
sep="","",
depVar="""",
dfte=None,
header=0,
verbose=0,
lowess=False,
chart_format=""svg"",
max_rows_analyzed=150000,
max_cols_analyzed=30,
)
123456789101112131415 
上述参数是默认的。有关更多信息，请查看这里。 
输出 
 
图片来源：作者 
我们可以通过一行代码查看所有的可视化并完成我们的 EDA。虽然有很多自动可视化库，但我真的很喜欢这个。 
4. Mito 
大家都喜欢 Excel，对吧？这是初步探索数据集最简单的方法之一。我几个月前遇到了 Mito，但直到最近才尝试，它真的让我爱不释手！ 
这是一个带有图形用户界面的 Jupyter-lab 扩展 Python 库，添加了电子表格功能。你可以加载你的 CSV 数据，并将数据集作为电子表格进行编辑，它会自动生成 Pandas 代码。非常酷。 
Mito 确实值得写一整篇博客。然而，今天我不会详细介绍。相反，我会给你一个简单的任务演示。有关更多详细信息，请查看这里。 
安装 
**#First install mitoinstaller in the command prompt** pip install mitoinstaller**# Then, run the installer in the command prompt**
python -m mitoinstaller install**# Then, launch Jupyter lab or jupyter notebook from the command prompt** python -m jupyter lab
12 
有关安装的更多信息，请查看这里。 
**# Importing mitosheet and ruuning this in Jupyter lab**import mitosheet
mitosheet.sheet()
12 
执行上述代码后，mitosheet 将在 jupyter lab 中打开。我使用的是 IRIS 数据集。首先，我创建了两列新数据。一列是平均萼片长度，另一列是总萼片宽度。其次，我更改了平均萼片长度的列名。最后，我为平均萼片长度列创建了一个直方图。 
按照上述步骤操作后，代码会自动生成。 
输出 
 
图片来源：作者 
以下代码是为上述步骤生成的： 
from mitosheet import * # Import necessary functions from Mito
register_analysis('UUID-119387c0-fc9b-4b04-9053-802c0d428285') # Let Mito know which analysis is being run# Imported C:\Users\Dhilip\Downloads\archive (29)\Iris.csv
import pandas as pd
Iris_csv = pd.read_csv('C:\Users\Dhilip\Downloads\archive (29)\Iris.csv')# Added column G to Iris_csv
Iris_csv.insert(6, 'G', 0)# Set G in Iris_csv to =AVG(SepalLengthCm)
Iris_csv['G'] = AVG(Iris_csv['SepalLengthCm'])# Renamed G to Avg_Sepal in Iris_csv
Iris_csv.rename(columns={""G"": ""Avg_Sepal""}, inplace=True)
1234567 
5. Gramformer 
另一个令人印象深刻的库，Gramformer 基于生成模型，帮助我们纠正句子的语法。这个库有三个模型，分别是检测器、高亮器和纠正器。检测器识别文本是否有不正确的语法。高亮器标记出有问题的词性，而纠正器则修复错误。Gramformer 完全开源，并处于早期阶段，但它不适合处理长段落，因为它只在句子级别工作，并且已经训练了 64 长度的句子。 
目前，纠正器和高亮器模型正在工作。我们来看一些示例。 
安装 
!pip3 install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git
1 
实例化 Gramformer 
gf = Gramformer(models = 1, use_gpu = False) # 1=corrector, 2=detector (presently model 1 is working, 2 has not implemented)
1 
 
示例 
#Giving sample text for correction under gf.correctgf.correct("""""" New Zealand is island countrys in southwestern Paciific Ocaen. Country population was 5 million """""")
1 
输出 
 
作者提供的图片 
从以上输出中，我们可以看到它纠正了语法甚至拼写错误。这是一个非常了不起的库，功能也很好。我还没有尝试高亮器，你可以尝试一下，并查看 GitHub 文档以了解更多细节。 
6. Styleformer 
我对 Gramformer 的积极体验促使我寻找更多独特的库。这就是我发现 Styleformer 的原因，它是另一个极具吸引力的 Python 库。Gramformer 和 Styleformer 都由 Prithiviraj Damodaran 创建，并且都基于生成模型。向创作者致以敬意，感谢开源。 
Styleformer 帮助将休闲句子转换为正式句子，将正式句子转换为休闲句子，将主动句转换为被动句，将被动句转换为主动句。 
来看看一些示例 
安装 
!pip install git+https://github.com/PrithivirajDamodaran/Styleformer.git
1 
实例化 Styleformer 
sf = Styleformer(style = 0)# style = [0=Casual to Formal, 1=Formal to Casual, 2=Active to Passive, 3=Passive to Active etc..]
1 
示例 
# Converting casual to formal sf.transfer(""I gotta go"")
1 
 
# Formal to casual 
sf = Styleformer(style = 1) # 1 -> Formal to casual# Converting formal to casual
sf.transfer(""Please leave this place"")
123 
 
# Active to Passive 
sf = Styleformer(style = 2) # 2-> Active to Passive# Converting active to passive
sf.transfer(""We are going to watch a movie tonight."")
123 
 
# passive to active
sf = Styleformer(style = 2) # 2-> Active to Passive# Converting passive to active
sf.transfer(""Tenants are protected by leases"")
123 
 
查看上述输出，它的转换非常准确。我在一次分析中使用了这个库，将休闲语言转换为正式语言，特别是用于社交媒体帖子。有关更多详细信息，请查看 GitHub。 
你可能对一些之前提到的库比较熟悉，但像 Gramformer 和 Styleformer 这样的库则是新兴的玩家。它们被极度低估，毫无疑问值得被了解，因为它们节省了我大量时间，并且我在 NLP 项目中大量使用了它们。 
感谢阅读。如果你有任何补充意见，请随时留下评论！ 
你可能还喜欢我之前的文章 五个酷炫的 Python 数据科学库 
个人简介： Dhilip Subramanian 是一名机械工程师，已获得分析硕士学位。他拥有 9 年的经验，专注于与数据相关的多个领域，包括 IT、营销、银行、电力和制造业。他对自然语言处理和机器学习充满热情。他是SAS 社区的贡献者，并喜欢在 Medium 平台上撰写有关数据科学的各种技术文章。 
原文。经许可转载。 
相关内容： 
 五个有趣的 Python 库用于数据科学Python 中的简单语音转文本学习数据科学和机器学习：第一步
相关阅读 
 数据科学、数据可视化与…的 38 个顶级 Python 库2022 年数据科学家应该知道的 Python 库可解释 AI：揭示模型决策的 10 个 Python 库数据清洗的 Python 库介绍超越 Numpy 和 Pandas：挖掘鲜为人知的…50 级数据科学家：需要了解的 Python 库
6 个数据科学证书，让你的职业生涯更进一步 
 
 原文：www.kdnuggets.com/2021/02/6-data-science-certificates.html 
 
评论 
 
由 Lewis Keegan 在 Unsplash 提供的照片。 
 
我们的前三推荐课程 
 1. 谷歌网络安全证书 - 快速进入网络安全职业的快车道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT 需求 
 
由于数据科学领域的吸引力以及高收入的前景，越来越多的人决定加入这个领域。有些人可能来自技术背景，而另一些人则因好奇而加入；无论你决定加入该领域的原因是什么，你的首要目标可能是拥有一个强大而扎实的作品集，以帮助你获得理想的工作。 
 
 那么，如何提升你的作品集的吸引力？ 
 
尽管进入数据科学领域并不一定需要任何学位或证书，但有时拥有一些证书可以帮助你在申请工作时脱颖而出。 
优秀的数据科学作品集的关键在于收集展示你技能、证明你知识并展示你构建稳固数据科学项目能力的项目。这是一个优秀作品集的核心，但你也可以添加一些证书来证明你投入了时间、精力和金钱来提升技能，成为更合格的数据科学家。 
幸运的是，并非所有证书都要求你去考试中心。事实上，大多数受欢迎的数据科学证书可以在舒适的家中获得。 
本文向你展示了 6 个极具吸引力的证书，这些证书能提高你获得实习或理想工作的机会。 
Microsoft 认证：Azure 数据科学家助理 
微软是技术和软件领域的领先企业之一；他们提供一个证书，旨在评估你进行实验、训练机器学习模型、优化模型性能并使用 Azure 机器学习工作区进行部署的能力。 
要获得该证书，你需要通过一场考试，你可以通过两种方式之一为这场考试做准备。微软提供免费的在线材料供你自学以备考。如果你更喜欢有老师指导，他们还提供了一个收费选项，可以让 Azure 机器学习讲师对你进行辅导。 
这次考试的费用大约为$165。费用因你所在的国家/地区而异。 
IBM 数据科学专业证书 
该证书由 IBM 颁发，并在一个系列课程结束时提供，该课程系列将带你从一个完全的数据科学初学者成长为一个在线的、按自己节奏的专业数据科学家。 
IBM 数据科学专业证书在Coursera和edX上均可获得。在任何一个平台上，你都需要完成一系列涵盖数据科学核心知识的课程，以获得证书和 IBM 徽章。 
要获得 Coursera 的证书，你需要每月支付$39 的费用，因此你完成系列课程的时间越早，你需要支付的费用就越少。另一方面，edX 要求支付$793 的费用以获得完整的课程体验，无论你需要多久才能完成它。 
Google 专业数据工程师认证 
Google 的专业数据工程师认证旨在考核你作为数据工程师所需的技能。数据工程师可以做出数据驱动的决策，构建可靠的模型，并对其进行训练、测试和优化。 
你可以通过直接在 Google 证书的官方网站申请来获得该证书，或者你可以参加Coursera上的课程系列和证书。这些课程将教授你有关机器学习和 AI 基础知识的所有内容，以及如何构建高效的数据管道和分析。 
要访问 Coursera 上的课程系列，你需要拥有 Coursera Plus 或支付每月$49 的费用，直到你完成系列课程并获得证书。 
Cloudera 认证专业数据工程师（CCP Data Engineer） 
Cloudera 针对开源开发者，提供CCP 数据工程师证书，让开发者测试他们在 Cloudera CDH 环境中高效收集、处理和分析数据的能力。 
要通过这次考试，你将被提供 5-10 个数据科学问题，每个问题都有自己的大数据集和 CDH 集群。你的任务是为这些问题找到高精度的解决方案并正确实施。 
要参加这次考试，你需要在考试中至少得分 70%。考试时间为 4 小时，费用为$400。你可以在任何地方在线参加这次考试。 
SAS 认证 AI 与机器学习专业人员 
与我们讨论过的证书不同，SAS AI & 机器学习专业证书是通过通过三场测试三个不同技能集的考试来获得的。你需要通过的三场考试包括： 
 机器学习考试将测试你构建、训练、测试性能和优化监督式机器学习模型的技能。预测和优化测试。在此测试中，将测试你处理、可视化数据、构建数据管道和解决优化问题的能力。自然语言处理和计算机视觉测试。
SAS 提供免费的 30 天备考资料，帮助你准备并通过这三场考试。 
TensorFlow 开发者证书 
TensorFlow 是广泛使用的机器学习、人工智能和深度学习应用程序包之一。TensorFlow 开发者证书授予开发者，以展示他们使用 TensorFlow 开发机器学习和深度学习解决方案的能力。 
你可以通过完成DeepLearning.AI TensorFlow 开发者专业证书的 Coursera 课程系列来为这个证书做好准备。一旦获得此证书，你的名字和照片将被添加到Google Developers网页上。 
TensorFlow 开发者证书有效期为 3 年。之后，你需要重新参加考试，以保持你的技能水平与 TensorFlow 包的最新更新同步。 
收获 
如果你问任何数据科学家他们是否需要学位或证书来获得他们的工作，大多数人会告诉你，他们是从非技术背景进入数据科学的，只有一个好奇的心态，想要更多地学习。 
尽管你可以通过自学数据科学的核心概念和构建实际项目或可以轻松应用于实际数据的项目来成为数据科学家并获得好工作，但有时拥有一个证书可以帮助你的简历脱颖而出，吸引招聘人员的注意。 
由于数据科学是目前流行的领域之一，你会发现网上有大量关于如何成为“优秀数据科学家”或“如何获得梦想的数据科学职位”的教程和指南，更不用说可以获得的证书和免费课程。我曾经也被数据科学领域的信息淹没，但我总是欣赏那些简单、直接的文章，它们直接切入要点，不拖延话题。 
原文。经许可转载。 
相关： 
 数据科学认证——为什么它很重要以及在哪里获得？10 个数据科学自学资源7 个最推荐学习的数据科学技能
更多相关主题 
 用这 3 个热门证书来提升你的技术职业生涯将你的数据科学职业生涯提升到下一个水平9 个专业证书可以带你获得学位……如果……提升你的机器学习技能的 15 个 YouTube 频道谷歌提供的 5 个 MLOps 课程来提升你的 ML 工作流提升技能的 5 个免费机器学习课程
2022 年值得工作的 6 家数据科学初创公司 
 
 原文：www.kdnuggets.com/2022/02/6-data-science-startups-work-2022.html 
 
 
Mika Baumeister 的照片，来源于 Unsplash 
数据科学推动了未来。它帮助我们创造创新产品，为客户提供更好的服务，做出智能决策，并为所有利益相关者提供价值。难怪数据科学已经成为核心业务职能！ 
研究表明，到 2025 年，大数据市场的全球收入将达到$68.09 billion，这也是为什么我们看到许多围绕这一行业发展的初创公司。 
 
我们的三大课程推荐 
 1. Google 网络安全证书 - 加速进入网络安全职业生涯 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织的 IT 工作 
 
数据科学是少数几个期望员工随着工作进步而不断发展的领域之一。因此，随着数据科学家职业生涯的进展和技术的进步，存在着持续的学习曲线。 
近年来，初创公司数量有所上升，其中包括数据科学、人工智能和机器学习模型。如果你想测试自己的技能，以下是 2022 年你应该考虑工作的六家顶尖初创公司。 
数据科学领域的 6 家顶尖初创公司 
研究表明，到 2025 年，全球组织将生成180 zettabytes的数据。这意味着许多新兴小型企业使用数据科学可以成为一个很好的工作场所，特别是对于刚进入这一领域的人来说。 
在初创公司工作可以帮助你提升技能，并且你可以期望随着组织的成长而成长。以下是六家利用数据科学的顶尖初创公司，对于刚刚开始职业生涯的人来说，它们可能是一个绝佳的工作场所。 
Avora 
Avora 总部位于伦敦，组织致力于增强分析。公司提供一个简化方式进行深入分析的平台。通过隐藏数据分析的复杂性，使非技术用户能够创建报告。此外，借助机器学习模型，该平台将数据准备时间缩短了 80%。 
此外，它帮助业务经理做出明智的决策并提高生产力。该平台可以服务于各种行业，包括供应链和金融服务、零售、广告及娱乐。希望提升技能并与公司一起学习的数据科学家可以将 Avora 视为一个很好的工作选择。 
Cognino AI 
当我们谈论人工智能与数据科学的结合时，Cognino AI 名列其中。尽管是一家初创公司，Cognino 在数据科学和 AI 领域取得了显著进展，创建了一种以研究为主导的 AI，具有深厚的自学习可解释 AI 专业知识。该解决方案使组织能够加快数据准备过程并获得有意义的洞察。他们有数据科学家培训 AI 以准备大型数据集来帮助客户。在 Cognino 工作可以帮助你磨练技能，并在 AI 主导的领域中朝着伟大的目标努力。 
Databand 
Databand 于 2018 年由 Joshua Benamram、Evgeny Shulman 和 Victor Shafran 创立。该组织在特拉维夫运营，为客户提供一个用于敏捷机器学习开发的软件平台。该平台使组织能够获得数据的全局视图，完成工作流管道，并管理资源消耗。 
Databand 还可以原生适配你的现代数据栈，并可集成到如 Apache Airflow、Kubernetes、Spark 及其他云提供商的机器学习工具中。希望了解各种工具及其集成方式的数据科学家可以在 Databand 获得很好的学习体验。 
Exceed.ai 
Exceed.ai 通过其 AI 驱动的销售平台迅速崛起，允许组织与潜在客户进行沟通，帮助营销和销售团队改善潜在客户的参与度和资格审核工作。 
利用自然语言和人工智能，该公司提供了一种通过聊天和电子邮件与潜在客户沟通的方式，从而释放资源并自动化重复任务。对人工智能有强烈兴趣和自然语言处理的数据科学家可以从 Exceed.ai 学到很多，这使其成为一个很棒的工作场所。 
Indico 
全球组织在准备非结构化数据和获得有意义的洞察时面临挑战，因为缺乏框架。Indico 推出了一个人工智能平台，帮助客户组织非结构化内容并自动化后台任务。Indico 因自动化手动流程和基于文档的工作流程而声名鹊起。 
该平台使用户能够训练机器学习模型，并消除对传统规则基础技术的需求。它使大规模非结构化数据集的组织能够简单易用地采用人工智能和机器学习模型。任何对数据科学和创建机器学习模型感兴趣的人都会在 Indico 学到很多东西。 
Algorithmia 
Algorithmia 被认为是机器学习操作领域最优秀的初创公司之一，它帮助组织以企业级治理和安全性生产机器学习模型。 
Algorithmia 提供机器学习部署、灵活的工具，并利用现有的软件开发生命周期和持续集成/持续交付实践。该组织已拥有超过 100,000 名工程师使用他们的解决方案，客户包括政府情报机构、财富 500 强公司甚至联合国。 
Algorithmia 是数据科学业务的终极圣杯，它能帮助你提升技能，并推动你的职业发展。 
总结 
数据科学在过去几年中经历了爆炸性增长。因此，数据科学家正在寻找能够帮助他们磨练技能并与不断变化的行业同步发展的组织。一些初创公司围绕数据科学展开，已在这一领域取得了快速进展。寻找良好工作场所的数据科学家可以选择上述提到的初创公司作为职业发展的强大跳板。 
**Nahla Davies**是一位软件开发者和技术写作专家。在全职投入技术写作之前，她还担任过 Inc. 5,000 排名的体验品牌组织的首席程序员，该组织的客户包括三星、时代华纳、Netflix 和索尼等。 
了解更多此话题 
 印度十大人工智能初创公司KDnuggets 新闻 2022 年 3 月 16 日: 学习数据科学基础与 5 个…2022 年数据科学最佳学习资源KDnuggets™ 新闻 22:n05, 2 月 2 日: 掌握机器学习的 7 个步骤…2022 年 8 个最佳数据科学课程，助力快速职业发展如何在 2022 年成为成功的数据科学自由职业者
你需要的 6 种数据科学技术来构建你的供应链管道 
 
 原文：www.kdnuggets.com/2022/01/6-data-science-technologies-need-build-supply-chain-pipeline.html 
 
供应链管道是将不同过程组合在一起，以生产和分发产品和服务给客户的系统。供应链中包含了各种各样的活动。下图展示了供应链管道的鸟瞰图： 
 
作者提供的图像 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业道路。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你组织的 IT 
 
全球各地的公司正在不断努力提高供应链管道的效率，以通过提供价值来满足客户。这可以通过数据科学来实现，通过数据驱动的判断和决策。根据这篇文章，员工每年在电话和电子邮件上花费超过 1 亿小时，将数据在贸易伙伴之间来回移动，给英国公司造成近 20 亿美元的费用。这些浪费的时间可以通过使用技术自动化沟通来减少。以下是构建全面顺畅供应链管道所需的一些数据科学技术。 
1. 大数据 
要能够做出数据驱动的决策，你需要数据。这些数据可以是结构化的、非结构化的或半结构化的，无论是机器生成还是人工生成的。但这些数据从哪里来呢？它可以是来自气象站、船只和集装箱的传感器数据。 
为了构建一个大数据解决方案以获得有价值的洞察，请考虑以下四个领域： 
 体量： 生成的数据量，例如 100 PB/年，500 TB/月多样性： 生成的数据类型，例如文本、图像速度： 需要处理和存储的数据频率，例如 1000 张图像/分钟 或 4000 吉字节/分钟真实性： 生成数据的质量。
一旦你涵盖了这四个领域，你可以使用基于云的解决方案来处理、存储和分析数据。例如，Amazon Simple Storage Service（Amazon S3）可以存储可扩展、安全且高效的数据。之后，你可以将这些数据源接入其他服务，如 Amazon QuickSight，以创建互动式商业智能仪表盘。还有其他云服务提供商和服务提供类似的产品。公司可以根据现有技术选择服务提供商，这将帮助他们实现更容易的集成。 
2. 机器学习 
你可以利用大数据并在其基础上增加层次，创建有价值的洞察，这些洞察可以转化为行动。通过使用机器学习和人工智能，你可以通过展示相关产品来提高客户满意度。另一方面，你可以使用预测算法来预测客户需求。此外，预测数据可以帮助自动化仓库操作，以提高库存周转率。 
通过这种方式，公司可以与其他企业竞争，并迅速将新产品推向市场。最后，供应链经理可以根据这些预测和预测计划并采取行动，做出关键决策，以促进业务增长。 
3. 物联网（IoT） 
通常称为物联网（IoT）的互联网技术，通过提供对操作的更多控制来帮助公司提高可见性。当你使用物联网传感器标记物品时，可以做出实时决策。例如，企业可以跟踪和监控船只，并根据交通情况重新路线，以实现可靠和更快的运输，从而减少运输延误。此外，公司可以通过监控冷藏运输容器的温度数据来避免因技术故障而损失资金。 
物联网（IoT）可以为供应链管道增加可靠性。公司可以轻松跟踪丢失或错放的容器，以更快地处理争议。企业可以通过实时更新他们所购买的产品来增加对客户的价值。 
4. 云计算 
云计算允许企业部署其供应链管道软件，如客户关系管理（CRM）。云计算的好处是无穷无尽的。它们可以根据流量进行扩展。此外，根据服务水平协议（SLA），应用程序将有保证的正常运行时间（高可用性）。像 AWS 这样的云平台使其服务可靠、安全且具有成本效益。此外，按需付费模式帮助企业节省开支。 
不仅是面向客户的应用，企业还可以部署其他应用来跟踪和监控库存系统。此外，BI 仪表盘可以提高公司的可见性。 
5. 类似 JavaScript 的语言 
上述应用程序可以根据需要强大，但需要一个前端才能使用。这个交互式用户界面可以使用 JavaScript 构建。前端数据验证可以帮助确保数据的真实性。 
现在，有许多工具可以使用 JavaScript 更快地构建应用程序，例如 Webpack、Babel 和 NPM，等。Webpack 允许将 JavaScript 应用程序进行打包。Babel 用于将 JSX 转换为纯 JavaScript。NPM 注册库包含了许多构建企业级应用程序的库。 
企业可以与其他第三方供应商集成，以提供诸如身份验证（auth0）、短信（twilio）、分析（powerbi）、消息传递（intercom）、文档（confluence）等服务。 
6. 类似 React 的框架 
React 是一个开源的 JavaScript 库，用于构建可组合的用户界面，它提供了构建企业级应用程序的最先进功能。诸如可重用组件、虚拟 DOM、JSX 等功能使得 React 成为一个优秀的 JavaScript 库。React 的懒加载功能可以动态加载组件，从而提高应用程序的性能。由于 React 是从 CDN 提供的，因此服务成本较低。此外，还有数以千计的 NPM 模块可以用于更快地构建应用程序。它提供了开箱即用的库，如 compromise。它可以用于进行自然语言处理。此外，企业还可以使用 React Native 创建跨平台的移动应用程序。 
如果您的企业有多个软件团队在开发应用程序，他们可以使用模块联邦构建微前端，而不是构建一个单体应用程序。这将提供完全的自由来独立设计和构建，而不会破坏整个应用程序。例如，N 个微前端可以提供跟踪、分析、身份验证、报告等服务。 
随着技术的进步，SCPs 已经采用了一些尖端技术，以优化和增强几乎每个人都依赖的管道。这些曾经新颖的六种技术，现在已成为供应链管道的关键部分。供应链经理应当关注这些数据科学技术，并尽早开始整合它们。这将帮助企业建立一个高效且具有成本效益的供应链管道，提高客户满意度。 
Zulie Rane 是一位自由撰稿人和编码爱好者。 
了解更多相关话题 
 5 种使用 AI 进行供应链管理的方法ETL 与 ELT：哪个更适合您的数据管道？使用 Bash 构建您的第一个 ETL 管道提升数据科学技能：你需要的关键 SQL 认证想用你的数据技能解决全球问题？这就是方法…你下一个晋升所需的数据分析师技能
使用 Tensorflow.js 实现计算机视觉应用的 6 个简单步骤 
 
 原文：www.kdnuggets.com/2020/06/6-easy-steps-implement-computer-vision-application-tensorflow-js.html 
 
评论 
由 Bala Venkatesh，数据科学家 
 
图片来源 
 
我们的三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析能力 
 3. 谷歌 IT 支持专业证书 - 支持组织中的 IT 工作 
 
介绍 
现在，很多人正在实现计算机视觉应用。你认为学习和实现它很困难吗？我的回答是否定的，因为现在有很多库可以实现如此强大的计算机视觉应用。最近你是否观看了 TensorFlow 峰会 2020？今年 TensorFlow 团队为我们宣布了许多酷炫的东西。现在我们将看到如何使用 Tensorflow.js 模型实现计算机视觉应用！！ 
TensorFlow.js 是什么？ 
 
 TensorFlow.js 是一个用于机器学习应用的开源库，支持在 JavaScript 中开发 ML 模型，并直接在浏览器或 Node.js 中使用 ML。 
 
Tensorflow.js 模型是什么？ 
 
 Tensorflow.js 模型是预训练模型，这意味着你无需准备/收集数据来训练模型。这些模型托管在 NPM 和 unpkg 上，因此可以立即在任何项目中使用。 
 
在这篇博客中，我们将介绍 Tensorflow.js 中的一个模型，即 MediaPipe Facemesh 模型。该模型专为移动设备上的前置摄像头设计，适用于面部在视野中占据相对较大画布比例的情况。 
Facemesh 模型演示 
我使用了 facemesh 模型来根据我的面部运动移动视频帧。请看下面的演示。我将告诉你实现下面的计算机视觉应用涉及的步骤。 
 
演示 
演示 
步骤 1：- 这些是实现计算机视觉应用的三个重要独立脚本标签代码。 
<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core""></script>
<script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter""></script>
<script src=""https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh""></script>
123 
 
 TensorFlow.js 核心，用于神经网络和数值计算的灵活低级 API。 
 TensorFlow.js 转换器，将 TensorFlow SavedModel 导入 TensorFlow.js 的工具。 
 facemesh 包在图像中找到面部边界和标志点。 
 
步骤 2:- 在正文内容中包含视频 HTML 标签，以通过网络摄像头读取面部。 
<video width=640 height=480 autoplay muted id=”camera”></video>
1 
步骤 3:- 使用 canvas 标签通过脚本（通常是 JavaScript）动态绘制图形。 
<canvas width=640 height=480 id=”augmented_canvas”></canvas>
1 
步骤 4:- 包含一个视频标签，在 canvas 标签上播放视频帧，根据面部运动移动视频帧。 
<video autoplay loop id=”movie” style=”visibility: hidden”>
<source src=”TensorFlowjs.mp4"" type=”video/mp4""></source>
</video>
123 
步骤 5:- 加载面部模型并估算面部，以找到图像中的面部边界和标志点。 
//load camera stream
const frame = document.getElementById(""camera"");//load movie stream
const movie = document.getElementById(""movie"");
movie.play();//prepare canvas
const canvas = document.getElementById(""augmented_canvas"");
const draw = canvas.getContext(""2d"");const result = await model.estimateFaces(frame, false);
123456 
步骤 6:- 使用以下代码在检测到的面部上绘制视频帧。 
//copy camera stream to canvas
draw.drawImage(frame, 0, 0, 640, 480);//check if face is detected
 if(result.length > 0)
 {
for (let i = 0; i < result.length; i++) {
const start = result[i].topLeft;
const end = result[i].bottomRight;
const size = [end[0] — start[0], end[1] — start[1]]; //Render a rectangle over each detected face.
 draw.drawImage(movie, start[0], start[1], size[0], size[1]);
}

}
123456789101112 
结论 
就这样，我们按照上述六个步骤实现了一个计算机视觉应用。你可以在这里获取完整的源代码。 
大家都说如果你想成为数据科学家，你应该熟悉 Python 或 R 编程语言，但现在我们可以使用 JavaScript 实现一个机器学习应用。 
 
来源: miro.medium.com/max/1400/1*F-1fq9TNjDnAYPAXnZP4Ww.png 
 
 祝大家学习愉快！如果你需要任何帮助或支持，请通过 LinkedIn 和 Twitter 与我联系。 
 
个人简介: Bala Venkatesh 是一名数据科学家。他热衷于从根本上理解技术，并分享想法和代码。 
原文. 经许可转载。 
相关: 
 利用 AI 对抗冠状病毒：通过深度学习和计算机视觉改进检测利用机器学习和计算机视觉进行作物疾病检测构建一个使用 TensorFlow 和 Streamlit 生成逼真面孔的应用
更多相关话题 
 计算机视觉中的 TensorFlow - 轻松实现迁移学习用 Python 在 10 个简单步骤中构建一个 AI 应用探索计算机视觉的世界：介绍 MLM 最新的……计算机视觉的 5 种应用关于数据管理及其重要性的 6 件事KDnuggets 新闻 2022 年 3 月 9 日：在 5 步内构建一个机器学习网络应用
每个有志数据科学家在 2019 年应设定的 6 个目标 
 
 原文：www.kdnuggets.com/2018/11/6-goals-every-wannabe-data-scientist-2019.html 
 
评论 
 
许多已经在技术领域工作的人意识到，他们想要开始新的路径，最终成为数据科学家。 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业的轨道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
这个目标本身是值得的，但人们也必须设定 2019 年的目标，以帮助自己更接近这个更广泛的目标。 
1. 成为相关组织的会员 
与其他对数据科学感兴趣的人建立联系，让人们了解现有的教育选项，理解数据科学行业中最突出的工具，并获得曾经也是数据科学初学者的人的鼓励。 
运筹学与管理科学学会（INFORMS）是 最大国际组织之一，拥有成千上万的成员。虽然该组织有线下活动，但也有仅限会员的在线论坛。 
然而，抱负的数据显示科学家也可以在他们的社区中探索本地选项。MeetUp.com 在全球提供各种会议，并 拥有超过 5,000 个与数据科学相关的活动。 
2. 熟悉新兴趋势并将其应用于职业目标 
数据科学是一个快速发展的行业，能够最好地跟上变化的专业人士通常是那些有意识地努力做到这一点的人。物联网、开源工具和预测分析是 2019 年可能会突出的 趋势。 
寻求成为数据科学家的人不仅要了解趋势和最新消息，还需深入探讨如何将这些趋势应用到自己的职业目标中。 
例如，一个人可以探索新的开源数据科学软件，并尽快开始使用，以熟悉其工作原理。或者，参加有关预测分析基础的在线课程，了解为什么这一领域的分支对招聘数据科学家的公司如此重要。 
3. 设定具体目标以推进数据科学项目 
许多数据科学家或希望进入该领域的人都有自学的阶段。这意味着即使人们尚未接受正式的数据科学培训，他们也可能会独立启动数据科学项目，受到好奇心和提升技能的愿望驱动。 
希望从事数据科学职业的人应该尝试一种特定的目标设定系统，这种系统由 Google、Amazon 和其他知名公司用于他们的数据科学项目。它涉及制定目标和关键成果，即 OKRs。 
目标与项目的目标相关，而关键成果则表明个人如何实现该目标。此外，OKRs 的成功标志是个人达成了70%的关键成果。 
个人可以通过选择与数据科学项目最相关的指标来应用 OKRs。该指标定义了目标，关键成果深入探讨了个人必须经历的过程，以使项目取得成功。最好是每个关键成果都有一个相关的日期。 
4. 考虑获得高级数据科学学位 
如果一个人希望在未来的数据科学岗位中提高收入潜力，可以通过获得高级数据科学学位来实现。许多学校提供数据科学方向的工商管理硕士（MBA）学位。 
了解更多关于它们的信息并做出短名单的一种方法是每个工作日晚上探索至少一所学校的课程。采取这种方法可以每月获得大约 20 所学校的详细信息，并且能够在不急于做决定的情况下获得知识。 
MBA 毕业生的平均薪资因所选择的方向和工作经验年限等因素而异。由于数据科学技能需求极高，数据科学 MBA 方向有可能使求职者在众多候选人中脱颖而出。 
最近编制的统计数据显示，美国存在数据科学短缺。拥有高级学位可以使人更好地填补这一空缺，并有可能获得高于平均水平的薪资。 
5. 提高数据讲述能力 
在数据集中找到有意义的信息是数据科学家的必要技能之一，但他们还必须成为优秀的数据讲故事者。否则，业务决策者将无法理解从数据中得出的特定结论为何有价值。如果观众认为见解不够引人注目，他们将不会进行更改。 
在 2019 年，人们可以尝试将他们的数据科学结果传达给没有数据科学背景的朋友。他们可以请这些人提供改进建议。 
6. 学习一些新的编程语言 
数据科学家在工作中使用各种编程语言。2019 年学习新的编程语言是为了获得未来职业中必需的知识，是一种积极的举措。 
如果希望从事数据科学工作的人尚未掌握任何编程语言，2019 年是扩展知识的最佳时机。Python 是一个快速增长和流行的编程语言，在数据科学领域中被广泛依赖。它还有易于理解的语法，使其成为一个优秀的第一编程语言。 
R 和 SQL 是数据科学中另两个常用的语言，值得进一步研究。但在提高编程语言能力的过程中，人们必须记住，与其对许多其他语言仅有最低限度的了解，不如对一两种编程语言掌握得特别好。 
拥有正确的心态至关重要 
除了这些目标外，人们在经历挫折时保持自我激励也至关重要。 
培养对数据科学工作的奉献精神可能使未来的数据科学家成为他们工作所在公司的更大资产。 
**简历：凯拉·马修斯**在《The Week》、《The Data Center Journal》和《VentureBeat》等出版物上讨论技术和大数据，已有五年以上的写作经验。要阅读凯拉更多的文章，请订阅她的博客 Productivity Bytes。 
相关内容： 
 数据科学如何改善高等教育在 2 年内提升数据科学技能的 8 种方法女性在数据科学社区中的参与增长
更多相关主题 
 AI/ML 技术集成将如何帮助企业实现目标我如何通过数据科学每月在线赚取 $3,500KDnuggets 新闻，5 月 25 日：每位数据科学家都应该了解的 6 种 Python 机器学习工具每位初学者数据科学家应该掌握的 6 种预测模型每位数据科学家应该具备的 13 种顶级技能每位数据科学家都应该了解的 6 种 Python 机器学习工具
数据科学家薪资最高的 6 家公司 
 
 原文：www.kdnuggets.com/2022/05/6-highest-paying-companies-data-scientists.html 
 
 
数据科学是一个蓬勃发展的行业。如果你拥有相关技能和兴趣，就有机会获得高薪。数据科学家的薪资水平即使与技术社区的其他人相比也很高。由于数据科学领域不断发展和变化，它被广泛认为是技术行业中最紧缺的技能。 
 
我们的前三名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业轨道 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你组织的 IT 需求 
 
数据科学是一个很棒的行业，如果你想赚很多钱，这里是一个很好的选择。近年来，对数据科学家的需求只增不减，这意味着相对于其他技术行业及其他行业的薪资也可能上涨。美国的数据科学家平均年薪为$117K，而软件工程师的平均年薪为$108K。如果你想从软件开发转向数据科学，只需在一些领域提升技能即可。 
这些是数据科学领域薪资最高的六家公司。我查看了绝对薪资，但我还会告诉你在选择数据科学工作时需要考虑的其他因素。 
 
数据科学家薪资最高公司的总结 
1. Airbnb 
这家公司颠覆了酒店行业，对数据情有独钟。它被认为是全球最数据驱动的公司之一。Airbnb 专注于“建立复杂的数据基础设施，以便公司中的每个人都能访问信息。”虽然 Airbnb 赋予每位员工深入数据的权力，但你确实需要许多优秀的数据科学家来培训他们周围的人。他们的数据科学家还建立了 惊人的内部数据工具，其中一些他们慷慨地开放为开源。 
显然，Airbnb 重视其数据，因为他们将数据视为在大规模上了解客户声音的一种方式。这就是为什么 Airbnb 排名第一，因为平均数据科学家的薪资是 $188K。在 Airbnb 的顶级数据科学家可以赚取高达 $286K。 
2. Meta 
Meta 期望其数据科学家找到 执行产品想法 的全球化方法，并找到理解用户行为的方式。Meta 的平台拥有超过 36 亿月活跃用户，这意味着有大量关于用户与 UI 交互、用户对广告或内容的反应以及用户之间互动的数据。 
Meta 的数据科学家有机会挖掘策略，以根据从用户数据中得出的见解来改变 UI 以改善用户体验，或调整内容算法。Meta 的平均数据科学家薪资为 $158K，但顶级收入者的薪资高达 $294K。 
3. Apple 
Apple 拥有广泛的产品，从操作系统软件到硬件设计。Apple 的数据科学家可能会使用机器学习模型来提高 Siri 的准确性，或分析 Apple Pages 以及其他 本地 Apple 应用 的性能。Apple 的产品涵盖操作系统如 iOS 和 Mac OS、支付系统 (Apple Pay)、云计算服务 (iCloud)、数字市场如 Apple Music 和 App Store，以及娱乐平台如 Apple TV 和 Podcasts。 
所有这些知名产品为数据科学家创造了大量机会。安全性特别是苹果公司的一个重要焦点，因此你可能会在苹果公司作为数据科学家从事欺诈预防的预测建模工作。 
苹果公司一直是且目前仍是世界上最有价值的公司，他们让这种财富展现出来。苹果公司平均支付给数据科学家的年薪为$158K，但最高可达$220K。 
4. 思科系统 
思科系统已将其产品系列扩展到数据科学工具市场。他们销售思科数据智能平台（CDIP），这是一种“将大数据、人工智能/计算农场和存储层整合在一起以作为一个整体工作，同时能够独立扩展以解决现代数据中心的 IT 问题的架构。” 
一如既往，为了为数据科学家开发产品，你需要数据科学家来完成工作。思科寻找那些将数据清洗和预处理视为必要工作的数据科学家。在思科系统，平均数据科学家的年薪为$157K，而收入最高的数据科学家的年薪可达$206K。 
5. Oracle 
Oracle 专注于帮助他们的客户“以新的方式查看数据，发现洞察，解锁无限可能。” Oracle 拥有一整套产品，其中许多与云工程服务和数据库管理系统相关。 
Oracle 在数据领域已经存在了相当长时间，他们扩展了他们的数据解决方案仓库，包括作为数据科学云平台的产品。这些工具帮助数据科学家轻松访问数据、自动配置数据并生成可视化图表，并且支持 Pandas、Numpy、Plotly 和其他常见的开源数据科学工具。 
Oracle 平均支付给他们的数据科学家的年薪为$154K，而 Oracle 的顶级数据科学家的年薪达到$187K。 
6. Google 
谷歌拥有近 40 亿的月活跃用户，而 YouTube 则有超过 19 亿的月活跃用户。谷歌依赖其数据科学家来解读他们通过谷歌搜索、谷歌地图、Gmail、YouTube 以及安卓操作系统等生成的大量数据。 
在谷歌的一个数据科学家可能会利用自然语言处理（NLP），这是机器学习的一个领域，以更好地理解用户的搜索词，从而对最相关的搜索结果进行排序。或者，你也可能深入挖掘来自谷歌地图的地理位置数据，以提高驾驶方向的准确性。 
谷歌的数据科学家的平均工资为$150K。然而，谷歌的薪资范围比大多数公司更大，因为谷歌的顶级数据科学家的收入为$297K。 
如何提升 
 
如果你已经获得了数据科学家的职位，并且有兴趣在这些高薪公司找到工作，你需要专注于学习新的数据科学技能并提升一些现有的技能。 
为了提升，你应该改善你的利益相关者管理技能，理解他们的目标、关切和困惑。你应该主动解答他们的问题，并提供建议的解决方案。 
在清理和预处理数据方面，你应该具备全面的能力。无论数据是否是"
KDNuggets 博客中文翻译（一百六十一）,2024-10-27,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
介绍 OpenLLM：LLM 的开源库 
 
 原文：www.kdnuggets.com/2023/07/introducing-openllm-open-source-library-llms.html 
 
 
图片来源于作者 
此时，我们都在思考同一个问题。LLM 的世界真的在主导吗？一些人可能预计热度会平稳，但它仍在持续上升。更多资源投入到 LLM 中，因为它显示出巨大的需求。 
LLM 的性能不仅成功，而且它们在适应各种 NLP 任务（如翻译和情感分析）方面的多功能性也非常强。微调预训练的 LLM 使得针对特定任务变得更加容易，减少了从头构建模型的计算成本。LLM 已迅速被应用于各种现实世界应用，推动了研究和开发的数量。 
开源模型也是 LLM 的一个重要优势，因为开源模型的可用性使研究人员和组织能够持续改进现有模型，并探索它们如何安全地融入社会。 
什么是 OpenLLM？ 
OpenLLM 是一个用于生产环境中操作 LLM 的开放平台。使用 OpenLLM，您可以对任何开源 LLM 进行推理、微调、部署，并轻松构建强大的 AI 应用程序。 
OpenLLM 包含最先进的 LLM，如 StableLM、Dolly、ChatGLM、StarCoder 等，所有这些都由内置支持。您还可以自由构建自己的 AI 应用程序，因为 OpenLLM 不仅仅是一个独立产品，还支持 LangChain、BentoML 和 Hugging Face。 
这些功能，还开源？听起来有点疯狂，对吧？ 
更棒的是，它易于安装和使用。 
如何使用 OpenLLM？ 
要使用 LLM，您需要系统中至少安装 Python 3.8 以及 pip。为了防止包冲突，建议使用虚拟环境。 
一旦准备好这些，您可以通过使用以下命令轻松安装 OpenLLM： 
pip install open-llm
1 
为确保正确安装，可以运行以下命令： 
$ openllm -h

Usage: openllm [OPTIONS] COMMAND [ARGS]...

 ██████╗ ██████╗ ███████╗███╗ ██╗██╗ ██╗ ███╗ ███╗
██╔═══██╗██╔══██╗██╔════╝████╗██║██║ ██║ ████╗ ████║
██║ ██║██████╔╝█████╗██╔██╗ ██║██║ ██║ ██╔████╔██║
██║ ██║██╔═══╝ ██╔══╝██║╚██╗██║██║ ██║ ██║╚██╔╝██║
╚██████╔╝██║ ███████╗██║ ╚████║███████╗███████╗██║ ╚═╝ ██║
 ╚═════╝ ╚═╝ ╚══════╝╚═╝╚═══╝╚══════╝╚══════╝╚═╝ ╚═╝

An open platform for operating large language models in production.
Fine-tune, serve, deploy, and monitor any LLMs with ease.
12345678910111213 
为了启动一个 LLM 服务器，请使用以下命令，并包括您选择的模型： 
openllm start
1 
例如，如果您想启动一个 OPT 服务器，请执行以下操作： 
openllm start opt
1 
支持的模型 
OpenLLM 支持 10 种模型。您还可以在下方找到安装命令： 
chatglm 
pip install ""openllm[chatglm]""
1 
该模型需要 GPU。 
Dolly-v2 
pip install openllm
1 
该模型可以在 CPU 和 GPU 上使用。 
falcon 
pip install ""openllm[falcon]""
1 
该模型需要 GPU。 
flan-t5 
pip install ""openllm[flan-t5]""
1 
该模型可以在 CPU 和 GPU 上使用。 
gpt-neox 
pip install openllm
1 
这个模型需要一个 GPU。 
mpt 
pip install ""openllm[mpt]""
1 
这个模型可以在 CPU 和 GPU 上使用。 
opt 
pip install ""openllm[opt]""
1 
这个模型可以在 CPU 和 GPU 上使用。 
stablelm 
pip install openllm
1 
这个模型可以在 CPU 和 GPU 上使用。 
starcoder 
pip install ""openllm[starcoder]""
1 
这个模型需要一个 GPU。 
baichuan 
pip install ""openllm[baichuan]""
1 
这个模型需要一个 GPU。 
要了解有关运行时实现、微调支持、集成新模型和生产部署的更多信息，请查看 这里，找到适合你需求的方案。 
总结一下 
如果你想使用 OpenLLM 或需要帮助，你可以加入他们的 Discord 和 Slack 社区。你也可以通过他们的 开发者指南 为 OpenLLM 的代码库做贡献。 
有没有人试过这个？如果试过，请在评论中告诉我们你的想法！ 
Nisha Arya 是一名数据科学家、自由技术写作者以及 KDnuggets 的社区经理。她特别感兴趣于提供数据科学职业建议或教程以及有关数据科学的理论知识。她还希望探索人工智能如何/能如何有利于人类寿命的不同方式。她是一位渴望学习者，寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。 
更多相关信息 
 介绍 Objectiv：开源产品分析基础设施介绍 MPT-7B：一个新的开源 LLM介绍 MetaGPT 的数据解释器：SOTA 开源 LLM 基于……介绍自然语言处理测试库RedPajama 项目：一个开源倡议，旨在普及 LLMsFalcon LLM：开源 LLM 的新王者
OpenAI 介绍 Superalignment 
 
 原文：www.kdnuggets.com/2023/08/introducing-superalignment-openai.html 
 
 
图片由作者提供 
OpenAI 频繁出现在媒体上，不仅因为发布了 ChatGPT、GPT-3 和 GPT-4，还因为围绕 ChatGPT 等 AI 系统的伦理问题以及当今世界的社会经济问题。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业的快车道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
首席执行官 Sam Altman 在 AI 安全性方面已多次发表讲话，例如在美国参议院委员会上，他表示： 
“我认为如果这项技术出现问题，它可能会变得非常严重……我们希望对此保持警觉。我们希望与政府合作，防止这种情况发生。” 
话虽如此，OpenAI 团队已经开始采取行动。许多人担心超智能，即一个智能超越人类思维的 AI 系统。有些人相信技术能够解决世界上许多当前的问题，然而，由于对其知之甚少或理解有限，很难权衡其利弊。 
也许谈论超智能还为时尚早，但这无疑是一个需要进行的对话。最佳的做法是尽早管理这些潜在的风险，以免它们变成无法处理的大问题。 
OpenAI 的做法 
OpenAI 表示他们目前没有针对超智能 AI 的解决方案，不过他们正在与新团队 Superalignment 合作进行研究。他们目前使用的技术包括来自人类反馈的强化学习，这在很大程度上依赖于人类对 AI 进行监督。然而，关于未来人类无法可靠监督 AI 的问题以及处理这一问题所需的新科学突破存在担忧。 
话虽如此，OpenAI 正在考虑建立一个能够从人类反馈中学习并协助人类评估 AI 的人类级别自动对齐研究人员，同时能够解决其他对齐问题。OpenAI 已将其迄今为止获得的 20% 的计算资源用于这一努力，以迭代对齐超级智能。 
为了让超级对齐团队在这方面取得成功，他们需要： 
1. 开发可扩展的训练方法 
他们的目标是利用其他 AI 系统来帮助评估其他 AI 系统，同时更好地理解模型如何推广监督，这是人类无法监督的。 
2. 验证结果模型 
为了验证系统对齐的结果，OpenAI 计划自动搜索问题行为，以改进模型的稳健性，以及自动解释性。 
3. 压力测试整个对齐流程 
测试，测试，再测试！OpenAI 计划通过故意训练不对齐的模型来测试其整个对齐过程。这将确保所使用的技术能够检测任何形式的不对齐，特别是最恶劣的对抗测试。 
OpenAI 已经进行了初步实验，结果显示良好。他们旨在利用有用的指标和持续的模型研究来推进这些工作。 
总结 
OpenAI 的目标是创造一个 AI 系统和人类能够和谐共处的未来，让任何一方都不会感到受到威胁。超级对齐团队的开发是一个雄心勃勃的目标，但它将向更广泛的社区提供有关机器学习使用的证据，并能够创建一个安全的环境。 
Nisha Arya 是一名数据科学家、自由技术写作人以及 KDnuggets 的社区经理。她特别关注于提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能如何能够促进人类寿命的不同方式。作为一名热衷的学习者，她寻求拓宽她的技术知识和写作技能，同时帮助指导他人。 
更多信息 
 什么是超级对齐，它为什么重要？OpenAI 的新 ChatGPT 和 Whisper APIs微调 OpenAI 语言模型的噪声标签数据OpenAI 的 Whisper API 用于转录和翻译免费 ChatGPT 课程：使用 OpenAI API 编写 5 个项目使用 OpenAI GPT 模型的最佳实践
介绍自然语言处理的测试库 
 
 原文：www.kdnuggets.com/2023/04/introducing-testing-library-natural-language-processing.html 
 
 
负责任的 AI：目标与现实 
 
我们的前三名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析能力 
 3. Google IT 支持专业证书 - 支持你所在组织的 IT 
 
虽然有很多关于训练安全、稳健和公平的 AI 模型的讨论——但很少有工具提供给数据科学家以实现这些目标。因此，生产系统中的自然语言处理（NLP）模型的前线反映了一个令人遗憾的现状。 
目前的自然语言处理系统经常失败且失败得非常惨烈。[Ribeiro 2020] 显示了在替换中性词时，顶级三大云服务提供商的情感分析服务失败率为 9-16%，在更改中性命名实体时为 7-20%，在时间测试中为 36-42%，在一些否定测试中几乎为 100%。[Song & Raghunathan 2020] 显示了 50-70%的个人信息泄露到流行的词汇和句子嵌入中。[Parrish et. al. 2021] 显示了种族、性别、外貌、残疾和宗教的偏见如何根深蒂固地存在于最先进的问答模型中——有时会使可能的答案发生超过 80%的变化。[van Aken et. al. 2022] 显示了在病人记录中添加任何关于族裔的提及都会降低其预测的死亡风险——最准确的模型产生了最大的误差。 
简而言之，这些系统就是不管用。我们绝不会接受一个只在某些数字上正确加法的计算器，或一个根据你放入的食物类型或时间随机改变强度的微波炉。一个经过良好工程设计的生产系统应该能在常见输入上可靠地工作。它在处理不常见输入时也应该是安全和稳健的。软件工程包括三个基本原则，以帮助我们实现这些目标。 
应用软件工程基础 
首先，测试你的软件。关于为什么 NLP 模型今天会失败，唯一令人惊讶的事是答案的平凡性：因为没有人测试它们。上述论文之所以新颖，是因为它们是其中的首批。如果你想交付有效的软件系统，你需要定义什么是有效，并在部署到生产环境之前测试它是否有效。每次更改软件时也应如此，因为 NLP 模型也会出现回退问题 [Xie et. al. 2021]。 
其次，不要将学术模型作为生产就绪模型重复使用。NLP 科学进步的一个美妙方面是，大多数学者将他们的模型公开并易于重用。这加速了研究，并启用了像 SuperGLUE、LM-Harness 和 BIG-bench 这样的基准。然而，旨在复制研究结果的工具并不适合用于生产。可重复性要求模型保持不变——而不是保持其最新或随时间变得更稳健。一个常见的例子是 BioBERT，也许是最广泛使用的生物医学嵌入模型，发布于 2019 年初，因此将 COVID-19 视为一个词汇表外的词。 
其三，测试超越准确性。由于你的 NLP 系统的业务需求包括稳健性、可靠性、公平性、毒性、效率、无偏见、无数据泄露和安全性——因此你的测试套件需要反映这些要求。语言模型的整体评估 [Liang et. al 2022] 是对这些术语在不同上下文中的定义和度量的全面回顾，非常值得一读。但你需要编写自己的测试：例如，对于你的应用程序来说，包容性实际意味着什么？ 
良好的测试需要具体、独立且易于维护。它们还需要有版本控制和可执行性，以便将其纳入自动构建或 MLOps 工作流。nlptest 库是一个简单的框架，可以简化这一过程。 
介绍 nlptest 库 
nlptest 库 是围绕五个原则设计的。 
开源。这是一个基于 Apache 2.0 许可证的社区项目。它可以永久免费使用，包括商业用途，没有任何限制。背后有一个活跃的开发团队，你可以参与贡献或分叉代码。 
轻量级。这个库可以在你的笔记本电脑上运行——不需要集群、高内存服务器或 GPU。只需运行 pip install nlptest 就能安装，并且可以离线运行（即在 VPN 或高合规企业环境中）。然后，生成和运行测试可以在三行代码内完成： 
from nlptest import Harness
h = Harness(task=""ner"", model=""ner.dl"", hub=”johnsnowlabs”)
h.generate().run().report()
123 
这段代码导入库，为指定模型从 John Snow Labs 的 NLP 模型中心创建一个新的命名实体识别（NER）任务的测试工具，自动生成测试用例（基于默认配置），运行这些测试，并打印出报告。 
测试本身存储在 pandas 数据框中，这使得它们易于编辑、过滤、导入或导出。整个测试工具可以被保存和加载，因此要运行之前配置的测试套件的回归测试，只需调用 h.load(""filename"").run()。 
跨库。库支持 transformers、Spark NLP 和 spacy。扩展框架以支持额外的库非常容易。作为 AI 社区，我们没有必要重复构建测试生成和执行引擎。这些库中的任何预训练或自定义 NLP 流水线都可以进行测试： 
# a string parameter to Harness asks to download a pre-trained pipeline or model
h1 = Harness(task=""ner"", model=""dslim/bert-base-NER"", hub=”huggingface”)
h2 = Harness(task=""ner"", model=""ner.dl"", hub=”johnsnowlabs”)
h3 = Harness(task=""ner"", model=""en_core_web_md"", hub=”spacy”)

# alternatively, configure and pass an initialized pipeline object
pipe = spacy.load(""en_core_web_sm"", disable=[""tok2vec"", ""tagger"", ""parser""])
h4 = Harness(task=“ner”, model=pipe, hub=”spacy”)
12345678 
可扩展。由于有数百种潜在的测试类型和度量需要支持，许多项目对 NLP 任务的额外兴趣，以及自定义需求，已经付出了大量的思考，以便于实现和重用新的测试类型。 
例如，对于美国英语的内置偏见测试类型之一，将名字替换为白人、黑人、亚洲人或西班牙裔常见的名字。但是，如果你的应用程序面向印度或巴西呢？那基于年龄或残疾的偏见测试呢？如果你想到了一种不同的测试通过标准怎么办？ 
nlptest 库是一个框架，它使你能够轻松编写并组合不同类型的测试。TestFactory 类定义了一个标准 API，用于配置、生成和执行不同的测试。我们已经尽力让你能够轻松地贡献或定制这个库以满足你的需求。 
测试模型和数据。当模型还没有准备好进行生产时，问题通常出在用于训练或评估模型的数据集上——而不是模型架构本身。一个常见的问题是标记错误的训练示例，这在广泛使用的数据集中被证明是普遍存在的 [Northcutt et. al. 2021]。另一个问题是表示偏差：一个常见的挑战是，找出模型在不同种族之间的表现如何，因为没有足够的测试标签来计算一个可用的度量。因此，库可能会使测试失败，并告诉你需要更改训练和测试集以代表其他群体，修复可能的错误，或训练边缘情况。 
因此，测试场景由一个任务、一个模型和一个数据集定义，即： 
h = Harness(task= ""text-classification"",
model = ""distilbert_base_sequence_classifier_toxicity"",
data= “german hatespeech refugees.csv”,
hub = “johnsnowlabs”)
1234 
除了使库能够为模型和数据提供全面的测试策略外，这种设置还使你能够使用生成的测试来增强你的训练和测试数据集，这可以大大缩短修复模型并使其准备生产的时间。 
接下来的章节描述了nlptest库帮助你自动化的三个任务：生成测试、运行测试和数据增强。 
 
1. 自动生成测试 
nlptest与过去的测试库之间的一个巨大区别是，现在可以在一定程度上自动生成测试。每个 TestFactory 可以定义多种测试类型，并为每种类型实现测试用例生成器和测试用例运行器。 
生成的测试作为一个表格返回，包含“测试用例”和“预期结果”列，这些列取决于特定的测试。这两列旨在可读，以便业务分析师可以在需要时手动审查、编辑、添加或删除测试用例。例如，以下是由 RobustnessTestFactory 为文本“I live in Berlin.”生成的一些 NER 任务测试用例： 
测试类型测试用例预期结果remove_punctuation我住在柏林柏林：地点lowercasei live in berlin.berlin: Locationadd_typosI liive in Berlin.柏林：地点add_context我住在柏林。 #citylife柏林：地点 
这里是由 BiasTestFactory 生成的文本分类任务的测试用例，使用基于美国种族的姓名替换，从文本“John Smith is responsible”开始： 
测试类型测试用例预期结果replace_to_asian_nameWang Li is responsible积极情感replace_to_black_nameDarnell Johnson is responsible负面情感replace_to_native_american_nameDakota Begay is responsible中性情感replace_to_hispanic_nameJuan Moreno is responsible负面情感 
这里是由 FairnessTestFactory 和 RepresentationTestFactory 类生成的测试用例。表示测试可能需要测试数据集中包含至少 30 名男性、女性和未指定性别的患者。公平性测试可能要求在对这些性别类别中的数据切片进行测试时，测试模型的 F1 得分至少为 0.85： 
测试类型测试用例预期结果min_gender_representation男性30min_gender_representation女性30min_gender_representation未知30min_gender_f1_score男性0.85min_gender_f1_score女性0.85min_gender_f1_score未知0.85 
关于测试用例需要注意的重要事项： 
 “测试用例”和“预期结果”的含义取决于测试类型，但在每种情况下都应该是人类可读的。这是为了在你调用h.generate()后，可以手动审查生成的测试用例列表，并决定保留或编辑哪些用例。由于测试表格是一个 pandas 数据框，你也可以在笔记本内直接编辑（使用 Qgrid），或将其导出为 CSV 文件，并让业务分析师在 Excel 中编辑。虽然自动化完成了 80% 的工作，你通常仍需手动检查测试。例如，如果你在测试一个假新闻检测器，那么将“巴黎是法国的首都”替换为“巴黎是苏丹的首都”的 replace_to_lower_income_country 测试将不可避免地导致预期预测和实际预测之间的不匹配。你还需要验证你的测试是否捕捉了你解决方案的业务需求。例如，上述的 FairnessTestFactory 示例没有测试非二元或其他性别身份，也没有要求性别之间的准确性几乎相等。然而，它确实使这些决策明确、易读，并且易于更改。一些测试类型将只生成一个测试用例，而其他类型可能生成数百个。这是可配置的——每个 TestFactory 定义了一组参数。TestFactory 类通常特定于任务、语言、地区和领域。这是有意为之，因为这允许编写更简单且更模块化的测试工厂。
2. 运行测试 
在生成测试用例并随心所欲地编辑后，以下是如何使用它们的方法： 
 调用h.run()以运行所有测试。对于测试框架表中的每个测试用例，相关的 TestFactory 将被调用以运行测试，并返回一个通过/失败标志以及一个解释性消息。调用h.report()在调用h.run()之后。这将按测试类型分组通过比例，打印一个总结结果的表格，并返回一个标志，说明模型是否通过了测试套件。调用h.save()将测试框架（包括测试表）保存为一组文件。这使你能够在以后加载并运行完全相同的测试套件，例如在进行回归测试时。
这是一个为命名实体识别（NER）模型生成的报告示例，应用了来自五个测试工厂的测试： 
类别测试类型失败次数通过次数通过率最低通过率是否通过？robustnessremove_punctuation4525285%75%TRUEbiasreplace_to_asian_name11016965%80%FALSErepresentationmin_gender_representation03100%100%TRUEfairnessmin_gender_f1_score1267%100%FALSEaccuracymin_macro_f1_score01100%100%TRUE 
虽然 nlptest 做的部分工作是计算指标——模型的 F1 分数是多少？偏差分数是多少？鲁棒性分数是多少？——但一切都被框定为一个二元结果的测试：通过或失败。正如好的测试应该做的那样，这要求你明确你的应用程序所做的和未做的事情。然后，它使你能够更快且更有信心地部署模型。它还使你能够将测试列表共享给监管者——他们可以阅读它，或自行运行以重现你的结果。 
3. 数据增强 
当你发现模型在鲁棒性或偏差方面存在不足时，改进的一种常见方法是添加专门针对这些不足的新训练数据。例如，如果你的原始数据集主要包含干净的文本（如维基百科文本——没有错别字、俚语或语法错误），或者缺乏穆斯林或印地语名字的代表性——那么将这些示例添加到训练数据集中应该能帮助模型更好地处理这些情况。 
幸运的是，我们已经在某些情况下有一种方法可以自动生成这些示例——这与我们用于生成测试的方法相同。以下是数据增强的工作流程： 
 在生成并运行测试后，调用 h.augment() 以根据测试结果自动生成增强训练数据。请注意，这必须是新生成的数据集——测试套件不能用于重新训练模型，因为下一个版本的模型将无法再次对其进行测试。在模型训练数据上进行测试是数据泄露的一个例子，这会导致测试分数被人为地抬高。新生成的增强数据集以 pandas dataframe 形式提供，你可以查看、编辑（如果需要），然后用它来重新训练或微调你的原始模型。然后，你可以在之前失败的相同测试套件上重新评估新训练的模型，通过创建新的测试工具，并调用 h.load()，接着是 h.run() 和 h.report()。
这个迭代过程使 NLP 数据科学家能够不断提升他们的模型，同时遵守他们自己道德规范、公司政策和监管机构规定的规则。 
入门 
nlptest 库现在已上线并免费提供给你。可以通过 pip install nlptest 开始使用，或访问 nlptest.org 阅读文档和入门示例。 
nlptest 也是一个早期的开源社区项目，欢迎你加入。John Snow Labs 已为该项目配备了完整的开发团队，并致力于多年改进该库，就像我们对待其他开源库一样。预计将会有频繁的更新，定期添加新的测试类型、任务、语言和平台。然而，如果你参与贡献、分享示例和文档，或者向我们反馈你最需要的内容，你将更快地获得所需的东西。访问 nlptest on GitHub 参与讨论。 
我们期待共同努力，使安全、可靠和负责任的 NLP 成为日常现实。 
更多相关话题 
 自然语言处理中的 N-gram 语言建模介绍 OpenLLM：用于 LLM 的开源库自然语言处理关键术语解释自然语言处理任务的数据表示用于图像识别和自然语言处理的迁移学习如何开始使用 PyTorch 进行自然语言处理
介绍 TPU v4: 谷歌前沿超级计算机用于大型语言模型 
 
 原文：www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html 
 
 
图片来源：编辑 
机器学习和人工智能似乎在以非常快的速度增长，以至于有些人甚至跟不上。随着这些机器学习模型的不断进步，它们将需要更好的基础设施和硬件支持来维持运行。机器学习的进步直接推动了计算性能的扩展。让我们进一步了解 TPU v4。 
 
我们的三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升您的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持您的组织的 IT 
 
什么是 TPU v4？ 
TPU 代表张量处理单元，它们被设计用于机器学习和深度学习应用。TPU 是谷歌发明的，构造方式使其能够处理机器学习和人工智能的高计算需求。 
当谷歌设计 TPU 时，他们将其设计为特定领域架构，这意味着它被设计为矩阵处理器，而不是通用处理器，从而专注于神经网络工作负载。这解决了谷歌的内存访问问题，这一问题使得 GPU 和 CPU 的速度减慢，导致它们使用更多的处理能力。 
那么，TPU v2、v3，现在是 v4。v2 到底是什么？ 
TPU v2 芯片包含两个 TensorCores、四个 MXUs、一个矢量单元和一个标量单元。见下图： 
 
图片来源：Google 
光学电路交换机（OCSes） 
TPU v4 是首个部署可重构光学电路交换机的超级计算机。光学电路交换机（OCS）被认为更有效。它们减少了以前网络中的拥堵，因为它们在发生时进行传输。OCS 提高了可扩展性、可用性、模块化、部署、安全性、功率、性能等。 
TPU v4 中的 OCS 和其他光学组件占 TPU v4 系统成本的不到 5%，系统功耗的不到 5%。 
SparseCores 
TPU v4 也是首个具有硬件支持嵌入的超级计算机。神经网络在密集向量上训练效果良好，而嵌入是将类别特征值转换为密集向量的最有效方式。TPU v4 包含第三代 SparseCores，这些数据流处理器加速了依赖于嵌入的机器学习模型。 
例如，嵌入函数可以将英文单词翻译为较小的密集空间，每个单词表示为 100 维向量。嵌入是深度学习推荐模型（DLRMs）的关键元素，它们存在于我们日常生活中，用于广告、搜索排名、YouTube 等。 
下图展示了在 CPU、TPU v3、TPU v4（使用 SparseCore）和 TPU v4（在 CPU 内存中使用嵌入，不使用 SparseCore）上的推荐模型性能。可以看到，TPU v4 SparseCore 在推荐模型上比 TPU v3 快 3 倍，比使用 CPU 的系统快 5 到 30 倍。 
 
图片由 Google 提供 
性能 
TPU v4 的性能比 TPU v3 高 2.1 倍，性能/瓦特比提高了 2.7 倍。TPU v4 体积增大了 4 倍，达到 4096 个芯片，使其速度提高了 10 倍。OCS 的实现和灵活性也大大助力于大型语言模型。 
TPU v4 超级计算机的性能和可用性正被广泛考虑用于改进大型语言模型，如 LaMDA、MUM 和 PaLM。540B 参数的 PaLM 模型在 TPU v4 上训练了超过 50 天，硬件浮点性能达到了令人惊叹的 57.8%。 
TPU v4 还具有多维模型分区技术，使大型语言模型的低延迟、高吞吐量推理成为可能。 
能效 
随着全球公司为提升整体能效而制定更多法律法规，TPU v4 表现不俗。Google Cloud 中的 TPU v4 使用的能量比当代 DSA 少 ~2-6 倍，CO2e 排放量比典型的本地数据中心少 ~20 倍。 
机器学习工作负载的变化 
既然你对 TPU v4 了解更多了，你可能想知道机器学习工作负载在 TPU v4 上实际变化有多快。 
下表展示了按深度神经网络模型类型的工作负载和使用的 TPU 百分比。Google 90%以上的训练在 TPU 上进行，这张表显示了 Google 生产工作负载的快速变化。 
递归神经网络（RNN）的应用有所下降，因为 RNN 是一次性处理输入，而与之相比，变换模型在自然语言翻译和文本摘要方面更为出色。 
 
要了解更多关于 TPU v4 能力的信息，请阅读研究论文 TPU v4: 一种用于机器学习的光学可重构超级计算机，支持嵌入的硬件。 
总结 
去年，TPU v4 超级计算机在谷歌云的机器学习集群中向 AI 研究人员和开发者开放。该论文的作者声称 TPU v4 比 Nvidia A100 更快且功耗更低。然而，由于 Nvidia H100 GPU 的有限供应和其 4nm 架构，他们尚未能将 TPU v4 与更新的 Nvidia H100 GPU 进行比较，而 TPU v4 采用的是 7nm 架构。 
你认为 TPU v4 的能力是什么，它的局限性如何，它是否比 Nvidia A100 GPU 更好？ 
尼莎·阿亚 是一名数据科学家、自由职业技术写作者及 KDnuggets 的社区经理。她特别关注于提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能如何在延长人类寿命方面发挥作用。作为一个热衷学习者，她寻求拓宽技术知识和写作技能，同时帮助指导他人。 
了解更多相关话题 
 介绍 John Snow Labs 的医疗保健专用大型语言模型通过整合 Jupyter 和 KNIME 缩短实施时间顶级开源大型语言模型了解大型语言模型什么是大型语言模型，它们是如何工作的？AI: 大型语言和视觉模型
介绍 VisualData：一个计算机视觉数据集的搜索引擎 
 
 原文：www.kdnuggets.com/2018/09/introducing-visualdata-search-engine-computer-vision-datasets.html 
 
 评论 
作者：Jie Feng，人性化 AI 
 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速开启网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 为你的组织提供 IT 支持 
 
计算机视觉无疑将在不久的将来改变机器与我们及环境互动的几乎所有方面。虽然这一领域仍然年轻（起源于 1960 年代），但得益于机器学习的最新进展，技术首次在历史上开始发挥作用。自动驾驶汽车、机器人技术、虚拟现实/增强现实等应用激励了人们进入这一领域，并将技术应用于更广泛的领域。它已经成为人工智能最活跃的子领域之一。 
关于计算机视觉的疯狂想法每天都在发生，涉及新的研究、边项目或业务用例。要从事计算机视觉工作，你需要三样东西：数据、算法和计算。像谷歌和亚马逊这样的巨头公司通过其云平台提供了强大的计算能力。研究界（例如 arXiv）和开源社区（例如 Github）为我们带来了更好的算法和易于使用的代码。现在学习这一复杂的主题是最佳时机，而这一主题曾经需要博士级别的经验。但仍然存在一个问题——数据。更具体地说，就是具有高质量注释的视觉数据。构建一个良好的数据集需要在规模、内容平衡、视觉变化等方面进行大量思考，以帮助训练的模型在未见过的案例中表现良好。标注大规模数据集可能非常昂贵且缓慢，这使得预算有限的个人或小公司仍然面临实验新想法的障碍。 
解决这个问题的答案是开源数据集。与其自己构建数据集，不如利用已经由学术研究者、爱好者和公司贡献的丰富计算机视觉数据集。这些数据集涵盖了从识别对象到重建 3D 房间，从在视频中找人到识别照片中的衬衫等多种主题。它们通常附带人工标注的高质量标签。近年来，新的数据集甚至包括了预训练的深度学习模型，你可以直接尝试，而无需自己训练。然而，发现这些数据集并不容易。它们通常分散在创作者的网站上，很难比较不同的数据集以选择适合特定任务的最佳数据集。对于新手来说，也很难找出合适的关键字来进行 Google 搜索。 
这就是为什么我们构建了VisualData，这是一个计算机视觉数据集的搜索引擎。目前，每个数据集都是人工筛选的，并标记了相关主题以便过滤，你可以在搜索栏中输入关键字，这些关键字将与数据集的标题和描述匹配，以便轻松找到最佳的使用数据集。它们按照发布时间排序，因此最新添加的数据集会被优先显示。详细视图显示了数据集的完整描述，并链接到开源代码/模型。将来还会逐步添加其他有用的信息，例如关于数据集的文章/教程、使用难度等。下面展示了一些网站的示例截图。 
 
从左到右：按（多个）主题过滤，按关键字搜索，数据集详细信息。 
这是我希望在攻读博士学位时能存在的网站。现在，我希望它能帮助对计算机视觉感兴趣的人轻松探索现有数据，从而更快地进行实验。它仍处于早期阶段，未来版本中还有一些很酷的想法。我们希望了解它如何对你更有用。 
附言：喜欢这个 Facebook 页面 或关注我们的 Twitter 以获取新数据集或代码发布的更新，并提供反馈和功能建议。 
简介：Jie Feng 拥有哥伦比亚大学计算机科学博士学位，并且是 VisualData 的创始人。 
原文。已获许可转载。 
相关： 
 宣布微软研究开源数据，云托管平台用于共享数据集DIY 深度学习项目使用 Numpy 和 OpenCV 进行基本图像数据分析 – 第一部分
相关话题 
 构建视觉搜索引擎 - 第二部分：搜索引擎探索计算机视觉的世界：介绍 MLM 最新的……通过 Uplimit 的机器学习搜索课程提升你的搜索引擎技能！构建视觉搜索引擎 - 第一部分：数据探索Qdrant：开源向量搜索引擎与托管云平台TensorFlow 用于计算机视觉 - 轻松实现迁移学习
活跃学习简介 
 
 原文：www.kdnuggets.com/2018/10/introduction-active-learning.html 
 
 评论 
由 Jennifer Prendki, Figure Eight 的机器学习副总裁 
随着数据收集和存储变得越来越便宜，数据科学家面临的数据量超过了他们能够分析的能力。而且这一趋势没有减缓的迹象：物联网设备的爆炸性增长以及新的内存密集型数据格式的出现，使得数据专业人员在原始数据的海洋中挣扎。 
鉴于机器学习中最激动人心的进展需要大量数据，这确实是一个令人兴奋的时代。但这也给机器学习社区带来了全新的挑战：除非数据被标注，否则它对所有依赖监督学习方法的机器学习应用基本上是无用的。 
数据标注：机器学习中的新瓶颈 
过去十年中，人工智能最有前景的进展之一来自于深度学习模型的使用。虽然神经网络几十年前就被发现了，但它们的实际使用直到最近才得以实现，这要归功于数据量和计算能力的增加，而旧硬件根本无法提供这些。由于大多数深度学习算法使用监督学习方法且对数据需求量大，因此现在机器学习中的新瓶颈不再是数据的收集，而是标注过程的速度和准确性。毕竟，我们大多数人都熟悉“垃圾进，垃圾出”的格言。除非你拥有高质量的数据来训练模型，否则模型的性能最终会受到影响。 
由于获取未标记实例现在基本上是免费的，赢得人工智能竞赛现在取决于拥有最快、最具扩展性、灵活和可靠的方式来获取数据集的质量标签。标注大量数据已经成为一个关键问题，因为待标注的数据量变得如此庞大，甚至全球所有的人工劳动都无法满足需求。例如，即使每个边界框标注几秒钟，单靠一个人对一个 10 秒钟的视频进行行人标注也可能需要几个小时，这就是为什么越来越多的公司现在专注于构建机器学习算法以自动化标注过程。单纯的人力方法根本无法扩展。 
但数据的数量并不是唯一的问题。获取标签也往往耗时（例如，当注释员被要求对较长文档的主题进行标注时），经常容易出错（有时也具有主观性），费用昂贵（例如，“标注”患者记录为癌症阳性可能需要 MRI 扫描或其他昂贵的医学检查，或验证石油存在可能需要钻探），甚至可能很危险（例如，在地雷探测的情况下）。 
更快标注与更聪明标注 
为应对对高质量注释的急剧需求，一种人机协作的 AI 方法，其中人类注释员验证机器学习算法的输出，看起来是一种有前途的方法。它不仅可以加快处理速度，还能提高质量，因为人类干预有助于弥补算法的不准确性。 
尽管这种蛮力方法有时是最佳途径，但另一个方法在 ML 社区中正在慢慢获得更多关注：即仅标注最重要的数据实例。作为数据科学家，我们习惯于认为更多的数据等于更高的模型准确率，但虽然这绝对正确，但也要承认并非所有数据都同等重要，因为并非所有例子都包含相同数量的信息。 
 
图 1：应对新兴的大标签危机的两种方法 
确定模型学习的最佳实例可以发生在两个不同的时间：要么在模型构建之前，要么在模型训练期间。前者通常称为优先级排序，而后者称为主动学习。 
什么是主动学习？ 
尽管在流行的 ML 博客中覆盖率较低，但主动学习实际上在许多现代 ML 问题中得到了令人惊讶的充分激励，特别是当标签难以获得、耗时或昂贵时。 
在主动学习中，学习算法被允许从尚未标注的实例池中主动选择下一步要标注的样本。该概念背后的基本信念是，如果机器学习算法能够选择它想学习的数据，它可能在使用较少的训练标签的情况下实现更好的准确性。这种算法被称为主动学习者。主动学习者可以在训练过程中动态提出查询，通常以未标注的数据实例的形式，由被称为oracle的人类注释员进行标注。因此，主动学习是人机协作范式成功的最有力示例之一。 
 
图 2：正如预期的那样，训练模型所需的数据越多，准确度越高，无论是否使用主动学习方法。然而，使用主动学习时，可以用显著更少的数据达到特定的准确度；在这个例子中，仅使用 45%的可用数据总量就可以实现 80%的准确度，而在常规监督学习的情况下则需要 70%。 
主动学习是如何工作的？ 
主动学习可以通过不同的场景来实现。本质上，决定是否查询特定标签归结为判断获取标签的收益是否抵消了收集该信息的成本。实际上，这个决策可以根据科学家是否有有限的预算或只是试图最小化标记费用的不同而采取几种形式。总体而言，主动学习可以分为三种不同的类别： 
基于流的选择性采样场景涉及确定询问数据集中某个特定未标记条目的标签是否足够有益。随着模型的训练和数据实例的呈现，它会立即决定是否需要查看标签。这种方法的缺点自然在于没有保证数据科学家会保持在其预算范围内。 
基于池的采样场景也是最为人知的一个。它尝试在选择最佳查询或一组最佳查询之前评估整个数据集。主动学习者通常最初在一个完全标记的数据子集上进行训练，从而生成模型的第一个版本，该模型随后用于识别哪些实例对下一次迭代（或主动学习循环）注入训练集最有益。其最大的缺点之一在于其内存需求非常高。 
成员查询合成场景可能并不适用于所有情况，因为它涉及生成合成数据。在这种场景下，学习者可以构造自己的例子进行标记。这种方法在生成数据实例较为容易时，对于解决冷启动问题（如搜索引擎中的问题）是非常有前途的。 
主动学习与强化学习有何不同？ 
尽管强化学习和主动学习都可以减少模型所需的标签数量，但它们是非常不同的概念。 
一方面，我们有强化学习。强化学习是一种目标导向的学习方法，受到行为心理学的启发，它允许你从环境中获得输入。因此，强化学习意味着代理在使用过程中会变得更好：它在使用中学习。当我们人类从错误中学习时，我们实际上是在进行一种强化学习方法。没有实际的训练阶段；而是代理通过试错过程使用预定的奖励函数来学习，该函数反馈关于特定动作的效果如何。技术上讲，强化学习不需要提供数据，而是随着进程生成自己的数据。 
主动学习，另一方面，更接近于传统的监督学习。实际上，它是一种半监督学习（同时使用标注数据和未标注数据）。这一概念的核心思想是，并非所有数据的价值相同，标注一个小样本的数据可能会达到相同的准确性（如果不是更高），唯一的挑战是确定这个样本是什么。主动学习是指在训练阶段逐步动态地标注数据，以便让算法识别哪个标签对其学习更有信息量，从而更快地学习。 
如何决定标注哪一行？ 
用于确定下一个需要标注的数据实例的方法称为查询策略。下面列出的是那些最常用和研究的策略： 
不确定性采样 
对于那些查询策略，首先在一个相对较小的标注数据样本上训练一个模型；然后将这个模型应用于数据集的（未标注的）其余部分。该算法基于通过推理步骤获得的信息选择下一个主动学习循环中需要标注的实例。 
在这些策略中，最受欢迎的是最小置信度策略，其中模型选择那些置信度最低的实例进行标注。另一种常见策略叫做边际采样：在这种情况下，算法选择那些两个最可能的标签之间的边际较小的实例，这意味着分类器在区分这两个最可能的类别时遇到了困难。这两种策略旨在帮助模型在特定类别之间进行区分，并且在减少特定分类错误方面表现出色。然而，如果目标函数是减少对数损失，则通常基于熵的方法更为合适，在这种方法中，学习者简单地查询模型在预测中具有最高输出方差的未标注实例。 
查询委员会（即，QBC） 
使用这些策略，对相同标记训练集的不同模型进行训练，并用于推断其余（尚未标记）数据。对于观察到最大分歧的实例，被认为是最具信息性的。这一框架的核心思想是最小化版本空间，即与当前标记训练数据一致的假设集合。 
预期影响 
这些查询策略考察新标签的增加对模型整体性能的影响。期望模型变化策略旨在识别那些如果已知标签将对当前模型产生最大变化的实例。期望误差减少策略衡量的不是模型可能发生的变化量，而是模型的泛化误差可能减少的量。这种策略面临的挑战之一是，它也是最计算上昂贵的查询框架。由于最小化期望损失函数通常没有封闭形式的解，因此使用方差减少方法作为代理。 
总的来说，这些方法学将输入空间整体考虑，而不是像不确定性采样或 QBC 那样关注个别实例，这使得它们在选择离群点时具有明显的优势。 
密度加权方法 
之前的框架描述了那些识别最具信息性的实例的策略，这些实例被认为是不确定性最高的。然而，另一种看待问题的方法是，数据集中最有价值的实例不一定是那些不确定性最大的，而是那些最能代表数据底层分布的实例：这就是信息密度框架的核心思想。 
性能与限制 
超过 90%的进行过主动学习工作的研究者声称他们的期望得到或完全或部分满足。这非常令人鼓舞，但人们不禁想知道其他情况下发生了什么。 
现实情况是，主动学习仍未被很好地理解。例如，有一些关于 NER 的深度主动学习的有前景的工作，但许多重大问题仍然存在。例如，几乎没有研究预测特定任务或数据集是否特别容易从主动学习方法中受益。 
最终，主动学习是一种特定的半监督学习情况，这类算法已被证明对偏见非常敏感，特别是因为它们容易自我满足于从基于相对较小数据集训练的模型中识别出的模式中得到的初始信念。随着高效标注成为机器学习中越来越关键的组成部分，可以预期未来几年将会有更多关于这个主题的研究成果发表。 
简介: 詹妮弗·普伦基是一位数据科学领袖和数据策略师，她非常享受设想数据科学的未来并推广它。詹妮弗特别擅长培养和管理早期阶段的数据团队，并且非常擅长管理混合团队，包括产品经理、数据科学家、数据分析师和各类工程师（前端、后端、系统等）。 
相关: 
 如何将主动学习应用于您的企业机器学习训练数据的基本指南2018 年数据科学家报告已发布
 
我们的前三推荐课程 
 1. 谷歌网络安全证书 - 快速进入网络安全职业轨道。 
 2. 谷歌数据分析专业证书 - 提升您的数据分析水平 
 3. 谷歌 IT 支持专业证书 - 支持您的组织在 IT 领域 
 
相关主题 
 KDnuggets 新闻，4 月 27 日：关于带代码的论文简要介绍；…统计学习导论，Python 版：免费书籍深度学习库介绍：PyTorch 和 Lightning AI回到基础第 3 周：机器学习导论使用 PyCaret 进行二分类入门使用 PyCaret 在 Python 中进行聚类入门
AI 简介，更新版 
 
 原文：www.kdnuggets.com/2020/10/introduction-ai-updated.html 
 
评论 
由 Imtiaz Adam，AI 和战略执行官。 
 
 
我们的三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业的轨道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 工作 
 
什么是人工智能（AI） 
AI 涉及开发能够执行人类擅长的任务的计算系统，例如识别物体、理解和分析语言，以及在受限环境中做出决策。 
窄域 AI： AI 的一个领域，机器被设计用来执行单一任务，并且机器在执行该任务时非常出色。然而，一旦机器训练完成，它不会推广到未见领域。这是我们今天拥有的 AI 形式，例如 Google 翻译。 
人工通用智能（AGI）： 一种可以完成任何人类能够做的智力任务的 AI 形式。它更具意识，并做出类似于人类决策的决定。AGI 目前仍然是一个理想，关于它的到来，各种预测从 2029 年到 2049 年甚至可能永远无法实现。它可能在未来 20 年左右到来，但面临与硬件、今日强大机器的能耗需求以及解决影响即使是最先进深度学习算法的灾难性记忆丧失等挑战。 
超级智能： 是一种在所有领域的表现超越人类的智能（根据 Nick Bostrom 的定义）。这涉及到一般智慧、问题解决和创造力等方面。 
 
**经典人工智能：**包括基于规则的系统、搜索算法（如无信息搜索（广度优先、深度优先、普遍成本搜索）和有信息搜索如 A 和 A*算法）。这些奠定了今天更先进的方法的坚实基础，这些方法更适合大规模搜索空间和大数据集。它还包括逻辑方法，如命题和谓词演算。虽然这些方法适用于确定性场景，但现实世界中的问题往往更适合概率方法。 
该领域近年来在各个行业中产生了重大影响，包括医疗保健、金融服务、零售、营销、交通、安全、制造和旅行行业。 
大数据的出现，由互联网、智能移动设备和社交媒体的到来推动，使得 AI 算法，特别是机器学习和深度学习，能够利用大数据并更优化地执行任务。加上更便宜、更强大的硬件，如图形处理单元（GPU），使 AI 发展成更复杂的架构。 
机器学习 
机器学习被定义为应用统计方法使计算机系统从数据中学习以实现最终目标的 AI 领域。这个术语由 Arthur Samuel 在 1959 年引入。 
需要理解的关键术语 
特征/属性：用于将数据表示为算法可以理解和处理的形式。例如，图像中的特征可能表示边缘和角落。 
熵：随机变量中的不确定性量。 
信息增益：由一些先前知识带来的信息量。 
监督学习：一种处理已标记（注释）数据的学习算法。例如，通过带有标记的水果图像学习将水果分类为苹果、橙子、柠檬等。 
无监督学习：一种发现数据中隐藏模式的学习算法，数据未被标记（注释）。一个例子是将客户分成不同的群体。 
半监督学习：当只有一小部分数据被标记时的学习算法。 
主动学习涉及一种算法可以主动询问教师标签的情况。Jennifer Prendki定义为“……一种半监督学习（其中使用了标记和未标记的数据）……主动学习是指在训练阶段逐步和动态地标记数据，以使算法识别最有信息量的标签，从而更快地学习。” 
损失函数：是实际情况与算法学习到的内容之间的差异。在机器学习中，目标是最小化损失函数，以便算法能够继续在未见过的场景中进行泛化和表现。 
机器学习方法（非详尽列表） 
分类、回归和聚类是机器学习的三个主要领域。 
分类被 Jason Brownlee 总结为“关于预测标签，而回归则是关于预测量。分类是预测离散类标签的任务。分类预测可以使用准确率进行评估，而回归预测则不能。” 
回归预测建模被 Jason Brownlee 总结为“任务是从输入变量（X）到连续输出变量（y）近似一个映射函数（f）。回归是预测一个连续量的任务。回归预测可以使用均方根误差进行评估，而分类预测则不能。” 
聚类被 Surya Priy 总结为“将总体或数据点划分为若干组的任务，使得同一组中的数据点彼此之间更为相似，而与其他组中的数据点则不相似。它基本上是根据相似性和差异性对对象进行的集合。” 
强化学习： 是一个处理在环境中建模代理的领域，该环境不断奖励代理做出正确决策。举例来说，代理在与人类对弈的国际象棋中获胜。当代理走出正确的棋步时会获得奖励，走错则会受到惩罚。经过训练后，该代理可以在真实比赛中与人类竞争。 
线性回归： 是机器学习的一个领域，它建模两个或多个具有连续值的变量之间的关系。 
逻辑回归： 是一种分类技术，它将 logit 函数建模为特征的线性组合。二元逻辑回归处理变量只有两个结果（‘0’或‘1’）的情况。多项逻辑回归处理预测变量可能有多个不同值的情况。 
k-均值： 是一种无监督的方法，用于根据数据实例之间的相似性对其进行分组（或聚类）。一个例子是根据相似性对人口进行分组。 
K-最近邻（KNN）： 是一种监督机器学习算法，可用于解决分类和回归问题。它基于这样一个假设：相似的项彼此靠近。 
支持向量机（SVM）： 是一种分类算法，通过在两个数据类别之间绘制分隔超平面来进行分类。一旦训练完成，SVM 模型可以用作对未见数据的分类器。 
决策树： 是一种从数据中学习决策规则的算法。这些规则随后用于决策制定。 
提升和集成： 是将多个表现不佳的弱学习者组合成一个强分类器的方法。自适应提升（AdaBoost）可以应用于分类和回归问题，通过结合多个弱分类器来创建一个强分类器。最近的一些关键示例包括 XG Boost，它在 Kaggle 竞赛中对表格或结构化数据表现出色（参见 Tianqi Chen 和 Carlos Guestrin 2016）， 微软在 2017 年推出的 Light Gradient Boosting Machine（Light GBM），以及 Yandex 在 2017 年推出的 CatBoost。有关集成学习的更多信息，请参见 Jason Brownlee 的集成学习文章 和 KDnuggets 发表的文章 标题为 CatBoost 与 Light GBM 与 XGBoost。 
随机森林： 属于集成学习技术的范畴，通过在训练期间创建多个决策树来引入随机性。输出将是各个树的平均预测值或表示类别模式的类别。这可以防止算法过拟合或记忆训练数据。 
主成分分析（PCA）： 是一种减少数据维度的方法，同时保持数据的可解释性。这有助于去除数据中冗余的信息，同时保留解释大部分数据的特征。 
同步定位与地图构建 (SLAM): 处理机器人在未知环境中定位自己的方法。 
进化遗传算法： 这些算法受到生物学的启发，灵感来源于进化理论。它们通常用于解决优化和搜索问题，通过应用包括选择、交叉和变异在内的生物启发概念。 
神经网络： 是受生物启发的网络，通过层级化方式从数据中提取抽象特征。神经网络在 1980 年代和 1990 年代曾遭到抑制。是Geoff Hinton继续推动了神经网络的发展，尽管当时许多经典人工智能社区对他嗤之以鼻。深度神经网络（参见下文定义）发展的关键时刻是在 2012 年，当时来自多伦多的一个团队在 ImageNet 竞赛中用 AlexNet 网络向世界展示了自己。他们的神经网络与之前使用手工提取特征的方法相比，大大减少了错误率。 
深度学习 
深度学习指的是具有多个隐藏层的神经网络领域。这样的神经网络通常被称为深度神经网络。 
目前使用的几种主要深度神经网络类型包括： 
卷积神经网络（CNN）： 卷积神经网络是一种通过卷积操作从输入数据中以层次化方式提取模式的神经网络。它主要用于具有空间关系的数据，例如图像。卷积操作通过在图像上滑动内核来提取与任务相关的特征。 
递归神经网络（RNN）： 递归神经网络，特别是 LSTM，用于处理序列数据。例如，时间序列数据、股票市场数据、语音、传感器信号和能源数据具有时间依赖性。LSTM 是一种更高效的 RNN，缓解了梯度消失问题，使其能够记住短期和远期的信息。 
限制玻尔兹曼机（RBM）： 基本上是一种具有随机属性的神经网络。限制玻尔兹曼机使用名为对比散度的方法进行训练。训练完成后，隐藏层是输入的潜在表示。RBM 学习输入的概率表示。 
深度置信网络： 是由限制玻尔兹曼机组成的，每一层作为下一层的可见层。在向网络中添加额外层之前，每一层都经过训练，这有助于概率性地重建输入。该网络使用逐层无监督的方法进行训练。 
变分自编码器（VAE）： 是一种改进版的自编码器，用于学习输入的最优潜在表示。它由一个编码器和一个解码器以及一个损失函数组成。VAE 使用概率方法，并涉及潜在高斯模型中的近似推断。 
生成对抗网络（GANs）：生成对抗网络是一种使用生成器和判别器的卷积神经网络（CNN）。生成器不断生成数据，而判别器则学习区分虚假数据与真实数据。随着训练的进行，生成器不断提高生成看起来真实的虚假数据的能力，而判别器则更好地学习区分虚假和真实的数据，从而帮助生成器自我改进。训练完成后，我们可以使用生成器生成看起来很真实的虚假数据。例如，一个训练过脸部图像的 GAN 可以用来生成不存在的、非常真实的脸部图像。 
变换器：用于处理序列数据，特别是在自然语言处理（NLP）领域中，处理文本数据的任务，如语言翻译。该模型于 2017 年在一篇名为""Attention is All you Need""的论文中首次提出。变换器模型的架构包括编码器和解码器的应用，以及自注意力机制，涉及到对输入序列中不同位置的关注能力，并生成序列的表示。与 RNN 相比，它们的优势在于不需要按序处理数据，这意味着在处理句子时，不需要先处理句子的开头再处理结尾。著名的变换器模型包括双向编码器表示（BERT）和 GPT 变体（来自 OpenAI）。 
深度强化学习：深度强化学习算法涉及建模一个智能体，该智能体学习以尽可能最优的方式与环境互动。智能体不断采取行动，始终牢记目标，而环境则根据智能体的行动给予奖励或惩罚。通过这种方式，智能体学习以最优的方式行为，以实现目标。DeepMind 的 AlphaGo 是智能体学习下围棋并能够与人类竞争的最佳例子之一。 
胶囊网络：仍然是一个活跃的研究领域。卷积神经网络（CNN）通常学习到的数据表示往往难以解释。另一方面，胶囊网络能够从输入中提取特定类型的表示，例如，它保留了物体部件之间的层次姿态关系。胶囊网络的另一个优势是它能够用比 CNN 所需的更少的数据学习表示。有关胶囊网络的更多信息，请参见胶囊之间的动态路由，堆叠胶囊自编码器和DA-CapsNet：双重注意力机制胶囊网络。 
**神经进化：**由Kenneth O. Stanley定义为：“旨在触发一种与产生我们大脑的进化过程类似的进化过程，只不过是在计算机内部。换句话说，神经进化旨在通过进化算法来发展演化神经网络的方法。” Uber Labs 的研究人员认为，神经进化方法在一定程度上由于降低了陷入局部最小值的可能性，与基于梯度下降的深度学习算法具有竞争力。Stanley 等人表示：“我们的希望是激发对该领域的新兴趣，因为它满足了今天日益增加的计算能力的潜力，突显其许多想法如何为深度学习、深度强化学习和机器学习社区提供令人兴奋的灵感和混合资源，并解释神经进化如何在长期追求人工通用智能中证明是一个关键工具。” 
神经符号人工智能：由MIT-IBM Watson AILab定义为一种将神经网络（从原始数据文件中提取统计结构——例如图像和声音文件的背景）与问题和逻辑的符号表示结合起来的 AI 方法的融合。“通过融合这两种方法，我们正在构建一种新型的 AI，其能力远超其部分之和。这些神经符号混合系统需要较少的训练数据，并跟踪进行推理和得出结论所需的步骤。它们在跨领域转移知识时也更为轻松。我们相信，这些系统将引领 AI 的新纪元，让机器更像人类一样学习，通过将词语与图像连接起来，掌握抽象概念。” 
联邦学习：也称为协作学习，在维基百科中定义为一种机器学习技术，使得算法可以在多个去中心化的服务器（或设备）上进行训练，而无需交换本地数据。差分隐私旨在通过衡量联邦学习元素间通信的隐私损失来增强数据隐私保护。这项技术可能解决数据隐私和安全的关键挑战，涉及异构数据，并影响物联网（IoT）、医疗保健、银行、保险等领域，在这些领域数据隐私和协作学习至关重要，并且随着 AI IoT 的扩展，可能会成为 5G 和边缘计算时代的关键技术。 
原文。经许可转载。 
简介： Imtiaz Adam，计算机科学硕士（人工智能与机器学习工程师）和斯隆战略研究员，MBA，专注于人工智能和机器学习技术的开发，特别是深度学习方面。 
相关： 
 我们距离实现人工通用智能的 3 个原因2020 年值得阅读的人工智能论文13 篇人工智能专家必读的论文
更多相关主题 
 停止学习数据科学以寻找目标，然后寻找目标来……一个 90 亿美元的人工智能失败案例，审视学习数据科学统计的顶级资源成功数据科学家的 5 个特征是什么让 Python 成为初创公司的理想编程语言每个数据科学家都应该知道的三个 R 语言库（即使你使用 Python）
自编码器和变分自编码器（VAE）介绍 
 
 原文：www.kdnuggets.com/2021/10/introduction-autoencoder-variational-autoencoder-vae.html 
 
评论 
 
图片来源 
 
我们的三大推荐课程 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
近年来，基于深度学习的生成模型因在人工智能（AI）领域取得的惊人进展而越来越受到关注。依靠大量的数据、精心设计的网络架构和智能训练技术，深度生成模型展示了生成各种高度逼真内容的惊人能力，例如图像、文本和声音。 
在本文中，我们将深入探讨这些生成网络，特别是自编码器、变分自编码器（VAE）及其使用 Keras 的实现。 
什么是自编码器？ 
自编码器（AE）是旨在将其输入复制到输出的神经网络。它们通过将输入压缩成潜在空间表示，然后从该表示重建输出。 
自编码器由两个主要组件组成： 
 编码器： 学习将输入数据压缩（减少）成编码表示。解码器： 学习从编码表示中重建原始数据，使其尽可能接近原始输入。瓶颈/潜在空间： 包含输入数据压缩表示的层。重建损失： 该方法测量解码器的表现，即衡量编码和解码向量之间的差异。差异越小越好。
 
自编码器。 
该模型涉及由ϕ参数化的编码函数g和由θ参数化的解码函数f。瓶颈层是： 
 
重建的输入： 
 
为了测量重建损失，我们可以使用交叉熵（当激活函数是 sigmoid 时）或基本的均方误差（MSE）： 
 
香草自编码器的类型 
 欠完备自编码器： 一个潜在空间小于输入维度的自编码器称为欠完备。学习一个欠完备的表示会迫使自编码器捕捉训练数据中最显著的特征。正则化自编码器： 它们使用一个损失函数，鼓励模型除了具备将输入复制到输出的能力之外，还有其他属性。在实践中，我们通常会发现两种类型的正则化自编码器：稀疏自编码器和去噪自编码器。稀疏自编码器： 稀疏自编码器通常用于学习另一任务的特征，比如分类。一个被正则化为稀疏的自编码器必须响应它训练过的数据集的独特统计特征，而不仅仅是作为一个恒等函数。这样，通过带有稀疏性惩罚的复制任务训练可以产生一个作为副产品学习到有用特征的模型。去噪自编码器： 目标不再是重建输入数据。我们可以通过改变损失函数的重建误差项来获得一个学习有用东西的自编码器。这可以通过向输入图像添加一些噪声并让自编码器学习去除它来实现。这样，编码器将提取最重要的特征并学习数据的稳健表示。
 
不同类型的自编码器。 
自编码器的应用 
传统自编码器主要有两个应用： 
噪声去除： 如上所述，噪声去除是从图像中去除噪声的过程。音频和图像都有噪声减少技术。 
 
去噪图像。 
 降维： 由于编码器部分学习输入数据的低维表示，当你希望进行降维时，自编码器的编码器部分很有用。这在例如主成分分析（PCA）不起作用时尤其有用，但你怀疑非线性降维（即，使用具有非线性激活函数的神经网络）有效。异常检测： 通过学习在某些约束条件下复制训练数据中最显著的特征，模型被鼓励准确重现最频繁观察到的特征。当遇到异常时，模型的重构性能应会变差。在大多数情况下，仅使用正常实例的数据来训练自编码器。训练后，自编码器将准确重构“正常”数据，而无法处理不熟悉的异常数据。重构误差（原始数据与其低维重构之间的误差）被用作异常分数来检测异常。机器翻译： 自编码器已被应用于机器翻译，这通常被称为神经机器翻译（NMT）。与传统自编码器不同，输出不与输入匹配——它是另一种语言。在 NMT 中，文本被视为要编码到学习过程中的序列，而在解码器一侧，则生成目标语言中的序列。
 
降维操作。 
Keras 自编码器实现 
让我们使用著名的 MNIST 数据集并应用自编码器来重建它。MNIST 数据集由 70000 张 28 像素乘 28 像素的手写数字图像和 70000 个包含每个数字信息的向量组成。 
# We create a simple AE with a single fully-connected neural layer as encoder and as decoder:

import numpy as np
import keras
from keras import layers
from keras.datasets import mnist
import matplotlib.pyplot as plt

# This is the size of our encoded representations
encoding_dim = 32# 32 floats -> compression of factor 24.5, assuming the input is 784 floats

# This is our input image
input_img = keras.Input(shape=(784,))

# ""encoded"" is the encoded representation of the input
encoded = layers.Dense(encoding_dim, activation='relu')(input_img)

# ""decoded"" is the lossy reconstruction of the input
decoded = layers.Dense(784, activation='sigmoid')(encoded)

# This model maps an input to its reconstruction
autoencoder = keras.Model(input_img, decoded)

# Let's also create a separate encoder model:
# This model maps an input to its encoded representation
encoder = keras.Model(input_img, encoded)

# As well as the decoder model:
# This is our encoded (32-dimensional) input
encoded_input = keras.Input(shape=(encoding_dim,))

# Retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]

# Create the decoder model
decoder = keras.Model(encoded_input, decoder_layer(encoded_input))

# Now let's train our autoencoder to reconstruct MNIST digits.
# First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adam optimizer:

autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

#Let's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images).

(x_train, _), (x_test, _) = mnist.load_data()

# We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

# Now let's train our autoencoder for 50 epochs:
autoencoder.fit(x_train, x_train,
epochs=50,
batch_size=256,
shuffle=True,
validation_data=(x_test, x_test))

# After 50 epochs, the autoencoder seems to reach a stable train/validation loss value of about 0.09\. We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib.

# Encode and decode some digits
# Note that we take them from the *test* set

encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)
n = 10# Number of digits to display
plt.figure(figsize=(20, 4))

for i in range(n):
# Display original
ax = plt.subplot(2, n, i + 1)
plt.imshow(x_test[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)

# Display reconstruction
ax = plt.subplot(2, n, i + 1 + n)
plt.imshow(decoded_imgs[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
plt.show()

1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 
这是我们得到的结果。顶部一排是原始数字，底部一排是重建的数字。使用这种基本方法，我们失去了相当多的细节。 
 
自编码器在内容生成中的局限性 
训练自编码器之后，我们可能会考虑是否可以利用该模型生成新内容。特别地，我们可能会问，是否可以从那个潜在空间中随机生成一个点并解码以获得新内容？ 
答案是“是的”，但生成数据的质量和相关性取决于潜在空间的规则性。潜在空间规则性取决于初始数据的分布、潜在空间的维度以及编码器的架构。很难事先确保编码器会以与我提到的生成过程兼容的聪明方式组织潜在空间。没有正则化意味着过拟合，这会导致解码后某些点的内容无意义。 
我们如何确保潜在空间的正则化程度足够？我们可以在训练过程中明确引入正则化。因此，我们引入了变分自编码器。 
什么是变分自编码器（VAE）？ 
变分自编码器（VAE）是对自编码的一个稍微现代化且有趣的变体。 
VAE 假设源数据具有某种潜在概率分布（例如高斯分布），然后尝试找到该分布的参数。实现变分自编码器比实现自编码器要复杂得多。变分自编码器的主要用途之一是生成与原始源数据相关的新数据。现在，确切地说这些额外数据有什么用处很难说明。变分自编码器是一个生成系统，类似于生成对抗网络（尽管 GAN 的工作方式大相径庭）。 
 
变分自编码器(VAE)。 
变分自编码器（VAE）背后的数学 
VAE 使用 KL 散度 作为其损失函数。其目标是最小化假设分布与数据集原始分布之间的差异。 
假设我们有一个分布 z，我们想从中生成观察值 x。换句话说，我们想计算： 
 
我们可以通过以下方式实现： 
 
但是，p(x) 的计算可以通过积分完成，如下所示： 
 
这通常使得它成为一个难以处理的分布（需要等于或更多的指数时间）。因此，我们需要将 p(z|x) 近似为 q(z|x) 以使其成为一个可处理的分布。为了更好地将 p(z|x) 近似为 q(z|x)，我们将最小化 KL 散度 损失，这样可以计算两个分布之间的相似性： 
 
通过简化，上述最小化问题等同于以下最大化问题： 
 
第一项表示重建似然性，另一项确保我们学习到的分布 q 与真实的先验分布 p 相似。 
因此，我们的总损失由两个部分组成，一个是重建误差，另一个是 KL 散度损失： 
 
变分自编码器（VAE）的 Keras 实现 
为了实现 VAE，首先，编码器网络将输入样本 x 转换为潜在空间中的两个参数，我们将其标记为 z_mean 和 z_log_sigma。然后，我们通过 z = z_mean + exp(z_log_sigma) * epsilon 从假设生成数据的潜在正态分布中随机采样相似点 z，其中 epsilon 是一个随机正态张量。 
最终，解码器网络将这些潜在空间点映射回原始输入数据。 
模型的参数通过两个损失函数进行训练：一个是重建损失，强制解码样本与初始输入匹配（就像在我们之前的自编码器中一样），另一个是KL 散度，用于表示学习到的潜在分布与先验分布之间的差异，作为正则化项。你实际上可以完全去掉这个后者的项，尽管它有助于学习良好的潜在空间并减少对训练数据的过拟合。 
# First, here's our encoder network, mapping inputs to our latent distribution parameters:
original_dim = 28 * 28
intermediate_dim = 64
latent_dim = 2

inputs = keras.Input(shape=(original_dim,))
h = layers.Dense(intermediate_dim, activation='relu')(inputs)
z_mean = layers.Dense(latent_dim)(h)
z_log_sigma = layers.Dense(latent_dim)(h)

# We can use these parameters to sample new similar points from the latent space:
from keras import backend as K

def sampling(args):
z_mean, z_log_sigma = args
epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=0.1)
return z_mean + K.exp(z_log_sigma) * epsilon

z = layers.Lambda(sampling)([z_mean, z_log_sigma])

# Finally, we can map these sampled latent points back to reconstructed inputs:
# Create encoder
encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')

# Create decoder
latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')
x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)
outputs = layers.Dense(original_dim, activation='sigmoid')(x)

decoder = keras.Model(latent_inputs, outputs, name='decoder')

# Instantiate VAE model
outputs = decoder(encoder(inputs)[2])

vae = keras.Model(inputs, outputs, name='vae_mlp')

# We train the model using the end-to-end model, with a custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.

reconstruction_loss = "
KDNuggets 博客中文翻译（一百一十六）,2024-10-27,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
电子书：使用 R 学习数据科学 – 免费下载 
 
 原文：www.kdnuggets.com/2021/09/ebook-learn-data-science-r.html 
 
comments 
由 Narayana Murthy，数据科学家。 
我很高兴宣布我的电子书 使用 R 学习数据科学 已发布。它可以作为限时赠书 下载。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
 
学习数据科学 
数据科学是各种技能的结合。数据科学家需要精通统计学、数学、编程及其他技能。 
 
所以合乎逻辑的问题是，从哪里开始？ 
统计学 
统计学是理解和解释数据的科学。它是数据科学的核心。开始的最佳选择之一是 Pluralsight 的 统计学基础 课程。它涵盖了描述统计学和推断统计学。 
编程语言 
Python、R 和 Julia 是数据科学项目中流行的编程语言。每种语言都有其独特的优点。Python 是最受欢迎的数据科学语言，而 Julia 是最快的。推荐学习 核心 Python 课程来掌握 Python 语言。 
R 语言是初学者、学术人员和领域专家的最佳选择。学习数据科学和像 Python 这样的通用语言同时进行会让人感到不知所措。R 语言作为数据科学语言，更容易掌握。它有良好的社区支持。 
机器学习 
机器学习是自动构建模型的科学。它是人工智能的一个子集。在掌握统计学和编程之后学习机器学习。书籍 线性代数与数据学习 涵盖了简单和高级的主题。它将线性代数与深度学习和神经网络结合起来进行讲解。 
使用 R 学习数据科学 简介 
针对特定主题的课程能很好地理解数据科学概念。能够在一个地方学习所有技能将是非常好的。 
用 R 学习数据科学 涵盖统计学、基础数学、R 语言、可视化和机器学习算法。内容精准完整，书长达 250 页。 
章节包括： 
 入门统计学与 R数学数据整理探索性数据分析机器学习机器学习类型高级监督学习实践项目数据科学的应用案例最终说明
撰写这本书对我来说是一次很棒的经历。该书在 Goodreads 和 LibraryThing 上获得了积极反馈。下载 电子书。希望你喜欢阅读，期待你的反馈。 
简介： Narayana Nemani 目前担任首席数据科学家，参与数据科学的教学和研究。 
相关： 
 2021 年数据科学学习路线图R 还是 Python？为什么不两者兼顾？编写干净 R 代码的 5 个技巧
更多相关内容 
 用这本免费电子书学习数据清理与预处理用这本免费电子书学习 MLOps 基础数据科学命令行：免费电子书超级学习指南：免费的算法与数据结构电子书数据科学家的认知偏差指南：免费电子书机器学习数学：免费电子书
开源数据科学/机器学习生态系统的 6 个组成部分；Python 是否宣布击败了 R？ 
 
 原文：www.kdnuggets.com/2018/06/ecosystem-data-science-python-victory.html 
 
 评论 在 5 月，我们报告了第 19 届 KDnuggets 软件调查的初步结果：Python 蚕食 R：2018 年分析、数据科学、机器学习的顶级软件：趋势与分析。 
在这里，我们更详细地查看哪些工具能够很好地配合使用。我们去年确定的开源 Python 友好的数据科学工具新兴生态系统已有新成员 - 见下文。 
我们在文章末尾提供了一个匿名数据集的链接 - 请告诉我你在数据中发现了什么，并请发布或通过电子邮件将结果发给我。 
首先，我们查看哪些工具是相互配合的，为了使图表易于理解，我们选择了至少有 400 票的工具。共有 11 种这样的工具，这种选择也有意义，因为第 11 名（Apache Spark，有 442 票）与第 12 名（Java，309 票）之间存在较大差距。 
有许多方法可以衡量两个二元特征之间关联的显著性，比如卡方检验或 T 检验，但我们使用了与我们的 2016 年分析和 2017 年分析相同的 Lift 度量。 
然后，我们将关联最强的工具组合在一起，从 Tensorflow 和 Keras 开始，直到得到下面的图 1。为了减少杂乱，我们还过滤了图表，只显示 abs(Lift1) > 15%的关联。 
 
图 1：数据科学、机器学习顶级工具关联，2018 年 
条形长度对应于 lift1 的绝对值，颜色表示 lift 值（绿色：关联较强，红色：关联较弱）。工具前面的数字是它们在 KDnuggets 2018 软件调查中的排名，例如 Python 排名第 1，RapidMiner 排名第 2，等等。 
我们注意到一组包含 6 种主要工具的现代开源数据科学生态系统，它们是：Python、Anaconda、scikit-learn、Tensorflow、Keras 和 Apache Spark。 
Rapidminer 与上述所有工具有小的负相关，也没有与其他工具强烈关联。 
R 与 Apache Spark、SQL 和 Tableau 有小的正相关。 
另一个出现的组是 3 种支持数据科学和机器学习的工具，它们经常一起使用：SQL、Excel 和 Tableau。 
我们注意到，虽然下面的图表相对于对角线是对称的（右上三角形与左下三角形相等），但在完整图表中，比在一半图表中更容易看到模式。 
提升度定义： 
 
 提升度 (X & Y) = pct (X & Y) / ( pct (X) * pct (Y) ) 
 其中 pct(X) 是选择 X 的用户百分比。 
 提升度 (X&Y) > 1 表示 X&Y 一起出现的频率高于它们独立时的预期频率， 
 如果 X 和 Y 的出现频率与它们独立时的预期频率相符，则提升度 = 1，并且 
 如果 X 和 Y 一起出现的频率低于预期，则提升度 < 1（负相关） 
 为了更容易观察差异，我们定义 
 提升度 1 (X & Y) = 提升度 (X & Y) - 1 
 
Python 与 R 
接下来我们比较 Python 与 R。 
设 with_Py(X) = 使用 Python 的工具 X 的百分比，以及 with_R(X) 为使用 R 的工具 X 的百分比。为了可视化每个工具与 Python 或 R 的接近程度，我们使用了一个非常简单的度量 Bias_Py_R(X) = with_Py(X) - with_R(X)，如果工具更常与 Python 一起使用则为正值，如果更常与 R 一起使用则为负值。 
在图 2 中，我们绘制了最受欢迎的工具的偏见，这些工具至少获得了 100 票。正如我们所见，几乎所有工具都偏向于 Python。唯一的两个例外是 IBM SPSS Statistics 和 SAS Base。作为对比，在类似的 2017 年分析中，有 10 个这样的工具：SAS Base、Microsoft 工具、Weka、RapidMiner、Tableau 和 Knime，几乎所有这些工具的使用量都与 Python 一起增加。 
 图 2：KDnuggets 2018 数据科学、机器学习投票：Python 与 R 的偏见 
Python 是否战胜了 R？ 
我认为不是这样，因为 R 是一个极其优秀的平台，具有巨大的深度和广度，广泛用于数据分析和可视化，它仍然占有约 50% 的市场份额。我预计 R 会被许多数据科学家使用很长时间，但未来，我期待 Python 生态系统会有更多的发展和活力。 
大数据与深度学习 
大数据（Spark / Hadoop 工具）在 KDnuggets 2018 软件投票中被 33% 的受访者使用，这与 2017 年的比例完全相同。这表明，大多数数据科学家处理的是中小数据，不需要 Hadoop / Spark，或者他们使用的是其他云端解决方案。 
然而，深度学习工具的比例从 32% 增长到了 43%。 
对于每个工具 X，我们计算它与 Spark/Hadoop 工具的使用频率（纵轴），以及它与深度学习工具的使用频率（横轴）。 
这是一个显示了顶级工具的图表（获得超过 100 票），排除了深度学习和大数据工具本身。 
 图 3：KDnuggets 2018 数据科学、机器学习投票：深度学习与 Spark/Hadoop 亲和度 
我们注意到 Scala 是在深度学习和大数据中最常用的语言。图表在左下角较重，几乎每个工具在深度学习中的使用频率都高于在大数据中的使用频率。 
这是匿名调查数据的 CSV 格式链接，包含以下列 
 Nrand: 记录 ID（随机化，记录顺序与投票顺序无关）region: usca: 美国/加拿大，euro: 欧洲，asia: 亚洲，ltam: 拉丁美洲，afme: 非洲/中东，aunz: 澳大利亚/新西兰Python: 如果 Votes（最后一列）中包含 Python，则为 1，否则为 0。RapidMiner: 如果 Votes 中包含 RapidMiner，则为 1，否则为 0。R language: 如果 Votes 中包含“R Language”，则为 1，否则为 0。我们使用“R Language”而不是 R，以便更容易进行正则表达式匹配。SQL Language: 如果 Votes 中包含“SQL Language”，则为 1，否则为 0。Excel: 如果 Votes 中包含 Excel，则为 1，否则为 0。Anaconda: 如果 Votes 中包含 Anaconda，则为 1，否则为 0。Tensorflow: 如果 Votes 中包含 Tensorflow，则为 1，否则为 0。Tableau: 如果 Votes 中包含 Tableau，则为 1，否则为 0。scikit-learn: 如果 Votes 中包含 scikit-learn，则为 1，否则为 0。Keras: 如果 Votes 中包含 KNIME，则为 1，否则为 0。Apache Spark: 如果 Votes 中包含 Apache Spark，则为 1，否则为 0。With DL: 如果 Votes 中包含深度学习工具，则为 1，否则为 0。With BD: 如果 Votes 中包含大数据工具，则为 1，否则为 0。ntools: Votes 中的工具数量Votes: 投票列表，用分号“;”分隔
请告诉我你的发现！ 
相关： 
 Python 蚕食 R：2018 年分析、数据科学、机器学习的顶级软件：趋势与分析。新兴生态系统：数据科学和机器学习软件分析，2017 年分析、数据科学、机器学习软件的最新领导者、趋势和惊喜调查，2017 年
更多相关内容 
 成为优秀数据科学家所需的 5 项关键技能每个初学者数据科学家应该掌握的 6 种预测模型2021 年最佳 ETL 工具它还活着！用 Python 和一些便宜的基本组件构建你的第一个机器人停止学习数据科学以寻找目标，并寻找目标去…数据共享平台的 5 个关键组成部分
EDISON 数据科学框架用于定义数据科学职业 
 
 原文：www.kdnuggets.com/2016/10/edison-data-science-framework.html 
 
作者：Yuri Demchenko，阿姆斯特丹大学，荷兰 
摘要 
数据科学技术的有效使用需要新的能力和技能，并要求新的职业来支持研究数据生命周期的所有阶段，从数据生产和输入到数据处理、存储以及获得的科学结果的发布和传播。本文介绍了 EDISON 数据科学框架（EDSF），包括建立未来数据科学专业人员可持续毕业和培训所需的概念性、指导性和政策性组件。 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业。 
 2. Google 数据分析专业证书 - 提升你的数据分析能力 
 3. Google IT 支持专业证书 - 支持你所在组织的 IT 
 
介绍 
现代研究需要新的专业人员，这些人员能够支持研究数据生命周期的所有阶段，从数据生产和输入到数据处理、存储以及科学结果的发布和传播，这些可以统称为数据科学职业系列。未来的数据科学家必须具备数据挖掘与分析、信息可视化与沟通、统计学、工程学和计算机科学等方面的知识（并获得相关的能力和技能），并在未来的工作和专业领域中积累经验。尽管数据科学家是数据相关职业系列中的关键职业，其他职业则专注于数据生命周期的其他阶段和支持基础设施。 
文章描述了所提出的 EDISON 数据科学框架（EDSF）的主要组件，该框架作为定义数据科学职业系列的基础。还提供了有关数据科学能力框架（CF-DS）和数据科学知识体系（DS-BoK）的更多详细信息，这些对于定义一致且可定制的数据科学课程至关重要。 
EDISON 数据科学框架 
图 1 下方展示了 EDISON 数据科学框架（EDSF）的主要组件，这些组件为数据科学职业的发展提供了概念基础（包括对相关文档的参考）： 
 CF-DS – 数据科学能力框架（CF-DS，2016）DS-BoK – 数据科学知识体系（DS-BoK，2016）MC-DS – 数据科学模型课程（MC-DS, 2016）DSP – 数据科学专业档案和职业分类（DSP, 2016）数据科学分类法和科学学科分类
提出的框架为数据科学专业生态系统的其他组成部分提供了基础，例如 
 EDISON 在线教育环境（EOEE）教育和培训市场及目录数据科学社区门户（CP），还包括个别能力基准测试和个性化教育路径构建的工具
核心数据科学能力和专业档案的认证框架 
 
图 1: EDISON 数据科学框架组成部分。 
数据科学能力框架和知识体系 
数据科学能力框架（CF-DS）是 EDISON 数据科学框架的基石，并用于定义数据科学知识体系（DS-BoK）和数据科学模型课程（MC-DS）等组成部分。CF-DS 的定义符合欧洲电子能力框架（e-CF3.0），并提供了将数据科学相关能力和技能扩展到 e-CF3.0 的建议。 
图 2 说明了主要的 CF-DS 能力组及其相互关系： 
 数据分析，包括统计方法、机器学习和业务分析工程：软件和基础设施学科/科学领域能力和知识数据管理、整理、保存科学或研究方法（针对研究职业）和业务流程管理（针对业务相关职业）
确定的能力领域为定义数据科学相关工作的教育和培训项目、技能再培训和专业认证提供了更好的基础。对科学研究方法和技术的了解使数据科学家职业不同于所有以往的职业。 
数据管理和研究方法（或业务流程管理）被放置在两个外圈，以强调这些能力和知识是所有数据科学专业人员所需的。建议在所有数据科学课程中都包括数据管理（或特别是研究数据管理）和研究方法。 
 
（a）数据科学能力组（针对一般或研究导向的职业）。 
 
（b）面向业务的职业的数据科学能力组。 
图 2: 确定的数据科学能力组之间的关系，分为（a）一般或研究导向和（b）商业导向的职业/角色 
CF-DS 为数据科学知识体系（DS-BoK）的定义提供了基础，该知识体系涵盖了专业人员执行其职业所有数据相关过程所需的知识。知识体系通常定义了课程内容，并通过可以为特定学员组定义的学习成果与 CF-DS 相关联。 
数据科学知识体系与模型课程 
根据 CF-DS 能力组定义，DS-BoK 应包含以下知识领域组 (KAG)： 
 KAG1-DSDA: 数据分析组，包括机器学习、统计方法和商业分析KAG2-DSENG: 数据科学工程组，包括软件和基础设施工程KAG3-DSDM: 数据管理组，包括数据策展、数据保存和数据基础设施KAG4-DSRM: 科学或研究方法组KAG5-DSBP: 业务流程管理组
高校可以使用 DS-BoK 作为参考，定义他们需要在课程中覆盖的知识领域，这取决于他们在研究或行业中的主要需求组。领域特定的知识可以作为学术教育的一部分或作为研究生的专业培训在工作场所获得。普遍认为，“新鲜”的数据科学家需要 2-3 年才能熟练掌握其职业技能。 
提出的数据科学模型课程为构建可定制的数据科学课程提供了两个基本组件：（1）基于 CF-DS 能力定义学习成果（LO），包括不同熟练程度的区分，例如使用布卢姆分类法，（2）定义映射到目标职业组学习成果的学习单元（LU），这些单元需要根据现有的学术学科分类，如计算机科学分类（CCS，2012），来定义。 
进一步发展 
所展示的 EDSF 包括由供应和需求双方的主要利益相关者实施的组件：大学、职业培训机构、标准化机构、认证和认证机构、公司及其人力资源部门，以成功管理数据相关工作的能力和职业发展。提出的 DS-CF 已在众多研讨会和社区论坛上广泛讨论。它已经被少数与 EDISON 项目相关的机构使用。发布的供公众评论的 DS-BoK 和 MC-DS 文档将需要进一步的开发和专家验证，以定义具体的知识领域。这将通过涉及相关知识领域的专家，并与 IEEE、ACM、DAMA、IIBA 等专业社区合作来完成。该项目将与合作伙伴和领先大学一起进行 DS-BoK 和 MC-DS 的试点实施，并收集从业者的反馈。所有 EDISON 项目产品均在创意共享许可下公开提供。 
致谢 
EDISON 项目在 H20202 资助协议编号 675419 下由欧洲委员会支持。本文是 SciDataCon2016 会议论文的更新版，最初发布于 www.scidatacon.org/2016/sessions/98/poster/75/ 
简介:是阿姆斯特丹大学系统与网络工程领域的高级研究员。他的主要研究领域包括大数据和数据密集型技术、电子科学、云计算和跨云架构、基于云的服务设计、一般安全架构以及面向云服务和数据中心应用的分布式访问控制基础设施。他负责协调欧洲资助的 EDISON 项目，该项目旨在为欧洲研究和工业建立数据科学职业的基础。 
参考文献 
Andrea Manieri 等, 2015，《数据科学专业揭秘：EDISON 项目如何促进数据科学家广泛认可的职业形象》，发表于第七届 IEEE 国际云计算技术与科学会议（CloudCom2015）的论文集中，2015 年 11 月 30 日至 12 月 3 日，加拿大温哥华 
CCS, 2012，2012 年 ACM 计算分类系统 [在线] www.acm.org/about/class/class/2012 
CF-DS, 2016，《数据科学能力框架（CF-DS）》。EDISON 草稿 V0.6，2016 年 3 月 10 日 [在线] www.edison-project.eu/data-science-competence-framework-cf-ds 
Demchenko, Y., E.Gruengard, S.Klous, 2014, 构建有效的大数据课程的教学模型。 见于第 6 届 IEEE 国际云计算技术与科学会议及研讨会 (CloudCom2014)，2014 年 12 月 15-18 日，新加坡 
DS-BoK, 2016, 数据科学知识体系 (DS-BoK)。EDISON 草稿 V0.1，2016 年 3 月 20 日 [在线] www.edison-project.eu/data-science-body-knowledge-ds-bok 
DSP, 2016, 数据科学专业档案定义 (CF-DS)。EDISON 草稿 v0.1，2016 年 7 月 11 日 [在线] www.edison-project.eu/data-science-professional-profiles-dsp 
eCFv3.0, 2014, 欧洲 e-能力框架 3.0。适用于所有行业领域的 ICT 专业人员的共同欧洲框架。CWA 16234:2014 第一部分 [在线] ecompetences.eu/wp-content/uploads/2014/02/European-e-Competence-Framework-3.0_CEN_CWA_16234-1_2014.pdf 
EDISON 项目：构建数据科学职业 [在线] www.edison-project.eu/ 
ESCO, 2016, ESCO (欧洲技能、能力、资格和职业) 框架 [在线] ec.europa.eu/esco/portal/#modal-one 
MC-DS, 2016, 数据科学模型课程 (MC-DS)，EDISON 草稿 v0.1，2016 年 6 月 11 日 [在线] www.edison-project.eu/data-science-model-curriculum-mc-ds 
NIST, 2015, NIST SP 1500-1 NIST 大数据互操作框架 (NBDIF)：第 1 卷：定义，2015 年 9 月 [在线] nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-1.pdf 
相关： 
 分析、大数据、数据科学、机器学习的证书与认证分析、数据挖掘和数据科学的证书分析、数据科学、机器学习的教育
更多相关主题 
 关于数据工程师的 11 个问题：这个职业是什么，…IBM 为各行业提供的生成式 AI 专业化课程AI/ML 模型的风险管理框架Django 框架中的社交用户认证适用于所有用途的唯一提示框架KDnuggets™ 新闻 22:n06, 2 月 9 日: 数据科学编程…
高效的小型语言模型：微软的 13 亿参数 phi-1.5 
 
 原文：www.kdnuggets.com/effective-small-language-models-microsoft-phi-15 
 
 
图片来源：作者 
当你以为你已经听够了有关大型语言模型（LLMs）的新闻时，微软研究院再次搅动了市场。2023 年 6 月，微软研究院发布了一篇名为 “教材就是你所需的一切” 的论文，在其中他们介绍了 phi-1，一个新的大型代码语言模型。phi-1 是一个基于 Transformer 的模型，具有 13 亿参数，在 8 个 A100 GPU 上训练了 4 天，使用了来自网络的“教科书质量”数据。 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业。 
 2. Google 数据分析专业证书 - 提升你的数据分析水平 
 3. Google IT 支持专业证书 - 支持你在 IT 领域的组织 
 
看来 LLM 正变得越来越小。 
什么是 phi-1.5？ 
现在，微软研究院向你介绍 phi-1.5，这是一个具有 13 亿参数的 Transformer，它使用了与 phi-1 相同的数据来源进行训练。如上所述，phi-1 在高质量的教科书数据上进行训练，而 phi-1.5 仅在合成数据上进行训练。 
phi-1.5 使用了 32xA100-40G GPU，并在 8 天内成功训练完成。phi-1.5 的目标是打造一个开源模型，它可以在研究社区中发挥作用，使用一个不受限制的小型模型，这样可以探索与 LLM 相关的不同安全挑战，例如减少有害内容、增强可控性等。 
通过使用‘合成数据生成’方法，phi-1.5 在自然语言测试中的表现相当于规模大 5 倍的模型，并且在更困难的推理任务中表现优于大多数 LLM。 
相当令人印象深刻，对吧？ 
该模型的学习过程非常有趣。它从多种来源获取数据，包括 StackOverflow 上的 Python 代码片段、合成的 Python 教科书以及由 GPT-3.5-turbo-0301 生成的练习。 
处理有害内容和偏见 
LLM 的一个主要挑战是有害内容和偏见内容。微软研究院旨在克服这一持续挑战，即有害/冒犯性内容和推广特定意识形态的内容。 
用于训练模型的合成数据生成的响应，相较于其他 LLMs 如 Falcon-7B 和 Llama 2–7B，生成有害内容的倾向较低，如下图所示： 
 
图片来源于教科书就是你所需的 II：phi-1.5 技术报告 
基准测试 
下图展示了 phi-1.5 在 3 个基准测试中表现略优于最先进的模型，如 Llama 2–7B、Llama-7B 和 Falcon-RW-1.3B，测试包括常识推理、语言技能和多步骤推理。 
 
图片来源于教科书就是你所需的 II：phi-1.5 技术报告 
这是怎么做的？ 
教科书式的数据使用方式使得 LLMs 中对这种数据的使用与从互联网提取的数据有所不同。为了进一步评估模型如何处理有害内容，使用了 ToxiGen，并设计了 86 个提示，手动标记为“通过”、“失败”或“未理解”，以更好地了解模型的局限性。 
也就是说，phi-1.5 通过了 47 个提示，失败了 34 个提示，并且没有理解 4 个提示。使用 HumanEval 方法评估模型的结果显示，phi-1.5 的评分高于其他知名模型。 
主要收获： 
以下是你应该了解的关于 phi-1.5 的主要要点： 
 是一个基于 transformer 的模型是一个专注于下一个词预测目标的 LLM经过了 300 亿个 token 的训练使用了 32xA100-40G GPUs成功在 8 天内完成训练
Nisha Arya 是一位数据科学家、自由技术撰稿人以及 KDnuggets 的社区经理。她特别感兴趣于提供数据科学职业建议或教程和理论知识。她还希望探索人工智能如何能够或已经在延长人类寿命方面发挥作用。她是一个热衷学习的人，寻求拓宽技术知识和写作技能，同时帮助指导他人。 
更多相关话题 
 PEFT 概述：最先进的参数高效微调如何每天处理 150 亿条日志并将大查询保持在 1 秒以内在本地 CPU 上运行小型语言模型的 7 个步骤自然语言处理中的 N-gram 语言建模顶级开源大型语言模型人工智能：大型语言与视觉模型
机器学习的有效测试 
 
 原文：www.kdnuggets.com/2022/01/effective-testing-machine-learning.html 
 我们在PyData Global 2021上展示了这个博客系列的简短版本。 

 我们的前三个课程推荐 

1. 谷歌网络安全证书 - 快速进入网络安全职业的捷径。 
2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 需求 

 点击这里查看第二部分，点击这里查看第三部分。 
 
 
本博客系列描述了一种我在过去几年中开发的策略，用于有效测试机器学习项目。鉴于机器学习项目的不确定性，这是一种逐步的策略，你可以随着项目的成熟而采纳；它包括测试示例，以提供这些测试在实践中的清晰概念，并且完整的项目实现可在 GitHub 上获取。到文章末尾，你将能够开发出更强健的机器学习管道。 
测试机器学习项目的挑战 
测试机器学习项目是具有挑战性的。训练一个模型是一个长时间运行的任务，可能需要数小时，并且具有非确定性的输出，这与我们测试软件所需的快速和确定性过程正好相反。一年前，我发布了一篇关于测试数据密集型项目的帖子，以使持续集成成为可能。我后来将这篇博客文章转化为演讲，并在PyData 2020上进行了展示。但之前的工作仅涵盖了数据管道测试的通用方面，并没有涉及机器学习模型的测试。 
需要明确的是，测试和监控是两回事。测试是一个离线过程，允许我们评估代码是否按预期工作（即，生成高质量的模型）。相反，监控涉及检查已部署的模型，以确保它正常工作。因此，测试发生在部署之前；监控发生在部署之后。 
我在本文中使用了pipeline 和 task 这两个术语。任务是一个工作单元（通常是一个函数或脚本）；例如，一个任务可以是下载原始数据的脚本，另一个任务可以是清理这些数据。另一方面，管道只是按预定义顺序执行的一系列任务。构建由小任务组成的管道的动机是使我们的代码更具可维护性和更易于测试；这与我们开源框架的目标一致，旨在帮助数据科学家使用 Jupyter 构建更具可维护性的项目。在接下来的部分中，你将看到一些示例 Python 代码；我们使用pytest、pandas 和Ploomber。 
机器学习管道的各个部分 
在我们描述测试策略之前，让我们分析一个典型的 ML 管道的样子。通过分别分析每个部分，我们可以清楚地阐明其在项目中的作用，并相应地设计测试策略。一个标准的 ML 管道具有以下组件： 
 特征生成管道。 一系列计算，用于处理原始数据并将每个数据点映射到特征向量。请注意，我们在训练和服务时都会使用这个组件。训练任务。 接收训练集并生成模型文件。模型文件。 来自训练任务的输出。它是一个包含学习到的参数的单一文件。此外，它可能包括预处理程序，如缩放或独热编码。训练管道。 封装了训练逻辑：获取原始数据，生成特征，并训练模型。服务管道。（也称为推理管道）封装了服务逻辑：获取新观察值，生成特征，通过模型传递特征，并返回预测结果。
 
可能会出什么问题？ 
为了激励我们的测试策略，让我们列举每个部分可能出现的问题： 
特征生成管道 
 无法运行管道（例如，设置问题、代码损坏）。无法重现之前生成的训练集。管道生成低质量的训练数据。
训练任务 
 无法训练模型（例如，缺少依赖项、代码损坏）。使用高质量数据运行训练任务生成低质量模型。
模型文件 
 生成的模型质量低于我们当前生产中的模型。模型文件与服务管道集成不正确。
服务管道 
 无法提供预测（例如，缺少依赖项、代码损坏）。训练和服务时的预处理不匹配（即训练-服务偏差）。传入无效原始数据时输出预测。传入有效数据时崩溃。
请注意，这不是一个详尽的列表，但涵盖了最常见的问题。根据你的使用情况，可能还有其他潜在问题，重要的是列出它们以便定制你的测试策略。 
测试策略 
在开发机器学习模型时，我们迭代的速度越快，成功的机会就越大。与传统的软件工程项目不同（例如，注册表单），机器学习项目存在很多不确定性：使用哪些数据集？尝试哪些特征？使用什么模型？由于我们事先不知道这些问题的答案，我们必须尝试一些实验并评估它们是否产生更好的结果。由于这种不确定性，我们必须在迭代速度和测试质量之间取得平衡。如果我们迭代 太快，可能会编写粗糙的代码；如果我们花太多时间彻底测试每一行代码，就无法足够快地改进我们的模型。 
这个框架稳步提高你的测试质量。策略包括五个层级；当达到最后一层时，你的测试已经足够健壮，可以自信地将新的模型版本推送到生产环境。 
测试层级 
 冒烟测试。 我们通过在每次 git push 时运行代码来确保其正常工作。集成测试和单元测试。 测试任务的输出和数据转换。分布变化和服务管道。 测试数据分布的变化以及测试我们能否加载模型文件并进行预测。训练与服务偏差。 测试训练和服务逻辑的一致性。模型质量。 测试模型质量。
简要介绍如何使用 pytest 进行测试 
如果你之前使用过 pytest，可以跳过此部分。 
测试是检查我们的代码是否正常工作的短程序。例如： 
# test_math.py
from my_math_project import add, subtract

def test_add():
assert add(1, 1) == 2

def test_subtract():
assert subtract(43, 1) == 42 
12345678 
测试是一个运行一些代码并 断言 其输出的函数。例如，上一个文件有两个测试：test_add 和 test_substract，组织在一个名为 test_math.py 的文件中；通常每个模块一个文件（例如，test_math.py 测试 math.py 模块中的所有函数）。测试文件通常放在 tests/ 目录下： 
tests/
test_math.py
test_stuff.py
...
test_other.py 
12345 
测试框架如 pytest 允许你收集所有测试，执行它们并报告哪些测试失败，哪些测试成功： 
# collect rests, run them, and report results
pytest 
12 
一个典型的项目结构如下： 
src/
exploratory/
tests/ 
123 
src/ 包含你项目的管道任务和其他实用函数。exploratory/ 包含探索性笔记本，你的测试放在 tests/ 目录中。src/ 中的代码必须可以从其他两个目录中导入。实现这一点的最简单方法是 打包你的项目。否则，你需要处理 sys.path 或 PYTHONPATH。 
如何导航示例代码 
示例代码可在 这里找到。该仓库有五个分支，每个分支实现了我将在接下来的部分中描述的测试级别。由于这是一种渐进策略，你可以通过从 第一个分支开始，逐步查看项目的演变。 
该项目使用了 Ploomber，我们的开源框架来实现管道。因此，你可以在pipeline.yaml文件中查看管道规格。要查看我们用于测试管道的命令，请打开 .github/workflows/ci.yml，这是一个 GitHub Actions 配置文件，用于告诉 GitHub 在每次git push时运行特定的命令。 
虽然不是严格必要的，但你可能会想查看我们的 Ploomber 入门教程以了解基本概念。 
请注意，本博客文章中显示的代码片段是通用的（它们不使用任何特定的管道框架），因为我们想要用一般术语解释概念；然而，仓库中的示例代码使用了 Ploomber。 
一级：冒烟测试 
示例代码可在这里找到。 
冒烟测试是最基本的测试级别，应在你开始项目时立即实施。冒烟测试不会检查代码的输出，只会确保它能够运行。虽然这看起来过于简单，但比完全没有测试要好得多。 
记录依赖项 
列出外部依赖项是启动任何软件项目的第一步，因此在创建你的 虚拟环境时，确保记录运行项目所需的所有依赖项。例如，如果使用pip，你的requirements.txt文件可能如下所示： 
ploomber
scikit-learn
pandas 
123 
创建虚拟环境后，创建另一个文件（requirements.lock.txt）来记录所有依赖项的已安装版本。你可以使用pip freeze > requirements.lock.txt命令（在运行pip install -r requirements.txt后执行）生成类似如下内容的文件： 
ploomber==0.13
scikit-learn==0.24.2
pandas==1.2.4
# more packages required by your dependencies... 
1234 
记录具体的依赖项版本可以确保这些包的更改不会破坏你的项目。 
另一个重要的考虑因素是将你的依赖列表保持尽可能简短。通常，你在开发时需要一组依赖项，但在生产环境中不需要。例如，你可能会使用matplotlib进行模型评估图，但在进行预测时不需要它。强烈建议将开发和部署的依赖项分开。依赖项过多的项目增加了遇到版本冲突的风险。 
测试特性生成管道 
你项目的第一个里程碑必须是获得一个端到端的特征生成管道。编写一些代码来获取原始数据，进行一些基本清洗，并生成一些特征。一旦你有了一个完整的端到端流程，你必须确保它是可重复的：删除原始数据，并检查你是否可以重新运行该流程并获得相同的训练数据。 
一旦你有了这个，就可以实施第一个测试；用原始数据的一个样本（比如 1%）运行管道。目标是使这个测试运行快速（不超过几分钟）。你的测试应如下所示： 
from my_project import generate_features_and_label

def test_generate_training_set():
my_project.generate_features_and_label(sample=True) 
1234 
请注意，这只是一个基本测试；我们没有检查管道的输出！然而，这个简单的测试允许我们检查代码是否运行。每当我们执行git push时，运行此测试是至关重要的。如果你使用 GitHub，可以通过GitHub Actions完成，其他 git 平台也有类似功能。 
测试训练任务 
在生成特征后，你会训练一个模型。训练任务接受一个训练集作为输入，并输出一个模型文件。测试模型训练过程具有挑战性，因为我们不能轻易定义给定某些输入（训练集）的期望输出（模型文件）——主要因为我们的训练集变化迅速（即添加、删除特征）。因此，在这个阶段，我们的第一个测试只检查任务是否运行。由于我们暂时忽略输出，我们可以使用数据样本来训练模型；记住，这个冒烟测试必须在每次推送时执行。因此，让我们扩展之前的示例，以涵盖特征生成和模型训练： 
from my_project import generate_features_and_label, train_model

def test_train_model():
# test we can generate features
X, y = my_project.generate_features_and_label(sample=True)
# test we can train a model
model = train_model(X, y) 
1234567 
在示例代码库中，我们使用 Ploomber，因此我们通过调用ploomber build来测试功能管道和训练任务，这会执行管道中的所有任务。 
第二级：集成测试和单元测试 
示例代码可在此处查看。 
将管道模块化为小任务，以便我们可以单独测试输出，这一点至关重要。在实现了第二个测试级别后，你将实现两件事： 
 确保用于训练模型的数据达到最低质量水平。分别测试代码中行为定义明确的部分。
让我们讨论第一个目标。 
集成测试 
测试数据处理代码是复杂的，因为其目标是主观的。例如，假设我让你测试一个接受数据框并清洗它的函数。你会如何测试？数据清洗的目的是提高数据质量。然而，这样的概念依赖于数据的具体情况和你的项目。因此，你需要定义清洁数据的概念，并将其转化为集成测试，尽管在这种情况下，我们可以使用数据质量测试这一术语来更精确。 
集成测试的理念适用于管道中的所有阶段：从下载数据到生成特征：你需要定义每个阶段的期望。我们可以在下图中看到集成测试的图形表示： 
 
例如，要为数据清理函数（我们称之为clean）添加集成测试，我们在函数体的末尾运行一些检查来验证其输出质量。常见的检查包括没有空值、数值列在预期范围内或分类值在预定义的值集合中： 
def clean(df)
# clean data frame with raw data
# ...
# ...

# integration test: check age column has a minimum value of 0
assert df.age.min() > 0 
1234567 
这种测试形式与我们在第一部分介绍的测试有所不同。单元测试存在于tests/文件夹中并可以独立运行，但集成测试在你执行训练管道时运行。 失败的测试意味着你的数据假设不成立，需要重新定义数据假设（这意味着需要相应地更新测试），或者你的清理程序应更改以确保测试通过。 
你可以通过在每个任务结束时添加断言语句来编写集成测试，而不需要额外的框架。然而，一些库可以提供帮助。例如，Ploomber 支持在任务完成时运行一个函数。 
这是实现示例的集成测试在我们的样本代码库中。 
单元测试 
在你的每个任务中（例如，clean内部），你可能会有更小的例程；这些代码部分应作为独立函数编写并进行单元测试（即，在tests/目录中添加测试）。 
编写单元测试的一个良好候选场景是对列中的单个值进行转换。例如，假设你正在使用心脏病数据集，并创建一个函数将chest_pain_type分类列中的整数映射到其对应的可读值。你的clean函数可能如下所示： 
import transform

def clean(df):
# some data cleaning code...
# ...

df['chest_pain_type'] = transform.chest_pain_type(df.chest_pain_type)

# ...
# more data cleaning code... 
12345678910 
与一般的clean过程不同，transform.chest_pain_type具有明确的、客观定义的行为：它应将整数映射到对应的可读值。我们可以通过指定输入和预期输出将其转化为单元测试。 
def test_transform_chest_pain_type():
# sample input
series = pd.Series([0, 1, 2, 3])

# expected output
expected = pd.Series([
'typical angina',
'atypical angina',
'non-anginal pain',
'asymptomatic',
])

# test
assert transform.chest_pain_type(series).equals(expected) 
1234567891011121314 
单元测试必须在所有即将到来的测试级别上持续进行。因此，每当你遇到具有明确目标的逻辑时，将其抽象成函数并单独测试。 
这是一个实现示例的单元测试在样本代码库中。 
接下来 
到目前为止，我们已经实施了一种基本策略，确保我们的特征生成管道生成的数据具有最低质量标准（集成测试或数据质量测试），并验证数据转换的正确性（单元测试）。在本系列的下一部分中，我们将添加更强大的测试：测试分布变化，确保我们的训练和服务逻辑一致，并检查我们的管道是否生成高质量的模型。 
如果你想知道第二部分何时发布，请订阅我们的 新闻通讯，关注我们的 Twitter 或 LinkedIn。 
爱德华多·布兰卡斯 是 Ploomber 的联合创始人之一。Ploomber 是一家获得 Y Combinator 资助的公司，帮助数据科学家快速构建数据管道。我们通过允许他们开发模块化管道并在任何地方部署它们来实现这一目标。在此之前，他是 Fidelity Investments 的数据科学家，负责部署首个面向客户的资产管理机器学习模型。爱德华多拥有哥伦比亚大学的数据科学硕士学位和蒙特雷科技大学的机电工程学士学位。 
原文。经许可转载。 
更多相关主题 
 假设检验与 A/B 测试机器学习中训练数据与测试数据的区别机器学习测试初学者指南：使用 DeepChecksA/B 测试：综合指南像专家一样测试：Python Mock 库的逐步指南假设检验详解
如何在数据过载时代有效获取消费者洞察 
 
 原文：www.kdnuggets.com/2020/09/effectively-obtain-consumer-insights-data-overload-era.html 
 
评论 
作者：Patrícia Osorio，共同创始人兼首席市场官，Birdie 
 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业道路。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 需求 
 
COVID-19 疫情对我们的日常生活产生了诸多影响。组织讨论的最重要因素之一是数字化转型的加速以及其在数据和技术方面的影响。正如《经济学人》所指出的，疫情的一个明显后果将是“数据驱动的服务将渗透到生活的方方面面。”这带来了另一个问题，就是大量的数据以及如何从中获取有价值的消费者洞察。大家都知道了解客户的重要性，但在信息过载的时代，如何做到这一点呢？ 
什么是数据过载？ 
信息过载是由 Debra Brass（前 J&J 全球总裁）在 2013 年创造的术语，描述了一种现实，即数据的过度可用性使得其分析变得复杂，以至于在某些情况下，它变得无效，无法对有洞察力的分析做出贡献。它阻碍了公司做出有效的决策，因为他们在面对如此多的数据时无法采取行动。 
这种问题背景很可能是由于公司在数据收集能力上的迅速演变，但未能跟上正确集成和分析这些数据的能力。虽然这一问题可能影响整个公司，但对营销、客户服务、销售和消费者洞察等面向客户的团队尤其成问题。 
一些研究估计，营销或消费者洞察高管最多将 80%的时间花在分析消费者数据上。在这些高管中，只有五分之一认为他们拥有合适的工具。电子商务平台上客户评论数量的指数级增长就是一个很好的例子。消费者不仅写更多评论，还在线上提出和回答更多问题，生成了大量具有挑战性的数据进行分析。 
这既是一个机会也是一个问题：随着消费者评论和对话的增加，公司有机会获取这些数据，并以非常客观的方式实时了解其消费者，比以往更快。但他们如何在仍在为数据过载而挣扎并且未能获得所需洞察时探索新的消费者信息来源呢？ 
如何将数据转化为消费者洞察 
答案是利用智能系统。这一概念由 Greylock Partners（软件行业最重要的投资基金之一）提出，许多人视其为数据分析领域下一波重大创新。智能系统作为一个额外的层次，连接了记录系统——存储数据的平台，如 CRM 或 ERP 平台——和互动系统——用于捕捉数据的互动平台（如客户服务或消息工具）。 
智能系统的主要价值主张在于连接来自不同来源的数据——这些数据在大多数情况下未被充分利用，因为公司甚至无法访问它们——以便从这些数据中生成深刻的见解。通过实施这些智能系统，公司不仅可以减少将自身数据与电子表格连接的工作，还可以更轻松地将新的数据源纳入分析中，减少技术挑战，实现流畅的工作流程，从而提高每个操作周期的质量。 
回到今天的背景，正是这样的解决方案似乎缺失。我们都有数据的访问权限，但在大多数情况下，我们不确定该寻找什么：我们分析错误的数据，只关注收集内容的一部分而看不到其与其他部分的关联，甚至选择了错误的度量标准来衡量。更重要的是，我们花更多的时间试图理解过去，而不是规划未来。 
因此，我们不仅做出错误的决策，还浪费了大量时间将精力集中在对成功无关的行动上。Gartner 的研究显示，到 2023 年，数据智能和消费者洞察领域的 60%将减半，仅仅是因为无法从捕获的数据中生成价值。 
市场营销和消费者洞察的高管需要减少在 Excel 上花费数小时处理和连接多个数据源的负担，以便- 也许- 得到相关的见解。 
消费者洞察的未来：决策智能 
随着""脏数据""每年使美国经济损失 3.1 万亿美元和每天产生 2.5 亿亿字节的数据，消费者洞察的未来与智能系统有直接关系并不足为奇。名字？决策智能。 
决策智能是一个框架，旨在帮助将人工智能（AI）、机器学习（ML）和文本分析应用于实际业务决策，优先考虑业务并将技术投入使用。根据 Gartner，到 2023 年，超过 33%的大型组织将拥有从事决策智能的分析师。 
它之所以如此强大，是因为它将商业目标、方面和问题与人工智能和预测分析的能力相连接，利用多个数据源来发展洞察力并预测未来。 
简介: Patrícia Osorio 拥有超过 10 年的市场营销和业务发展经验。她在 USP 获得法学学位，在 FGV-EAESP 获得商业管理学位后，于 2007 年加入 Arizona，领导市场营销部门，并负责产品创新和新市场开发项目，如公司国际化（向阿根廷、智利、哥伦比亚和英国的客户销售，并在阿根廷开设办公室）以及数字产品开发。她还共同创办了 HomeRefill，一个在线订阅电子商务平台，以及 GVAngels，一个已经在巴西初创企业投资超过 100 万美元的天使投资集团。Pat 是由 Growth Tribe（欧洲）毕业的增长黑客，拥有在巴西和美国的 B2B 增长和获取经验。她与 Alex 一起看到了利用产品数据获得见解的机会，并在 2017 年底起草了**Birdie**的第一个版本。 
相关内容: 
 坦率地说：我们在数据中淹没向大数据提问的 3 个关键数据科学问题客户流失预测：全球绩效研究
更多相关内容 
 在 GPT-4 时代如何成为数据科学家GenAI 时代的 AI 转型战略如何在生成式 AI 时代脱颖而出并保护你的职位数据分析：分析数据的四种方法及如何…数据可视化：有效展示复杂信息如何有效使用 Pandas GroupBy
如何有效使用 Pandas GroupBy 
 
 原文：www.kdnuggets.com/2023/01/effectively-pandas-groupby.html 
 
Pandas 是一个功能强大且广泛使用的开源库，用于使用 Python 进行数据操作和分析。它的一个关键特性是能够使用 groupby 函数通过基于一个或多个列将数据框分成组，然后对每个组应用各种聚合函数。 
 
图片来自 Unsplash 
 
我们的前三个课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织进行 IT 维护 
 
groupby 函数非常强大，因为它允许你快速总结和分析大数据集。例如，你可以根据特定列对数据集进行分组，并计算每个组中剩余列的均值、总和或计数。你还可以通过多个列进行分组，以获得对数据的更详细理解。此外，它还允许你应用自定义聚合函数，这对于复杂的数据分析任务是一个非常强大的工具。 
在本教程中，你将学习如何使用 Pandas 的 groupby 函数来对不同类型的数据进行分组并执行不同的聚合操作。通过本教程，你应该能够使用这个函数以多种方式分析和总结数据。 
实践代码示例 
当概念经过充分练习后就会内化，这也是我们接下来要做的，即实际操作 Pandas 的 groupby 函数。建议使用 Jupyter Notebook 来完成本教程，因为你可以在每一步查看输出结果。 
生成示例数据 
导入以下库： 
 Pandas: 创建数据框并应用分组Random - 用于生成随机数据Pprint - 用于打印字典
import pandas as pd
import random
import pprint
123 
接下来，我们将初始化一个空的数据框，并填入每一列的值，如下所示： 
df = pd.DataFrame()
names = [
""Sankepally"",
""Astitva"",
""Shagun"",
""SURAJ"",
""Amit"",
""RITAM"",
""Rishav"",
""Chandan"",
""Diganta"",
""Abhishek"",
""Arpit"",
""Salman"",
""Anup"",
""Santosh"",
""Richard"",
]

major = [
""Electrical Engineering"",
""Mechanical Engineering"",
""Electronic Engineering"",
""Computer Engineering"",
""Artificial Intelligence"",
""Biotechnology"",
]

yr_adm = random.sample(list(range(2018, 2023)) * 100, 15)
marks = random.sample(range(40, 101), 15)
num_add_sbj = random.sample(list(range(2)) * 100, 15)

df[""St_Name""] = names
df[""Major""] = random.sample(major * 100, 15)
df[""yr_adm""] = yr_adm
df[""Marks""] = marks
df[""num_add_sbj""] = num_add_sbj
df.head() 
1234567891011121314151617181920212223242526272829303132333435363738 
额外提示 – 更简洁的方法是创建一个包含所有变量及其值的字典，然后将其转换为数据框。 
student_dict = {
""St_Name"": [
""Sankepally"",
""Astitva"",
""Shagun"",
""SURAJ"",
""Amit"",
""RITAM"",
""Rishav"",
""Chandan"",
""Diganta"",
""Abhishek"",
""Arpit"",
""Salman"",
""Anup"",
""Santosh"",
""Richard"",
],
""Major"": random.sample(
[
""Electrical Engineering"",
""Mechanical Engineering"",
""Electronic Engineering"",
""Computer Engineering"",
""Artificial Intelligence"",
""Biotechnology"",
]
* 100,
15,
),
""Year_adm"": random.sample(list(range(2018, 2023)) * 100, 15),
""Marks"": random.sample(range(40, 101), 15),
""num_add_sbj"": random.sample(list(range(2)) * 100, 15),
}
df = pd.DataFrame(student_dict)
df.head() 
123456789101112131415161718192021222324252627282930313233343536 
数据框的外观如下所示。在运行此代码时，由于我们使用的是随机样本，某些值可能不会匹配。 
 
创建分组 
让我们按“Major”科目分组数据，并应用组过滤以查看有多少记录落入此组。 
groups = df.groupby('Major')
groups.get_group('Electrical Engineering')
12 
所以，四名学生属于电气工程专业。 
 
你也可以按多个列分组（在此情况下为 Major 和 num_add_sbj）。 
groups = df.groupby(['Major', 'num_add_sbj'])
1 
注意，所有可以应用于单列组的聚合函数都可以应用于多列组。在接下来的教程中，让我们以单列为例深入探讨不同类型的聚合。 
让我们使用“Major”列上的groupby创建组。 
groups = df.groupby('Major')
1 
应用直接函数 
假设你想找到每个 Major 的平均分数。你会怎么做？ 
 选择 Marks 列应用均值函数应用round函数将成绩四舍五入到两位小数（可选）
groups['Marks'].mean().round(2)
1 
Major
Artificial Intelligence63.6
Computer Engineering 45.5
Electrical Engineering 71.0
Electronic Engineering 92.0
Mechanical Engineering 64.5
Name: Marks, dtype: float64
1234567 
聚合 
另一种实现相同结果的方法是使用如下所示的聚合函数： 
groups['Marks'].aggregate('mean').round(2)
1 
你也可以通过将函数作为字符串列表传递给组来应用多个聚合。 
groups['Marks'].aggregate(['mean', 'median', 'std']).round(2)
1 
 
但如果你需要对不同的列应用不同的函数怎么办？不用担心。你也可以通过传递{column: function}对来做到这一点。 
groups.aggregate({'Year_adm': 'median', 'Marks': 'mean'})
1 
 
变换 
你可能需要对特定列执行自定义变换，这可以通过使用groupby()轻松实现。让我们定义一个类似于 sklearn 的预处理模块中的标准缩放器。你可以通过调用transform方法并传递自定义函数来变换所有列。 
def standard_scalar(x):
return (x - x.mean())/x.std()
groups.transform(standard_scalar)
123 
 
注意，“NaN”表示标准差为零的组。 
过滤 
你可能想检查哪个“Major”表现不佳，即平均“Marks”低于 60 的那个。这需要你对组应用一个带有函数的过滤方法。下面的代码使用一个 lambda 函数来实现过滤结果。 
groups.filter(lambda x: x['Marks'].mean() < 60)
1 
 
首先 
它给你按索引排序的第一个实例。 
groups.first()
1 
 
描述 
“describe”方法返回给定列的基本统计数据，如计数、均值、标准差、最小值、最大值等。 
groups['Marks'].describe()
1 
 
大小 
Size，如名称所示，返回每组的大小，即记录数量。 
groups.size()
1 
Major
Artificial Intelligence5
Computer Engineering 2
Electrical Engineering 4
Electronic Engineering 2
Mechanical Engineering 2
dtype: int64
1234567 
计数和唯一值 
“Count”返回所有值，而“Nunique”仅返回该组中的唯一值。 
groups.count()
1 
 
groups.nunique()
1 
 
重命名 
你也可以根据自己的偏好重命名聚合列的名称。 
groups.aggregate(""median"").rename(
columns={
""yr_adm"": ""median year of admission"",
""num_add_sbj"": ""median additional subject count"",
}
) 
123456 
 
充分利用 groupby 函数。 
 明确 groupby 的目的： 你是想通过一列对数据进行分组，以获取另一列的均值？还是通过多列对数据进行分组，以获取每组中的行数？了解数据框的索引： groupby 函数使用索引来分组数据。如果你想通过某列对数据进行分组，确保该列被设置为索引，或者可以使用.set_index()使用适当的聚合函数：可以与各种聚合函数一起使用，如 mean()、sum()、count()、min()、max()。使用 as_index 参数： 当设置为 False 时，此参数告诉 pandas 将分组列用作常规列而不是索引。
你还可以将 groupby()与其他 pandas 函数（如 pivot_table()、crosstab()和 cut()）结合使用，以从数据中提取更多洞察。 
摘要 
groupby 函数是数据分析和处理的强大工具，因为它允许你根据一个或多个列对数据行进行分组，然后对这些组执行聚合计算。教程展示了如何通过代码示例使用 groupby 函数的各种方法。希望它能帮助你理解这些选项及其在数据分析中的作用。 
Vidhi Chugh 是一位人工智能策略师和数字化转型领导者，她在产品、科学和工程交汇处工作，致力于构建可扩展的机器学习系统。她是一位获奖的创新领导者、作者和国际演讲者。她的使命是使机器学习民主化，并打破行话，让每个人都能参与这场变革。 
了解更多相关话题 
 数据分析：分析数据的四种方法及其有效应用如何有效管理 Docker 镜像版本的标签数据可视化：有效呈现复杂信息如何使用条件格式化增强 Pandas 中的数据可视化如何使用 pivot_table 函数进行高级数据汇总如何在 Pandas 中使用 MultiIndex 进行层次化数据组织
ChatGPT 在学校的影响以及为什么它被禁止 
 
 原文：www.kdnuggets.com/2023/06/effects-chatgpt-schools-getting-banned.html 
 
 
图片由编辑提供 
ChatGPT 在许多应用场景中既引起了兴奋也引发了担忧。它最显著——也是最具争议的——用途之一是它在学校中的使用。随着 ChatGPT 的受欢迎程度持续飙升，许多学区正在禁止它，但它对教育的潜在积极作用也在增长。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升您的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持您的组织在 IT 方面 
 
学校为什么禁止 ChatGPT 
理解这个问题的两个方面很重要。首先，这里列出了学校禁止 ChatGPT 的一些主要原因。 
抄袭问题 
对 ChatGPT 在学校中的最普遍担忧是它的抄袭倾向。像所有 AI 模型一样，ChatGPT 并不一定创造新内容，而是编译、总结和重新表述现有工作。学生可以轻松地使用它来完成他们的作业，就像偷别人的论文并交上去一样。 
许多工具可以检测到 ChatGPT 生成的内容，因此发现试图作弊的学生相对容易。然而，即使学生只是将其作为研究工具，他们也可能无法找到 ChatGPT 提供的信息的原始来源，从而导致意外抄袭。模型训练数据背后人员缺乏同意也引发了进一步的抄袭问题。 
准确性存疑 
在学校使用 ChatGPT 也可能妨碍学生教育的质量。ChatGPT 提供的答案并不总是准确的，即使它们很有说服力，这可能导致学生相信误导性或完全错误的信息。 
这些事实上的不准确性可能比抄袭更难检测，尤其是当涉及教师不太熟悉的话题时。因此，在研究或作为教学工具中使用它可能会传播错误信息。 
隐私问题 
ChatGPT 还存在一些隐私问题。该平台收集了大量用户数据，其隐私控制有限且不明显。考虑到教育行业在去年最容易遭受勒索软件攻击，这确实是一个重大问题。 
学生可能不知道在使用 ChatGPT 时，他们正在提供多少个人信息或该网站如何使用这些信息。这使大量高度敏感的数据面临风险。 
ChatGPT 的作弊以外的用途 
尽管存在这些担忧，但 ChatGPT 在学校中有几个积极方面。以下是教育系统如何利用这个聊天机器人来使学生和教师都受益。 
简化行政管理 
教师和其他员工可以使用 ChatGPT 来自动化一些行政任务，比如安排时间表和数据录入。这种自动化将使他们有更多时间与学生互动，从而改善教育成果。 
近50%的独立学校使用五种或更多在线解决方案来管理行政工作。这会导致很多断层、低效和不必要的成本。利用像 ChatGPT 这样的工具来简化这些幕后工作流程，将有助于减少时间和费用。 
提高参与度 
教育工作者还可以使用 ChatGPT 来使课程对学生更具吸引力。研究发现，当技术在他们的学习中发挥作用时，学生会更加投入和舒适，而像 ChatGPT 这样的新技术可能特别有用。 
学生可以使用 ChatGPT 来制定他们自己充实的研究论文大纲，这有助于他们学习什么是高质量的文章。或者，教师可以使用 ChatGPT 来展示 AI 的工作原理，或强调在在线资源中关注偏见和虚假信息的重要性。 
为 AI 未来做准备 
学校也可以考虑教学生如何使用 ChatGPT 及类似工具。生成性 AI 无疑将在许多未来的业务中发挥作用，因此学习如何负责任地使用 AI 是即将到来的这一代的关键技能。 
一些专家预测58%的现有劳动力将需要提升技能，以便在新技术改变他们的角色和职责时有效工作。教育系统可以通过在学生进入职场前为他们提供 AI 素养来领先于这一趋势。ChatGPT 的易用性使其成为实现这一目标的理想工具。 
生成性 AI 可能永远改变教育 
生成式 AI 平台如 ChatGPT 将改变许多行业的运作方式。教育领域可能也会发生类似变化，无论是通过采纳新政策来限制技术，还是积极拥抱它。 
在学校使用 ChatGPT 有许多好处，但仍面临重大障碍。如果学校系统能以谨慎的态度对待，并考虑这些缺点，他们可以利用这些优势来改善未来的教育。 
April Miller 是 ReHack 杂志的消费者技术管理编辑。她有创建优质内容的丰富经验，这些内容能够吸引流量到她所工作的出版物中。 
更多相关话题 
 ChatGPT 是如何工作的？为什么它有效？为什么 DuckDB 越来越受欢迎？Visual ChatGPT：微软结合 ChatGPT 和 VFMChatGPT CLI：将你的命令行界面转变为 ChatGPT开始使用刚刚打败 GPT-4 和 Gemini 的 Claude 3 Opus开始使用 PyTest：轻松编写和运行 Python 测试
高性能深度学习，第一部分 
 
 原文：www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html 
 
评论 
 
机器学习在今天的应用中无处不在。它在没有单一算法完美解决问题的领域中天然适用，并且在算法需要很好地预测正确输出的情况下，有大量未见过的数据。与我们期望确切最优解的传统算法问题不同，机器学习应用可以容忍近似答案。深度学习与神经网络在过去十年里一直是训练新机器学习模型的主流方法。它的崛起通常归因于 2012 年的 ImageNet [1] 竞赛。那一年，多伦多大学的一个团队提交了一个深度卷积网络（AlexNet [2]，以首席开发者 Alex Krizhevsky 命名），表现比下一个最佳提交好 41%。 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业的捷径。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织的 IT 
 
深度和卷积网络之前已经被尝试过，但总是未能兑现承诺。卷积层最早由 LeCun 等人在 90 年代提出 [3]。同样，80 年代、90 年代等也提出了几种神经网络。深度网络为什么花了这么长时间才超越手工调整特征工程的模型？ 
这次的不同之处在于多个因素的结合： 
 计算：AlexNet 是早期依赖图形处理单元（GPU）进行训练的模型之一。算法：一个关键的修复是激活函数使用了 ReLU。这使得梯度可以更深地反向传播。之前的深度网络使用了 sigmoid 或 tanh 激活函数，这些函数在非常小的输入范围内饱和到 1.0 或 -1.0。因此，输入变量的变化导致梯度非常微小（如果有的话），当层数很多时，梯度实际上消失了。数据：ImageNet 拥有超过 1000 个类别的 100 万张以上的图像。随着互联网产品的出现，从用户行为中收集标注数据也变得更加便宜。
深度学习模型的快速增长 
由于这项开创性的工作，出现了创建更深网络、参数数量越来越大的竞赛。几种模型架构如 VGGNet、Inception、ResNet 等，在随后的几年中相继打破了 ImageNet 比赛的记录。这些模型也已在实际应用中投入使用。 
 
 
图 1：流行的计算机视觉和自然语言深度学习模型参数数量的增长趋势 [4]。 
我们在自然语言理解（NLU）领域看到了类似的效果，其中 Transformer 架构显著超过了 GLUE 任务的之前基准。随后，BERT 和 GPT 模型在 NLP 相关任务上均显示出了改进。BERT 产生了几种优化其各种方面的相关模型架构。GPT-3 通过生成与给定提示相匹配的逼真文本引起了关注。这两者都已投入生产。BERT 用于 Google 搜索以提高结果的相关性，而 GPT-3 作为 API 供感兴趣的用户使用。 
如所推测，深度学习研究一直集中于提高最先进的技术，因此，我们在图像分类、文本分类等基准测试中看到了持续的进步。每一次神经网络的新突破都导致了网络复杂度的增加、参数数量的增加、训练网络所需的资源量、预测延迟等。 
像 GPT-3 这样的自然语言模型现在训练一次就需要花费数百万美元 [5]。这还不包括尝试不同超参数组合（调优）或手动或自动实验架构的成本。这些模型通常还拥有数十亿（或万亿）个参数。 
与此同时，这些模型的卓越性能也推动了在之前受限于现有技术的新任务上的应用需求。这就产生了一个有趣的问题，即这些模型的传播受到其效率的限制。 
更具体地说，我们在进入这一深度学习新时代时，面临以下问题，其中模型变得越来越大，并且跨越不同领域： 
 可持续的服务器端扩展：训练和部署大型深度学习模型的成本很高。虽然训练可能是一次性成本（或者使用预训练模型时可能免费），但长时间部署和进行推理仍可能是昂贵的。还有一个非常实际的关注点是用于训练和部署这些大型模型的数据中心的碳足迹。像谷歌、脸书、亚马逊等大型组织每年在数据中心的资本支出上花费数十亿美元。因此，任何效率提升都是非常重要的。启用设备端部署：随着智能手机、物联网设备的普及，部署在这些设备上的应用必须是实时的。因此，需要设备端机器学习模型（即模型推理直接在设备上进行），这使得优化模型以适应运行设备变得非常重要。隐私与数据敏感性：在用户数据可能涉及敏感处理或受各种限制（如欧洲的 GDPR 法规）的情况下，使用尽可能少的数据进行训练至关重要。因此，高效地使用少量数据训练模型意味着需要收集的数据更少。类似地，使设备上的模型能够运行意味着模型推理可以完全在用户设备上进行，而无需将输入数据发送到服务器端。新应用：效率还将使得在现有资源约束下无法实现的应用成为可能。模型爆炸：通常，可能会有多个机器学习模型在同一设备上同时服务。这进一步减少了单个模型的可用资源。这可能发生在服务器端，多个模型共存在同一台机器上，或者在应用中，不同的模型用于不同的功能。
高效深度学习 
我们上述识别出的核心挑战是效率。虽然效率可能是一个过载的术语，但让我们探讨两个主要方面。 
 推理效率：这主要涉及部署模型进行推理（计算给定输入的模型输出）时需要问的问题。模型是否小巧？是否快速等等？更具体地说，模型有多少参数？磁盘大小、推理期间的内存消耗、推理延迟等是多少？训练效率：这涉及到训练模型时需要问的一些问题，比如模型需要多长时间训练？需要多少设备进行训练？模型是否可以适应内存？还可能包括诸如，模型需要多少数据才能在给定任务上达到预期的性能？
如果我们有两个模型在给定任务上表现相同，我们可能会选择在上述一个或理想情况下两个方面表现更好的模型。如果在推理受限（如移动和嵌入设备）或昂贵（云服务器）的设备上部署模型，关注推理效率可能是值得的。同样，如果在有限或昂贵的训练资源下从头开始训练大型模型，开发针对训练效率设计的模型将有所帮助。 
 
图 2: 帕累托最优性：绿色点代表帕累托最优模型（共同形成帕累托前沿），其中没有其他模型（红色点）在相同的推理延迟下获得更好的准确性，反之亦然。 
无论我们优化的目标是什么，我们都希望达到帕累托最优性。这意味着我们选择的任何模型都是在我们关心的权衡中表现最好的。例如，在图 2 中，绿色点代表帕累托最优模型，其中没有其他模型（红色点）在相同的推理延迟下获得更好的准确性，反之亦然。总之，帕累托最优模型（绿色点）形成了我们的帕累托前沿。帕累托前沿的模型从定义上讲比其他模型更高效，因为它们在给定的权衡中表现最好。因此，当我们寻求效率时，我们应该考虑发现和改进帕累托前沿。 
高效的深度学习可以定义为一组算法、技术、工具和基础设施，它们共同作用，使用户能够训练和部署帕累托最优模型，这些模型在训练和/或部署时消耗较少的资源，同时达到类似的结果。 
现在我们已经激发了这个问题，在下一篇文章中，我们将讨论深度学习效率的五个支柱。 
参考文献 
[1] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 和 Li Fei-Fei. 2009. ImageNet: 一个大规模层次化图像数据库。2009 年 IEEE 计算机视觉与模式识别大会。248–255. https://doi.org/10.1109/CVPR.2009.5206848 
[2] Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton. 2012. 使用深度卷积神经网络进行 Imagenet 分类。神经信息处理系统进展 25 (2012), 1097–1105。 
[3] 卷积网络: yann.lecun.com/exdb/lenet/index.html 
[4] PapersWithCode: paperswithcode.com/ 
[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, 等. 2020. 语言模型是少量样本学习者。arXiv 预印本 arXiv:2005.14165 (2020)。 
简介： Gaurav Menghani（@GauravML）是 Google Research 的高级软件工程师，他领导着旨在优化大型机器学习模型的研究项目，以实现高效的训练和推理，这些模型可以在从微控制器到基于 Tensor Processing Unit（TPU）的服务器等各种设备上运行。他的工作对 YouTube、Cloud、Ads、Chrome 等领域的超过 10 亿活跃用户产生了积极的影响。他还是即将出版的 Manning Publication 的《高效机器学习》一书的作者。在 Google 之前，Gaurav 在 Facebook 工作了 4.5 年，并对 Facebook 的搜索系统和大规模分布式数据库做出了重要贡献。他拥有纽约州立大学石溪分校的计算机科学硕士学位。 
相关： 
 视觉变换器：自然语言处理（NLP）提高效率和模型泛化能力深度学习的最重要思想关于强化学习的 2 件事——计算效率和样本效率
更多相关主题 
 数据科学家的高薪副业KDnuggets™ 新闻 22:n04, 1 月 26 日：高薪副业…人工智能人员管理：构建高效能 AI 团队7 个高薪的数据科学副业7 个平台获取高薪数据科学职位Kubernetes 中的高可用性 SQL Server Docker 容器
效率是生物神经元与人工神经元之间的区别 
 
 原文：www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html 
 
 
图片由 macrovector 提供 在 Freepik 上 
 

 我们的前三名课程推荐 

1. 谷歌网络安全证书 - 快速进入网络安全职业的快车道。 
2. 谷歌数据分析专业证书 - 提升您的数据分析能力 
3. 谷歌 IT 支持专业证书 - 支持您的组织 IT 

 机器学习取得了重大进展，但正如本系列所讨论的，它与大脑的工作方式没有太多相似之处。本系列第八部分探讨了生物神经元的一个方面，到目前为止，这一方面使它们远远领先于人工神经元：它们的效率。 
 
您的大脑包含约 860 亿个神经元，这些神经元被挤在略超过一升的体积中。虽然机器学习可以做许多人脑做不到的事情，但大脑能够进行连续的语音识别、视觉解读以及许多其他任务，同时消耗约 12 瓦特的能量。相比之下，我的笔记本电脑消耗约 65 瓦特，而我的台式机消耗超过 200 瓦特，它们都无法运行当前使用的"
KDNuggets 博客中文翻译（一百一十一）,2024-10-27,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
AI、分析、机器学习、数据科学、深度学习研究 2021 年的主要发展和 2022 年的关键趋势 
 
 原文：www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html 
 
comments 
又一年将要结束，再次，KDnuggets 联系了专家，以了解他们对今年发生的事情以及未来可能发生的事情的看法。 
今年，我们向一些 AI、分析、机器学习、数据科学、深度学习研究领域的领导者提出了以下问题： 
 
我们的前三课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业轨道 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT 
 
2021 年 AI、数据科学、机器学习研究的主要发展是什么？你对 2022 年有哪些关键趋势的看法？ 
虽然本文从研究的角度探讨了这个问题，但在接下来的几天里，我们还将分享从技术和行业的角度讨论同一问题的文章。 
我想感谢本轮意见中的每位参与者，感谢他们在这个繁忙时节抽出时间提供见解和意见：Anima Anandkumar、Louis Bouchard、Andriy Burkov、Charles Martin、Gaurav Menghani、Ines Montani、Dipanjan Sarkar 和 Rosaria Silipo。 
现在，让我们不再拖延，来看看 AI、分析、机器学习、数据科学、深度学习研究在 2021 年的主要发展和 2022 年的关键趋势。 
 
Anima Anandkumar 是 NVIDIA 的 ML 研究主任及加州理工学院的 Bren 教授 
AI4Science 在过去一年中显著成熟，疫情作为重要催化剂，将来自多个领域的科学家聚集在一起。我们看到突破性的十亿原子分子模拟，以了解 Covid-19 病毒及其与气溶胶的相互作用，借助人工智能方法增强了这些研究。我们看到能够解决复杂科学模拟的创新人工智能方法，例如首次解决湍流流体流动问题。我们看到许多医院联手，通过保留隐私的联邦学习平台协作训练 AI 患者护理模型。语言模型变得更大，但对偏见问题的广泛意识导致对这些模型的更深入检查，并开发了少量样本和微调方法以减少伤害。 
路易斯·布歇 专注于在 YouTube 和 Medium 上的“What’s AI”使人工智能更易于获取 
第一个问题对我来说相对容易回答。我实际上维护着一个 GitHub 仓库，记录了人工智能的所有主要发展，所以我的答案很快：CLIP。它介绍了许多令人激动的可能性，将文本与图像连接起来。当然，这是我首先想到的，但今年还有许多其他惊人的发现和进展，我强烈邀请你查看我制作的精心挑选的列表，也在 KDnuggets 上共享，涵盖了 2021 年最有趣的人工智能研究，包括视频演示、文章和代码（如果适用）。 
我相信我们将在 2022 年继续在图像合成和文本到图像应用领域取得许多激动人心的发现，并迈出比以往更大的步伐，许多技术进步也将再次发生。当然，如果你想跟上最新的研究趋势，我将继续在我的 YouTube 频道和博客上覆盖这些令人兴奋的趋势！ 
安德烈·布尔科夫 是数据科学主任——机器学习团队负责人，同时是《一百页机器学习书》和《机器学习工程书》的作者 
2021 年人工智能的主要突破是 DALL·E 和类似的技术，这些技术可以从文本中创建图像。这些技术为创造性的人提供了全新的工具，并使创作过程更加民主化。我认为 2022 年我们将看到更多创造性 AI 的例子：在视频和音乐中。模型将变得更大，我们将看到新的多模态模型。 
查尔斯·马丁 是一位 AI 专家和自然语言处理及搜索领域的杰出工程师 
在 2021 年，疫情仍在全面爆发中，我们看到在线零售和一般在线存在的显著增长，越来越多的企业尝试将数据科学和机器学习运用于提升在线销售和运营。这导致了从纯数据科学作为孤立活动的转变，推动更多的机器学习/人工智能模型投入生产，增加了对机器学习工程、机器学习运维和数据中心人工智能的需求。虽然传统的机器学习方法（即 XGBoost）仍主导企业，但现代人工智能正找到自己的位置，包括向量空间搜索、图神经网络，以及计算机视觉应用。因果机器学习也受到关注，因为企业需要了解机器学习方法的工作原理。 
在 2022 年，机器学习和人工智能将越来越成为标准软件产品开发生命周期的一部分，并且将出现更好的企业工具来管理它们的开发、部署和监控。 
 
图片由 Gaurav Menghani 提供 
Gaurav Menghani 是 Google Research 的一个软件工程师 
 可持续人工智能：随着对人工智能需求的增长，其碳足迹也在增加。减少这种环境影响对于可持续人工智能至关重要。可解释人工智能：为什么这个模型会做出这个具体的预测？理解模型行为背后的理由将帮助我们理解人工智能带来的偏见，从公平性和伦理角度来看。合成人工智能：Cadbury 最近推出了一项广告活动，使小企业主能够创建自己的广告，并有一位受欢迎的电影明星为他们的杂货店做宣传。无/低代码人工智能：像MindsDB这样的公司通过使人工智能训练和预测能够直接通过 SQL 进行，从而赋能用户，让他们可以无缝利用人工智能和预测分析的力量。设备端人工智能：随着硅芯片每单位能量消耗提供更多的功率，设备端人工智能将变得更具吸引力，因为它更快、更响应迅速且更私密。关键任务人工智能：当前的人工智能实践可能不适合关键任务应用（例如医疗保健中的安全/可靠性），在这些应用中，即使最后 0.1%的准确性也非常重要。人工智能的规范：由于现有的规范无法涵盖不断扩大的人工智能需求，我们需要额外的规范和治理，以确保使用人工智能的机构不会忽视关键的保护措施。
Ines Montani 是 Explosion 的首席执行官兼创始人。 
我们在 2021 年看到很多有趣的发展，但最让我印象深刻的是：对炒作驱动的开发的持续减少。人们大多已接受，自驾车并不是即将到来，人工智能不会治愈 COVID，这个新模型也不是仅差一步就能成为通用人工智能，GPT-3 及更大的语言模型不会神奇地解决所有实际问题，甚至最新论文中的这个奇特技巧可能也无法帮助你的生产应用。 
仍然充满了兴奋和热情，但这次更为务实，且源于该领域有了更多时间成熟。现在有很多人已经在人工智能和机器学习领域工作了几年，2020 年和 2021 年远程工作的广泛接受帮助了合适的人找到合适的角色，从而真正完成工作。在 2022 年，我认为将会少写把人工智能呈现为这种陌生的新奇事物的文章。人工智能的发展就像软件开发，遵循相同的趋势。它大多是在内部完成的。维护的开支大于开发。工具大多是开源的。每个项目都有自己的挑战，因此没有灵丹妙药。 
Dipanjan Sarkar 是苏黎世施法豪森技术学院的数据科学负责人，Google 机器学习开发专家，出版作者以及顾问。 
根据我去年的预测，2021 年在迁移学习和表示学习领域取得了巨大进展，特别是变换器成为了理解、表示和构建有效解决方案于非结构化数据（包括文本、图像以及音频和视频）的突破性工具。我们还看到在使用低代码和自动化机器学习工具来自动化机器学习训练的领域取得了很多进展，以及可解释人工智能和机器学习运营的持续增长。 
对于 2022 年，我预见到编码器-解码器模型架构（如变换器）在解决复杂的多模态数据问题和创建新基准方面将继续上升。我们还应看到生成式深度迁移学习领域的更多进展，以及更容易微调这些预训练模型来解决各种任务，使用的模型甚至比 GPT-3 更强大。生成式深度学习也应该密切关注，其在数据生成和内容创建等新兴领域中的应用。最后，机器学习、数据中心机器学习和 MLOps 的自动化将以稳定的步伐继续发展，更多高效工具的出现将帮助我们更快地构建、部署、监控和维护机器学习模型。 
Rosaria Silipo是 KNIME 的数据科学布道负责人 
过去的一年是 AI 生产化的一年。新的工具和流程应运而生，以便于部署、实施和监控基于数据科学的解决方案。多亏了数据科学生命周期中的这一新分支，AI 现在已成为主流学科。它不再是一个研究领域，而是数据分析社会的越来越多的领域都在寻求接触它。 
市场分析师、护士、医生、首席财务官、会计师、机械工程师、审计专业人员等具有不同背景和不同程度编程及 AI 算法知识的专业人士，都需要在陌生领域内迅速开发数据解决方案——无论是编码、大数据还是 AI。在这种情况下，低代码工具的易用性可能成为非数据科学专业人士创建复杂 AI 解决方案的关键。 
个人而言，我希望 2022 年能看到更多女性和其他少数代表性类别在数据科学领域的出现。 
相关： 
 2020 年 AI、分析、机器学习、数据科学、深度学习研究的主要进展及 2021 年关键趋势2020 年主要进展和 2021 年 AI、数据科学、机器学习技术的关键趋势2021 年 AI、分析、数据科学、机器学习行业预测
更多相关话题 
 2021 年主要进展和 2022 年 AI、数据科学…的关键趋势2021 年数据科学与分析行业的主要进展及关键…2022 年数据科学、机器学习、AI 和分析的关键进展2022 年及未来的 10 个关键 AI 与数据分析趋势5 个关键数据科学趋势与分析趋势百度研究发布 2022 年十大技术趋势预测
2021 年数据科学与分析行业的主要发展及 2022 年的关键趋势 
 
 原文：www.kdnuggets.com/2021/12/developments-predictions-data-science-analytics-industry.html 
 
随着 2021 年的结束，我们也结束了我们的专家观点系列文章，为读者带来了对今年重大发展的不同观点，并展望了明年的重要故事。 
为此，我们向行业领先公司的专家征求意见，询问： 
 
我们的三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析能力 
 3. Google IT 支持专业证书 - 支持你的组织 IT 
 
2021 年 AI、数据科学、机器学习的主要发展是什么？你预期 2022 年会有什么关键趋势？ 
本文从行业的角度来探讨这个问题，在过去的一周里，我们还发布了类似的文章，从研究和技术的角度探讨了同样的问题： 
 2021 年 AI、分析、机器学习、数据科学、深度学习研究的主要发展及 2022 年的关键趋势2021 年 AI、数据科学、机器学习技术的主要发展及 2022 年的关键趋势
我想感谢这轮意见征集中的每一位参与者，在如此繁忙的时间里抽出时间提供他们的见解和观点：Yashar Behzadi（Synthesis AI）、Dipti Borkar（Ahana）、Matthew Carroll（Immuta）、Kendall Clark（Stardog）、Brian Gilmore（InfluxData）、Raj Gossain（Alation）、Alan Jacobson（Alteryx）、Ashley Kramer（Sisense）、Haoyuan Li（Alluxio）、Buno Pati（Infoworks）、Jared Peterson（SAS）、John Purcell（DoiT International）、Ravi Shankar（Denodo）、Dan Sommer（Qlik）、Muddu Sudhakar（Aisera）、Marco Varone（expert.ai）、Ryan Welsh（Kyndi）、Brett Wujek（SAS）。 
现在，不再拖延，让我们来看看 2021 年 AI、分析、机器学习、数据科学、深度学习行业的主要发展和 2022 年的关键趋势。 
 
Dipti Borkar，共同创始人兼首席产品官 (CPO)，Ahana 
在 2021 年，我们看到云计算在 AI 和 ML 应用中的采用加速，这是由于云数据仓库/数据湖/数据湖屋的普及。随着更多的数据迁移到云端，公司面临着一个架构决策——将特定的结构化数据存储在一个高成本的数据仓库中，虽然可以进行高速分析，价格/性能比非常好，还是使用数据湖存储所有数据——结构化和非结构化数据——成本更低但没有内置的查询或分析机制。 
到 2022 年，我们将看到更多的 AI 和 ML 工作负载迁移到数据湖/数据湖屋，因为开放数据湖分析栈的出现——这是一个专为云数据仓库工作负载构建的栈，包括一个开源高性能查询引擎（Presto）用于数据湖上的 SQL 分析、开放格式和开放云。云计算的下一个增长阶段将包括开放数据湖，以增强云数据仓库，更多的开源分析和 AI 支持，以及开箱即用的云解决方案来推动创新，这意味着数据平台团队将花费更少的时间在管理复杂的分布式系统上，而更多时间专注于推动以业务为驱动的创新。 
Muddu Sudhakar，Aisera的联合创始人兼首席执行官 
对话式人工智能的迅猛崛起及更好的语言建模 — 到 2022 年，人工智能将继续演进，变得比以往更具变革性和直观性。从人力资源到营销，对话式人工智能技术被设计用来简化生活。对话式人工智能能够并将会接管日常的内部和客户服务任务，使得人工客服可以专注于更紧迫的问题。 
对话式人工智能，如 Aisera 创建的那种，旨在与员工紧密协作，建立一个集成的、更高效、更快速的客户服务体验。 
自然语言处理将继续发展，理解语音节奏以及我们所有的人类特有的语音模式、嗯声、啊声和含义混杂的词汇。它将继续学习哪些适用，使其更加反映人类语言，并更好地引导查询和解决问题。 
Raj Gossain，Alation的首席产品官 
公司将通过利用公共和私人数据市场释放关键业务价值 — 如今，公司已经在购买数据集以进行创新或获取数据不足的见解。到 2022 年，我们将看到更多组织转向公共数据市场，使用两种方法。首先，利用数据目录访问、使用和理解组织内丰富数据的公司，将认识到将企业数据与第三方数据集结合起来比以往释放更多价值和生产力。相比之下，传统公司将意识到专有内部数据集可以被货币化，并打包供其他公司使用，从而创造新的收入来源，使企业更容易发现和使用数据。 
李浩源，Alluxio 的创始人兼首席执行官 
主流 AI 和深度学习 — 随着 AI 应用工具集的不断发展，机器学习和深度学习平台已进入主流，并将达到与专业数据分析相同的成熟度。正如我们目前看到基于 Apache Spark 和 Presto 的大量完全集成的托管服务一样，在 2022 年，我们将看到基于 PyTorch 和 TensorFlow 的垂直整合出现。MLOps 用于管道自动化和管理将变得至关重要，进一步降低障碍，加速 AI 和 ML 的采用。 
艾伦·雅各布森，Alteryx 的首席数据和分析官 
数字化转型 2.0 将在各业务单元中带来分析文化，因为更多的大型企业提供自助服务技术和培训，以确保普通知识工作者能够成功并直接进行分析。 
随着分析的持续民主化，数据科学家需要从“问题解决者”转变为“教师”。组织现在希望填补这些角色的人能够清晰地表达和解释——不仅仅是编写代码，以鼓励人们富有创意并进行批判性思考。然而，数据科学家作为从业者和作为教师之间存在技能差距。 
数据和分析领域的碎片化将趋于平稳。近年来，AI/ML 领域变得复杂，进入这一领域的公司比前一年更多。然而，随着我们进入一个更加成熟的领域，并且在 2022 年出现更多整合，我们将开始看到这一趋势趋于平稳。 
拉维·香卡，Denodo 的高级副总裁和首席营销官 
数据网格架构变得越来越诱人。随着组织规模和复杂性的增长，中央数据团队被迫处理各种功能单位和相关数据消费者。这使得理解所有跨职能团队的数据需求并向其消费者提供合适的数据产品变得困难。数据网格是一种新的去中心化数据架构方法，旨在消除瓶颈，将数据决策权移交给那些理解数据的人。 
从 2022 年及以后，拥有分布式数据环境的大型组织将实施数据网格架构，以减少数据孤岛，避免重复工作，并确保一致性。数据网格将创建一个统一的基础设施，使领域能够创建和共享数据产品，同时强制执行互操作性、质量、治理和安全的标准。 
John Purcell，首席产品官，DoiT International 
在预测的季节里，很可能许多人会再次预言人工智能或机器学习的进展。公司和企业确实在寻找创新的方法来利用这些技术和方法，但它们在采用方面还未真正取得突破。主要挑战将保持不变：在数据中提出正确的问题，融合人类和机器智能来回答这些问题，以及克服复杂性。超大规模公司将继续推出新服务，公司将花费资金来确定这些服务如何真正帮助他们的业务。 
Marco Varone，创始人兼首席技术官 expert.ai 
在过去几年，自然语言理解的重要进展是将不同技术组合起来，以提高整体效果并更好地解决复杂问题。这种混合（或复合）方法结合了符号学和机器学习，赋予我们更强大的能力和灵活性来应对现实世界中的语言问题。 
我们将在 2022 年看到这种方法的更广泛应用，因为它可以节省大量时间和金钱，同时提高准确性、效率和速度。它还增加了可解释性（仅机器学习很难做到这一点），并通过知识图谱使从之前的实施中重用知识变得更简单。 
Matthew Carroll，首席执行官，Immuta 
云计算将决定远程工作的成败 —— 云计算如今是企业的必备之物，并且在 COVID-19 大流行期间至关重要。全球的数据驱动型组织正在寻找解决方案，以加快数据获取速度、安全地与更多用户共享数据，并减轻数据泄露和漏洞的风险。云计算的特点是远程工作，未来将继续对寻求业务连续性、提高可扩展性和成本效益的组织至关重要。 
根据 Immuta 数据工程状态调查，组织越来越多地采用多种云技术，以跟上现代数据团队所需的规模、速度和用例。近三分之二（65%）的受访者将他们的公司描述为 100%基于云或主要基于云，这表明对自动化云数据访问控制的市场需求很大。 
布赖恩·吉尔莫尔，InfluxData 的物联网项目经理 
智能城市技术变得无处不在：我们将不再使用“智能城市”这一术语——这不是因为技术失败，而是因为“城市”这一概念随着人口增长和无处不在的连接而消退。随着更多人群获得或选择高度成熟且易于访问的连接和服务，个体“智能城市”技术的采用将大幅增加。 
AI/ML 驱动公民体验：智能政府应用将更像消费者应用，而不像企业内部网。最聪明的城市将把机器学习和人工智能集成到推荐引擎中，支持自然语言交互，通过数字方式提供所有服务，并将公民体验视为首要要求。 
布诺·帕蒂，首席执行官，Infoworks 
数据织物的上升趋势 —— 2022 年将看到数据织物解决方案的显著增长和兴趣，因为公司寻求利用共同管理层来加速分析迁移到云，确保安全和治理，通过支持跨混合多云的实时、可信数据快速交付业务价值——所有这些都在推动数字化转型。我们相信，这项技术将在未来五年得到广泛采用。 
瑞安·韦尔什，Kyndi 的创始人兼首席执行官 
企业将期望供应商为业务团队提供全面的 AI 驱动解决方案，而不是专注于 IT 的开发工具和技术——AI 行业的大部分，甚至大多数，都专注于为内部 IT 团队或咨询组织开发强大的工具，以将技术应用于企业应用中的特定用例。在 2022 年，组织将要求 AI 供应商开始开发能够立即实施的特定 AI 驱动解决方案，而无需编码。通过专注于为业务用户提供以人为本的解决方案，供应商将使个人能够立即生成驱动决策的洞察。因此，组织将会调整对 AI 的投资，从高度定制的解决方案转向可配置的（现成的）选项。 
丹·索默，Qlik 全球市场情报领导高级总监 
合作与商业智能（BI）自疫情开始以来密不可分。在全球尝试恢复一定程度的正常状态时，合作的需求变得更加明确，需要尽早合作，并且没有数据孤岛阻碍。在努力改善我们围绕数据、网络和流程的协作方式时，我们将见证“协作挖掘”的出现，从而使决策得以跟踪。企业也了解到，如果他们想真正实现数据驱动，就必须找出在正确的地方运行正确查询的方法。最后，API 经济为企业联合进行共同倡议开辟了全新的途径，同时减少了买与自建的相关性。自动化是一个强劲的新兴领域，它消除了对这些集成进行编码的需求，我预计这项技术将在 2022 年产生持久影响。 
贾瑞德·彼得森，SAS 工程部门的高级副总裁 
人工智能（AI）正在慢慢地走向现实世界。虽然许多机器学习和 AI 的进展在常见任务或在线竞赛中表现出惊人的准确性，但这些进展仍需时间才能进入行业，最终为客户解决实际问题。这其中的一部分原因是需要领域特定的标注数据或运行这些系统/模型所需的计算能力。由于这些限制，我们将看到从研究到现实的进展将是缓慢但稳定的。 
布雷特·伍杰克，SAS 分析部门的首席产品经理 
AI 带来了现实世界的成果。过去，组织在 AI 投资上几乎没有什么回报，因为过度关注模型构建和模型性能。AI 不仅会用于独特的突破性项目，组织还会发现将 AI 技术应用于既有项目中，以实现最佳结果的价值。一个 AI 产品或服务要成功，它必须包含能够提升结果或加快、降低过程成本的元素。AI 的价值将不在于它如何建模现实世界，而在于它如何改善现实世界。 
艾什莉·克雷默，Sisense的首席产品和市场营销官 
多年来，我们一直听说分析的未来将超越描述性分析（发生了什么）和预测性分析（将会发生什么），进入处方指导（应对措施）。AI 的进步与自动化的结合终于使这一点成为可能，通过动态结合相关数据并提前提醒知识工作者采取行动。在 2022 年，处方分析将从仅仅告诉我们数字走向哪里，发展到帮助我们做出更智能、更主动的决策。 
组织也开始意识到，并不是每个人都有时间/兴趣成为数据分析师或具备数据素养。在 2022 年，许多组织将重新定义“分析文化”的含义，通过以更易于消化的方式将见解带给员工——转向像嵌入式分析这样的方式，这不会要求新技能或额外的时间投入。 
肯达尔·克拉克，Stardog的创始人兼首席执行官 
大数据中心化和整合的时代已经结束——中央化或整合数据存储的重要性也将在 2022 年成为焦点。明确来说，这一趋势并不是存储的终结，而是中心化整合的数据存储方法的终结，特别是针对分析和应用开发。 
到 2022 年，我们将看到数据分析领域持续的大斗争，旧有的企业数据管理方法，侧重于整合和中央化模式，将达到顶峰，然后开始趋向下降。我们即将看到的 Snowflake 和 Databricks 之间的重大斗争部分源于它们对中央化整合的不同方法。 
但这不仅仅是技术上的压力。在混合多云世界中不可避免的数据移动的经济状况并不好，也没有改善的迹象。客户和投资者正在反对随中央化方法而来的那种锁定现象，因此预期未来一年内，数据分析堆栈将朝向去中心化和去中介化的方向发展。 
Yashar Behzadi，Synthesis AI的首席执行官兼创始人 
围绕 AI 的数据对话将被优先考虑 — 关于 AI 的数据讨论已经开始，但尚未得到足够的关注。数据是构建 AI 系统的最关键方面，我们刚刚开始讨论和思考获取、准备和监控数据的系统，以确保性能和避免偏见。组织在 2022 年必须在企业架构中优先考虑数据优先的方法，以使 AI 和分析能够解决问题并促进新的收入来源。 
合成数据将成为构建元宇宙的必需品 — 没有合成数据就无法构建元宇宙。为了将现实重建为数字双胞胎，必须深入了解人类、物体、3D 环境及其相互作用。创建这些 AI 能力需要大量高质量标记的 3D 数据——这些数据对人类来说几乎无法标记。我们无法在 3D 空间中标记距离，推断材料属性或标记重建空间所需的光源。利用生成 AI 模型和视觉效果（VFX）技术组合构建的合成数据将成为支持新元宇宙应用所需 AI 模型的关键推动力。 
相关： 
 AI、分析、机器学习、数据科学、深度学习研究 2021 年主要进展及 2022 年关键趋势2021 年主要进展及 2022 年 AI、数据科学、机器学习技术的关键趋势2021 年 AI、分析、数据科学、机器学习的行业预测
更多相关主题 
 AI、分析、机器学习、数据科学、深度学习…2021 年主要进展及 2022 年 AI、数据科学的关键趋势…2022 年数据科学、机器学习、AI 和分析的关键进展2022 年及以后 10 大 AI 与数据分析趋势5 大数据科学与分析趋势云机器学习透视：2021 年的惊喜，2022 年的预测
2020 年的主要进展和 2021 年 AI、数据科学、机器学习技术的关键趋势 
 
 原文：www.kdnuggets.com/2020/12/developments-trends-ai-data-science-machine-learning-technology.html 
 
评论这是非常不寻常且困难的一年，我很高兴它很快就会过去。但我们仍然可以拥有美好的事物，其中之一是 KDnuggets 每年的专家预测总结。 
早些时候，我们涵盖了 2020 年 AI、分析、机器学习、数据科学、深度学习研究的主要进展和 2021 年的关键趋势。 
COVID 改变了人们的生活和工作方式，也突显了预测模型对假设的依赖以及对突然变化的脆弱性。 
本博客将更多关注技术。我们询问了我们的专家小组： 
 
 2020 年 AI、数据科学、机器学习、深度学习技术的主要进展是什么？你对 2021 年有何关键趋势的期待？ 
 
一些常见的主题包括 AutoML、自动化 / RPA、AI 偏见、COVID 影响、深度学习的限制、伦理 AI、GPT-3、医学和医疗保健以及 MLOps。 
在提交截止日期之后发生了一项巨大的 AI/ML 发展。来自 DeepMind 的AlphaFold显然解决了长达 50 年的蛋白质折叠问题，对医学和生物学具有巨大的潜力。 
这是专家们的意见，来自 Marcus Borba、Kirk Borne、Tom Davenport、Carla Gentry、Jake Flomenberg、IPFConline（Pierre Pinna）、Nikita Johnson、Doug Laney、Ronald van Loon、Bill Schmarzo、Kate Strachnyi 和 Mark van Rijmenam。 
享受吧！ 
 
 
Marcus Borba，全球思想领袖与影响者：#AI #MachineLearning #DataScience #BI #BigData #Analytics #IoT #DigitalTransformation。 
在 2020 年，对 AI 工作原理的关注日益增加，同时也评估了偏见问题，这使得 AI 需要越来越具可解释性和责任感。AutoML 的使用在资源工程和算法测试等问题上为数据科学家带来了若干好处。 
在 2021 年，我预见到人工智能和机器学习在所有知识领域的使用将大幅增加，通过超自动化的概念，这涉及到除了 AI 和机器学习外的许多技术，如增强分析、物联网、iBPM 和 RPA。人工智能和机器学习将在网络安全领域、企业系统、智能城市和智能家居中被越来越多地使用。人工智能与物联网的结合（AIoT）也将是一个关键趋势，使得创建具有更高效率操作的解决方案成为可能，改善机器与人类之间的互动，同时在数据分析中获得更多的稳健性。 
 
柯克·D·博恩，布兹艾伦的首席数据科学家。全球演讲者。顶级#大数据 #数据科学 #人工智能 影响者。博士，天体物理学家。 
我在 2020 年数据相关领域的主要趋势包括：自动化叙述文本生成（如 GPT-3）的惊人（且令人惊恐）的进展；RPA 部署的无处不在；用于操作企业机器学习活动的 MLOps 工具的增长；AI 和 DevSecOps 在 IT 操作中的融合，作为 AIOps；数据科学家对我们部署模型中的数据漂移和概念漂移重要性的警示，特别是在 COVID 疫情期间之前成功模型的失败；包括边缘计算、边缘到云计算和雾计算在内的多样化混合数据架构的兴起；分布式数据挖掘的复兴（10-15 年前流行）在“联邦机器学习”标签下，允许在分布式数据集上进行增量模型训练而无需移动数据；在所有组织和市场领域对数据素养的巨大广泛兴趣和投资；以及对物联网的认识逐渐觉醒，不仅仅是监控活动（作为你做的事情），而是作为新的企业可观测性战略的技术推动者（作为你这样做的原因）。 
我希望在 2021 年看到更多这样的趋势，特别是 MLOps、可观测性、数据素养和智能流程自动化（AI 增强 RPA）。 
 
汤姆·达文波特、@tdav 是巴布森学院的 IT 和管理学杰出教授；德勤分析与人工智能实践的高级顾问；麻省理工学院数字经济倡议的数字研究员。 
2020 
 我去年预测，广泛称为“MLOps”的机器学习模型持续管理将变得流行。这个概念已经变得流行，但我认为实际使用仍然主要集中在大型金融服务和科技公司中。我还预测翻译员将变得对数据科学团队很重要，许多公司确实使用他们，但这个角色尚未成为制度化。我预测了“模型对数据的适配只是考虑其是否有用的一个因素”，我认为这实际上并没有发生。强调这一信息的供应商并没有真正繁荣起来。
2021 
 大规模深度学习模型的受欢迎程度将在 2021 年达到峰值；数据和能源需求正变得过高，无法继续这一趋势。数据科学团队将采纳成功实施项目所需的角色和技能分类；“产品经理”将成为确保部署的重要角色。外部公司将专注于“数据科学即服务”的各个方面，包括提供训练模型的数据和通过 MLOps 监控模型。
 
这是这些预测中的一个词云 
 
 
Carla Gentry，数据科学家，分析解决方案 
2020 
当我们回顾过去的一年时，我发现最有趣的关于人工智能、机器学习和数据科学的事情是：许多人仍在谈论 Python/编程/软件，但很少有人展示这些伟大领域如何帮助社会“解决问题”的现实例子——人工智能被简化为聊天机器人，机器学习变成了在某些“大型网站”上进行角色和物体识别，用于分类产品或寻找类似产品或推荐……我并不是说没有一些非常酷的东西在做，尤其是考虑到 Covid 是大家关注的焦点，数据科学和机器学习也被用来寻找热点和/或帮助接触追踪，这将有助于遏制新感染的增加。你知道，数据科学、分析和机器学习曾经并继续在所有疫苗的研发中发挥作用，但正如我们在“可靠性”和“再现性”部门即“AstraZeneca/Oxford COVID 疫苗数据”中看到的，数据质量和能够涉及代表性样本仍然是大问题。 
尽管我们已经取得了进展，基本的“数据 101”问题仍然非常严峻——所以我们在开始任何项目时，请记得检查数据、查询并跟进异常，不要假设任何事情。有趣的是，在感恩节晚餐时，户外且保持社交距离，我妈妈给我看了一个两个“人跳舞”的视频，我说“哦，这很好”并微笑——她说——你注意到这两个“人”是机器人吗？嗯，跳舞就是数学，对吧（眨眼，不过，类似于 AI 画作，没有灵魂）2021 年及以后，我们可以做的事情无限，因此让我们通过记住基础知识——再现性、透明性、准确性和它是否代表真实生活和形式——来推动机器学习和数据科学的发展——我们必须摆脱黑箱思维——仅仅因为它在大学环境或实验室环境中有效，并不意味着它在现实生活中也会有效…… 
记住这一点，那些“短距离”内有上匝道和下匝道的高速公路——使得进出合流混乱——是由认为这样设计更高效的工程师设计的……对谁高效？在设计算法或模型时，了解你的受众……知道你的最终目标是什么，目的或功能是什么，然后——当然要有逻辑——但也要实际…… 
 
杰克·弗洛门贝格是 Wing Venture Capital 的合伙人，专注于 AI/ML 驱动的应用，他最近发布了一份关于数据科学领导者面临的最大问题的调查。 
2020 年： COVID-19 是一个黑天鹅事件，使许多模型经受了考验。不幸的是，大多数 AI 尚未能很好地适应快速变化的情况。随着公司被迫适应，AI 可观测性和模型监控成为所有从事生产模型工作者的明显需求。 
NLP 领域的 AI 正在上升。人类 NLP 基准测试正在被超越。像 GPT-3 这样的语言处理模型正在解锁新的应用场景，如总结和代码生成。利用 NLP 从文本中发掘价值的初创公司继续激增。 
医疗保健领域的 AI 终于开始发挥作用——无论是化学、药物发现、蛋白质组学等。在过去 2 年中，涉及 AI 的生物学出版物占比超过一半。 
2021 年： 随着数据科学家努力在组织内部各方——利益相关者、监管者和最终用户之间建立信任，并更好地理解他们的模型并提供性能置信区间，模型可解释性成为主流需求。各种问题，如不公平的偏见、模型不稳定性、监管要求，推动了这一点。 
计算机视觉不断普及。我预计除了自动驾驶车辆外，将会看到更多的现实世界应用案例以及更多的风险投资。我也希望看到计算机视觉开始受益于在 NLP 中解锁价值的自注意力策略。 
数据标注变得更加自动化。随着少量样本学习和迁移学习的进步，计算机终于能够介入，减少人工手动标注所有数据的需求。 
 
IPFConline，数字化转型咨询，法国马赛。 
对于 IPFConline 团队来说，2020 年在 AI 和数据科学领域标志着 3 个重要事实： 
 OpenAI GPT-3 的发布是自然语言处理领域的一个重大进步，对于深度学习的真实认识在于，深度学习算法对数据的需求过于贪婪，这使得它们的解释变得非常困难。在世界许多地方，禁止在大规模监控中使用面部识别技术。
2021 年将与 2020 年有很强的相似性，深度强化学习和迁移学习可能会开辟出新的算法类型，这些算法更公平，因此更值得信赖且偏见更少。 
 
尼基塔·约翰逊，创始人，RE.WORK 深度学习与人工智能活动，#reworkDL #reworkAI #AIforGood 
这一年充满挑战，但或许在我们保持社交距离的同时，关于如何将人工智能应用于社会影响的讨论使我们更紧密地团结在一起，并且由于探索如何利用人工智能应对疫情及造福社会，这些讨论变得更加突出。当然，DeepMind 近期关于蛋白质折叠突破的公告也是今年人工智能在医疗领域的一大进步。 
我希望我们能看到人工智能在医疗和数字健康领域的力量在 2021 年进一步发展，包括隐私和个性化的进展，以及远程医疗和心理健康应用的改进。 
 
道格·拉尼，创新研究员，数据与分析战略 
在 2021 年，应用人工智能和数据科学的主要工业焦点将是提高生命科学研究和医疗服务的效率。新冠疫情危机突显了医疗政策、供应链、运营、诊断、预防和治疗中的差距和机遇。 
随着个人继续寻找更有效的远程工作方式，智能代理将逐渐融入我们用来协作的应用程序中。它们将帮助我们与彼此及大量的数据、内容和其他应用程序进行集成和沟通。智能设备和助手将从以问答为主的发展到真正具备对话性和情境意识。此外，它们将学习并模仿我们个人在处理事务时的伦理标准。 
此外，我们将看到人工智能开始自主管理数据。数据的体量和速度一直只是基础设施扩展的问题。然而，日益增长和不断演变的数据种类需要整合、清洗、组织、存储和访问，用于事务和分析目的，这一直是一项艰巨的手动建筑和工程任务。自组织的数据管理能力将会出现，以减轻这一负担。 
 
罗纳德·范·伦，帮助数据驱动型公司创造价值；#AI、#BigData、#DataScience、#IoT、#MachineLearning、#Analytics、#Cloud、#5G 领域的 Top10 影响者 
在 2020 年，随着企业应对 COVID-19 大流行带来的挑战，对话 AI 的加速发展达到了一个非凡的速度。对新数字服务的紧急需求、客户需求的变化以及远程工作的转变推动了这一技术的大规模战略性采用。AI API 的兴起，如对话 AI API 和文档 AI API，使得推动 AI 的可及性变得更加容易和快捷，并消除了对专业 AI 技能或经验的需求。 
对于 2021 年，嵌入式 AI 将成为一个主要趋势，因为公司将专注于通过直观、简化的 AI 功能推动数字化转型的势头，并快速进行 AI 创新。这将确保 AI 的好处和价值能迅速实现，同时允许组织快速现代化业务。类似地，针对业务用户的机器学习应用将通过精简的实施和流程、自动化工作流以及更快的洞察生成推动企业中的 AI 民主化。 
 
比尔·施马佐, @schmarzo, 大数据学主任 | 认可的创新者、教育者、实践者 数据科学、设计思维、数据货币化 
2020 年的主要发展： 
 利用 AI 创建在使用过程中增值而非贬值的自主资产已经成为现实（感谢埃隆·马斯克）孤立分析 - 为一次性分析目的构建的机器学习模型，但从未设计为可重复使用或持续学习和适应 - 继续成为组织的失望开源工具在 AI/ML 模型开发、操作化和管理方面的持续进展低估了由于错误估计误报和漏报错误成本而产生的意外后果
2021 年的关键趋势： 
 尽管数据货币化仍然是 CIO 的首要挑战，但许多组织缺乏以业务为中心的价值工程方法来识别和优先考虑 AI/ML 如何产生新的客户、产品和运营价值来源组织开始认识到数据和分析的经济价值，这些数字资产不会磨损，而且使用越多，它们的价值就越高。自主分析开始在领先组织中取代孤立分析
 
凯特·斯特拉赫尼, Story by Data 和 DATAcated Academy 创始人 
在 2020 年，我们见证了从静态仪表板向交互式数据故事的转变，这些故事自动开发并向利益相关者传递关键洞察。数据消息是有上下文的，并根据受众（人们看到对他们有意义的洞察）量身定制的。我们还看到了一些软件，允许业务用户通过在搜索栏中输入问题来“询问”问题；这使他们能够更接近数据，减少获取数据驱动答案所需的时间和精力。 
在 2021 年，我们可以期待看到推动提升所有行业和角色（不仅仅是数据专家）专业人士的数据素养技能的努力。将会有一个教育员工如何阅读、分析、解读和沟通数据的趋势。借助创新技术，我们将减少构建仪表盘的时间，更多地关注做出基于数据的决策。 
 
马克·范·瑞门南博士，VanRijmenam，思考技术及其对商业和社会的影响。三本管理书籍的作者。Datafloq.com的创始人。通过Mavin.org打击虚假新闻、机器人和网络喷子。 
在 2020 年，我们见证了拥有数十亿参数的先进语言模型的出现，这些模型凭借其准确性让人惊叹。**《卫报》**的一篇文章甚至展示了这一模型的先进性。 
然而，在 2021 年，我们可能会看到首个拥有万亿参数的语言模型将技术提升到另一个层次。虽然这些模型可能最初会停留在研究领域，但如 GPT-3 这样的技术将会进入企业，并开始直接与客户互动。2021 年我们还会看到对伦理人工智能的关注和要求的增加。人工智能在社会中变得越来越普遍，公民将要求对人工智能采取伦理方法。在 2020 年，我们见证了阿姆斯特丹和赫尔辛基推出算法注册册，以带来人工智能公共部署的透明度，2021 年我们将看到这种情况更加频繁。 
相关： 
 人工智能、分析、机器学习、数据科学、深度学习研究 2020 年的主要进展和 2021 年的关键趋势人工智能、分析、机器学习、数据科学、深度学习研究 2019 年的主要进展和 2020 年的关键趋势人工智能、分析、机器学习、数据科学、深度学习技术 2019 年的主要进展和 2020 年的关键趋势
更多相关话题 
 2021 年主要进展和 2022 年人工智能、数据科学等关键趋势人工智能、分析、机器学习、数据科学、深度学习…数据科学与分析行业 2021 年的主要进展及关键…2021 年最佳 ETL 工具成为伟大数据科学家所需的 5 项关键技能每个初学者数据科学家都应该掌握的 6 种预测模型
《数据科学家的 DevOps：驯服独角兽》 
 
 原文：www.kdnuggets.com/2018/07/devops-data-scientists-taming-unicorn.html 
 
 评论 
作者 Syed Sadat Nazrul，分析科学家 
 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
当大多数数据科学家开始工作时，他们通常具备了从教科书中学到的所有数学概念。然而，很快他们就会意识到，大多数数据科学工作涉及将数据转换成模型所需的格式。更进一步地，正在开发的模型是最终用户应用的一部分。现在，一个合格的数据科学家应该将他们的模型代码版本控制在 Git 上。然后，VSTS 将从 Git 下载代码，VSTS 随后将被封装在 Docker 镜像中，然后被放到 Docker 容器注册表上。一旦在注册表中，它将使用 Kubernetes 进行编排。对普通数据科学家来说，这些术语可能会让他们感到完全困惑。大多数数据科学家知道如何提供静态报告或带有预测的 CSV 文件。然而，我们如何对模型进行版本控制并将其添加到应用程序中？用户如何基于结果与我们的网站互动？如何扩展！？所有这些都涉及到信心测试，检查是否有任何内容低于设定的阈值，来自不同方的签署和不同云服务器之间的编排（以及所有丑陋的防火墙规则）。这就是一些基本 DevOps 知识派上用场的地方。 
什么是 DevOps？ 
简而言之，DevOps 是那些帮助开发者（例如数据科学家）和 IT 部门协作的人。 
 
开发者和 IT 之间的常见斗争 
开发人员有自己的指挥链（即项目经理），他们希望尽快推出功能。对于数据科学家来说，这意味着更改模型结构和变量。他们根本不在乎机器发生了什么。数据中心冒烟？只要他们能够获取数据以完成最终产品，他们根本不在乎。另一方面是 IT 部门。他们的工作是确保所有服务器、网络和精美的防火墙规则都得到维护。网络安全对他们来说也是一个巨大关注点。他们对公司的客户毫不关心，只要机器正常运作。DevOps 是开发人员和 IT 之间的中介。常见的 DevOps 职能包括： 
 集成测试打包部署
博客的其余部分将详细解释整个持续集成和部署过程（或者至少是对数据科学家相关的内容）。在阅读博客的其余部分之前，请注意：理解业务问题，不要对工具过于依赖。博客中提到的工具可能会改变，但基础问题在可预见的未来将大致相同。 
源代码管理 
想象一下将你的代码推送到生产环境。它正常工作了！完美。没有任何抱怨。时间流逝，你不断添加新功能并持续开发。然而，其中一个功能引入了一个严重的错误，严重影响了你的生产应用。你本希望你的多个单元测试能发现这个错误。然而，仅仅因为某个功能通过了所有测试，并不意味着它没有漏洞。它只是意味着它通过了所有当前编写的测试。由于是生产级别的代码，你没有时间进行调试。时间就是金钱，你还有愤怒的客户。是否可以简单地回到代码正常工作的那个点呢？这就是版本控制发挥作用的地方。在敏捷风格的代码开发中，产品在不确定的时间段内不断开发。对于这种应用，某种形式的版本控制将非常有用。 
 
Bitbucket 仓库 
就个人而言，我喜欢Git，但SVN用户仍然存在。Git 适用于所有形式的平台，如GitHub、GitLab和BitBucket（每个平台都有其独特的优缺点）。如果你已经熟悉 Git，考虑阅读更为高级的 Git 教程。我推荐查看的一个高级功能是Git 子模块，你可以存储多个独立 Git 仓库的特定提交哈希，以确保你可以访问一组稳定的依赖项。当必要时，重要的是要有 README.md，概述仓库的详细信息以及打包（例如，使用 setup.py 进行 Python）。如果你存储二进制文件，考虑查看Git LFS（不过我建议如果可能的话避免使用这个）。 
 
在 Git 上合并 Jupyter 笔记本 
数据科学中特定的版本控制问题是使用 Jupiter/Zeppelin 笔记本。数据科学家绝对喜欢笔记本。然而，如果你将代码存储在笔记本模板中并尝试在版本控制中更改代码，那么在执行差异比较和合并时，你会得到疯狂的 HTML 垃圾。你可以完全放弃在版本控制中使用笔记本（仅从版本控制的库中导入数学函数），或者可以使用现有工具如nbdime。 
自动测试 
从数据科学家的角度来看，测试通常分为两类。一类是常规的单元测试，用于检查代码是否正常工作或是否按照你的要求运行。另一类更具体于数据科学领域，是数据质量检查和模型性能。你的模型是否为你提供了准确的分数？现在，我相信许多人会想知道为什么这是一个问题。你已经做了分类分数和 ROC 曲线，模型也足够令人满意，可以进行部署。好吧，有很多问题。主要问题是，开发环境中的库版本可能与生产环境完全不同。这意味着不同的实现、近似值，因此，模型输出也不同。 
 
如果集成和部署做得正确，模型输出在开发和生产环境中应该是相同的。 
另一个经典示例是为开发和生产使用不同语言。让我们设想这个场景。你，尊贵的数据科学家，希望用 R、Python、Matlab 或者上周刚刚发布白皮书的许多新语言之一来编写模型（这些语言可能还没有经过充分测试）。你将模型交给生产团队。生产团队怀疑地看着你，笑了 5 秒钟，然后才意识到你是认真的。他们可能会嗤之以鼻。生产代码是用 Java 编写的。这意味着需要将整个模型代码重写为 Java 以用于生产。这也意味着完全不同的输入格式和模型输出。因此，自动化测试是必要的。 
 
Jenkins 主页 
单元测试是非常常见的。JUnit可供 Java 用户使用，而unittest库适用于 Python 开发人员。然而，有可能有人在将代码推送到生产环境之前忘记在团队中正确运行单元测试。虽然你可以使用crontab来运行自动化测试，但我推荐使用更专业的工具，如Travis CI、CircleCI或Jenkins。Jenkins 允许你安排测试、从版本控制库中选择特定分支、如果出现问题会发送邮件提醒，甚至可以启动 Docker 容器镜像，如果你希望将测试隔离在沙盒中。基于容器化的沙盒化将在下一部分中更详细地解释。 
容器化 
 
容器与虚拟机 
沙盒化是编码的重要组成部分。这可能涉及为各种应用程序设置不同的环境。它可能只是将生产环境复制到开发环境中。它甚至可能意味着拥有多个具有不同软件版本的生产环境，以满足更大客户群的需求。如果你考虑使用带有Virtual Box的虚拟机，我相信你已经注意到，你要么需要在多轮测试中使用完全相同的虚拟机（这是很糟糕的 DevOps 卫生），要么需要为每次测试重新创建一个干净的虚拟机（这可能需要接近一个小时，具体取决于你的需求）。一种更简单的替代方案是使用容器而不是完整的虚拟机。容器只是一个 UNIX 进程或线程，表现得像虚拟机。它的优点是功耗低，内存消耗少（这意味着你可以随意启动或关闭它……在几分钟内）。流行的容器化技术包括Docker（如果你只希望使用一个容器）或Kubernetes（如果你喜欢协调多个容器以实现多服务器工作流）。 
 
Kubernetes 工作流 
容器化技术不仅有助于测试，还能提高可扩展性。特别是当你需要考虑多个用户使用你的模型基础应用程序时，这一点尤为重要。这可能涉及训练或预测。 
安全 
安全很重要，但在数据科学领域往往被低估。一些用于模型训练和预测的数据涉及敏感信息，如信用卡信息或医疗数据。在处理这些数据时，需要遵守 GDPR 和 HIPPA 等合规政策。不仅仅是客户端需要安全。贸易秘密模型结构和变量，在部署到客户端服务器时，也需要一定级别的加密。这通常通过将模型部署到加密的可执行文件（例如 JAR 文件）中，或在将模型变量存储到客户端数据库之前进行加密来解决（不过，请不要自己编写加密代码，除非你完全知道自己在做什么……）。 
 
加密 JAR 文件 
此外，最好按租户建立模型，以避免意外的迁移学习，这可能导致一个公司向另一个公司泄露信息。在企业搜索的情况下，数据科学家可以使用所有可用数据构建模型，并根据权限设置筛选出特定用户未授权查看的结果。虽然这种方法看起来合理，但用于训练模型的数据中一部分信息实际上是由算法学习并转移到模型中的。因此，无论如何，这使得用户有可能推断出被禁止页面的内容。没有绝对的安全。然而，它需要足够好（这一点的定义取决于产品本身）。 
协作 
 
当与 DevOps 或 IT 合作时，作为数据科学家，明确需求和期望是非常重要的。这可能包括编程语言、包版本或框架。最后但同样重要的是，彼此尊重也很重要。毕竟，DevOps 和数据科学家都面临着极具挑战性的工作。DevOps 对数据科学了解不多，而数据科学家在 DevOps 和 IT 方面也不是专家。因此，沟通是成功业务结果的关键。 
额外信息 
软件开发设计原则 
当人们开始作为自学编程的程序员时，我们经常会想到创建一个简单的应用程序…… 
如何让你的软件开发体验变得无痛…… 
在各种形式的组织中工作（从大型软件开发公司到小型创业公司，再到学术实验室），我… 
数据科学面试指南 
数据科学是一个相当大且多样的领域。因此，很难成为一个全能型人才… 
简介：Syed Sadat Nazrul 白天利用机器学习追捕网络和金融犯罪分子，晚上则撰写有趣的博客。 
原始。已获许可转载。 
相关： 
 操作性机器学习：成功 MLOps 的七个考虑因素数据科学面试指南接收器操作特征曲线揭秘（Python 版）
更多相关内容 
 通过这个免费的 DevOps 速成课程释放你的潜力每个初学者应该学习的 10 种必备 DevOps 工具数据工程师和数据科学家都适用的高保真合成数据我们不需要数据科学家，我们需要数据工程师数据分析师和数据科学家有什么区别？掌握数据讲故事的艺术：数据科学家的指南
IBM Watson 如何征服世界 
 
 原文：www.kdnuggets.com/2016/02/dezyre-ibm-watson-taking-world.html 
 
作者：Khushbu Shah，DeZyre 
人工智能（AI）让我们着迷已有数十年，随着每年的推移，我们离那台能像人类一样互动的机器越来越近。一方面，我们在数据分析领域取得了巨大的进展，另一方面，我们的技术也变得足够智能，能够识别我们的提问并以我们提问的语言作出回应。 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 需求 
 
人工智能 是一个复杂的领域，需要在分析、机器学习、神经网络、语音识别和大数据等多个方面进行前所未有的协调。现代智能手机操作系统配备了基于语音的虚拟助手，通过支持语音搜索并以机械化的英语回应结果来展示人工智能的初级阶段。虽然目前看来这已经足够，但仍然有许多工作要做。 
IBM 一直处于创新的前沿。通过推出其超级计算机 IBM Watson（以 IBM 创始人 Thomas J. Watson 命名），它在人工智能领域迈出了坚实的一步。 
什么是 IBM Watson？ 
它是一个“问答（QA）”机器（占据的空间足以容纳 10 台冰箱）。IBM Watson 具有扫描大量数据以“学习”主题的能力，然后像人类一样解释和回答问题。它的目标是复制（甚至超越）一个智能人类理解问题并提供答案的能力。 
对于那些对数字感兴趣的人，IBM Watson 的处理能力为 80 teraflops，即每秒一万亿次浮点运算。为了能够像高效运作的人脑一样理解、分析和回答问题，Watson 从 90 台服务器上挖掘超过 2 亿页的信息。这些数据随后通过六百万条逻辑规则来得出相关的答案。 
想成为数据科学超级英雄？学习 Python 中的数据科学 
 
IBM Watson 与搜索引擎有何不同？ 
搜索引擎通常会对问题给出机械的回应。它们无法解释单词在句子中的顺序。它们只会查找搜索的短语并提供答案，而不考虑该短语出现的“上下文”。这意味着一旦 Watson 得到适当的训练，向它提出问题就像与一个实际的有意识的人的对话，而不是机械地显示结果。 
IBM Watson 应用 
2011 年，Watson 通过在危险边缘节目中与前冠军 Brad Rutter 和 Ken Jennings 竞争，证明了其认知计算的能力。最终赢得了 100 万美元的头奖。通过这次竞赛，主要研究员 David Ferrucci 领导的研究团队能够确立人工智能的变化面貌。大约四年后，Watson 的计算能力已被各个行业有效地利用来解决实际问题。在这一部分，我们列出了一些成功使用 IBM Watson 认知处理能力的行业。 
USAA：帮助军人过渡到平民生活 
美国劳工统计局指出，每年约有 155,000 名军人过渡到平民生活。在这一过程中，他们有许多与退伍军人补偿福利、9/11 后 GI 法案等相关的问题。相关信息散布在数十万页文档中。IBM Watson 已经吸收了记录在 3000 多份与军事过渡相关的文档中的知识，使其能够令人满意地回答军人们的问题。由于 3000+文档的知识库需要数百人阅读并整理观察结果，才能形成答案，因此 IBM Watson 所节省的成本可想而知。 
澳新银行全球财富 
金融行业拥有大量特定的数字，正确处理这些数字以生成最佳的金融规划建议对业务成功至关重要。澳新银行全球财富（澳大利亚和新西兰全球银行集团的私人银行部门）的 400 多名金融规划师使用 IBM Watson Engagement Advisor 来帮助他们改善金融建议流程。其目标是将客户收到建议声明的时间从几天减少到仅一次会话。 
MD 安德森癌症中心 
德克萨斯大学 MD 安德森癌症中心已经将 IBM Watson 引入其抗癌使命。在处方方面，肿瘤学专家顾问能够利用 Watson 的认知计算能力，整合 MD 安德森研究人员的庞大知识库。另一方面，它在收集和分析大量有关患者医疗历史的非结构化数据方面发挥了关键作用。一旦这两者连接，临床医生将获得宝贵的信息，这些信息要么需要很长时间才能收集，要么可能会被遗漏。 
IBM Watson 的未来将会怎样？ 
如前所述，Watson 的应用涵盖了许多行业。如果您的业务需要处理大量数据，从中学习并提供超越数字计算的建议，您可以通过以下方式大幅缩短整个过程的周期时间： 
IBM Watson 将使您的旅行计划无压力。 
现在有很多网站可以帮助您找到旅行票的“最佳价格”。在争夺成为“最快”获取这些信息的在线门户/应用的竞争中，企业往往忽视了旅行的人文方面，即创造终生难忘的体验。Watson 能够从全球的旅行相关文档中获取信息，分析并准备针对诸如“4 月份我和妻子去哪里度浪漫海滩假期？”这样的问题的具体答案。在回答这个问题时，Watson 会考虑到并非所有海滩目的地都是浪漫的，它们的“最佳旅行月份”也不同。客户可能没有时间扫描成千上万的评论再做决定。试想一下，当查询用简单英语在几秒钟内得到回答时，旅行网站能够达到的效率！ 
IBM Watson 将使购物变得更简单。 
想象一下，您走进一家商店购买露营相关的物品。我们通常会查找互联网，询问朋友，准备一份我们需要的物品清单，然后购买它们。如果有一台超级计算机，您可以简单地询问它，它会给您提供一份综合清单，并附加关于天气、最佳时间和露营地点以及邻近其他活动的信息？IBM Watson 的开发平台正被 Fluid Inc. 用来通过做类似的事情来颠覆购物者的购买体验。 
IBM Watson 将为您提供烹饪大师级的体验。 
沃森能够学习任何东西，它不局限于假设、事件和技术。其创造力可以用来创建新颖的食谱。就像其他应用程序一样，它需要经过烹饪方面的训练。一旦建立了足够大的知识库，就可以随时调用它来提供适合用户需求的食谱。经过足够的训练后，用户可以向沃森提问 
提供一种意大利风味的素食菜肴建议，这道菜可以用特定的方式（例如烤制、煮沸、空气炸等）来烹饪，并且可以快速引导用户了解可能的食材、烹饪时间和配比。这些只是我们日常生活中的一些应用，它们能让我们的生活变得更简单、更美好。沃森的计算能力在农业、欺诈检测、风险评估、娱乐甚至太空探索等领域没有限制。因此，下次当你听到有人与机器对话并得到准确、富有创意且人类语气相似的回应时，你可能正在使用其中一个 IBM Watson 应用。 
学习数据科学来构建令人惊叹的机器学习系统！ 
相关: 
 Spark SQL 实时分析通过 IBM Watson 和 Twitter 解锁 Spark 的力量独家采访：Apache Spark 创始人 Matei Zaharia 讨论 Spark、Hadoop、Flink 及 2020 年的大数据
更多相关内容 
 谷歌推荐你在参加他们的机器学习课程之前做的事…免费的数据科学学习路线图：适用于所有级别的 IBMIBM 针对各行各业的生成式 AI 专业课程Tensorflow 的 “Hello World” 程序AIoT 革命：AI 和物联网如何改变我们的世界帮助你解决实际问题的数据科学项目
Bagging 和 Boosting 有什么区别？ 
 
 原文：www.kdnuggets.com/2017/11/difference-bagging-boosting.html 
 
评论 
由 xristica，Quantdare提供。 
 
Bagging 和 Boosting 的相似之处在于它们都是集成技术，在这些技术中，一组弱学习器被组合以创建一个强学习器，从而获得比单一学习器更好的性能。所以，让我们从头开始： 
什么是集成方法？ 
集成是一个机器学习概念，其思想是使用相同的学习算法训练多个模型。集成方法属于更大的方法群体，称为多分类器，在这些方法中，数百或数千个具有共同目标的学习器被融合在一起以解决问题。 
第二组多分类器包含混合方法。它们也使用一组学习器，但可以使用不同的学习技术进行训练。Stacking 是最著名的。如果你想了解更多关于 Stacking 的信息，你可以阅读我之前的文章，“梦之队组合分类器”。 
学习中的主要误差原因是噪声、偏差和方差。集成有助于最小化这些因素。这些方法旨在提高机器学习算法的稳定性和准确性。多个分类器的组合减少了方差，尤其是在不稳定分类器的情况下，并可能产生比单一分类器更可靠的分类。 
要使用 Bagging 或 Boosting，你必须选择一个基础学习算法。例如，如果我们选择一个分类树，Bagging 和 Boosting 将由一个数量随意的树池组成。 
 
Bagging 和 Boosting 如何获得 N 个学习器？ 
Bagging 和 Boosting 通过在训练阶段生成额外的数据来获得 N 个学习器。通过随机采样带替换从原始集合中生成 N 个新的训练数据集。通过带替换的采样，一些观察值可能在每个新训练数据集中重复出现。 
对于 Bagging，任何元素在新数据集中出现的概率是相同的。然而，对于 Boosting，观察值是加权的，因此其中一些会更频繁地参与新集合： 
 
这些多个集合用于训练相同的学习算法，因此产生了不同的分类器。 
为什么数据元素会被加权？ 
在这一点上，我们开始处理这两种方法之间的主要区别。虽然 Bagging 的训练阶段是并行的（即每个模型独立构建），但 Boosting 以顺序方式构建新的学习器： 
 
在 Boosting 算法中，每个分类器都是在数据上训练的，并考虑了之前分类器的成功。每次训练步骤后，权重会重新分配。被错误分类的数据增加其权重，以强调最困难的案例。这样，后续学习器在训练过程中会集中关注这些案例。 
分类阶段是如何工作的？ 
要预测新数据的类别，我们只需将 N 个学习器应用于新观察数据。在 Bagging 中，结果是通过对 N 个学习器的响应进行平均（或多数投票）获得的。然而，Boosting 为 N 个分类器分配第二组权重，以便对它们的估计进行加权平均。 
 
在 Boosting 训练阶段，算法会为每个结果模型分配权重。对训练数据分类效果好的学习器会被分配比效果差的学习器更高的权重。因此，在评估新的学习器时，Boosting 还需要跟踪学习器的错误。我们来看看过程中的差异： 
 
一些 Boosting 技术包括额外的条件来决定是否保留或丢弃单个学习器。例如，在最著名的 AdaBoost 中，需要误差小于 50% 才能维持模型；否则，迭代将重复进行，直到达到比随机猜测更好的学习器。 
之前的图像展示了 Boosting 方法的一般过程，但存在几种替代方法，它们在确定下一训练步骤和分类阶段使用的权重方面有所不同。如果你想详细了解，请点击这里：AdaBoost、LPBoost、XGBoost、GradientBoost、BrownBoost。 
Bagging 和 Boosting 哪个更好？ 
没有绝对的赢家；这取决于数据、模拟和具体情况。 
Bagging 和 Boosting 通过结合来自不同模型的多个估计来降低单个估计的方差。因此，结果可能是一个更高稳定性的模型。 
如果问题在于单个模型性能非常低，Bagging 很少能获得更好的偏差。然而，Boosting 可以生成一个综合模型，具有较低的错误，因为它优化了单个模型的优点并减少了陷阱。 
相反，如果单个模型的困难是过拟合，那么 Bagging 是最佳选择。Boosting 本身并不能避免过拟合；实际上，这一技术本身面临这个问题。因此，Bagging 比 Boosting 更常有效。 
总结： 
相似之处差异两者都是集成方法，将 N 个学习者从 1 个学习者中获取…… 但 Bagging 是独立构建的，而 Boosting 尝试添加在之前模型失败的地方表现良好的新模型。两者都通过随机抽样生成多个训练数据集…… 但只有 Boosting 为数据确定权重，以便将偏差倾斜到最困难的案例。两者都通过对 N 个学习者的结果取平均（或取多数）来做出最终决定…… 但 Bagging 采用的是等权重平均，而 Boosting 采用加权平均，更多权重分配给在训练数据中表现更好的学习者。两者都擅长减少方差并提供更高的稳定性…… 但只有 Boosting 尝试减少偏差。另一方面，Bagging 可能解决过拟合问题，而 Boosting 可能增加过拟合。 
原文。经许可转载。 
相关： 
 集成学习以提升机器学习结果数据科学入门：初学者的基本概念必须了解：集成学习的基本思想是什么？
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织进行 IT 管理 
 
更多相关话题 
 SQL 与对象关系映射（ORM）的区别是什么？数据分析师和数据科学家的区别是什么？效率决定生物神经元与…L1 与 L2 正则化的区别机器学习中训练数据和测试数据的区别GBM 与 XGBoost 的区别是什么？
数据科学家与机器学习工程师的区别 
 
 原文：www.kdnuggets.com/2021/08/difference-between-data-scientists-ml-engineers.html 
 
评论 
 
图片由 Stillness InMotion 提供，来源于 Unsplash 
 
我们的前 3 门课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织在 IT 领域 
 
数据科学家和机器学习工程师的角色之间常常存在混淆。虽然他们肯定会友好地合作，并且在专业知识和经验方面有一些重叠，但这两个角色服务于截然不同的目的。 
实际上，我们正在区分那些寻求理解其工作背后科学的科学家和那些寻求构建可以被他人访问的事物的工程师。这两个角色都极其重要，在一些公司中，角色是可以互换的——例如，某些组织的数据显示科学家可能会执行机器学习工程师的工作，反之亦然。 
为了明确区分，我将差异分为 3 类：1）职责 2）专业知识 3）薪资预期。 
职责 
数据科学家遵循数据科学过程，这也可以称为 Blitzstein 和 Pfister 工作流。Blitzstein 和 Pfister 最初创建了这个框架，旨在教导 哈佛 CS 109 课程的学生如何处理数据科学问题。 
数据科学过程包括 5 个关键阶段 
 阶段 1： 理解业务问题阶段 2： 数据收集阶段 3： 数据清理与探索阶段 4： 模型构建阶段 5： 传达和可视化见解
数据科学家所做的大多数工作都是在研究环境中进行的。在这个环境中，数据科学家执行任务以更好地理解数据，从而构建能够最佳捕捉数据固有模式的模型。一旦他们构建了一个模型，下一步就是评估它是否达到项目的预期结果。如果没有，他们将迭代地重复这个过程，直到模型达到预期结果，然后将其交给机器学习工程师。 
机器学习工程师负责创建和维护机器学习基础设施，使其能够将数据科学家构建的模型部署到生产环境中。因此，机器学习工程师通常在开发环境中工作，他们关注的是在研究环境中重现数据科学家构建的机器学习流程。同时，他们在生产环境中工作，使模型可以被其他软件系统和/或客户访问。 
从本质上讲，机器学习工程师负责维护机器学习基础设施，使其能够部署和扩展数据科学家构建的模型。而数据科学家则是使用机器学习工程师构建的机器学习基础设施的用户。 
专业知识 
人们对这两个角色之间的区别感到困惑的原因在于，它们的技能有很多重叠的地方。例如，数据科学家和机器学习工程师都被期望具备良好的知识； 
 监督学习与无监督学习机器学习与预测建模数学和统计学Python（或 R）
角色之间的主要重叠导致一些组织，特别是较小的组织和初创公司，将这些角色合并为一个。因此，一些组织让数据科学家做机器学习工程师的工作，而一些则让机器学习工程师做数据科学家的工作。这只会导致从业者之间的更多困惑。 
然而，这两个角色所需的专业知识之间还是存在一些关键区别。 
数据科学家通常是极其优秀的数据讲述者。有些人认为，这一特质使他们比机器学习工程师更具创造力。另一个区别是，数据科学家可能使用像 PowerBI 和 Tableau 这样的工具来向业务部门分享见解，他们不一定需要使用机器学习。 
弥补对方不足的伴侣通常更强。当你这样考虑时，前面提到的专业知识可能是机器学习工程师的薄弱点，他们被期望在计算机科学和软件工程方面有坚实的基础。机器学习工程师需要了解数据结构和算法，并理解创建可交付软件所需的基本组件。 
尽管如此，机器学习工程师通常也能很好地掌握其他编程语言，如 Java、C++ 或 Julia。 
 2021 年学习数据科学的课程如何成为一名机器学习工程师
薪资预期 
精确确定薪资期望是困难的。两个职位的薪资因多种因素而异，例如你的经验、资质、所在地点以及你所在的行业。 
组织还预计提供各种福利。无论职位如何，你可以期待加入公司养老保险计划、灵活或远程工作、绩效奖金和私人医疗保险。 
联合王国（UK） 
 初级数据科学家的起薪范围为£25,000 — £30,000（根据经验可能涨至£40,000）。 [来源: Prospects]。毕业生或入门级机器学习工程师的起薪为£35,000 — £40,000。 [来源: Prospects]根据 Glassdoor，在英国，数据科学家的平均薪资为£46,818。 Prospects 指出，在英国，机器学习工程师的平均薪资为£52,000。根据 Prospects，高级和首席数据科学家的薪资通常超过£60,000（在某些情况下超过£100,000）。相比之下，经验更丰富的机器学习工程师可以期待获得高达£170,000的薪资（特别是如果他们为像谷歌或脸书这样的跨国公司工作） [来源: Prospects]。
美利坚合众国（USA） 
在美国，数据科学家的平均基础薪资为*$120,089* [来源: Indeed]。相比之下，美国的机器学习工程师的平均基础薪资为*$150,660* [来源: Indeed]。 
总的来说，机器学习工程师的薪资通常比数据科学家高，平均而言。 
终极思考 
尽管这两个职位有许多相似之处，但数据科学家和机器学习工程师在职责、专业技能和薪资方面差异很大。根据我听过的大多数相关访谈，许多人认为从数据科学家转型为机器学习工程师比从机器学习工程师转型为数据科学家要困难得多。这是因为数据科学家通常不熟悉软件工程和计算机科学基础，这是一大学习曲线。 
感谢阅读！ 
如果你喜欢这篇文章，可以通过订阅我的免费每周通讯来与我联系。不要错过我关于人工智能、数据科学和自由职业的任何文章。 
相关工作 
 数据相关角色概述数据科学之旅数据科学家究竟做什么？数据科学职业路径排名
简介：Kurtis Pykes 是一名机器学习工程师，数据科学领域的顶级作者，也是 Upwork 的顶级自由职业者。关注他的Medium 博客。 
原文。经许可转载。 
相关： 
 我如何在 8 个月内提升我的数据科学技能最重要的数据科学项目我希望在数据科学职业生涯中避免的 5 个错误
更多相关话题 
 建立一个稳固的数据团队使用管道编写干净的 Python 代码数据分析师和数据科学家之间的区别是什么？机器学习中训练数据和测试数据的区别效率决定了生物神经元和……之间的差异SQL 和对象关系映射（ORM）之间的区别是什么？
数据分析师与数据科学家的区别 
 
 原文：www.kdnuggets.com/2022/03/difference-data-analysts-data-scientists.html 
 
 
图片来源：Alexander Sinn于Unsplash 
数据科学家和数据分析师是 2022 年最受欢迎且薪资最高的职业之一。随着数据的不断增加，数据驱动的洞察也在不断增长。能够筛选、分析和解释大量信息的数据专业人员在今天至关重要。虽然数据分析师和数据科学家都处理数据，但有时不清楚二者之间的区别。主要的区别之一在于他们对处理的数据的不同用途。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业道路。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT 
 
数据分析师通常使用结构化数据来解决特定的业务问题。数据分析师最常使用 SQL、R、Python 等工具。他们还使用数据可视化软件和统计分析。 
相比之下，数据科学家处理的是未知的数据，并使用更先进的数据分析技术来预测未来。数据科学家构建自动化机器学习算法，开发能够处理结构化和非结构化数据的预测建模技术。通常，这个角色被认为是数据分析师的更高级版本。不过，这还不是全部。 
本文将帮助你详细了解数据分析师和数据科学家如何处理数据。 
什么是数据科学？ 
在深入了解数据分析师和数据科学家之前，了解一些关键术语是很重要的。虽然“数据科学”的定义没有统一标准，但谷歌决策智能负责人 Cassie Kozyrkov 表示：“数据科学是使数据变得有用的学科。” 
另一个重要的术语是数据挖掘和描述性分析。数据挖掘是识别数据中的模式并提出关于这些模式成因的假设的过程。这一步骤专注于通过数据寻找趋势。 
数据科学与许多区块链相关技术的交集越来越多。区块链本质上是一个不断增长的记录列表，通常作为一个使用加密技术链接的分布式账本。 
数据的使用方式：数据分析师与数据科学家 
数据科学家通常是处理原始数据和数据集的第一接触点。数据科学家将数据拆解并帮助理顺数据。他们处理的数据集通常很大，结构松散，并需要抽象分析来发现数据中的洞察。 
另一方面，数据分析师接收由数据科学家处理过的数据并加以利用。他们获得带有洞察的已知数据集，然后寻找数据中的新趋势。由于数据更为结构化，数据分析师可以创建报告来回答特定的业务问题，并向非技术观众传达他们的发现。 
越来越多的数据专业人士将区块链技术作为他们工作的一部分。区块链最为人知的是其作为比特币核心技术的角色。许多企业使用区块链来管理数据。其他企业甚至可能买卖比特币作为其财务计划的一部分。 
可以把数据科学家视为数据的原始处理者，而数据分析师则是利用数据为业务带来利益的专家。 
行业工具 
一旦数据专业人士获取了数据，他们需要将其投入使用。然而，选择正确的工具进行数据分析很重要。由于工作的性质，数据分析师和数据科学家往往会使用相同的工具，但使用方式不同。 
通常工具可以分为五类： 
 数据库 
比如 DuckDB 和 PostgreSQL网络爬取 
 Beautiful Soup，一个用于从 HTML 和 XML 中提取数据的 Python 库。Zyte 是一个用于运行网络爬虫的云平台。 数据分析 
 编程语言是数据分析的核心。Python 是数据科学家中最常用的语言。R，是数据分析和统计学家中流行的程序。Julia，一种用于解决科学问题的新兴编程语言。Tableau，一个无需编码的工具，能让你可视化各种数据。 机器学习 
 FastAI 是一个适合初学者的编程库，用于构建机器学习性能。Scikit-learn 用于数据处理和机器学习任务。TensorFlow 是一个完整的机器学习生态系统，是构建机器学习模型的端到端解决方案。 报告 
 Jupyter notebook 允许你在本地或云端构建科学报告Deepnote，一个云笔记本平台Dash 允许你构建和部署具有交互式用户界面的数据应用程序。 
这些只是数据分析师和数据科学家可用的部分工具。具体使用哪些工具取决于他们对数据如何利用的终极目标。 
教育和技能 
现在你了解了这两类专业人士如何使用数据以及他们使用的工具类型，那么这两组人需要哪些技能和教育背景呢？ 
教育 
数据分析师通常比数据科学家专业性低，他们通常需要至少拥有统计学、数学、计算机科学或金融学的学士学位。 
相对而言，数据科学家（以及更高级的分析师）至少拥有信息技术、数据科学、统计学或数学领域的硕士学位（甚至博士学位）。 
然而，证书和专业培训的作用越来越大。Google 和 IBM 都提供数据分析的专业证书，让你获得成为入门级数据分析师所需的技能。最终，可以将数据分析师视为入门点，数据科学家则是更高级的数据管理者。 
数据技能 
由于这两组人都处理数据，他们在所需技能上往往有重叠，但也有一些差异。最重要的是，数据科学家往往经验更丰富。 
 数学 
 数据分析师：统计学、基础数学数据科学家：高级统计学、预测分析 编程 
 数据分析师：Python、R、SQL 基础流利数据科学家：高级面向对象编程语言培训 工具 
 数据分析师：Excel、SAS、商业智能软件数据科学家：Hadoop、MySQL、TensorFlow、Spark 其他技能 
 数据分析师：数据可视化、分析思维数据科学家：数据建模、机器学习 
这些只是基础，但一般来说，数据分析师是成为数据科学家的入门点，通常需要经过广泛的培训和教育。许多在线平台提供数据科学免费课程和数据分析课程，如果你想深入了解。 
数据专业人员在商业中的角色 
数据科学家和数据分析师在商业团队中都至关重要。数据分析师和数据科学家在商业中扮演的角色以及技能的运用是不同的。 
数据科学家通常向首席数据官汇报工作。他们通常在一个团队中工作，不同的业务部门将利用他们的工作来建立数据集，以便各自的数据分析师进行提取。数据科学家可能一天查看供应链数据，第二天则分析收益信息。 
数据分析师通常会被限制在一个业务领域内。数据分析师最常在一个部门内工作，如财务或会计。然后他们会 专注于与其业务领域最相关的指标和报告。 
数据科学家的主要商业价值在于他们能够深入挖掘数据，找到非结构化数据中的洞察。然后，数据分析师将这些洞察转化为可操作的见解。 
数据科学职业的核心要点 
数据科学家和数据分析师在组织中扮演着重要但不同的角色。然而，两者都是从商业数据中获取最大收益所必需的。数据科学家凭借其专业培训，可以从原始数据中识别出洞察。然后，数据分析师利用筛选后的数据来回答具体的商业问题。两者共同利用数据，帮助公司获得利益。 
Nahla Davies 是一位软件开发者和技术作家。在全职从事技术写作之前，她还曾担任过包括三星、时代华纳、Netflix 和索尼在内的 Inc. 5,000 体验品牌组织的首席程序员，负责许多引人入胜的项目。 
更多相关话题 
 机器学习中训练数据和测试数据的区别效率决定了生物神经元与…之间的区别SQL 和对象关系映射（ORM）有什么区别？L1 和 L2 正则化的区别GBM 和 XGBoost 之间的区别是什么？数据科学家和分析师的统计学基础
数据科学家和数据分析师之间的区别是什么？ 
 
 原文：www.kdnuggets.com/2021/11/difference-data-scientist-data-analyst.html 
 
评论 
 
 “数据科学家是被雇佣来分析和解释复杂数字数据的人，例如网站的使用统计信息，特别是为了帮助企业做出决策。” 
 
 
我们的前三个课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织的 IT 
 
—Oxford Dictionary 
 
 “数据分析师是一位与数据工作以提供洞察的专业人士，他们处理原始或非结构化的数据，并进行分析，以生成高层管理人员和其他人可以用来做决策的易于理解的结果。” 
 
—Techoped"
Algolia 博客中文翻译（二十一）,2024-10-29,"原文：Algolia Blog 
 协议：CC BY-NC-SA 4.0 
 
什么是无头电子商务？ 
 
 原文：https://www.algolia.com/blog/product/what-is-headless-ecommerce-and-how-does-it-work/ 
 
当你听到“无头商业”时，你可能会情不自禁地想到无头骑士，这个来自 《断头谷传奇》、 的心怀不满的家伙骑着他的黑色种马离开了。别担心，没人在谈论砍掉网上购物者的头。在在线购物领域，“headless”指的是许多品牌用来简化其电子商务平台的酷新技术，以便跟上购物者的需求并增加收入。 
为什么网购者的需求如此迫切？自从新冠肺炎疫情开始以来，人们已经成群结队地转向零售商的网站。最初，随着实体店关门，没有其他选择。然而，尽管实体店重新开业，网上购物的趋势仍在继续，因为消费者发现了在家购物的优势。因此，对更加互动和个性化的购物体验的需求推动企业适应、改造和优化其在线实践。 
无头电子商务:一个定义 
那么到底是什么， 无头电子商务 功能呢？要想出一个无头电子商务的定义，首先确定电子商务应用程序的前端和后端之间的差异是有帮助的。电子商务网站的前端体验是购物者使用和互动的店面。后端系统用于处理数据输入、产品图像、定价、促销和订单细节。 
在传统的电子商务应用程序(如 Shopify)中，前端和后端是在一起的。然而，这种模式的问题是，在后端添加应用程序(例如，创建站点弹出窗口的小部件)可能会影响前端，可能会降低页面速度或导致 404 错误。 
当我们谈论无头商务时,“无头”部分指的是切断电子商务商店应用程序的前端(头部)。切断前端和后端之间的联系会减少开发依赖性:这意味着开发人员可以在不损害前端完整性的情况下对后端进行更改。对于希望扩展其在线产品并满足客户对更复杂购物体验的需求的企业来说，无头解决方案是理想的选择。 
一个无头的电子商务平台是如何运作的？ 
把一个网店的前后段分开的想法似乎很奇怪，不是吗？这就像把仓库从商店里拿出来，搬到几英里以外的地方。然而，网站前端和后端的区别不仅仅是“面向客户”和“幕后” 
对于传统的电子商务平台，所有对前端的定制都发生在后端。例如，对网站设计的任何更新都是在后台实现的。前部依赖于后部，后部做了双倍的工作(结果是更慢)。 
有了无头电子商务，做出这些改变的能力就转移到了前端。后端是为纯粹的功能保留的:像计费、定价、报告和分析这样的过程。应用程序编程接口(API)，一个让双方“对话”的软件中介，被放置在这两个部分之间。 
换句话说，在无头架构中，前端承担了更多的工作。它更加独立和自包含，并且，通过 API，它只在必要时才与后端通信。 
无头商业与传统商业 
我们已经讨论了无头商业架构和传统模式之间的区别。但是这两者在功能上有什么不同呢？让我们来看看。 
刚性与柔性架构 
传统的电子商务开发者经常抱怨在设计方面有太多的限制。像 Shopify 这样的电子商务内容管理系统(CMSes)有一定数量的设计模板，更复杂的设计更改通常需要一个插件，这可能会降低网站的页面加载速度或导致网站崩溃。 
无头电子商务解决方案开发团队拥有更大的灵活性。前端开发人员对网站设计和模板有更多的控制权，因此他们可以创建更好的用户体验。中介 API 使开发人员能够进行动态创新——在不影响整个平台的情况下进行快速更改。因此，对于希望快速扩展网站并适应不断变化的客户需求的在线企业来说，无头电子商务是一个很好的选择。 
快与慢店面 
我们已经讨论了 API 如何提高网站的页面加载速度，以及与传统 CMS 相比，这是无头 CMS 的一个重要优势。在传统模式下，对后端进行更改会影响页面加载速度、下载时间和结账时间。所有这些都会影响销售。移动缓慢或笨拙的电子商务界面会让购物者想离开网站。因此，为了避免高客户跳出率，传统开发商已经尝试性地做出了改变。 
有了无头 CMS，消费者会注意到页面加载更快，他们享受到更好的用户体验，这对公司来说，等同于更高的客户忠诚度和转化率。 
随着向网上购物的持久转变，以及消费者对更复杂的用户界面的期待，许多企业不得不迎头赶上。在尽可能短的时间内提供无缝、便捷的购物体验可以大大提高销售额。一些公司能够做到这一点的唯一方法是实施无头电子商务模式。 
个性化与一般化体验 
无头电子商务的一个标志是个性化。自从疫情站稳脚跟后，传统的商业网站就不再受顾客欢迎了。谁希望必须导航回一个产品页面(并向下滚动)，在注销然后重新登录后发现他们的购物篮是空的，或者得到客观的搜索建议？ 
糟糕的客户体验是不被允许的。开发传统商务应用程序的开发人员已经能够使用插件来创建个性化，但是正如我们所讨论的，这些插件降低了网站的速度。 
借助 headless commerce，开发人员可以跨不同的接触点自由创建更加个性化的功能，从而改善客户体验。来自后端的产品数据和客户信息仍然可以用于通知前端的个性化功能和电子商务体验。 
头(肩)高于传统商业 
既然你已经牢牢掌握了 无头电子商务 是如何运作的，你无疑就能理解为什么这么多企业会走向无头。在电子商务中，客户需求总是推动变革，无头化运动解决了各种问题。无头应用程序促进了商业网站上的高级开发，允许企业更快地进行更改和扩展。适应性强的框架使得实现个性化变得容易，这有助于电子商务网站更好地满足购物者的标准。 
希望利用无头架构的优势，打造经得起未来考验的电子商务网站，并通过个性化的产品信息取悦客户？联系 我们的团队 以了解更多关于我们的 API-first 平台或请求一个 免费演示 。 
什么是智能搜索，它是如何工作的？ 
 
 原文：https://www.algolia.com/blog/ux/what-is-intelligent-search-and-how-does-it-work/ 
 
一台电脑曾经在象棋上赢过我，但在拳击上却不是我的对手。 —Emo 飞利浦 
有没有想过(更确切地说是什么时候)计算机会想出好主意，赶走它们的人类霸主，取而代之？ 
你并不孤单。人工智能正在为人类的更多技术提供动力，计算机和软件每天都在“学习”并变得越来越聪明。 
搜索领域无疑是智能技术正在接管的一个领域。人工智能搜索工具现在正被用于从企业数据管理到电子商务购物体验的各种应用中。 
例如，你可能已经注意到最近谷歌在从网络上给你提供相关信息方面变得多么聪明。这是因为它是一个智能搜索引擎，事实上是这个星球上最聪明的一个。但为企业知识库和其他用例定制的小规模企业搜索引擎版本在采用这种扩展功能方面并没有落后太多。 
什么是智能搜索？ 
智能搜索能力是你从一个搜索引擎那里得到的，这个搜索引擎能够根据用户在搜索框中输入的内容“理解”用户意图。也被称为认知搜索和 AI 搜索 ，它利用一系列技术，如自然语言处理(NLP)和机器学习，来更好地理解每个用户想要的东西。一旦确定了这一点，就可以提供正确的搜索查询结果。 
当你意识到就在不久前，唯一可用的搜索是“哑”(因为没有更好的词)搜索，现在被称为“传统搜索”时，这是非常令人惊讶的 
传统搜索有什么例子？ 
几年前，你可能会在一个使用传统搜索引擎的网站上进行文本搜索；也许你输入了类似“纯素食冰淇淋”的东西你会看到这个词排名最高的页面，基本上是一个出售纯素食品牌的各种杂货店网站上的产品链接列表。 
如果你正在寻找在哪里可以买到一加仑纯素食冰淇淋的基本信息，这种搜索方法可能会很有效。但你可能一直在寻找与纯素食冰淇淋更相关的东西。 
这就是现代智能搜索解决方案派上用场的地方。 
智能搜索有什么例子？ 
如果你在谷歌的搜索框中输入“纯素食冰淇淋”，你的体验会更加令人印象深刻。人工智能驱动的智能引擎仍然使用网络爬虫和网页排名，但该软件也分析用户意图。 
当你像这样搜索一个宽泛的术语时，一个智能搜索应用程序会将结果分组为基于意图的类别，比如购物选项、食谱和常见问题的答案。然后，你可以快速锁定你心目中的类别，并在搜索结果中深入搜索。 
“纯素冰淇淋”涵盖面很广。与其知道在哪里可以买到，你可能只是想知道是什么决定了真正的纯素食冰淇淋，或者里面有什么成分。 
智能人工智能支持的搜索引擎技术考虑了所有这些变量，所以你不太可能因为第一个搜索查询失败而不得不重复搜索查询。搜索结果通常更准确，自然符合你的具体需求，无论你是想吃纯素食冰淇淋，还是只想知道它与素食冰淇淋有何不同(有这样的东西吗？).万一你还没有清楚地说出你正在寻找的确切的纯素食冰淇淋数据，你会看到“人们也会问”的问题，比如“纯素食冰淇淋比普通冰淇淋健康吗？” 
智能搜索是如何工作的？ 
现在我们知道，智能搜索是基于识别搜索者的特定意图，以便快速提供正确的信息。但是智能搜索信息解密过程实际上是如何工作的呢？ 
智能搜索结合了包括自然语言处理和机器学习在内的 技术 。例如，它可以在语义术语之间建立联系，这是传统搜索引擎(只查看关键字的搜索引擎)无法识别的。它还可以做“思考”类的事情，比如理解文档的结构。 
让我们详细了解一下这些功能。 
过滤内容 
网络爬虫搜索大量的结构化和非结构化数据，以识别最准确的搜索结果。由于这项繁琐的工作，智能搜索引擎采用先进的过滤器，缩小搜索范围，以便找到最相关的内容。 
分析文档结构 
智能搜索还有助于内容管理活动，如查找 pdf、模板和可视单页内容。这是因为它使用文档扫描技术，可以识别文档的类型和结构，例如，它是信息图、合同还是发票；它是否包含表格、目录、页眉或页脚。该软件可能对搜索者正在寻找的文档格式进行“有根据的猜测”,并在此基础上，为搜索者优先考虑最相关的项目。 
应用自然语言处理 
自然语言处理 是计算机程序理解人类口头和书面语言的能力。NLP 使搜索引擎能够理解从多个数据源提取的数字内容，并解释抽象的语言结构，如包含同义词、细微差别或单词之间复杂关系的语言结构。能够学习人们语言背后含义的搜索应用程序可以更有效地预测搜索意图。 
应用机器学习 
机器学习是人工智能的一个分支，利用数据和算法来复制人类的学习方式。这种学习是对检测到的东西的反应。 
例如，如果某人一直在搜索特定主题，然后开始对看似相关的东西进行不同的搜索，机器学习功能可以假设这两个搜索确实相关，并主动建议包含这两个主题的材料。随着时间的推移，随着每个额外的查询，搜索引擎在预测最准确的搜索结果方面变得更好(更聪明)。 
机器学习的另一个应用是当智能搜索引擎检测到信息模式时，这可以帮助它识别垃圾邮件和重复内容。这意味着搜索者不必受制于大量不想要的数字信息。 
为你的网站做出明智的选择 
你的公司还在为你的网站或应用程序使用传统的搜索技术吗？既然你已经读了这篇文章，你可能想知道你是否应该通过添加智能搜索来优化。 
如果这个想法有道理，那就去查查阿哥利亚完全主持的 SaaS 搜索解决方案 。我们提供人工智能支持的智能搜索，让用户轻松找到他们想要的东西，沿途发现感兴趣的新项目，并对他们刚刚享受的出色客户体验感到完全满意。 
从开发的角度来看，无论您的网站是专注于电子商务、医疗保健、媒体还是其他领域，我们强大的 API 都可以让您创建和简化搜索体验，以满足您的用户和团队的需求。 联系我们 获取详情，或者现在就开始免费构建伟大的企业内容 搜索功能 。 
什么是意图数据？ 
 
 原文：https://www.algolia.com/blog/ux/what-is-intent-data-and-how-can-you-use-it-to-predict-user-intent-and-increase-roi/ 
 
如果你有一家实体商店，详尽地采访每一个走进来的不知情的人，以获得他们想要的东西的详细了解，会怎么样？包括他们在哪里寻找背景信息(如果适用的话)，他们从与朋友和邻居的交谈以及在亚马逊上查看推荐中了解到的信息，他们对你的产品在价格方面的预期，他们对你商店的整体用户体验的感受，以及其他看似相关的趣闻。 
你肯定会让他们怀疑你在干什么，因为那太过分了。但是你可能也能够自信地辨别 确切地说 他们想要什么——他们真正的 意图——以及这些基准数据是否与你的公司碰巧提供的产品或服务的好处或功能重叠。 
继续……想象你很满意你花了最后一个小时来收集他们的想法，并且你已经让他们浏览你的店内商品(如果他们在经历了这个小小的磨难后还有兴趣这么做的话)。 
你建立了一个基线级别的电子表格，开始摆弄你的临时意图预测模型，并试图建立关联，希望能找到一些自然的方法来预测用户意图:找到他们想要的东西。 
最后，基于这种认识，你对你的网站做一些优化，以尽可能最吸引人的方式向他们推销，你努力提升你的客户体验，以满足他们的期望。 
不管你的购物者是消费者还是企业买家，这种笨重的预测形式相当于现代的信息检索和购买意图验证。 
意向数据可以通过计算机科学的进步来收集，如 自然语言处理 和 语义搜索 ，这要归功于 97%的消费者在做出购买决定之前会研究产品和服务。有了人工智能和先进算法增强的软件，研究人员可以准确找到用户在寻找什么，就电子商务而言，他们是否可能很快点击购买。 
什么是意向数据？ 
买家意向 是一回事；买家意向 来自独特买家旅程的数据——记录该意向的重要证据——是另一个例子。 
意向数据是关于潜在买家在线行为的信息，表明他们接下来可能会做什么或购买什么:他们的界面阐明了他们的购买意向。它利用 IP 地址和浏览器 cookies 等标识符，以及关键词和内容的点击量来收集信息。例如，某人的内容消费可能包括阅读在线评论、细读博客文章和点击其他内容的链接、观看视频、获取门控材料以及比较产品。 
意向数据通常也围绕特定的主题、产品和服务。它专注于与主题相关的(积极或消极)情绪，提供了比传统的分析或行为数据更全面的视图。 
预测分析和意图数据有什么区别？ 
预测分析 (也称为预测意图)大多与过去发生的事情联系在一起。它使用大数据来预测谁将在何时购买什么。信息来自各种数据源，如客户资料和销售，这种方法融入了数据挖掘、数据建模、机器学习模型、历史信息和人工智能等元素。 
意向数据 可以说是卓越的，因为它专注于跟踪和记录购物者的实际在线探索。因此，它可以更好地帮助营销人员和销售代表识别潜在的潜在客户，并确定他们准备购买的时间，例如他们在搜索栏中输入相关关键词、观看网络研讨会或下载相关内容的时间。 
值得注意的是，一家公司当然可以同时使用这两种方法来获得完整的数据。然而，收集买家意向数据并不复杂(需要更少的集成)，而且与预测意向不同，它可以立即提供具体的实时洞察。 
B2B 意向数据生成主要销售线索 
当涉及到收集和分析意向数据时，B2B 买家特别感兴趣。事实上，汇总的商业买家意向数据非常有价值，以至于 Gartner 将其称为“B2B 销售线索生成的未来”并且Gartner预测，到 2022 年底，超过 70%的 B2B 营销人员将利用第三方意向用户数据在购买过程中锁定 B2B 销售前景，或在选定的客户中吸引买家群体。 
两种意向数据 
意图数据分为两种类型:主题和上下文。 
一个 题目 类似于一般意义上的:一个主题。在这种情况下，它是在线内容的主题。主题是通过自然语言处理(NLP)和深度学习模型的应用正式确定的。主题可以是概念、地点、人物、产品、公司或其他实体。 
当你搜索某样东西时，你表明了你对这个话题所包含的任何东西的兴趣。 
然而，通过编辑意图数据，理解人员角色的背景和他们的搜索会有很大帮助，从而使数据真正相关。要获得准确的购买信号，真实世界的背景是关键。例如，搜索者可能是一名正在做研究的学生，或者是一名正在寻找要在国际会议上展示的数据的人，而不是有权为一家大公司进行采购的企业采购员。 
当你有一个清晰的主题和清晰的背景时，你就对真实意图有了一个很好的了解，你的营销人员就可以充满信心地前进了。 
三类意向数据 
数据根据第一方、第二方和第三方提供的信息进行分组。 
第一方意向数据 
这是从你自己的站点收集的访问者意图信号。企业通过使用分析工具或营销自动化平台来收集目标市场的第一方数据。 
第一方数据可能是匿名的，或者你可以使用 IP 身份或该人填写的表格来挖掘他们的具体细节。 
数据可以来自人们在你的网站上填写的表格，他们完成的下载，他们输入的搜索词，他们登陆和查看的页面，他们放入购物车的物品(无论他们是否购买)，任何基于活动的东西。 
更广泛地说，第一方意向涵盖了您在与网站访问者、潜在买家和现有客户的互动中收集的任何信息，包括您从任何电子邮件营销活动和社交媒体投放中收集的信息。 
第二方意向数据 
第二方数据本质上可以被认为是另一个组织从第一方收集的数据。对于第三方信息，数据是由第二家公司收集的，例如评论网站(想想 Yelp)或发布网络或链接您的产品或服务的网站集合。当用户在这些类型的网站上注册时，他们授予网站所有者分享和出售他们的联系方式和他们留下的任何意向数据的权利。 
与第一方数据一样，这些收集的材料可能包括来自网站、应用程序和社交媒体活动的客户参与度数据、店内购买历史、调查反馈等。 
搜索引擎是第二方客户意图数据收集器的一种类型。比如你用 Google 做广告，你的关键词数据技术上是第二方数据，而不是第一方。人们搜索提供关键字给谷歌，而不是直接在你的网站上给你。你以非正式的伙伴关系与谷歌签约，以获得对这些关键词的访问权。 
第三方意向数据 
最后一种意向数据包括从另一个站点收集的信息。与第一方数据一样，它可能是从填写的表格和 IP 地址、与聊天机器人的交互以及其他活动中收集的。 
第三方数据通常由从事收集意向数据业务的公司收集。这些意向数据提供商通常可以提供一系列的研究和购物接触点，由用户在访问您的网站之前着手进行。 
哪种类型的意向数据最适合你的企业？ 
第一、第二或第三:你应该选择哪一个？ 
这取决于诸如营销活动的规模等因素。如果您收集意向数据的预算有限，可以考虑以下几点: 
第一方意向数据的优点 
数据是 完全属于您的: 一个独特的数据集，阐明了您的买家的兴趣和痛点(相比之下，如果您选择购买第三方意向数据，您的竞争对手也可以访问这些数据)你 控制并能以任何最符合你需求的方式分割 你的数据你的营销团队可以 利用你收集的数据 来强化你的销售流程和营销策略 
第一方意向数据的缺点 
第一方数据仅构成 购物旅程的一个片段你可能会错过那些没有访问你自己网站的潜在客户的数据，例如，那些只在第三方网站上研究竞争对手的人 
第三方数据的优点 
你可以看到一个潜在客户的活动、他们所走的整个路线以及他们所走的每一步。第三方数据是 可用的最早 的任何类型。这意味着它可以帮助公司计划如何解决问题，主动击败竞争对手，并在购物者虚拟踏上自己的网站时提供更个性化的体验。 
鉴于这些优势和局限性，公司通常会结合第一方和第三方数据，以获得其潜在客户旅程的最完整图片。 
如何利用意图数据？ 
现在你明白了为什么意图数据对预测用户行为和增加收入如此重要。本质上，如果你能确定潜在客户在销售周期中的位置，你就可以适当地向他们提供他们想要的相关内容，以促使他们更接近购买，从而有可能大幅增加收入。 
这一切都归结于使用你的意向数据来识别准备购买的潜在客户，然后相应地调整你的营销活动。 
大多数企业开始利用意向数据来帮助销售人员确定要接洽的目标客户的优先顺序。然后，销售团队可以在他们可能感兴趣的时候，直观地联系基于客户的营销(ABM)潜在客户，从而提高转化率。 
意向数据也可用于未来潜在客户的相关工作，因为人们揭示了他们正在进行的购物和购买模式。 
想让您的意向数据利用率更上一层楼吗？您可以使用这些数据来创建基于特定活动的独特群体和目标列表，然后让市场营销以更加个性化的方式进行受众拓展。 
用例:利用搜索意图数据 
如果你有一个零售网站，你无疑会关心你的搜索意图数据的质量。 
您可以立即开始从您网站访问者的行为中收集 搜索意图 数据，以便提供 最先进的结果 。Algolia API 平台 将让你 预测 你的用户的意图，并在你的网站和应用中提供正确的搜索结果。 
此外，您的决策者可以使用我们现成的搜索分析来筛选数据并调整他们的内容营销工作。您可以通过将用户的搜索分解为流行搜索、没有结果的搜索数量和过滤器使用等指标来模拟用户的行为。 
包括 Lacoste、Slack 和 Medium 在内的公司依靠我们来满足其潜在客户在独特购买流程中的搜索需求。 
想加入这些成功的网站并享受可能的留存率或转换率的提升吗？ 联系我们 了解如何利用意向数据来提高 您的 客户对搜索的满意度，以及我们认为肯定会带来的所有回报。 
什么是以知识为中心的服务？ 
 
 原文：https://www.algolia.com/blog/ux/what-is-knowledge-centered-service/ 
 
您知道您的 IT 服务台很可能面临越来越大的压力吗？由于数字化转型项目的增加而涌入的需求意味着许多支持组织正在努力为其员工提供足够的 IT 支持。 
为了帮助简化支持流程并提高利益相关方的服务水平，许多服务台经理正在转向以知识为中心的服务(KCS)方法。KCS 包括持续分享和提炼知识。拥有客户支持的 KCS 有很多好处。服务台代理通过搜索访问历史公司内容并不断优化该内容，所有这些都是为了帮助快速跟踪解决过程。 
KCS:最佳实践方法论 
以知识为中心的服务(也称为以知识为中心的支持)是一种描述组织如何更好地利用其拥有的信息来改善其服务交付的方法。KCS 方法让 IT 服务台工作人员不仅解决系统、员工和利益相关方的问题，还记录解决方案流程，以便在未来实现有效的知识共享和客户成功。在 KCS 进程中，知识库的不断更新改善了服务的提供。 
什么是知识库？ 
知识库是一个在线图书馆，包含关于产品、服务、问题和主题的知识文章。它可能包括产品常见问题解答、故障排除指南、用户手册和其他文档。为了支持知识的收集、检索和共享，今天最好的知识库往往是人工智能驱动的和可搜索的。对于处理许多服务请求的支持团队来说，知识库中的集体知识对于加快新代理和老代理的响应时间至关重要，因为他们会定期参考公司的历史内容。 
知识型服务的好处 
像所有好的 知识管理 一样，KCS 的实践可以帮助您组织的服务台捕捉知识，以多种方式提高客户满意度。这对你的团队有帮助: 
破案更快 
支持团队随手可得的可搜索数据库意味着不再需要第二次猜测服务请求。过去的问题解决事件都有详细记录，让代理有所参考。常见问题可以快速解答。 
打造专业文化 
专业文化是指以知识和技能为特征的服务组织文化。丰富的资源库有助于团队成员培养专业知识文化，使代理能够处理案例。KCS 处理服务请求的方法可以产生消息更灵通的服务代理和更熟练的查询解决方案。 
优化利用资源 
拥有一个 可搜索的知识库 意味着内容可以进行优化搜索。特别是对于大型知识库，使用关键字标记内容可以让搜索正确信息的代理更快地访问特定内容，从而减少搜索正确信息的困难，并有助于确保运营效率。 
启用自助成功策略 
自助式数据库为支持代理提供了对其工作的自主权和所有权，并加快了新代理的培训过程(他们可以使用该数据库查看案例是否已被记录)。 
增加首次接触分辨率 
优化的资源数据库不仅加快了解决流程，还增加了案例首次得到解决的可能性，有助于持续改进。如果第一次接触的解决率很高，这应该表明客户以高效的方式获得了良好的服务。 
创建以 KCS 为中心的知识库 
创建有效的知识库需要时间，并且是一个持续的过程。然而，使用 KCS 方法可以帮助你更快地创建一个知识库，并有效地维护它，使它总是最新的。 
以下是如何创建一个反馈知识库: 
第一步:创建好文档 
当 IT 服务台的询问得到解决时，创建适当的文档(如果它们不存在)是很重要的。为每个案例中的客户问题创建支持文档的过程——无论是以常见问题解答、文章、一页纸、案例研究、故障排除指南，甚至是完整的手册的形式——都将快速构建一个全面的有用资源库。 
第二步:使用一致的文档结构 
创建标准化的文档结构能够实现整个库的一致性，并简化创建额外文档的过程。代理人可以在整理文档时从内容模板开始。 
第三步:运用你组织的知识 
一个伟大的知识库仅仅存在是不够的。支持团队必须接受如何使用该平台、如何创建文档并将其添加到库中，以及如何有效地使用搜索在他们需要时找到正确的信息方面的培训。 
搜索第一流知识库的重要性 
您的知识库应该以直观的方式组织，并且易于搜索。为了优化可访问性，您可以按主题或内容类型排列内容。搜索栏确保用户可以在他们需要的时候立即调出正确的内容。 
通过可搜索的知识库，您可以使用关键词按产品、主题或任何其他相关类别对信息进行细分。 
关键词最佳实践 
关键词是一个单词、短语或一组单词，最能描述内容所涵盖的内容。有了知识库，工程师可以使用关键字来查找与其问题相关的文档。例如，对于丢失密码的困境，代理可以通过在搜索栏中输入关键字(如“密码”或“忘记密码”)来查找相关信息。 
支持团队可以定义他们自己的记录和查找文档的规则。这里有一些正确做这件事的技巧 
选择代理可能搜索的关键词: 了解代理的想法。他们可能会使用什么搜索词？他们如何表达他们的搜索查询？将关键词与内容联系起来: 每个关键词都应该描述内容的内容。使用长尾关键词: 长尾关键词通常是三到五个单词的短语。长尾关键词更长，因此更具体，有助于缩小内容范围。在内容中使用链接: 让服务管理内容创建者添加超链接，将搜索者导向其他相关内容。建立关键字目录: 作为参考文档，创建与代理可能需要的信息相关的关键字列表。比如这里有一个 关键词列表 帮助人们查询微软知识库。 
拥有可搜索知识库的好处 
由人工智能软件支撑的搜索栏是一个成功知识库的重要特征。当您的代理拥有这些信息时，他们可以: 
快速检索内容 并快速满足客户需求轻松找到自己需要的 享受愉悦的用户体验建立文档管理流程: 定期用关键词组织文档，直观地组织内容 
添加最佳搜索来升级您的知识库 
总而言之，您可以集中您的 IT 历史支持信息，并确保持续优化您的内容，以便在您的服务团队最需要时为他们提供支持。你可以让所有相关的文章都容易找到(而不是从孤立的文件夹中一篇一篇地挖掘)。 
Algolia 的高级搜索 API 随时准备在这两方面提供帮助。我们的搜索功能是著名和可靠的，具有成熟的算法、集成和用户界面库。无论您是否正在实施 KCS 方法，帮助您的服务台代理毫不费力地挖掘他们需要的信息并能够重复使用知识都是至关重要的。 
免费发现搜索如何帮助您大幅提高服务台的效率(更不用说用户体验了)。如果你喜欢你所看到的，没有大量的前期投资，你可以选择灵活的支付方式。 让我们知道 聊天的好时机，我们再联系！ 
搜索意图是什么意思？ 
 
 原文：https://www.algolia.com/blog/ux/what-is-meant-by-search-intent-and-what-are-the-different-types/ 
 
你现在想要什么？ 
也许是午睡，吃点蘑菇披萨，一天中更多的时间；差不多吧。 
比方说，你想吃一个新鲜的薄脆皮蘑菇比萨饼，里面有奶酪。 
听起来很完美。但是冰箱里的剩菜可能足够了。或者车库里有冷冻烤焦的微波意大利香肠。如果你有食谱，用农贸市场的蘑菇从头开始做就很棒了。但是现在，你最好的选择是去当地的比萨饼店吃一个热的奶酪馅饼，上面覆盖着新鲜的香菇，橄榄油和少许蒜末。(这对于内容营销来说怎么样？) 
什么是搜索意图？ 
所有这些都是说，当你在网上或特定网站上进行搜索时，你有一种特定类型的意图，或者你可以在几种相关类型的意图之间切换。营销人员想知道那是什么，或者那些是什么，这样他们的搜索引擎就能给你正确的搜索结果。 
当人们在网上搜索时，他们可以在他们的搜索查询中有任何数量类型的搜索意图。因此，考虑到这些关于蘑菇的想法，你在搜索栏中指出的在搜索引擎结果页面(SERP)上调出结果以便导航到正确的网页的意图可能是这样的: 
蘑菇披萨靠近我 
交付数量披萨我的心 
食谱蘑菇披萨 
当地最好的蘑菇披萨 
披萨为什么会上瘾？ 
香菇和牛肝菌的区别 
所以搜索查询可以包含多种类型的搜索意图。那些类型的意图是什么？ 
理解用户意图 
你在搜索框中键入的单词或短语——你输入的关键词——通常能很好地表明你的意图。例如，搜索引擎可以判断你是否: 
正在寻找名牌产品正在浏览，以便在做出购买决定之前比较信息你已经做好了购买的准备，只需要找到合适的产品页面你很好奇，并且有一个你想要回答的特定问题想找菜谱正在旅行，想找一个受欢迎的地方吃饭 
为什么搜索意图很重要？ 
当人们的意图明确时，营销部门可以正确地设置他们的搜索引擎优化(SEO)来包含正确的关键词，以吸引你访问他们的网站登陆页面。 
用户搜索意图——搜索者进行搜索背后的动机——已经成为管理 SEO 的商业人士的首要考虑因素。意图现在被认为是一个比以前更有效的指标，以前是根据输入的关键字进行推断。 
营销人员热衷于找出整个搜索意图，他们已经确定了四种类型的意图。前三个由专家一致命名，而第四个命名略有不同，但涵盖了基本相同的概念。 
类型 1:信息意图 
信息搜索的意图非常简单:使用这种类型的查询，搜索者感兴趣的是挖掘一些内容，想知道他们看到的一些东西或他们想到的一些想法；他们很好奇。 
在他们的搜索查询中，他们会问一个“谁”、“什么”、“何时”、“何地”、“为什么”或“如何”的问题(并且不一定会把它表述为一个问题)。 
示例信息查询: 
Spotify 上排名第一的艺人是谁？ 
什么是 virga 
2023 年劳动节 
阿姆斯特丹的火车站在哪？ 
蝙蝠为什么倒挂 
没有经验如何找到工作 
一个信息搜索查询 不能 的一件事就是与购买某样东西有关——这被认为是交易意图。 
例如，判断谷歌搜索是否是信息性的一种方法是通过搜索引擎回应的细节。第一个结果通常是高质量的内容元素，如定义、地图、图表或图形等视觉效果、食谱、特色片段(简短的广告)或“人们也问”内容。 
类型二:导航意图 
这种类型的内容查询听起来像是表明搜索者想要知道如何去一个物理位置，不是吗？然而，导航查询实际上是想找到一个特定网站的快捷方式，比如一个特定品牌的网站(想想苹果或脸书)。搜索者不知道 URL 或名称的确切拼写，或者他们懒得键入。所以他们只需输入公司名称。有了导航式搜索查询，人们不再需要寻找特定的产品来购买(至少现在还没有)。导航搜索的目的是以最快的方式到达搜索者选择的网站的主页。 
类型三:交易意图 
你猜对了:这种类型的查询可能表明某人想要并准备购买某样东西——某种特定的产品或服务。如果你有一个电子商务网站，以产品名称作为目标关键字的交易查询将是预期的。搜索词可能是(但不总是)品牌名称，因为用户准备放弃公司网站或在线市场上的旋转产品传送带和其他元素，并支付他们的现金。这些搜索者使用听起来像交易的词，比如“购买”、“花费”或“注册”。(【成本】也可能是 而不是 表明事务性搜索意图；它可能只是面向研究的查询中的一个单词，如下所述。)交易搜索者也可以寻求捐赠。 
下面是一些事务性搜索的例子: 
克托在乔氏购物 
买最好的洗发水治疗脱发 
亚马逊折扣券 
去哪里捐旧 iPhone 
指向目标 
第四类:商业意图(又称商业调查、考虑、优惠) 
通过“商业”或另一个名称的搜索，某人正在做他们的研究(尤其适用于 B2B 交易)，可能在他们决定访问产品页面买东西、注册服务或与某个组织做生意之前比较产品和选项。例如: 
点评登山鞋男 
技术风格 vs 缝合修复 
最好的狗 
比较名人邮轮价格 
以上总结了搜索意图的类型。听起来很简单，对吧？ 
不完全是。虽然许多搜索很容易被归类为体现了这些类型的用户意图之一，但其他搜索仍然不会准确地揭示搜索者的意图。这是因为搜索者可能不知道他们当时想要什么，或者他们可能以一种模糊的方式表达他们的查询，这种方式可能被搜索引擎以多种方式解释。 
这意味着公司能够准确识别用户的意图是至关重要的 
如何高手破译关键词意图 
由于用户进行模糊搜索的可能性，考虑如何设置搜索意图(例如，根据搜索排名因素)以及它在搜索引擎软件中的工作效率是值得的。 
Algolia 使用 机器学习 来检测意图，并采用平局决胜算法来确定适用于任何给定查询的所有规则的优先级。如果需要澄清， 图形 被创建。 
当您确定了特定查询术语和短语的预期效果后，您可以在用户的搜索与这些术语匹配时动态更改用户的特定搜索结果。 
你用 规则 配置这个。这里有一个 的例子，说明意图匹配规则如何为零售商 工作。 
意图也很重要 搜索 vs 浏览 。通过提供更丰富的用户界面来超越标准的搜索功能，可以提高搜索框的使用率，并确保满足搜索者的需求。为什么？人们并不总是有足够的技术知识来知道如何有效地搜索，或者如果他们最初的搜索不起作用，如何调整搜索。一个更好的搜索界面可以帮助人们不放弃地搜索——如果空手而归，还可以继续搜索。 
你是想提高网站点击率的开发者吗？你可以开始 构建 一个有前途的新搜索解决方案(而且是免费的)。该视频进入 意图检测，规则 。 
你是一个想要改善你的数字营销策略以获得更高转化率和降低跳出率的商业领袖或营销人员吗？ 
不管怎样，我们都愿意提供帮助。要了解如何增强对网站或应用中搜索意图的检测，以改善用户体验， 与 Algolia 团队联系 。 
什么是自然语言处理(NLP) 
 
 原文：https://www.algolia.com/blog/product/what-is-natural-language-processing-and-how-is-it-leveraged-by-search-tools-software/ 
 
语言是我们最基本的交流方式之一，但它也是丰富的信息来源，我们无时无刻不在使用它，包括在网上。如果我们能以自动化的方式使用这种语言，包括书面和口头语言，会怎么样？这就是自然语言处理要做的事情。 
自然语言处理，简称 NLP，将语言处理成软件可以使用的信息。有了这些信息，软件就可以执行无数其他任务，我们也将对此进行研究。 
自然语言处理为什么有必要？ 
但是首先，为什么自然语言处理是必要的？首先，每天都有大量的信息通过自然语言被创造和分享。每天都有数十亿的社交媒体帖子出现。数万亿次搜索发生在大大小小的搜索引擎上。通话记录。电子邮件。分类广告。新闻文章。其中一些，如搜索查询，直接受益于 NLP。其他的，比如新闻文章，可以通过 NLP 处理来创造价值。 
让我们更详细地看几个例子。我们将从看新闻文章开始。 
琼斯将出任 Acme 公司总裁 
马库斯·l·琼斯今天宣布他将成为 Acme 公司历史上的第四任总裁。他将带领这家小部件制造商进入下一个篇章，研究向欧洲、墨西哥和加拿大等新市场的扩张。 
现在想想我们可能想对这篇文章做的所有事情。例如，我们可能想知道哪些公司、主题、国家和其他关键的 实体 被提及，以便我们可以对类似的文章进行标记和分类。我们可以这样做的一个方法是首先决定只有名词和形容词有资格被考虑作为标签。为此，我们将使用一个 词性标签 ，它将指定文本中每个单词是什么词性。 
NLP 规范化和标记化 
但是，即使我们识别了这些单词，事情也很棘手，因为“widget”和“widgets”有什么不同吗？当然不是！所以我们需要使用一些 规范化 ，将单词折叠到它们的核心，以便不同的变体可以被认为是等价的。规范化可能很复杂，例如“欧洲”和“欧盟”，或者“马库斯·l·琼斯”和“马库斯·琼斯”。 
这种方法甚至忽略了与标记和分类相关的项目可能不是单个单词，甚至可能是一个短语，例如“Acme Corp”。识别这些项目是 标记化 的工作。标记化将一个较大的文本分解成较小的部分。它可以将文档分解成段落，段落分解成句子，句子分解成“记号”(这里我们就不说 字了 ，因为 Acme Corp .可以是一个 token 但不是一个字，而“不是”是一个字，但往往会分解成两个 token:是 和n ’ t。)标记化可能非常困难。例如，即使像识别段落中的句子这样“简单”的事情也是棘手的，因为当你有一个像文章中第一个这样的句子时会发生什么？“马库斯·l .”是因为以段落结尾，后面跟着一个大写字母的单词而成为句子吗？) 
使用 NLP 进行命名实体识别 
总而言之，识别关键概念就是所谓的 命名实体识别 。命名实体识别不仅仅是识别名词或形容词，而是识别文本中的重要项目。在这篇新闻文章中，我们可以确定 Marcus L. Jones、Acme Corp .、欧洲、墨西哥和加拿大都是指定的实体。 
最后，我们可能要了解单词之间的联系。这将有助于我们的程序理解第二句话中“他”是谁，或者“小部件制造商”描述的是 Acme Corp. 背后的语义 
自然语言处理搜索 
自然语言处理对于搜索 查询同样重要，但是有其自身的挑战和需求。说明这一点的一个好方法是讨论自然语言处理的一个重要因素:世界上有成千上万种自然语言。不同的语言会有不同的需求，虽然英语是许多 NLP 软件开始使用的语言，但它并不代表所有的语言。 
举个例子，英语中很少有 复合词在一起 不用一些分隔符，不管是空格还是标点。事实上，它是如此罕见，以至于我们有了 这个词来形容它。其他语言不遵循这种惯例，单词会相互碰撞，完全形成一个新单词。德语中“hundehütte”一词的意思是狗屋。不是两个词，而是一个，只是把这两个概念合起来指。 
一个幼稚的搜索引擎会将hundehütte匹配到hundehütte足够好，但它不会将该查询词匹配到短语“hütte für gro e Hunde”，这意味着 大狗之家 。自然语言处理将查询词分解成单个的部分，这样搜索者就可以看到正确的产品。这说明了 NLP 的深度学习元素是有用的另一个领域，以及 NLP 如何经常需要特定于语言。 
自然语言处理为程序构造数据 
通过自然语言处理的所有这些步骤(统称为自然语言处理 流水线 )返回以软件可以理解的方式构建的信息。到目前为止，你已经看到语言中隐藏了大量的信息。一个 40 字的段落可以涉及一个公司，一个人，三个地区，以及关于这些项目的许多信息。人类非常擅长识别语言的重要部分，并理解它是如何组合在一起的，但不擅长从数百、数千或数百万的文本中寻找趋势或将它们组合在一起。大多数软件程序正相反:它们可以发现趋势或对文本进行分类，但它们不擅长文本本身。这就是为什么我们使用定制的软件，自然语言处理，将文本组织成那些程序可以使用的方式。(顺便问一下，为什么不把所有步骤合并成一个程序呢？拥有小而集中的程序可以使每一步都做得更好，并允许我们为不同的目的组合不同的工具。) 
结论 
上面，我们看了一篇新闻文章和一个搜索查询的例子，以及我们如何使用自然语言处理来更好地转换文本。现在想想我们讨论过的文本内容的其他例子，比如通话记录、分类广告或电子邮件。这些文本可能需要什么样的处理？ 有关 Algolia 的搜索和发现 API 如何利用 NLP 的更多信息，或者了解我们如何帮助您在您的网站或应用中实现这一强大技术以获得更具吸引力的用户体验的更多信息， 请联系我们的专家团队 。 
什么是自然语言搜索？ 
 
 原文：https://www.algolia.com/blog/product/what-is-natural-language-search/ 
 
自从互联网诞生以来，搜索已经发生了很大的变化。用户现在在浏览网站或搜索引擎时期望快速和个性化的结果，他们不太可能为了找到他们想要的东西而尝试许多不同的关键词。随着新计算技术的兴起，网站开始提供更自然的搜索体验，提供创新的方式来探索内容——主要是通过自然语言搜索。搜索正迅速成为一种双向对话。 
什么是自然语言搜索？ 
自然语言搜索允许用户使用他们的日常语言而不是关键字在设备中说话或打字。用户可以用他们的母语使用完整的句子，就像他们在和另一个人说话一样，让计算机将查询转换成它可以理解的东西。 
自然语言搜索与关键词搜索 
多亏了谷歌和其他搜索引擎，用户已经习惯了使用关键词搜索。但是关键词搜索并不是用户提问的直观方式，用户实际上很不擅长使用它们来找到他们需要的东西。它们迫使用户去掉疑问词和其他连接语言，形成搜索引擎可以用来查询数据的文字字符串。还可能需要业务部门努力从关键字搜索中挖掘意图。 
虽然关键词搜索系统通常允许某种形式的复合问题，但它们经常迫使用户手动构建复杂的 搜索结构 。例如，不要问一个简单的问题，如“有西红柿和奶酪的素食食谱是什么？”，你应该搜索更像“素食食谱” 番茄奶酪 的东西。 
然而，随着 Siri 和 Alexa 等数字语音助手的兴起，人们越来越习惯于用完整且语法复杂的句子与他们的设备 进行对话。其结果是，许多用户现在在不同的设备和平台上形成类似问题的查询。用户正变得习惯于使用自然语言来获取信息，并期望快速得到结果。因此，所有类型的搜索系统都能够开始接受自然语言搜索是至关重要的。 
历史自然语言搜索 
虽然计算机科学和计算速度的进步使得自然语言搜索取得了突破，但实现这些系统的尝试实际上可以追溯到互联网和 web 的早期。 
1993 年，MIT 人工智能实验室开发了 START 自然语言问答系统 。虽然从技术上讲，它不是一个互联网搜索引擎，但 START 系统允许用户使用完整的自然语言句子搜索在线信息百科全书。 
几年后的 1996 年， 问吉维斯 推出了。这是第一个允许用户通过自然语言探索网络的搜索引擎。然而，事实证明，吉夫斯有点超前于他的时代。此后不久，谷歌推出了一个关键词搜索引擎，并迅速建立了一个强大的系统，具有令人印象深刻的相关性评分，轻松击败了竞争对手的结果。 
近二十年后，谷歌和其他搜索引擎开始意识到自然语言搜索的价值，并进一步发展 Ask Jeeves 试图提供的体验。 
自然语言搜索如何工作 
自然语言搜索使用一种叫做自然语言处理(NLP)的高级计算机科学技术。这个过程使用大量数据来运行统计和机器学习模型，以推断复杂语法句子的意义。随着互联网公司收集越来越多的数据，这在过去十年变得更加可行。计算能力正以指数速度增长，以便处理这些数据。 
自然语言的力量不仅来自解析问题的能力，还来自分解复合句和基于上下文的句子的意义的能力。例如，如果一位顾客问一家电子商务商店“你们为我的孩子准备了什么尺寸的 t 恤？”，搜索系统可以确定客户正在寻找儿童类别中的 t 恤衫，并且想要知道有什么尺寸的存货。如果商店有这位顾客过去的购买和搜索历史，它甚至能够确定衣服的最佳尺寸和首选款式。 
自然语言搜索不再仅仅是从个人助理那里获取天气等基本信息的工具。越来越多的消费者开始直接通过语音助手或手机语音搜索开始他们的购物和品牌探索之旅。因此，至关重要的是，公司要确保优化他们的技术和销售渠道，以确保这些消费者能够用对话语言与他们交流。 
设计自然语言搜索友好网站的四个技巧 
在为自然语言搜索优化网站时，许多网站过度关注 SEO，而没有优先考虑用户体验。然而，最终，自然语言搜索的目标是为客户提供一个有帮助的、直观的、吸引人的界面来浏览网站。以下是一些牢记用户体验的设计技巧: 
1。设计一个减少大海捞针的语音搜索引擎 
搜索系统应该利用所有可用的信息和上下文。例如，用户档案和过去的搜索可以帮助提供关于用户可能想要什么的有价值的信息。如果语音查询有点模糊，这一点特别有用，因为搜索引擎可以根据上下文来推断含义。此外，通过设置过滤器，按照预定义的类别对索引数据进行分类，有助于优化搜索，为用户提供更相关的结果。 
2。研究并理解用户如何进行对话式搜索 
虽然自然语言处理工具在理解一般含义方面非常强大，但大多数企业会发现他们的行业或领域中存在需要微调的细微差别。查看和定期分析用户搜索可以帮助揭示搜索中的这些趋势，以便可以相应地优化模型。 
3。使用自然搜索查询测试网站内容的排名 
除了审查搜索准确性之外，重要的是实际网站内容的构建方式要确保自然语言搜索引擎能够正确匹配查询。尝试运行常见的搜索查询来查看内容的排名，并逐渐调整内容来查看它对结果的影响。 
4。在网站内容中使用日常语言，回答客户需求 
在网站内容中使用对话式语言将有助于确保用户的问题和需求得到回答。对于可能无法在内容中直接找到答案的常见问题，在 FAQ 部分或专用页面中添加这些问题的答案会很有用，这样用户仍然可以找到答案。 
总之，设计一个自然语言搜索友好的网站包括使用数据为搜索提供上下文，微调搜索算法和过滤器以适应特定的业务领域，以及构建网站内容以适应对话式搜索模式。这些过程将有助于客户习惯于过渡到与您的网站更具对话性的体验。 
你的站点自然语言搜索准备好了吗？ 
随着消费者越来越多地将搜索转向对话和自然语言，企业必须跟上步伐来回答这些问题。 观看我们的网络研讨会“ 打造卓越语音体验的最佳实践 ”，了解您的网站如何为自然语言和语音搜索革命做准备 
NLU 和 NLP 有什么区别 
 
 原文：https://www.algolia.com/blog/product/what-is-natural-language-understanding-and-how-is-it-different-from-nlp/ 
 
自然语言理解，也称为 NLU，是一个术语，指的是计算机如何理解人说的和写的语言。是的，这几乎是重复的，但它值得一提，因为虽然 NLU 的建筑是复杂的，结果可能是不可思议的，但 NLU 的潜在目标是非常清楚的。 
为例，据估计每天有 3200 亿封电子邮件发送。这是大量自然语言的创造和消费，如果计算机能够更好地理解它，它可以帮助那些与电子邮件互动的人。NLU 可以确定电子邮件是否是垃圾邮件，电子邮件是否具有高优先级，或者是否有其他相关的电子邮件要与收件人共享。所有这些努力都有助于人们充分利用电子邮件。 
NLU 和 NLP 的区别 
当然，自然语言理解和自然语言处理或 NLP 之间的区别也是一个一直存在的问题。答案还是在名字里。自然语言处理是关于 处理 自然语言，或者取文本，转换成更容易让计算机使用的片段。一些常见的 NLP 任务有 去除停用词、分词或者拆分复合词 。NLP 还可以识别词类或文本中的重要实体。 
回到自然语言理解的用途，我们可以想到其他的例子，比如: 
总结新闻文章和博客文章检测网页的语言以提供翻译确定销售拜访记录中的关键主题对推文中表达的情绪进行分类服务客户服务请求的机器人为搜索请求提供正确的产品智能语音助手 
这些例子只是自然语言理解的一小部分。你能想到的任何可以从理解自然语言交流中受益的领域都可能是 NLU 的领域。 
为什么自然语言理解很重要 
自然语言的理解是复杂的，而且看起来像魔术一样，因为自然语言是复杂的。语言在很小的空间里包含了大量的信息。一个明显的例子是句子“ 奖杯放不进棕色的手提箱，因为它太大了。 “你可能马上就明白什么是太大了，但这对一台计算机来说真的很难。 
我们不能简单地编写一个程序来检查短语“was too big”并理解该短语指的是第一项。首先，因为这个短语可能改为“太大了”或“太重了”或“太大了”第二，因为有公式表明这种“规则”是不成立的，比如“棕色手提箱因为太大而不适合奖杯。”甚至有一些措辞可能会让人们感到困惑，比如“我没有把奖杯放在棕色的行李箱里，因为它太大了。”是奖杯太大放不下行李箱，还是行李箱太大带不动？ 
自然语言理解是建立在机器学习之上的 
正是因为这个原因，NLU 非常依赖机器学习。机器学习(ML)可以获取大量文本，并随着时间的推移学习模式。这可以用所谓的 分布假说 来解释，该假说认为“通过一个词所结交的朋友”，你可以了解这个词的很多信息以“帽子”这个词为例。一个 ML 模型可能会看到这样的短语，“那个人头上戴着一顶帽子”或者“我戴上帽子是为了遮挡阳光。”如果模型看到了足够多这样的短语，它就开始发现一些模式。然后，抛出这句话，“我戴了一顶棒球帽来遮挡阳光”，它可以感觉到“帽子”和“棒球帽”之间可能有相似之处。加上短语“这个男人头上戴着一顶棒球帽”，相似性会更强。 
可以想象，这些 ML 模型需要大量的数据。OpenAI 在 上训练了他们的 GPT-2 模型 15 亿个参数 ，紧接着又在 上训练了 GPT-3 175 亿个参数 。这些数据通常是从网上公开可用的数据中抓取的，但随后会在特定的数据集上进行微调。这种微调允许模型更好地理解给定的数据集。例如，微调可以帮助模型更好地理解医疗数据。 
过去十年，计算和机器学习的进步增强了 NLU 的力量和能力。我们可以预期，在未来几年内，NLU 将变得更加强大，并更多地集成到软件中。 
有关自然语言理解 应用的更多信息 ，以及了解如何在您的网站或应用中利用 Algolia 的搜索和发现 API， 请联系我们的专家团队 。 
什么是自然语言理解| NLU 定义 
 
 原文：https://www.algolia.com/blog/product/what-is-natural-language-understanding/ 
 
人和机器通常通过语音或文本界面交换信息。但是，机器能够理解——并对——人的情绪状态、微妙的语气或低调的意图做出适当的反应吗？越来越多的人认为答案是肯定的。支持这种突破性能力的科学被称为自然语言理解(NLU)。 
NLU 是被称为自然语言处理(NLP)的更广泛领域的子集，它已经在改变我们与技术的互动方式。 
NLP vs NLU 
NLP 涉及通过将自然口语或文本语言数据分解成可以分析的更小元素来处理自然口语或文本语言数据。常见的 NLP 任务包括标记化、词性标注、词汇化和词干化。NLP 主要致力于将文本转换成结构化数据。 
NLU 是自然语言处理的一个子集，它教会计算机一段文本或口头语言意味着什么。NLU 利用人工智能来识别语言属性，如情感、语义、上下文和意图。它使计算机能够理解语言中的微妙之处和变化。使用 NLU，计算机可以识别人们说同样事情的许多方式。 
一键区别 
本质上，自然语言处理的是所说或输入的内容，而 NLU 则努力理解其含义。人们写或说的意图会因为拼写错误、断句和发音错误而被扭曲。NLU 通过这些错误来判断用户的意图，即使他们的书面或口头语言有缺陷。 
因此，NLU 允许计算机软件和应用程序在响应书面和口头命令时更加准确和有用。对于开发者来说，在设计对话式搜索功能时，考虑 NLP 和 NLU 之间的区别是很重要的，因为它会影响对用户所说和所指的解释的质量。 
【NLU】实例及应用 
常用的自然语言处理和 NLU 的例子包括: 
通过智能个人助理提供客户支持和服务 
由 NLU 驱动的聊天机器人实时工作，根据用户意图和基本对话元素立即回答查询。无论是指导用户使用产品、回答支持问题，还是将用户分配给人工客户支持操作员，NLU 聊天机器人都提供了一种有效、高效且经济实惠的方式来实时支持客户。 
基于语音的智能个人助理，如 Siri、Cortana 和 Alexa，也受益于 NLU 的进步，能够更好地理解用户请求并提供更个性化的响应。 
机器语言翻译 
语言翻译——其诱人的前景是让用户用一种语言说话或输入文本，并立即获得另一种语言的准确翻译——一直是应用程序开发者的圣杯。但是实现这个目标的问题就像任何自然语言本身一样复杂和微妙。虽然这一领域远非完美，但 NLU 的应用在最近几年已经促进了巨大的进步。虽然翻译仍然很少是完美的，但它们往往足够准确，以合理的准确度传达复杂的意思。 
数据收集与分析 
越来越多的公司发现，NLU 解决方案为分析元数据(如客户反馈和产品评论)提供了强大的优势。在这种情况下，NLU 被证明比传统的方法，如手工编码，更加有效和准确。 
NLP 和 NLU 通常用于使用 5 种技术从文本中提取信息:命名实体识别、情感分析、文本摘要、方面挖掘和主题建模。一旦使用这些方法从非结构化文本中提取了信息，它就可以立即被机器学习模型使用，以提高它们的准确性和性能。 
NLU 商业用例 
电子商务 
传统的搜索引擎很适合基于关键词的搜索，但对于更复杂的查询，NLU 搜索引擎可以使这个过程更有针对性和回报。假设一个购物者询问“给我看看 500 美元以下的黑色礼服。”该查询定义了产品(连衣裙)、产品类型(黑色)、价格点(低于 500 美元)以及个人品味和偏好(经典)。 
使用 Algolia Understand 等工具的 NLU 驱动搜索将此类请求的重要部分分解开来，以准确把握客户想要什么。通过理解更复杂、更细致的搜索请求，NLU 更快地将顾客从浏览转向购买。对于那些确切知道自己想要什么的人来说，NLU 可以节省大量时间。 
聊天机器人 
聊天机器人可能是 NLU 和自然语言处理技术中最著名和最广泛使用的应用，许多部署它的公司已经获得了丰厚的回报。例如，服装零售商 Asos 使用 Facebook Messenger Chatbox 将订单增加了 300%,并获得了 250%的投资回报率，同时达到了几乎 4 倍多的用户目标。同样，化妆品巨头丝芙兰通过使用 Facebook Messenger Chatbox 将其化妆预约增加了 11%。 
电视、流媒体、视频 
支持 NLU 的流媒体和点播服务可以帮助观众找到内容，从而显著提高客户满意度和忠诚度，即使他们不确定自己到底在找什么。如果一个观众说:“给我看一些由《阿波罗 13 号》的主要演员主演的有趣的电影，”尽管很模糊，NLU 还是能推断出并生成一个符合所有这些标准的电影列表。可能是一种令人厌倦和沮丧的猜谜搜索体验，反而是一种短暂而富有成效的体验，往往会导致购买或租赁销售。 
记者和出版商 
通过使用自然语言与内容档案进行交互，NLU 可以极大地帮助记者和出版商从内容深处提取复杂问题的答案。 
Algolia Answers 等工具支持自然语言交互，可快速找到现有内容，减少记者撰写报道所需的时间。读者还可以受益于 NLU 驱动的内容访问，这有助于他们在一系列来源之间建立联系，并在几秒钟内找到非常具体的问题的答案。 
客户服务与支持 
像 Alexa 和 Siri 这样的语言交互平台已经广泛使用 NLU 技术来处理大量的用户请求，从产品搜索到诸如“我该如何退货？”以及“我的保修期有多长？”客户服务和支持应用程序非常适合让 NLU 在制造商和经销商最少参与的情况下提供准确的答案。 
NLU 是问答系统的核心，问答系统增强了企业中的语义搜索，并将员工与业务数据、图表、信息和资源联系起来。它也是客户支持应用程序的核心，这些应用程序回答大量、低复杂性的问题，重新路由请求，将用户引向手册或产品，并降低全面的客户服务成本。 
博彩 
在线游戏已经变得极其复杂，以至于玩家不断地参考规则书和游戏指南来寻找特定问题的答案。此外，游戏通常以极快的速度进行，玩家希望立即得到诸如“我如何在这个游戏中击败 3 级？”这样的竞争焦点问题的答案以及“这个游戏里的魔法药水在哪里可以找到？” 
在战斗过程中，玩家可以求助于 NLU 驱动的聊天机器人来获得他们需要的信息，而不是翻阅厚厚的纸质手册，不会错过怪物攻击或射线枪爆发。 
阿洛利亚对 NLU 的态度 
NLU 和 NLP 已经在 Algolia 的下一代搜索工具的开发和推出中发挥了核心作用。例如: 
Agolia Understand是一款强大的多功能 NLU 驱动应用，将 NLU 和人工智能引入电子商务搜索，以提高客户参与度，并将访客转化为买家。Algolia Answers旨在理解最具挑战性的自然语言搜索，以更好地对文章进行排序，并从内容深处提取答案，使其成为出版商和记者的一个特别有价值的工具。 
有关 NLU 和 NLP 的 Algolia 方法的更多信息，请参见: 
神经搜索 101 
 
 原文：https://www.algolia.com/blog/ai/what-is-neural-search-and-how-does-it-work/ 
 
想到搜索，你首先想到的是什么？ 
你可能会想象有人在一个网站上，在搜索框中输入一个查询，他们的搜索结果会根据搜索引擎在其网络架构中将关键词与正确的内容进行匹配而出现。毕竟， 否则 搜索引擎如何识别信息，而不是通过即时匹配关键词？ 
基于关键词的搜索永远是现状。但现在，新技术开始改变搜索应用程序的幕后运作。关键字作为在网上追踪信息的唯一途径的日子正在消逝。现代新的搜索方式正被用于关键字搜索，以创建更准确的搜索结果，并为用户提供更好的搜索体验。 
以下是全球公司目前使用的最新发展的搜索方法概要: 
传统关键词搜索 
用老派的 关键词搜索——你已经认识并喜爱的那种——你肯定会得到大量的搜索结果。诚然，必须提前为搜索引擎创建规则，以便它能够有效地理解数据，并在所有可用的存储库中为您提供最佳信息。根据你的关键词匹配的方式，你的搜索结果可能并不总是超级准确或者按照你的目的排名最优，尽管它们会在眨眼之间为你展示出来，所以你可以方便地点击看起来很好的选择。 
【人工智能搜索】 
随着人工智能(AI)领域的重大进步，传统的关键词搜索方法和功能一直在稳步发展。大多数搜索引擎用例仍然依赖于关键字匹配方法；其他人则更积极地为新来者让路。为了提高搜索结果的质量，AI-powered search加入了基线关键词搜索。有了人工智能，通过收集和考虑用户活动的数据，搜索结果的相关性得到了提高。AI 搜索利用了一系列现代技术，包括 机器学习 和 自然语言处理 (NLP)，以更好地了解每个用户需要定位什么信息。 
矢量搜索 
许多较新的基于人工智能的搜索引擎整合了 矢量搜索 ，它使用机器学习模型来检测 语义关系 。 
搜索向量是文本的数学表示(这适用于搜索查询和索引信息，如产品信息和网页)。向量让搜索引擎理解输入文本的 上下文 。在将单词转换成向量的过程中，意义被编码。 
例如，一个基于关键词的搜索引擎不一定知道“温暖”、“热浪”和“夏天”是相关的。相比之下，基于向量的搜索引擎确实理解这些类型的单词关系，这使它们能够胜过单独的关键字搜索。 
由于增加了这个维度，向量成为同义词自动化、documents‍聚类、检测查询中的含义和意图以及搜索结果排名等任务的首选。 
与关键词搜索引擎的输出相比，矢量搜索可能速度较慢，扩展性较差，而且价格昂贵。 
神经搜索 
然后是神经搜索，最热门的新兴搜索方法，准备在搜索界掀起风暴。这听起来像是“有头脑”的东西，事实上，它包含了基于算法组件的奇特、互连节点的“思维”，这些组件被称为 神经网络 (也称为人工神经网络；ANNs)。神经搜索结合了向量的力量，以快速的性能和自学习能力准确地表示数据。 
神经搜索的出现是大数据推出的一个成果，没有大数据，搜索引擎就没有足够的信息来进行准确的分析。我们社会积累的可用现代数据集允许神经网络筛选大量信息(其中大部分是非结构化数据)，并将其吸收用于广泛的用例。 
理解语义 
神经搜索有助于提高 语义 理解人们在搜索时想要什么。它利用人工智能来检测人、内容和数据之间的关系，以及用户兴趣与当前和过去的搜索查询之间的联系。 
它还可以从数据中学习任意复杂的表示，并利用数据点之间的关系。在理解搜索查询的语义时，这给了它一个优势。 
由于能够利用语义和关键词检索方法，对搜索者意图的更好理解转化为提供更相关的结果。 
灵活、独立、时刻学习 
神经搜索也具有适应性:它可以用于任何类型的数据，包括图像、视频、音频和 3D 信息。例如，表示图像的向量可以包含关于其形状、颜色和内容的信息，这些信息对于将图像与其他图像进行匹配都是非常有用的数据。 
神经搜索促进了相关搜索结果的交付，同时降低了对持续管理的需求。例如，它消除了大多数同义词和语言规则的必要性。这意味着网站搜索经理不需要照看和微调他们的搜索功能，添加同义词，制定复杂的搜索规则，提出关键字填充标题，或插入额外的语言包。 
你可以从一个训练有素的神经网络开始，而不是依赖于为机器学习模型制定和更新规则，然后模型可以逐渐得到更好的“教育”由于神经搜索基于学习算法，它可以随着时间的推移适应不断变化的现实世界条件(通过微调)。 
简而言之，神经搜索就是: 
擅长理解搜索查询强大的灵活非常适合在系统训练数据和输入不断变化的情况下使用，例如在 电子商务 中 
神经网络和机器学习 
神经网络也被认为是机器学习的一种形式——人工智能(AI)的一个分支，它使用数据和算法来复制人类的学习方式。 
有了机器学习，数据会随着时间的推移变得更加精确。例如，如果某人搜索一个主题，然后开始另一个看似相关的搜索，面向机器学习的系统可以假设这两个搜索是相关的，并建议搜索者检查包含这两个主题的材料。随着处理的附加查询的集成，搜索引擎学习并更好地在下一次提供准确的搜索结果。 
子集:深度学习 
深度学习是机器学习的一个子集(显然，如果你想知道的话，还有一种叫做“浅层学习”的东西)。 
深度学习的概念是基于人类对神经网络如何运作的理解而诞生和发展起来的。1943 年，科学家根据人脑中神经网络的运作方式创建了一个计算机模型。他们的目标是利用算法和数学来复制人类的思维过程。 
今天，深度学习的计算算法为各种编程方法提供了一个强大的框架。 
深度学习和神经网络有什么关系？而它对搜索质量的影响呢？ 
神经搜索引擎依靠深度学习算法为用户提供准确的搜索结果以及出色的搜索体验。 
搜索引擎由索引器、查询处理器、检索算法和排序算法组成。与神经搜索引擎的最大区别是检索部分，它使用机器学习来确定查询和搜索结果之间的相似性度量。这样，神经搜索引擎实际上可以理解概念 ，这可以带来更好的性能，特别是对于关键词/结果对不完全匹配的长尾查询。 
深度神经网络 
深度学习、神经网络……不可避免地，我们会进入词汤变异的下一个变异: 深度神经网络。 
深度神经网络就像它们听起来的那样:注入了更多深度的人工神经网络:在输入和输出层之间插入了额外的层。 
深度学习的重点是机器如何利用深度神经网络计算模型进行学习。 
由于在基准问题和应用中的优异表现，多层深度神经网络的概念最近已经成为机器学习中非常成功和热门的研究课题。 
神经搜索简单来说就是深度神经网络驱动的信息检索。神经搜索利用深度神经网络(DNN)的能力来构建搜索系统的每个组件。 
有什么最新的思维？ 
这总结了各种在线搜索技术的最新进展。 
那么，相关搜索下一步将走向何方？回顾一下大约十年前，当全文关键词和向量搜索技术的合并使优化成为可能时，将会蓬勃发展，确保比传统搜索平台或向量搜索目前单独提供的搜索结果更准确。 
与此同时，如果你想在下一个十年的某个时候更快地改进你的网站搜索，你仍然可以做一些事情，就像现在的 。T13 
检出神经搜索 
那是什么东西？看看 Algolia 新收购的技术neural search，它利用矢量搜索与 神经散列 相结合，为您的用户提供快速、准确的搜索结果。它允许我们在一个 单一 API 中结合传统关键字搜索的速度和神经搜索的准确性。 
通过这一改变游戏规则的收购，我们现在可以为您的网站或应用程序提供真正的混合搜索，结合全文人工智能关键词搜索和基于向量的语义搜索来处理每个用户查询。是的，你可以在毫秒内为你的用户或客户获得 最先进的搜索 结果，无论规模还是查询吞吐量。 
了解如何利用我们先进的搜索算法来提高贵公司的搜索和底线指标，现在就开始吧。联系我们的 团队 ，成功结束您对正确搜索解决方案的搜索！ 
电子商务营销:科学与艺术 
 
 原文：https://www.algolia.com/blog/ecommerce/what-is-online-retail-merchandising-an-introduction/ 
 
最近在电子商务网站上购物过吗？如果是这样，你知道一个顺畅的网上购物体验不是可有可无的。如果你打算逗留一会儿，也许会买一件东西，那么对你来说，做任何事情都一定非常容易。 
同样，作为一名电子商务商人，你希望你的目标客户享受在你的电子商务商店度过的平稳体验。当他们登陆你的主页或产品详细信息页面时，你希望他们能够专注于你的产品，并开始热情地看伟大的产品图片、阅读产品描述和浏览产品评论。他们的购买之旅不应该是一项复杂的任务，否则你可能会失去他们的兴趣和你可能获得的利润丰厚的零售销售。 
在线零售商品展示是打造顺畅购物之旅的关键。他们让购物者探索你的产品，发现他们想要的东西，购买它们，然后也许以后会购买更多，因为第一次的体验是如此愉快( 和 他们喜欢你的产品)。 
一门科学一门艺术 
在线零售营销(也称为电子商务营销)与零售网站上的产品战略布局有关。有效的营销既是一门科学也是一门艺术:数据驱动的布局是科学，产品布局是艺术。它显然与传统的实体店现场营销有很大不同——其元素包括橱窗展示和商店设计、彩色标牌、穿着考究的人体模特、奢华的产品展示和试衣间——然而，它仍然符合一些店内原则:漂亮的店面、展示不同类型产品的直观商店布局、强大的品牌以及智能分类。 
然而，网上销售更多的是关于而不是你在网站上放产品的地方和你的各种分类页面的外观。它需要对你的潜在客户的购物需求和行为有深入的了解。 
作为一门学科，在线零售营销已经受到技术进步、消费者行为变化和电子商务平台功能现代化的影响。随着网上购物变得越来越流行，网上零售商的竞争也越来越激烈，他们需要想出新的方法脱颖而出，吸引顾客，并保持领先地位，包括店内销售 和 网上销售。 
如果利用得好，在线零售营销还可以融合虚拟和实体购物体验，可以通过多种渠道为消费者创造一致、无缝的品牌体验(见证亚马逊的品牌)。在零售商的电子商务业务工具包中，简化 全渠道 是构建引人入胜、个性化的在线零售体验的重要流程，可促进商店销售并激发客户忠诚度。 
电子商务营销策略的好处 
作为更大的零售营销计划的一部分，在线零售营销是推动品牌成功不可或缺的。当你做对了，你可以收获这两大好处: 
更顺畅的客户互动 
推动高质量的用户参与和购物体验可以为你的购物者和你的 电子商务网站 带来显著的成功。当你能吸引顾客的欲望，让他们购买他们需要的东西(此外，你可能会向他们推销与他们购物车里的东西相关的商品，或者在结账时向他们交叉销售补充产品)，让他们想回来，并把他们变成快乐的长期顾客时，你通常可以预期一些相当丰厚的回报。 
通过应用在线零售营销的原则来提供更好的用户体验——从登录页面到产品页面，以及沿途的奖励体验——您还可以提高品牌的声誉。然后，当你的顾客想到你的公司时，他们会记得找到和购买他们想要的东西是多么容易；他们很高兴购买并能够使用该产品。但如果做错了，代价可能会很大: 86%的消费者在两次糟糕的网站体验后很可能会离开一家公司。 
收入大增 
归结起来，愉快、轻松的网上购物和顾客体验必然会带来更高的转化率。如果顾客问:“我能找到我要找的东西吗？”并且可以肯定地说，收入即将到来。此外，您满意的客户可能会再次进行网上购物，因为根据 Salesforce 的调查，89%的人在获得积极体验后更有可能再次购买。 
如何钉钉线上零售商品销售 
准备好开始构建或改造你的在线零售营销战略了吗？首先，问自己这些问题: 
你的品牌塑造如何？ 
你的网站品牌告诉你的顾客什么？网上销售空间显然与零售商店完全不同，但理想情况下，它应该与购物者在实体店销售区的体验相似。随着时间的推移，强大的品牌会建立回头客，因为他们知道他们在你的网站上会有什么样的体验，所以他们会毫不犹豫地首先访问你。 
你的网站好看吗？ 
在零售网站上，颜色、标识和风格都是创造吸引顾客的体验的一部分，让顾客感到舒适熟悉。引人入胜的视觉营销技术吸引用户，并促进成功的导航。基于你的团队的编码和设计能力，你可能会遇到一些挑战，但你仍然可以优先提供专业的产品销售体验，让你的顾客满意。 
你的搜索栏状态好吗？ 
你的搜索栏在客户体验中扮演着重要角色。目的是帮助你的客户快速找到他们想要的东西，所以你的搜索栏需要容易定位，并连接到一个强大的搜索引擎，可以几乎实时地提供快速、可预测的网站搜索结果。 
你的产品分组好了吗？ 
就像你在店内展示一样，通过在你的在线商店中为多种商品创建建议的“套件”,你可以为顾客的购物之旅增添便利。例如，如果你是一个极限运动品牌，你可能会把登山夹克、登山靴和攀岩装备归为一类。即使购物者只寻找组合中的一部分，他们也会知道在准备好的时候去哪里买其他的商品(可能是在某些 SKU 降价之后)。做得好的话，直观的产品分组促进了销售，一些在线零售商的平均订单价值(AOV)增长了 20%。 
个人说明 
此外，个性化的互动通过向人们展示他们可能特别喜欢的商品来增强购物之旅。个性化产品推荐和其他以客户为中心的路标由一种 算法 生成，该算法考虑了客户的数据点，如浏览和购买历史。可以肯定地说， 个性化营销 有助于赢得客户体验。个性化有助于确保购物者的快乐，从而获得更高的收入。研究压倒性地证实了这一点。例如，Salesforce 发现，76%的顾客说他们会从提供个性化体验的品牌那里购买，其中 78%的人可能会重复购买。 
现在需要结果 ？T37 
想要尽快为您的在线购物者提供无缝的用户旅程吗？在 Algolia，我们提供了 工具 ，可以帮助你简化用户体验以增加销售，潜在的大幅增加。 
不要只相信我们的话。我们已经让 零售巨头 提供最佳搜索和发现，这带来了一些非常惊人的结果。从改善塞曼的搜索体验到将 Lacoste 的销量提高 150%以上，我们有着辉煌的业绩。这意味着你可以自信地运用我们的销售技巧，将你的电子商务销售提升一个档次。 
好奇？ 联系我们 让我们开始您的 在线零售营销 改进之旅。 
什么是搜索即服务？ 
 
 原文：https://www.algolia.com/blog/ux/what-is-search-as-a-service/ 
 
“搜索即服务”是一种使用软件即服务(SaaS)模式的网站搜索。 
通常在 SaaS，许多不同的服务通过云按需提供，现收现付。有了搜索即服务，公司可以更快地创新，因为他们不必担心托管、运营和维护。相反，他们可以专注于完善最终用户体验和优化相关性。搜索即服务提供了一系列好处，包括搜索分析功能、直观的搜索 UI 以及用户可以定期即时获得的创新，因为该服务托管在云上，所以其效率超过了典型的网站搜索。 
搜索即服务模式对于帮助企业持续改进网站搜索功能和满足客户需求至关重要，而无需大量的 IT 投资。 
搜索即服务的好处 
当 43%的网站访问者立即进入搜索框时，出色的现场搜索体验意味着更好的参与度和更多的转化。 
网站搜索不仅能快速将用户与他们的需求联系起来，还能通过提高转化率和客户保持率来帮助业务增长。但是最好的搜索即服务解决方案提供的 远不止是一个搜索栏 。它们让探索成为可能，将访问者与他们甚至不知道自己需要的东西联系起来。 
搜索与在线体验如此紧密地结合在一起，如果没有它，一个网站很难与之竞争。然而，专有搜索解决方案的构建成本很高，难以维护，并且需要大量的 IT 投资来测试和实施改进。凭借 云架构 的效率，将开发、部署和创新外包给搜索专家通常更容易，而企业则专注于补充营销和产品开发工作。 
搜索即服务模式为企业提供了多种优势: 
敏捷 
搜索即服务解决方案创建了一个更加简化的运营环境。搜索提供商负责保持运行的细节，因此一旦实现，内部团队就不必花费大量时间进行故障排除、测试、解决后勤问题或创建新功能。事实上，搜索即服务为企业提供了自助服务能力，因此 it 可以将更多的时间用于实现组织目标，而将更少的时间用于调整搜索。 
拥有/运营成本 
有了搜索即服务，企业承担的基础设施和运营成本就会减少。他们不必为搜索工具雇佣专门的运营团队，这简化了技术堆栈，加快了工作完成的速度。这些因素从根本上为开发人员创造了更好的体验。 
性能和速度 
快速是伟大搜索体验的基础。搜索即服务依赖于可靠的基础设施，如 分布式搜索网络 ，即使在业务增长和客户需求变化的情况下，也能大规模提供闪电般的结果。搜索即服务允许网站以最少的开发投资处理更复杂的搜索需求，如替代语言搜索、允许输入错误、个性化等等。 
可靠性和可扩展性 
随着业务的增长，搜索功能也必须随之增长和发展。内部搜索解决方案要求开发人员管理系统备份，并解决 网络延迟 、服务器停机和/或搜索无响应的问题。另一方面，搜索即服务允许企业将支持可靠性和可扩展性的基础设施外包给第三方专家。 
多通道用途 
搜索即服务还允许组织用一个搜索解决方案支持多个渠道，而不是为每个渠道构建新的解决方案。因此，无论用户使用哪种搜索渠道，包括桌面 web、移动 web、移动应用程序和语音，都可以快速获得相关信息。 
范围 
大多数公司在自己的网站上都有多个部门、资产和数据仓库。一个好的搜索服务工具可以很容易地跨越所有这些，在一个直观、易于理解的界面上给用户一个关于你的所有内容、产品和服务的统一视图。 
分析 
竞争性搜索即服务解决方案的主要优势之一是能够收集用户行为，并使网站所有者能够根据这些数据采取行动。 网站搜索分析 提供了对各种页面和内容性能的可见性，揭示了哪些资源是最有影响力的，以及为什么。Analytics 还能洞察转化率、关键词、查询、CTR 的有效性等等。这些信息由“搜索即服务提供商”以直观的仪表板形式提供，有助于做出有关产品和营销策略的更大决策。通常，公司可以发现一些未满足的需求，这些需求代表着巨大的商机。 
定制&灵活性 
有了强大的搜索服务提供商，公司可以让业务目标和业务特定需求驱动搜索配置和搜索 UI 设计。定制确保搜索与业务案例和挑战最匹配。公司可以调整排名因素、优先级和其他元素，以不断改进搜索工具来满足客户的需求。 
个性化 
个性化是 未来的搜索 。搜索即服务提供商拥有数据、分析和基础设施来支持大规模个性化，这是内部解决方案和通用搜索插件通常无法做到的。 
用户越觉得搜索结果迎合他们的需求，就越有机会交叉销售和追加销售相关内容和产品。组织可以同时针对多种语言进行优化，而不是一次构建一种语言，然后再逐步构建另一种语言。每次，他们还可以针对变化、复数和单数情况以及其他相关因素进行优化，以提供相关的结果。 
视觉设计 
当你 构建自己的搜索功能 时，你还得投资工具的 UX 和视觉设计。通过搜索即服务，搜索提供商提供了与您的网站无缝协作的视觉设计和模板。在该工具中，您可以调整这些视觉效果并测试不同的图形元素，以获得最吸引人的视觉布局。您可以从与您的搜索相互影响的不同设计元素中进行选择，如分页、过滤器和方面，以确保客户拥有直观、顺畅的体验。 
功能 
效率是有效功能的结果。作为服务提供商的搜索在后端功能上投入了大量资金——从及时的 索引 到对查询的快速响应，再到 联合搜索——因此您可以专注于调整和优化自己的搜索结果。虽然默认的 CMS 或电子商务平台搜索结果是通用的，通常是无优先级的列表，但优化的搜索即服务解决方案为每个网站访问者提供了高度相关的定制结果。 
搜索即服务如何运作 
一旦一家公司选择了 他们首选的网站搜索解决方案 ，就该让服务在网站上运行了。要实现搜索即服务解决方案，客户首先需要让云提供商能够访问格式正确的数据。 
供应商通过其搜索引擎处理这些数据，因此客户可以通过 API 提供的软件访问搜索结果。该处理包括创建索引，或者指示内容包含什么的标记，其可被搜索引擎访问。当用户查询或在搜索栏中输入单词开始搜索时，该引擎会筛选索引以快速向用户提供结果。那些搜索结果根据与查询的相关性 被分类为 。 
搜索供应商维护所需的硬件、计算、内存和处理能力。对于其他解决方案，客户负责购买和维护基础架构，如服务器、操作系统和补丁程序。将搜索作为服务的 API 方法不需要这些投资。 
谁需要搜索即服务？ 
任何拥有可搜索内容或产品的网站都可以从“搜索即服务”中受益。 
搜索即服务在许多不同的行业中尤其有用。在媒体或新闻业，每天或每周都会产生大量的新内容，因此访问者很难找到最相关的信息。在零售和电子商务中，不断变化和/或扩展的产品目录可能很难让用户浏览。在许多情况下，组织依赖多个搜索引擎来搜索不同的内容类型，这增加了开支，给内部开发团队带来压力，并导致用户体验不佳。 
搜索即服务有助于缓解这些问题。通过一个工具，公司可以帮助用户找到广泛的内容，从存档的文章到新的视频，再到最新的调查报告。由于频繁的网站索引，结果会不断刷新，媒体网站可以运行分析来查看哪些内容是最重要的，哪些内容缺口需要填补。 
和搜索即服务 
搜索即服务是一种云产品，让用户、开发者和企业的搜索变得更加容易。 
Algolia 是尖端的搜索即服务，旨在规模化运作。我们在高可用性架构、持续创新和个性化体验方面的投资有助于您随时为每位用户提供出色的搜索体验。 
了解更多关于 购买与构建您的搜索工具 或 观看演示 了解 Algolia 的实际应用！ 
在浏览、发现和推荐的时代，搜索相关性是什么？ 
 
 原文：https://www.algolia.com/blog/product/what-is-search-relevance-in-the-era-of-browsing-discovery-and-recommendations/ 
 
良好的搜索相关性是指找到正确的信息，以及 将客户和在线业务置于平等的地位。搜索相关性不仅试图满足顾客在他们的搜索查询中表达的 意图 ，它还使在线商家能够最有利地呈现他们的产品和服务，而不打破他们的顾客的相关性期望。 
在本文中，我们将看到这种更广义的搜索相关性如何扩展到包括 浏览 和 发现 ，这些活动不一定需要从搜索框开始。 
有什么关联？或者更好地说，相关性试图完成什么？ 
相关性算法试图(1)将查询文本与一些潜在内容进行匹配，同时(2)预测并满足客户和企业的需求。这个定义是一个相当好的总结，在某些方面很简单，但承载着大量未言明的潜力和力量——或复杂性，这取决于你看到的是半满还是半空。 
推/拉的关联性 
如果我们把 搜索 关联简化为关于 寻找信息 ，那么更一般意义上的 关联 可以指除此之外的东西。我们的文章对 搜索和浏览 讨论了 拉动 和 推送 内容的优点，即搜索和浏览。用户 在主动搜索并期望找到合适信息的时候拉 内容。一个业务 在用户搜索(或浏览)公司的各种数字界面时，当它主动建议或浮出信息时，就会推送 内容。 
我们来对比一下两者: 
| 拉取/搜索内容 | 推送/浏览内容 | | 
将字母、单词和短语与内容进行匹配，并过滤以向下钻取权衡某场比赛的强弱(下面讨论)结果排名:根据相关性对结果进行排序(首先显示最强的匹配)添加自定义排名，根据受欢迎程度、利润或其他此类业务指标以及业务需求进行排序 
| 
通过营销活动推广您的产品和服务管理内容和媒体使用用户资料个性化搜索结果根据快速变化的市场趋势和用户行为动态调整结果推荐相关和“经常一起购买”的商品还有更多 
| 
拉和推是一"
KDNuggets 博客中文翻译（一）,2024-10-27,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
使用 Python、SpaCy 和 Streamlit 构建结构化金融新闻源 
 
 原文：www.kdnuggets.com/2021/09/-structured-financial-newsfeed-using-python-spacy-and-streamlit.html 
 
评论 
由 Harshit Tyagi，数据科学讲师 | 导师 | YouTuber 
 
自然语言处理的一个非常有趣且广泛使用的应用是命名实体识别（NER）。 
从原始和非结构化数据中获取洞察至关重要。上传文档并从中提取重要信息被称为信息检索。 
信息检索一直是自然语言处理中的一项主要任务/挑战。命名实体识别（或命名实体链接）在多个领域（如金融、药品、电子商务等）中用于信息检索目的。 
在这个教程帖子中，我将展示你如何利用 NEL 开发一个自定义的股票市场新闻源，列出互联网上的热门股票。 
先决条件 
没有特别的先决条件。你可能需要对 Python 和 NLP 的基本任务（如分词、词性标注、依存解析等）有一些了解。 
我将详细介绍重要的部分，所以即使你是完全的初学者，也能理解发生了什么。 
所以，让我们开始吧，跟着做，你将会有一个简洁的股票新闻源，你可以开始研究了。 
你需要的工具/设置： 
 Google Colab 用于数据和 SpaCy 库的初步测试和探索。使用 VS Code（或任何编辑器）来编写 Streamlit 应用程序。股票市场信息（新闻）的来源，我们将对其进行 NER 和后续的 NEL。需要一个虚拟的 Python 环境（我使用的是 conda），以及 Pandas、SpaCy、Streamlit 和 Streamlit-Spacy（如果你想展示一些 SpaCy 渲染结果的话）等库。
目标 
本项目的目标是学习并应用命名实体识别，提取重要的实体（在我们的示例中为上市公司），然后使用知识库（Nifty500 公司名单）将每个实体与一些信息进行链接。 
我们将从互联网上的 RSS 源获取文本数据，提取热门股票的名称，然后获取这些股票的市场价格数据，以在对这些股票采取任何操作之前测试新闻的真实性。 
 
 注意：命名实体识别可能不是最前沿的问题，但在行业中有许多应用。 
 
继续使用 Google Colab 进行实验和测试： 
第一步：提取热门股票新闻数据 
为了获得可靠的真实股市新闻，我将使用Economic Times和Money Control RSS 源进行本教程，但你也可以使用/添加你所在国家的 RSS 源或 Twitter/Telegram（群组）数据，以使你的源更具信息性/准确性。 
机会是巨大的。本教程应作为应用 NEL 构建不同领域应用程序的垫脚石，解决不同类型的信息检索问题。 
如果你查看 RSS 源，它看起来像这样： 
 
economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms 
我们的目标是从这个 RSS 源中获取文本标题，然后使用 SpaCy 提取标题中的主要实体。 
标题位于 XML 的标签中。 
首先，我们需要捕获整个 XML 文档，可以使用**requests**库来完成。确保你在 colab 的运行时环境中安装了这些包。 
你可以运行以下命令来从 colab 的代码单元中安装几乎任何包： 
!pip install <package_name>
1 
发送一个GET请求到提供的链接以获取 XML 文档。 
import requestsresp = requests.get(""https://economictimes.indiatimes.com/markets/stocks/rssfeeds/2146842.cms"")
1 
运行单元以检查响应对象中得到的内容。 
它应该会给你一个带有 HTTP 代码 200 的成功响应，如下所示： 
 
现在你有了这个响应对象，我们可以将其内容传递给 BeautifulSoup 类来解析 XML 文档，如下所示： 
from bs4 import BeautifulSoupsoup = BeautifulSoup(resp.content, features='xml')
soup.findAll('title')
12 
这将给你一个包含所有标题的 Python 列表： 
 
图片由作者提供 
太棒了，我们已经得到了文本数据，我们将使用 NLP 从中提取主要实体（在本例中是上市公司）。 
现在是将 NLP 应用到实践中的时候了。 
第 2 步：从标题中提取实体 
这是激动人心的部分。我们将使用来自**spaCy**库的预训练核心语言模型来提取标题中的主要实体。 
关于 spaCy 和核心模型的简要介绍。 
spaCy是一个开源 NLP 库，以超快的速度处理文本数据。它是 NLP 研究中的领先库，被广泛用于企业级应用中。spaCy 以其适应问题的能力而闻名，并且支持超过 64 种语言，能够很好地与 TensorFlow 和 PyTorch 兼容。 
说到核心模型，spaCy 具有两个主要类别的预训练语言模型，这些模型在不同大小的文本数据上进行训练，以提供最先进的推断。 
 核心模型——用于通用的基础 NLP 任务。起始模型——用于需要迁移学习的特定应用程序。我们可以利用模型的学习权重来微调我们的自定义模型，而无需从头开始训练模型。
由于我们在这个教程中的用例是基本的，我们将继续使用 en_core_web_sm 核心模型管道。 
那么，让我们将它加载到笔记本中： 
nlp = spacy.load(""en_core_web_sm"")
1 
注意： Colab 已经为我们下载了这个模型，但如果你尝试在本地系统中运行，你需要使用以下命令首先下载模型： 
python -m spacy [download](https://spacy.io/api/cli#download) en_core_web_sm
1 
en_core_web_sm 基本上是一个针对 CPU 优化的英语管道，具有以下组件： 
 tok2vec — 将令牌转换为向量（对文本数据进行标记化），tagger — 为每个令牌添加相关的元数据。spaCy 利用一些统计模型来预测每个令牌的词性（POS）。更多信息请参见 文档。parser — 依赖解析器在令牌之间建立关系。其他组件包括 senter、ner、attribute_ruler、lemmatizer。
现在，为了测试这个模型能为我们做什么，我会将一个单独的标题传递给实例化的模型，然后检查句子的不同部分。 
# make sure you extract the text out of <title> tagsprocessed_hline = nlp(headlines[4].text)
1 
该管道执行从标记化到命名实体识别（NER）的所有任务。这里我们首先得到令牌： 
 
图片来源于作者 
你可以使用 pos_ 属性查看标记的词性。 
 
图片来源于作者 
每个令牌都带有一些元数据。例如，“Trade”是专有名词，“Setup”是名词，“:`” 是标点符号，等等。所有标签的完整列表可以在 这里 找到。 
然后，你可以通过查看依赖图来了解它们之间的关系，使用 dep_ 属性： 
 
图片来源于作者 
这里，“Trade”是复合词，“Setup”是根词，“Nifty”是同位语修饰语。再次说明，所有语法标签可以在 这里 找到。 
你还可以使用以下的 displacy render() 方法来可视化令牌之间的关系依赖： 
spacy.displacy.render(processed_hline, style='dep',jupyter=True, options={'distance': 120})
1 
这将生成如下图表： 
 
图片来源于作者 
实体提取 
要查看句子的主要实体，你可以在同一代码中将 **'ent’** 作为样式传递： 
 
图片来源于作者 — 我使用了另一个标题，因为我们上面用的那个没有任何实体。 
我们对不同的实体有不同的标签，例如“day”有 DATE 标签，“Glasscoat”有 GPE 标签，可以是国家/城市/州。我们主要寻找带有 ORG 标签的实体，这些标签能给我们公司、机构、组织等信息。 
我们现在能够从文本中提取实体。让我们来提取所有标题中的组织实体。 
这将返回如下的公司列表： 
 
图片来源于作者 
很简单，对吧？ 
这就是 spaCy 的魔力！ 
下一步是查找所有这些公司在知识库中，以提取该公司的正确股票符号，然后使用像 yahoo-finance 这样的库提取市场详情，如价格、收益等。 
第三步 — 命名实体链接 
了解市场上哪些股票在活跃，并在你的仪表板上获取其详细信息是这个项目的目标。 
我们有公司名称，但为了获取它们的交易详情，我们需要公司的交易股票符号。 
由于我在提取印度公司的详细信息和新闻，我将使用Nifty 500 公司（一个 CSV 文件）的外部数据库。 
对于每家公司，我们将使用 pandas 在公司列表中查找它，然后使用yahoo-finance库捕获股票市场统计数据。 
图片由作者提供 
你应该注意到的一点是，我在将每个股票符号传递给yfinance库的Ticker类之前，添加了一个.NS后缀。这是因为印度 NSE 股票符号在yfinance中以.NS后缀存储。 
之后，流行的股票将会出现在如下的数据框中： 
 
图片由作者提供 
太好了！这是不是很棒？这样一个简单却深刻的应用程序，可以帮助你找到正确的股票方向。 
现在，为了使其更易于访问，我们可以使用 Streamlit 将刚刚编写的代码创建为 Web 应用程序。 
第四步 — 使用 Streamlit 构建 Web 应用程序 
该是移动到编辑器，创建一个新项目和虚拟环境来进行 NLP 应用程序的时候了。 
开始使用 Streamlit 对于这样的演示数据应用程序非常简单。确保你已经安装了 streamlit。 
pip install Streamlit
1 
现在，让我们创建一个名为 app.py 的新文件，并开始编写功能代码以准备应用程序。 
在顶部导入所有所需的库。 
import pandas as pdimport requestsimport spacyimport streamlit as stfrom bs4 import BeautifulSoupimport yfinance as yf
1 
给你的应用程序添加一个标题： 
st.title('Buzzing Stocks :zap:')
1 
通过在终端中运行streamlit run app.py 来测试你的应用程序。它应该会在你的 Web 浏览器中打开一个应用程序。 
我添加了一些额外的功能，以从多个来源捕获数据。现在，你可以将你选择的 RSS 源 URL 添加到应用程序中，数据将被处理，趋势股票将在数据框中显示出来。 
要访问完整的代码库，你可以查看我的仓库： 
GitHub - dswh/NER_News_Feed 
你可以添加多个样式元素、不同的数据源和其他类型的处理，以提高其效率和实用性。 
我当前状态下的应用程序看起来像横幅中的图片。 
如果你想逐步跟随我，请在这里观看我编写这个应用程序的过程： 
下一步！ 
除了选择一个金融应用案例外，你也可以选择其他你喜欢的应用。医疗保健、电子商务、研究等等。所有行业都需要处理文档并提取和链接重要实体。尝试另一个想法。 
一个简单的想法是提取研究论文中所有重要的实体，然后使用谷歌搜索 API 创建一个知识图谱。 
此外，如果你想将股票新闻推送应用提升到另一个水平，你还可以添加一些交易算法来生成买卖信号。 
我鼓励你放飞你的想象力。 
如何与我联系！ 
如果你喜欢这篇文章并想看到更多类似内容，你可以订阅我的新闻通讯或我的 YouTube 频道，我会继续分享这样有用且快捷的项目。 
如果你是刚开始编程的人或想进入数据科学或机器学习领域，你可以查看我在WIP Lane Academy的课程。 
感谢 Elliot Gunn。 
简介：Harshit Tyagi 是一位具有综合网页技术和数据科学（即全栈数据科学）经验的工程师。他已经指导了超过 1000 名 AI/网页/数据科学的求职者，并设计了数据科学和机器学习工程学习课程。此前，Harshit 与耶鲁大学、麻省理工学院和加州大学洛杉矶分校的研究科学家一起开发了数据处理算法。 
原文. 经许可转载。 
相关： 
 2021 年数据科学学习路线图机器学习如何利用线性代数解决数据问题学习数据科学和机器学习：路线图后的第一步
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业道路。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能。 
 3. 谷歌 IT 支持专业证书 - 支持你的组织进行 IT 维护。 
 
更多相关内容 
 构建机器学习模型的结构化方法使用 spaCy 进行 NLP 入门使用 spaCy 进行自然语言处理使用 DAGsHub 将 Streamlit WebApp 部署到 Heroku使用 HuggingFace Pipelines 和 Streamlit 回答问题使用 Streamlit 进行 DIY 自动化机器学习
=人工智能= 
在边缘 AI 应用中最大化性能 
 
 www.kdnuggets.com/maximize-performance-in-edge-ai-applications 
 
随着 AI 从云端迁移到边缘，我们看到这项技术在不断扩展的应用场景中得到使用——从异常检测到包括智能购物、监控、机器人技术和工厂自动化的应用。因此，没有一种通用的解决方案。但随着摄像头启用设备的快速增长，AI 已被广泛应用于分析实时视频数据，以自动化视频监控，提升安全性，提高操作效率，并提供更好的客户体验，最终在其行业中获得竞争优势。为了更好地支持视频分析，你必须了解优化边缘 AI 部署系统性能的策略。 
优化 AI 系统性能的策略包括 
选择合适大小的计算引擎，以满足或超越所需的性能水平。对于 AI 应用，这些计算引擎必须执行整个视觉管道的功能（即，视频的前处理和后处理、神经网络推理）。 
可能需要一个专用的 AI 加速器，无论是独立的还是集成在 SoC 中（而不是在 CPU 或 GPU 上运行 AI 推理）。 
 理解吞吐量和延迟之间的区别；其中吞吐量是系统中数据处理的速率，而延迟则衡量数据通过系统的处理延迟，并且通常与实时响应性相关。例如，系统可以以每秒 100 帧的速度生成图像数据（吞吐量），但图像通过系统的时间是 100 毫秒（延迟）。考虑未来轻松扩展 AI 性能以适应不断增长的需求、变化的要求和不断发展的技术（例如，更先进的 AI 模型以提高功能性和准确性）。你可以通过使用模块化格式的 AI 加速器或额外的 AI 加速器芯片来实现性能扩展。
理解可变 AI 性能要求 
实际的性能要求依赖于应用。通常，可以预期对于视频分析，系统必须处理来自摄像头的数据流，速率为每秒 30-60 帧，分辨率为 1080p 或 4k。一台 AI 启用的摄像头将处理单个流；一个边缘设备将并行处理多个流。在任何情况下，边缘 AI 系统必须支持预处理功能，将摄像头的传感器数据转换为符合 AI 推理部分输入要求的格式（见图 1）。 
预处理功能接收原始数据，执行如调整大小、归一化和颜色空间转换等任务，然后将输入数据提供给运行在 AI 加速器上的模型。预处理可以使用高效的图像处理库，如 OpenCV，以减少预处理时间。后处理涉及分析推理的输出。它使用如非极大值抑制（NMS 解释大多数目标检测模型的输出）和图像显示等任务生成可操作的见解，如边界框、类别标签或置信度分数。 
 
图 1. 对于 AI 模型推理，预处理和后处理功能通常在应用处理器上执行。 
AI 模型推理可能面临额外的挑战，即每帧处理多个神经网络模型，具体取决于应用的能力。计算机视觉应用通常涉及多个 AI 任务，需要多个模型的流水线。此外，一个模型的输出通常是下一个模型的输入。换句话说，应用中的模型通常彼此依赖，并且必须按顺序执行。要执行的模型集合可能不是静态的，甚至可能在每帧之间动态变化。 
运行多个模型动态的挑战需要一个外部 AI 加速器，该加速器需配备专用且足够大的内存来存储这些模型。由于 SoC 中共享内存子系统和其他资源的限制，SoC 内部集成的 AI 加速器通常无法管理多模型工作负载。 
例如，基于运动预测的物体跟踪依赖于连续检测来确定一个向量，这个向量用于识别未来位置的跟踪物体。由于缺乏真正的重识别能力，这种方法的效果有限。使用运动预测时，物体的轨迹可能因错过检测、遮挡或物体暂时离开视野而丢失。一旦丢失，就无法重新关联物体的轨迹。增加重识别功能可以解决这个限制，但需要一个视觉外观嵌入（即图像指纹）。外观嵌入需要第二个网络来通过处理包含在第一个网络检测到的物体边界框内的图像来生成特征向量。这个嵌入可以用来重新识别物体，无论时间或空间如何。由于必须为视野中的每个检测到的物体生成嵌入，因此随着场景变得更加繁忙，处理要求也会增加。带有重识别的物体跟踪需要在执行高精度/高分辨率/高帧率检测和保留足够的开销以便嵌入可扩展性之间进行仔细权衡。解决处理需求的一种方法是使用专用的 AI 加速器。如前所述，SoC 的 AI 引擎可能会受到共享内存资源不足的影响。模型优化也可以用来降低处理需求，但可能会影响性能和/或精度。 
不要让系统级开销限制 AI 性能 
在智能摄像头或边缘设备中，集成的 SoC（即主处理器）获取视频帧并执行我们之前描述的预处理步骤。这些功能可以由 SoC 的 CPU 核心或 GPU（如果有的话）来执行，也可以由 SoC 中的专用硬件加速器（例如图像信号处理器）来完成。在这些预处理步骤完成后，集成在 SoC 中的 AI 加速器可以直接访问系统内存中的量化输入，或者在离散 AI 加速器的情况下，输入则通过 USB 或 PCIe 接口传递进行推理。 
集成 SoC 可以包含一系列计算单元，包括 CPU、GPU、AI 加速器、视觉处理器、视频编码器/解码器、图像信号处理器（ISP）等。这些计算单元共享相同的内存总线，从而访问相同的内存。此外，CPU 和 GPU 可能还需要在推理中发挥作用，并且这些单元在部署系统中将忙于运行其他任务。这就是我们所说的系统级开销（见图 2）。 
许多开发人员错误地评估了 SoC 内置 AI 加速器的性能，而没有考虑系统级开销对整体性能的影响。例如，考虑在 SoC 中运行一个 50 TOPS 的 AI 加速器上的 YOLO 基准测试，可能会得到 100 次推断/秒 (IPS) 的基准结果。但在实际部署的系统中，所有其他计算单元都在活动时，这 50 TOPS 可能会减少到约 12 TOPS，而整体性能只会得到 25 IPS，假设使用了宽松的 25% 利用率因素。系统开销始终是一个因素，特别是当平台持续处理视频流时。相反，使用离散 AI 加速器（例如，Kinara Ara-1、Hailo-8、Intel Myriad X），系统级利用率可能会超过 90%，因为一旦主 SoC 启动推断功能并传输 AI 模型的输入数据，加速器会自主运行，利用其专用内存访问模型权重和参数。 
 
图 2. 共享内存总线将决定系统级性能，此处显示了估计值。实际值将根据你的应用使用模型和 SoC 的计算单元配置有所不同。 
边缘的视频分析需要低延迟 
到目前为止，我们讨论了以每秒帧数和 TOPS 作为衡量标准的 AI 性能。但低延迟是提供系统实时响应的另一个重要要求。例如，在游戏中，低延迟对于无缝和响应迅速的游戏体验至关重要，尤其是在动作控制游戏和虚拟现实 (VR) 系统中。在自动驾驶系统中，低延迟对于实时物体检测、行人识别、车道检测和交通标志识别至关重要，以避免影响安全。自动驾驶系统通常要求从检测到实际动作的端到端延迟小于 150 毫秒。类似地，在制造业中，低延迟对实时缺陷检测、异常识别和机器人指导至关重要，依赖于低延迟视频分析以确保高效操作并减少生产停机时间。 
一般来说，视频分析应用中的延迟有三个组成部分（图 3）： 
 数据捕获延迟是指从相机传感器捕获视频帧到帧可用于分析系统处理的时间。你可以通过选择具有快速传感器和低延迟处理器的相机、选择最佳帧率和使用高效的视频压缩格式来优化这个延迟。数据传输延迟是指从相机捕获并压缩的视频数据传输到边缘设备或本地服务器的时间。这包括每个端点发生的网络处理延迟。数据处理延迟是指边缘设备执行视频处理任务的时间，如帧解压和分析算法（例如，基于运动预测的物体跟踪、人脸识别）。正如前面提到的，对于需要在每个视频帧上运行多个 AI 模型的应用程序，处理延迟尤为重要。
 
图 3. 视频分析管道包括数据捕获、数据传输和数据处理。 
数据处理延迟可以通过使用设计为最小化芯片上数据移动和计算与各种内存层次之间数据移动的 AI 加速器来优化。此外，为了提高延迟和系统级效率，架构必须支持模型之间的零（或接近零）切换时间，以更好地支持我们之前讨论的多模型应用。另一个提升性能和延迟的因素与算法灵活性有关。换句话说，一些架构仅针对特定 AI 模型优化行为，但随着 AI 环境的迅速变化，每隔一天似乎就有新的高性能和更高精度的模型出现。因此，选择一种对模型拓扑、操作符和大小没有实际限制的边缘 AI 处理器。 
在边缘 AI 设备中，最大化性能需要考虑许多因素，包括性能和延迟要求以及系统开销。成功的策略应该考虑使用外部 AI 加速器，以克服 SoC 的 AI 引擎中的内存和性能限制。 
C.H. Chee 是一位成功的产品营销和管理高管，Chee 在推广半导体行业的产品和解决方案方面具有广泛经验，专注于基于视觉的 AI、连接性和视频接口，涵盖企业和消费市场。作为一名企业家，Chee 联合创办了两家视频半导体初创公司，这些公司后来被一家上市半导体公司收购。Chee 领导过产品营销团队，喜欢与专注于取得出色成果的小团队合作。 
更多相关信息 
 通过组织工作最大化您的数据科学家生产力通过第三名最佳在线数据硕士课程最大化您的价值…边缘上的机器学习Windows on Snapdragon 将混合 AI 引入边缘应用介绍 TPU v4：谷歌前沿超级计算机用于大型语言模型边缘 AI 的承诺及其有效采纳的方法
我们可以期待 GPT-5 带来什么？ 
 
 原文：www.kdnuggets.com/2023/06/expect-gpt5.html 
 
 
编辑提供的图片 
跟上 AI 和技术的快速发展可能看起来非常困难。每周或每月，总会有新的东西出现，现在你又在这里学习新的内容！ 
这次是 GPT-5。 
GPT-4 于 2023 年 3 月发布，自那时以来，大家一直在等待 GPT-5 的发布。Siqi Chen 在 3 月 27 日发推称“GPT-5 计划在今年 12 月完成训练。”然而，OpenAI 首席执行官 Sam Altman 在 4 月的MIT 活动上被问及 GPT-5 时澄清道，“我们不会，也不会在一段时间内发布”。 
这就澄清了这一点。然而，一些专家建议 OpenAI 在 2023 年第三季度或第四季度发布 GPT-4.5，作为 GPT-4 和 GPT-5 之间的一个中间版本。当前模型总是在不断改进，这可能成为 GPT-4.5 的一个潜在发布。许多人认为 GPT-4.5 具备多模态能力，这在 2023 年 3 月的 GPT-4 开发者直播中已有展示。 
尽管对 GPT-5 有很高的期望，GPT-4 仍然需要解决一些问题。例如，GPT-4 的推理时间非常长，同时运行成本也很高。还有其他挑战，比如访问 GPT-4 的 API。 
尽管还有很多工作要做，但我们可以说的是，每一次 GPT 的发布都推动了 AI 技术及其能力的边界。AI 爱好者们对探索 GPT-5 的突破性特性感到兴奋。 
那么我们可以期待 GPT-5 带来哪些功能呢？让我们来了解一下。 
减少幻觉 
这全关乎信任，这是大多数用户不相信 AI 模型的主要原因。例如，GPT-4 在内部事实评估中的得分比 GPT-3.5 高 40%，如下面的图片所示。这意味着 GPT-4 在回应不允许的内容方面的可能性更小，产生事实性回应的可能性比 GPT-3.5 高 40%。 
随着新版本不断改进当前挑战，据说 GPT-5 将把幻觉减少到 10%以下，使大型语言模型（LLM）更值得信赖。 
 
图片来源于OpenAI 
计算效率 
如前所述，GPT-4 的计算成本非常高，每个 token $0.03。这与 GPT-3.5 的$0.002 成本相比差距巨大。GPT-4 在一万亿参数数据集和基础设施上训练，反映了其成本。 
而谷歌的 PaLM 2 模型仅在 3400 亿参数上进行训练，并且性能高效。如果 OpenAI 计划与谷歌的 PaLM 2 竞争，他们需要寻找降低成本和缩小 GPT-4 参数规模的方法，同时保持性能。 
另一个需要关注的方面是更好的推理时间，即深度学习模型预测新数据所需的时间。GPT-4 的功能和插件越多，计算效率也就越高。开发者们已经向 OpenAI 抱怨 GPT-4 的 API 经常停止响应，这迫使他们使用 GPT-3.5。 
考虑到所有这些，我们可以期待 OpenAI 通过推出更小、更便宜、更高效的 GPT-5 来克服这些挑战。 
多感官 
在 GPT-4 发布之前，很多人对其多模态能力感到疯狂。尽管这尚未添加到 GPT-4 中，但这可能是 GPT-5 出现并真正成为明星的地方，使其真正实现多模态。 
我们不仅可以期待它处理图像和文本，还可以处理音频、视频、温度等更多内容。萨姆·奥特曼在一次采访中表示*“我非常期待看到当我们能够处理视频时会发生什么，世界上有很多视频内容。很多事情用视频学习比用文本更容易。”* 
增加可以用来使对话更具动态性和互动性的数据类型。多模态能力将是通向人工通用智能（AGI）的最快途径。 
长期记忆 
GPT-4 的最大令牌长度为 32 千个令牌，这在当时令人印象深刻。但随着世界上不断推出新模型，我们已经有如 Story Writer 这样的模型能够输出 65 千个令牌。 
为了跟上当前的竞争，我们可以期待 GPT-5 引入更长的上下文长度，使用户能够拥有能够记住他们的个性和历史多年之久的 AI 朋友。 
改进的上下文理解 
作为一个大型语言模型（LLM），我们可以期待的第一件事是对上下文理解能力的提升。如果我们将其与上面提到的长期记忆结合起来，GPT-5 可能具有在长时间对话中保持上下文的潜力。作为用户，你将获得更多量身定制且符合要求的有意义的回应。 
随之而来的是对语言的更高级理解，其中自然语言的主要组成部分是情感。GPT-5 在上下文理解方面的潜在能力可以使其更具同理心，并生成适当的回复以继续参与对话。 
总结 
关于 GPT-5 潜在能力还有很多未知，我们要等到发布时才能了解更多信息。本文基于 GPT-4 和 GPT-3.5 目前面临的挑战，以及 OpenAI 如何利用这些障碍克服困难，推出高性能的 GPT-5。 
尼莎·阿利亚 是一位数据科学家、自由技术写作人以及 KDnuggets 的社区经理。她特别关注提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能在延长人类寿命方面的不同应用方式。作为一个热衷学习的人，她寻求扩展自己的技术知识和写作技能，同时帮助指导他人。 
更多相关话题 
 2023 年 AI 指数报告：未来我们可以预期的 AI 趋势作为数据科学家的职业发展预期2023 年人工智能质量趋势的预期数据科学家为何期望从 Google Bard 得到有缺陷的建议ODSC East 2022 免费访问的 15 个热门 MLOps 讲座人工智能如何改变数据集成
什么是谷歌 AI Bard？ 
 
 原文：www.kdnuggets.com/2023/03/google-ai-bard.html 
 
 
作者提供的图片 
当大家都在疯狂讨论 ChatGPT 时，谷歌突然发布了他们自己实验性的 AI 驱动聊天机器人 - 谷歌 Bard。你可以看到竞争非常激烈，谷歌需要作出回应。但这是否是对 ChatGPT 的回应，还是谷歌 Bard 已在筹备中？ 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业的快车道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 
 
大揭密：谷歌 AI Bard 
所以现在我们知道谷歌 Bard 是谷歌对 OpenAI 的 ChatGPT 的回应。让我们进一步了解它。它具有相同的主要特点 - 它是一个人工智能聊天机器人。它能够以对话的方式回应不同的查询。 
它利用网络上的信息，提供高质量、最新且易于用户理解的回应。谷歌 Bard 结合了机器学习和自然语言处理（NLP）来为用户提供这些高质量而现实的回应。 
谷歌 Bard 旨在将其增强到谷歌的搜索工具中，并为企业提供自动化支持和其他可以完成的类似人类的交互任务。必应已经将 ChatGPT 融入其网站，实现了更好的搜索、浏览和聊天体验。 
除了作为谷歌搜索工具的一部分外，它还可以集成到网站、消息平台、桌面和移动应用程序等更多领域。这一切将在他们有限的测试服务完成后提供。你可以通过查看下面的谷歌互动图像了解这一点： 
 
图片由 谷歌 提供 
它是如何工作的？ 
谷歌的知识图谱是他们连接信息的一种方式。知识图谱卡片包含与特定搜索相关的背景信息。谷歌希望 Bard 与知识图谱卡片协调工作，然而，Bard 对 NORA 问题做出回应。NORA 代表“没有唯一正确答案”。 
为了使其有效，Bard 使用了 LaMDA，一个对话应用语言模型，以更好地理解请求/问题并应用上下文。然后，Bard 利用网络从各种信息中提取内容来形成回答。接着，这些内容被转换成你通常与人类进行的对话回应。 
什么是 LaMDA？ 
两年前，Google 揭示了他们的对话应用语言模型（LaMDA）。 
Google 的语言模型 LaMDA 基于 Transformer，这是一种 Google Research 在 2017 年发明的神经网络架构。它还结合了多模态用户意图、强化学习和建议。 
该模型经过训练以阅读和理解文字，从句子到段落。然后，它会利用这些理解寻找关系，以预测下一个可能出现的词。我们可以说 LaMDA 是一种统计方法，因为它基于之前的词预测未来的词。这使得聊天机器人能够以流畅的方式进行对话，从一个话题转到另一个话题。 
LaMDA 的主要优点是它使用更少的计算资源，允许可扩展性，并提供更有效的反馈系统，这由 Google 自身的内部测试完成。 
我如何使用 Google Bard？ 
在撰写本文时，Google Bard 目前仅对有限的测试人员开放。然而，近期有计划使其变得更加可及。 
如果你是这些测试人员之一，可以通过打开 Google 应用程序，点击聊天机器人图标，开始对话或提出请求来使用 Google Bard。 
Google 已达到其测试小组的限制，将不再接受申请。在此期间，Google 旨在优化 Bard 的准确性、质量和速度。 
Google Bard 与 ChatGPT 的比较 
现在我们对 Google Bard 有了很好的理解和范围，那么 Google Bard 和 OpenAI 的 ChatGPT 有什么区别呢？ 
如果你需要快速了解 ChatGPT，请阅读：ChatGPT: 一切你需要知道的 
那么，让我们深入了解一下 Google Bard 和 ChatGPT 之间的区别。 
 
图片由作者提供 
结论 
看到 Google 在 Bard 方面有什么新进展会很有趣。它会比 ChatGPT 更有效吗？ 
如果你想看看 Google Bard 和 ChatGPT 之间的回应差异，可以查看亚利桑那大学助理教授 Henk van Ess 对 Bard AI 和 ChatGPT 的比较：ChatGPT 是否对 Google 构成威胁？ 
如果你有机会使用 Google Bard 测试版服务，请在评论中告诉我们你的想法。 
Nisha Arya 是一名数据科学家，自由技术写作者，以及 KDnuggets 的社区经理。她特别关注提供数据科学职业建议或教程以及与数据科学相关的理论知识。她还希望探索人工智能如何/可以提升人类生命的延续性。作为一个热衷学习的人，她寻求拓宽自己的技术知识和写作技能，同时帮助他人。 
更多相关主题 
 为何数据科学家期望从 Google Bard 获得有缺陷的建议ChatGPT 与 Google Bard：技术差异的比较8 个开源 ChatGPT 和 Bard 替代品超级 Bard：无所不能的 AI，且表现更优检测 ChatGPT、GPT-4、Bard 和 Claude 的十大工具Bard 数据科学备忘单
人工智能不是来取代我们的 
 
 原文：www.kdnuggets.com/2023/02/ai-replace-us.html 
 
 
作者提供的图片 
当我听说人工智能能够写出体面的代码时，我不得不承认，我的脊背有些发凉。每天似乎都有新的职业受到人工智能的威胁。SEO 博客作者都在提心吊胆。艺术家们则惊恐地看着用户涌向 Midjourney 的 Discord 服务器。现在还有这个。 
 
我们的三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织在 IT 领域 
 
 
Chat GPT 的截图 
我一直主张技术不会消除工作，而是增加工作。是的，文字处理器使打字员变得过时。在此之前，汽车使马车司机变得过时。但尽管技术快速涌入，工作岗位——好工作、有趣的工作、引人入胜的工作——的整体数量还是在增加。 
数据与我的直觉一致。世界经济论坛估计到 2025 年技术将增加 1200 万个工作岗位。经济历史学家詹姆斯·贝森建议过去两个世纪工资的部分上涨得益于生产的增长，而这又归功于自动化和技术的发展。 
但这是我第一次面对自己被取代的可能性。ChatGPT 可以写代码。 
这是否意味着人工智能最终会取代我们？ 
我相信这不仅不太可能，而且是不可能的，这要归功于人工智能的训练方式。有些技能（例如这些数据科学技能）无论技术多么先进，人工智能都无法取代。 
什么技能是不可替代的，为什么？ 
人工智能永远不会有创造力。人工智能永远不会有创新性。人工智能永远无法进行推理、思考或争论。 
目前，它只是一个非常有能力的模仿者。人工智能可以模式匹配，并且做得非常好。但它无法完成那些使人类工人和人脑如此宝贵的事情。 
为什么会这样？考虑一下人工智能生成图像的过程作为例子。Midjourney 已经在来自网络各个部分的数百万张图像上进行了训练。每张图像都有一些关联的文字。一个笑容灿烂的女性图像会被描述为“笑容灿烂的女性”。一件拉斐尔风格的艺术作品会被描述为名字、风格和画作内容。 
当你要求 Midjourney 生成一幅笑容灿烂的女性拉斐尔风格的画作时，它会将所有的训练数据放入一个搅拌机中，然后输出一个综合结果。这并不具有创新性或创造性。人工智能并没有“思考”或“推理”来创建那个图像。它只是使用模式识别来跟随你的提示。 
 
由 Midjourney 创建的图像 | 提示：笑容灿烂的女性，拉斐尔风格，画作 
Nick Cave 对 ChatGPT 用他的风格创作的歌曲感到厌恶。在解释原因时，他理解到 ChatGPT 所做的是“模仿，或者复制，或者模仿的拼贴”。这正是写好歌曲所不具备的。然后他解释了它是什么：“这是一种自我谋杀的行为，它摧毁了一个人过去所努力创造的一切。正是那些危险的、令人心跳骤停的离经叛道，才将艺术家推向超越他们所认知的自我的极限。”在这方面，我认为 Nick Cave 解释了人类与人工智能之间的区别。我理解的意思是，我们应该区分生成和创造。所有人工智能做的只是生成，而人类则有创造的能力。 
看来 ChatGPT 同意 Nick 的观点！我让它写一篇关于人工智能是否会被用来取代软件工程师的博客文章。它可以吸收成千上万篇关于这个主题的博客文章，并输出最接近的内容。它并没有考虑你的问题，也没有思考如何最佳回应。它只是使用训练数据来完成你的提示。 
ChatGPT 说得最好： 
 
来自 ChatGPT 的图像 
这意味着，按照目前的使用和训练方式，人工智能永远无法真正生成独特的艺术作品。它永远无法产生原创思想或分享独特且个人化的经历。它也不能成为一名软件工程师。 
它能做的只是消费和反复输出。 
这也是为什么人工智能有时会犯错，比如给人手添加过多的手指，或者在博客文章中提供错误的信息/虚假的报告。它不能知道什么是对的，什么是错的，只能知道它所消费的内容。 
它可以用于什么？ 
尽管如此，我确实发现 AI 当前的应用场景令人印象深刻且有价值。除了编写 Python 代码生成五个随机数字之外，AI 还能做更多的事情。大多数这些技能都属于“让我的工作更有趣且少些枯燥”的范畴。 
例如，我们雇用的博客作者常常花费大量时间优化博客帖子以便于搜索引擎。（我们这样做是为了帮助感兴趣的读者更容易找到我们的博客帖子。）这可能有点枯燥。我们的作者喜欢专注于使博客帖子有趣、信息丰富和有趣。确保博客帖子具有优化的关键词数量以便于读者发现，则少了一些乐趣。 
AI 也已经用于拼写检查。例如，Grammarly 使用 相同的模式匹配能力来确保你写的内容符合它所学的规范。 
你也可以用 AI 来进行研究。它在整合和传达信息方面非常有效，因为它已经扫描了比任何人一生中能阅读的文本还要多的内容。例如，ChatGPT 可以向我解释相机是如何工作的，飞机是如何飞行的，甚至是量子力学。 
最后，软件工程师和数据科学家可能依赖 AI 来调试他们编写的代码。我们都使用过 StackOverflow。可以把 AI 看作是一个阅读过所有问题和答案的 StackOverflow 助手，能够将发现汇总成一个简单的回答。 
事实是，如果 AI 能够取代你的工作，这意味着你原本做的工作相当琐碎。AI 的当前版本应该能让你腾出时间做更多令人兴奋、智力密集或基于技能的工作。 
AI 可以并且应该增强人类工作 
有一个领域，AI 不仅可以自动化你工作流程中的基本步骤，还能使人类的工作变得更好。那就是医疗领域。 
AI 可以处理大量数据并发现模式，因此在帮助医疗专业人员做出诊断方面特别出色。尽管大多数临床医生仍然更喜欢与患者交谈以全面了解症状，但 AI 助手可以监测血压并将症状与潜在疾病进行匹配。 
医疗领域的 AI 仍然有些争议，自从 IBM 购买了可以击败《危险边缘》竞赛者的“Watson” AI 并试图用它来诊断患者之后。最终，这款 AI 在处理复杂的患者档案时遇到了麻烦，无法做出可靠的诊断。但随着 AI 在处理复杂和多样的信息源方面变得越来越出色，以及临床医生越来越习惯依赖 AI 来解释医学扫描和症状，这将是 AI 能够发挥作用并挽救生命的一个领域。 
所以，AI 会抢你的工作吗？ 
当我开始与 AI 技术互动时，我很快就克服了我的震惊。我使用得越多，越能清楚地看出作为一个拥有大脑的人，我的工作是 AI 无法替代的。AI 永远无法像我一样解决问题，也无法像我一样讲述个人经历。 
在我看来，AI 并不是为了取代我们而来的。它是为了帮助我们。如果它能取代你，那么你应该尽快在简历中添加一些额外的技能。 
内特·罗西迪 是一位数据科学家，专注于产品战略。他也是一名兼职教授，教授分析学，并且是 StrataScratch 的创始人，该平台帮助数据科学家通过顶级公司提供的真实面试问题来准备面试。你可以在 Twitter: StrataScratch 或 LinkedIn 上与他联系。 
更多相关话题 
 别害怕，因为 AI 编程来帮助你了！DeepMind 的 AlphaCode 会取代程序员吗？ChatGPT 会取代数据科学家吗？AI 会取代人类吗？你应该用什么来替代 Google 的 8 个 AI 搜索引擎这里是我用的 AI 工具以及我的技能，帮助我每月赚取 $10,000…
机器学习并不像你的大脑第六部分：精确突触权重的重要性及其快速设置的能力 
 
 www.kdnuggets.com/2022/08/machine-learning-like-brain-part-6-importance-precise-synapse-weights-ability-set-quickly.html 
 
据我们所知，突触的权重只能通过连接它的两个神经元的近乎同时的发放来改变。这完全与机器学习反向传播算法的基本架构相悖。 
你可以把反向传播想象成一个坐在神经网络旁边的小人，他查看网络输出，将其与期望输出进行比较，然后为网络中的突触指示新的权重。在生物系统中，没有机制可以指示任何特定突触的权重。你可以尝试通过发放连接的两个神经元来增加突触权重，但也没有办法做到这一点。你不能仅仅要求发放神经元 1000 和 1001 来增加它们之间的突触，因为没有办法在网络中发放特定的神经元。 
我们确定的唯一调整突触权重的机制被称为赫布学习。它是一个常被俏皮地表述为“同时发放的神经元会连接在一起”的机制。但正如所有生物学现象一样，它并不那么简单。在“突触可塑性”的研究中，得到的曲线如图所示，表明要增强连接源神经元和目标神经元的突触，源神经元需要在目标神经元之前稍微发放。要减少突触权重，目标神经元必须在源神经元之前稍微发放。这在整体上是有意义的，因为如果一个神经元对另一个神经元的发放有贡献，那么连接这两个神经元的突触应该被加强，反之亦然。 
图中还有一些需要注意的问题。首先，尽管整体概念在图 B 中进行了总结，图 A 显示了观察数据中大量的散布。这意味着，将突触设定为任何特定值的能力非常有限，这一点已通过模拟得到确认。 
你还可以观察到，要对突触权重进行任何实质性的改变需要多次重复。即使在理论环境中（没有干扰的情况下），你也可以得出结论：对突触值的精确度要求越高，设定它所需的时间就越长。例如，如果你希望一个突触有 256 种不同的值之一，你可以定义每对增强性尖峰将权重增加 1/256。可能需要 256 对尖峰（到源头和目标）才能设置权重。在生物神经元的缓慢速度下，这将需要整整一秒钟。 
想象一下构建一台计算机，其中单个字节的内存写入需要大约一秒钟。此外，设想需要支持电路来设置一个值 x，安排准确的 x 次脉冲到源神经元和目标神经元。这假设它从 0 的权重开始，这又是另一个问题，因为没有办法知道任何突触的当前权重。最后，想象一下网络中的任何使用如何修改突触权重，使得这样的系统无论如何都无法存储准确的值。在特定突触中存储特定值的整个概念完全不切实际。 
还有另一种看法更为合理。考虑将突触视为一个二进制设备，值为 0 或 1（或者在抑制突触的情况下为-1）。现在，突触的特定权重代表了该突触的重要性以及遗忘它所代表的数据位的可能性。如果我们考虑神经元发射脉冲（可能是 5 次），那么任何超过 0.2 的权重代表 1，任何低于 0.2 的权重代表 0。这样的系统可以在一次脉冲中学习，并且对内存内容的随机变化具有免疫力。这是一个完全合理的情景，但它与现代机器学习方法完全相悖。 
 
 到目前为止，集中讨论了 ML 和感知机可以做到而神经元不能做到的事情，我将在本系列的第七部分扭转局面，描述一些神经元特别擅长的事情。 
 
 
图 A：显示源神经元和目标神经元的相对脉冲时间如何影响突触权重。图 B：一个理想化的 Hebbian 学习在仿真中可用的表示。摘自：Piochon, Claire & Kruskal, Peter & Maclean, Jason & Hansel, Christian. (2012). 小脑回路中的非 Hebbian 脉冲时间依赖性可塑性。神经回路前沿。6. 124. 10.3389/fncir.2012.00124. 
欲了解更多信息，请访问 www.youtube.com/watch?v=jdaAKy-XkA0 
查尔斯·西蒙 是一位全国知名的企业家和软件开发者，也是 FutureAI 的首席执行官。西蒙是《计算机会反叛吗？：为人工智能的未来做准备》的作者，以及 Brain Simulator II 的开发者，这是一个 AGI 研究软件平台。欲了解更多信息，请访问这里。 
更多相关主题 
 机器学习不像你的大脑 第四部分：神经元的……机器学习不像你的大脑 第一部分：神经元很慢，……机器学习不像你的大脑 第二部分：感知器与神经元机器学习不像你的大脑 第三部分：基本架构机器学习不像你的大脑 第五部分：生物神经元机器学习不像你的大脑 第七部分：神经元的作用
欧洲 AI 法案：简化版 
 
 译文：www.kdnuggets.com/2022/06/european-ai-act-simplified-breakdown.html 
 
 
Sasun Bughdaryan via Unsplash 
2020 年 2 月 19 日，发布了一份关于 AI 的白皮书——“欧洲卓越与信任的方法”。随后，在 2021 年 4 月 21 日，欧洲委员会发布了关于 AI 使用法案的立法。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 需求 
 
根据欧盟的说法，一些 AI 系统非常复杂、不可预测且非常不透明。他们的目标是确保这些不同类型的 AI 系统符合基本权利并提供信任，同时减少市场碎片化。 
《法律》的主要目标是： 
 欧洲市场上的 AI 系统是安全的。这些 AI 系统尊重欧盟的价值观和公民权利。这些 AI 系统确保法律确定性，以帮助促进 AI 的投资和创新。这些 AI 系统值得信赖，以确保市场不会失败。它改进了现有的安全性和权利要求立法。
然而，这项法律可能不仅适用于欧盟；其他国家已经开始探索提供更透明 AI 系统的新方法。2021 年 9 月，巴西通过了一项创建法律框架的法案。 
AI 风险框架 
该法律将这些 AI 系统分类为四个风险领域：不可接受风险、高风险、有限风险和最低或无风险。 
最低或无风险 
这允许使用低风险或无风险的 AI 系统，没有限制。虽然没有任何限制，但 AI 系统的提供者必须遵守自愿的行为规范。委员会预测，大多数 AI 系统将归入“低风险或无风险”类别。 
这些类型的 AI 系统包括垃圾邮件过滤。 
有限风险。 
这些 AI 系统也被允许，但需提供更深入的信息和更高的透明度，如技术文档。他们也可以选择遵守自愿的行为规范。 
这些类型的 AI 系统包括聊天机器人。 
高风险 
这些是最具风险的人工智能系统，尽管允许使用，但它们必须符合要求和事前/事后合规评估。在系统投入市场前需进行合规评估。 
这些人工智能系统将会是： 
 人工智能系统被用作产品安全组件，例如医疗设备独立的高风险人工智能系统，例如执法系统
不可接受的风险 
这些类型的人工智能系统完全禁止，因为它们已知对人们的安全和权利构成‘不可接受的风险’。 
例如对儿童或精神残疾人士的剥削。这可能通过包含集成语音助手的玩具娃娃，促使用户进行危险行为。 
高风险人工智能系统的要求 
高风险人工智能系统由于其要求而面临最大的风险。这些要求包括 
数据和数据治理 - 确保这些人工智能系统使用高质量的相关和代表性数据 
文档和记录保存 - 创建文档和记录功能以帮助追踪和审计，同时确保人工智能系统的合规性。 
透明度和用户信息提供 - 向用户提供信息，例如如何使用系统以确保透明度。 
人工监督 - 在人工智能系统的构建阶段以及实施阶段，人工干预是至关重要的。 
强健、准确、网络安全 - 这些元素对任何人工智能系统都至关重要，以保护企业和用户。 
罚款/处罚 
从公司到这些人工智能系统的制造商再到分销商，都可能面临严重的罚款。这些罚款分为三个级别，具体取决于违规的严重性。 
高达 1000 万欧元 
这是《人工智能法》中规定的最低罚款级别。这可能是由于向当局提供了不完整或虚假的信息。罚款金额可能高达1000 万欧元或公司全球年营业额的 2%。 
高达 2000 万欧元 
这是下一个潜在的罚款，可能是由于人工智能系统要求的违反。例如，缺乏提供透明度的技术文档。罚款金额可能高达 2000 万欧元或公司全球年营业额的 4%。 
高达 3000 万欧元 
这是最高的罚款金额，可能由于使用了禁止的人工智能系统或系统质量未达到标准。罚款金额可能高达 3000 万欧元或公司全球年营业额的 6%。 
结论 
尽管还有更多内容需要审查，一切尚未确定，但《人工智能法案》已经引起了很多关注。对企业、制造商和分销商，无论是在欧盟内部还是外部，都是如此。《人工智能法案》的目的是确保欧盟在人工智能领域的卓越，提供正确的条件以促进人工智能的发展，并确保人工智能系统对人类有益。 
Nisha Arya 是一名数据科学家和自由职业技术写作人员。她特别关注提供数据科学职业建议或教程以及围绕数据科学的理论知识。她还希望探索人工智能如何有助于人类寿命的不同方式。她是一位热衷于学习的人，寻求扩展自己的技术知识和写作技能，同时帮助指导他人。 
更多相关话题 
 SQL 简化：使用 CTE 制作模块化和易于理解的查询数据科学家细分：技能、认证和薪资
AI 有自己的蝙蝠侠吗？ 
 
 原文：www.kdnuggets.com/2022/05/ai-get-batman.html 
 
 
图片由Michael Marais拍摄，来自Unsplash 
每个轰动一时的故事都有一个英雄和一个反派，AI 范式也不例外。随着 AI 和数据的发展以及在我们日常生活中扮演越来越重要的角色，AI 多年来一直需要发出蝙蝠信号。你知道这个故事 - 数百万的投资却回报甚微。 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业的捷径 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT 工作 
 
要理解英雄的角色，重要的是注意到阻碍 AI 充分发挥潜力的反派。对于 AI 来说，反派是双面派。虽然数据是构建可靠且强大的 AI 模型的关键组成部分，但数据也是 AI 采纳的主要障碍之一。 
根据 Gartner，数据质量和数量的缺乏是 AI 采纳的一些最大障碍。成功的 AI 项目需要大量的数据来获取最佳应对情况的信息。没有足够的数据或新场景与过去的数据不匹配时，AI 可能会出现问题。情况越复杂，AI 现有的数据越可能不够。 
起初训练和完善 AI 模型所需的数据量并不总是明确的。隐私问题也可能使得获取所需数据变得困难。 
引入合成数据 
在视觉领域，合成数据在创建更具能力和伦理的 AI 模型方面显示出潜力。合成数据是计算机生成的图像数据，用于模拟现实世界。视觉特效行业的技术与生成神经网络结合，以创建广泛、多样化且真实感强的标记图像数据。合成数据集是人工创建的，而不是通过现实世界数据生成的，从而使训练数据的开发成本和时间大大降低。像布鲁斯·韦恩一样，合成数据也有许多巧妙的应用。 
穿斗篷的十字军 
目前，大多数人工智能系统依赖于“监督学习”，即人工标记并有效地教导人工智能如何解释图像的过程。这个过程既耗时又耗费资源，并且存在根本性限制，因为人类无法扩展，更重要的是，无法标记如 3D 位置、交互等关键属性。此外，关于人工智能的群体偏见和消费者隐私的担忧不断增加，使得获取具有代表性的人类数据变得越来越困难。 
对于以消费者为中心的应用程序，如智能手机和智能家居，确保隐私至关重要。合成数据最终可以消除在构建以消费者为中心的应用程序中使用真实人类的需求。由于合成数据是人工生成的，这消除了许多传统数据集收集过程中存在的偏见和隐私问题。 
比子弹还快 
捕获和准备真实世界数据用于模型训练是一个漫长且繁琐的过程。对于复杂的计算机视觉系统，如自动驾驶汽车、机器人或卫星图像，部署所需的硬件可能非常昂贵。一旦数据被捕获，人类需要标记和注释关键特征，这容易出错且成本高昂。 
合成数据能够按需提供数据，从而降低计算机视觉模型和产品的成本并加快市场速度。它比传统的人工标注真实数据的方法快和便宜几个数量级，并将加速新型更强大模型在各行业的部署。 
针对新兴技术的强大模型 
如同蝙蝠侠的能力一样，合成数据的能力超越了普通公民。如前所述，人类在准确标记帮助计算机视觉系统解释周围世界的关键属性方面有限。公司受制于足够多样化和准确标记的人类数据集的可用性。目前，获取和标记图像数据的时间和成本巨大。这种方法的根本限制在于，人类工作者无法标记公司可能感兴趣的所有属性。 
与需要手动标记的真实数据不同，合成数据是人工生成并标记的，模拟真实世界。合成数据方法提供的关于 3D 位置、深度和新传感器系统的新标签将允许开发适用于自主、机器人和 AR/VR/元宇宙等应用的新型更强大的模型。 
适用于所有人的超级英雄 
人工智能系统可能包含固有的偏见，这会影响到人群的各个群体。驱动人工智能模型的数据集可能在某些数据类别上不平衡，并且某些群体可能被过度或不足代表。这可能导致人工智能数据集中的性别、种族和年龄偏见。引入合成数据。 
合成数据并非从现实世界事件或现象中提取，而是部分或完全人工生成的。如果数据集不够多样或庞大，人工智能生成的数据可以填补空白，形成更全面、公正的数据集。这使得人工智能科学家能够创建平衡的数据集，帮助组织满足监管和合规要求，并构建更公平且具有伦理的人工智能系统。 
世界将需要强大且具有伦理的人工智能系统来支持未来的应用。自动驾驶的车辆将把旅行者送往机场，虚拟工作会议将在元宇宙中进行，或者送货机器人将送来本周的食品，这些都将依赖于由庞大数据集驱动的计算机视觉应用。合成数据的超能力将在确保这些数据集具有伦理性、公正性、经济性和鲁棒性方面发挥至关重要的作用。 
Yashar Behzadi, Ph.D. 是 Synthesis AI 的首席执行官兼创始人。他是一位经验丰富的企业家，在人工智能、医疗技术和物联网市场上建立了变革性的企业。 
更多相关主题 
 什么是 K-Means 聚类及其算法如何工作？停止在 ChatGPT 上做这些事情，并超越 99%的用户数据网格及其分布式数据架构KDnuggets™ 新闻 22:n07, 2 月 16 日: 如何学习机器数学…辛普森悖论及其在数据科学中的影响伯克森-杰克尔悖论及其对数据科学的重要性
=职业发展= 
5 个需求高但未得到足够认可的 IT 职位 
 
 原文: www.kdnuggets.com/5-it-jobs-that-are-high-in-demand-but-dont-get-enough-recognition 
 
 
作者提供的图片 
当人们考虑进入技术领域时，他们通常会追求高度技术性的工作，如软件工程或数据科学。然而，技术领域的发展是由不同领域的不同专家共同推动的。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业轨道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析能力 
 3. 谷歌 IT 支持专业证书 - 支持你的组织在 IT 方面 
 
虽然软件工程师和数据科学家是许多组织的基础，但其他利益相关者将组织凝聚在一起，使其成功。 
IT 支持 
链接: 谷歌 IT 支持专业证书 
级别: 初学者 
时长: 6 个月，每周 10 小时 
IT 支持在技术领域中非常重要。他们的主要职责是安装和配置计算机系统，诊断硬件和软件故障以及解决技术和应用问题 - 这些问题在技术行业中经常发生。 
在谷歌提供的这门课程中，你将获得成功进入初级 IT 职位所需的技能，并学习执行日常 IT 支持任务，包括计算机组装、无线网络、程序安装和客户服务。你还将学习如何提供端到端的客户支持，从识别问题到故障排除和调试，并使用包括 Linux、域名系统、命令行界面和二进制代码在内的系统。 
网络安全 
链接: 谷歌网络安全专业证书 
级别: 初学者 
时长: 6 个月，每周 7 小时 
我会一直说这一点， 世界需要更多的网络安全分析师。 网络安全分析师的主要职责是保护组织的网络和系统免受攻击。 
在 Google 提供的课程中，你将了解网络安全实践的重要性及其对组织的影响，识别常见的风险、威胁和漏洞，以及减轻这些问题的技术。你将学习如何使用安全信息和事件管理（SIEM）工具保护网络、设备、人员和数据免受未经授权的访问和网络攻击，并获得 Python、Linux 和 SQL 的实际操作经验。 
云解决方案架构师 
链接：AWS 云解决方案架构师专业证书 
级别：中级 
时长：2 个月，每周 10 小时 
云计算与安全是 IT 领域中需求量最大的领域之一，目前 AWS 平台是全球使用最广泛的云平台。 
在 AWS 提供的课程中，你将学习如何在何时以及如何应用关键的 AWS 服务（计算、存储、数据库、网络、监控和安全）做出明智的决策。你还将学习设计架构解决方案，无论是为了成本、性能和/或运营卓越，以应对常见的业务挑战。同时，还将学习以安全且可扩展的方式创建和操作数据湖，将数据导入并组织到数据湖中，以及优化性能和成本。 
最后但同样重要的是，你将为认证考试做准备，识别每个领域的优势和不足，并制定识别错误回答的策略。 
项目管理 
链接：Google 项目管理：专业证书 
级别：初级 
时长：6 个月，每周 10 小时 
在技术领域，项目经理的价值极高。他们负责管理信息技术项目的时间表和预算，以确保实施过程的顺利。 
在 Google 提供的课程中，你将深入了解成功担任初级项目管理角色所需的实践和技能。你还将学习如何在项目的各个阶段创建有效的项目文档和成果物，以及敏捷项目管理的基础知识，重点是实施 Scrum 事件、构建 Scrum 成果物和理解 Scrum 角色。 
技术营销 
链接：Google 数字营销与电子商务专业证书 
级别：初级 
时长：6 个月，每周 10 小时 
当你的公司已经开发出 10/10 的产品并且它在正常运行时，这是很棒的。但是如果你没有营销专家来推动公司成功，那么你将没有销售或潜在客户。 
在 Google 提供的课程中，你将学习数字营销和电子商务的基础知识，以获得进入初级职位所需的技能。你将了解如何通过搜索和电子邮件等数字营销渠道吸引和参与客户，并通过分析衡量营销表现和呈现见解。 
总结 
技术行业不仅需要高技能的专业人士来构建产品。它们还需要一个拥有广泛技能的完整生态系统，以将你的最终产品推向市场。 
Nisha Arya 是一名数据科学家、自由技术作家、以及 KDnuggets 的编辑和社区经理。她特别关注提供数据科学职业建议或教程，以及围绕数据科学的理论知识。Nisha 涵盖了广泛的主题，并希望探索人工智能如何有利于人类寿命的不同方式。作为一个热衷学习者，Nisha 希望拓宽她的技术知识和写作技能，同时帮助指导他人。 
更多相关内容 
 获得高薪数据科学职位的 7 个平台微软的 4 个入门级证书助你获得热门职位如何在没有相关学位的情况下进入数据分析领域图像识别和自然语言处理的迁移学习语音识别指标的演变MLOps 乱象，但这是可以预期的
科技行业各领域的热门 Google 认证 
 
 链接：www.kdnuggets.com/popular-google-certification-for-all-areas-in-the-tech-industry 
 
 
图片由作者提供 
当人们说他们在科技行业工作时，许多人假设他们是软件工程师，掌握 3 种不同的编程语言，并能在一夜之间构建应用程序。但科技行业远不止这些。 
随着科技的不断发展，我们不仅需要软件工程师和数据科学家，还需要网络安全分析师、市场营销人员、设计专业人士等。如果你正在寻找职业转型，但希望保持选项开放而不涉及编码，继续阅读…… 
数据分析 
链接：Google 数据分析专业认证 
对于那些有兴趣处理数据、准备数据并分析数据以进行决策的人，我们从最技术性的开始。 
Google 数据分析专业认证使你能够理解关联数据分析师使用的实践和过程。你将学习如何清理数据、分析数据，并使用 SQL、R 编程和 Tableau 等工具创建可视化。 
随着我们理解数据的价值，我们也明白数据分析师的价值将继续增长。 
项目管理 
链接：Google 项目管理专业认证 
科技行业发展迅速，每天都有新的项目发布。这就是我介绍项目管理及其在任何行业中重要性的地方。如果没有项目管理，许多新工具可能无法部署，让我们能够使用它们。 
项目管理是应用过程、方法、技能、知识和经验以实现特定目标，确保项目成功。在这个 Google 项目管理专业认证中，你将学习如何有效记录项目，了解 Agile 项目管理、Scrum 的基础，并实践战略沟通，提升你的问题解决能力。 
网络安全 
链接：Google 网络安全专业认证 
数据是新的黄金，就像黄金一样，组织也有相应的流程和工具来确保其安全。 
在这个 Google 网络安全专业认证中，你将学习最佳网络安全实践及其对组织的影响。你将识别常见的风险和漏洞，并应用技术来缓解这些问题。 
网络安全全在于保护，因此深入了解保护网络、设备、数据和人员的各种工具，以及使用 Python、Linux 和 SQL 的实践经验。 
IT 支持 
链接：Google IT 支持专业认证 
科技行业内容丰富。它就像从零开始建造一座房子，每个承包商都负责确保不同层级始终符合他们专业领域的黄金标准。这就是 IT 支持的作用所在。 
在这个 Google IT 支持专业认证中，你将了解 IT 支持日常处理的任务，包括计算机组装、无线网络、安装和客户服务。你还将学习如何识别问题，利用如 Linux、域名系统、命令行界面和二进制代码等工具进行故障排除和调试。 
市场营销与电子商务 
链接: Google 的市场营销与电子商务专业认证 
你有软件工程师在构建产品。你有数据分析师在分析数据。你有项目经理确保产品顺利投入生产。你有网络安全和 IT 支持确保一切顺利运行，组织不受攻击。那么一切都准备好了，现在怎么办？ 
销售产品。确保每个人都知道它。通过这个伟大的产品赚取收入！这就是 Google 的市场营销与电子商务专业认证发挥作用的地方。 
你将学习数字营销和电子商务的基础知识，以及如何通过各种数字营销渠道吸引和吸引客户。接着，你将学习如何通过分析来衡量这些渠道的表现，并提供见解。 
总结一下 
一个行业，有 5 种潜在的可能性来帮助你打开大门。所有这些专业人士都是必需的，并构成了组织成功的基础。 
科技世界将继续增长，与此同时，还有更多领域和部门可以让你转型进入。 
今天就开始学习吧！ 
Nisha Arya 是一名数据科学家、自由技术作家，以及 KDnuggets 的编辑和社区经理。她特别关注提供数据科学职业建议或教程以及数据科学的理论知识。Nisha 涉及广泛的话题，并希望探索人工智能如何有利于人类生命的延续。作为一个热衷学习者，Nisha 希望拓宽她的技术知识和写作技能，同时帮助指导他人。 
更多相关内容 
 通过这三个热门证书推进你的科技职业庆祝科技行业的女性领导角色成为科技行业专业人士的终极路线图人工智能对科技行业的影响科技行业的裁员潮怎么回事？2023 年 Google 数据分析认证评测
科技裁员的原因是什么？ 
 
 原文：www.kdnuggets.com/2023/02/layoffs-tech.html 
 
 
作者提供的图片 
如果你在科技行业，你可能享受了一个辉煌的十年。数据科学是最炙手可热的工作。每位大学教授都告诉你，你的技能将让你保持就业，直到世界末日。每个公司都需要数据科学家，这些职位暂时不会消失。 
 
我们的前三个课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业之路。 
 2. Google 数据分析专业证书 - 提升你的数据分析能力 
 3. Google IT 支持专业证书 - 支持你组织的 IT 需求 
 
除非你最近打开了报纸或查看了头条新闻，否则你会知道这并不完全准确。Twitter、Meta、Netflix、Microsoft 和其他数十家科技公司最近都进行了大规模裁员。那些避免裁员的公司也实施了招聘冻结。总体来看，2022 年几乎有 9 万名科技行业的员工被裁员。 
在 2023 年的前几周，另有 16,000 人被裁员，根据 Crunchbase 的数据。 
突然间，最热门的就业市场变得冷却了不少。 
大型科技公司发生了什么？如果你在科技行业，未来会怎样？你应该担心吗，还是考虑换个职业？ 
这篇文章将回答所有这些问题及更多内容。剧透：如果你在科技行业，你的技能仍然很受欢迎。 
我们是如何到达这里的？ 
解释科技裁员的因素有几个，不过很难确定这些因素中有多少具体导致了裁员。 
首先，科技公司前所未有的增长速度一直有些不可持续。COVID 使得许多电子商务和广告基础公司经历了快速增长，但消费者开始恢复正常后，这种增长难以维持。 
在经历了十年的高估、过度招聘和风投补贴增长后，裁员的“报应”终于到来了。 
其次，正如你无疑已经厌倦听到的那样，目前经济形势很奇怪。供应链问题层出不穷。通货膨胀高得离谱。地缘政治动荡。这一切导致科技行业的增长放缓——足够让科技公司开始恐慌。 
第三，来自多个方面的压力。以 Twitter 为例，目前在埃隆·马斯克的不可预测和混乱的领导下。马斯克对员工数量进行了大幅削减，截至目前，Twitter 仍在运营（尽管有些故障）。这一成功让其他公司董事会质疑，为什么他们要支付如此多员工的薪水，而 Twitter 证明了在员工数量削减一半的情况下仍能运作。 
还有一种社会传染的因素。斯坦福大学商学院教授杰弗里·费弗认为，模仿行为可能是导致我们现在看到的许多裁员的原因。“会有科技衰退吗？有。估值有泡沫吗？绝对有。Meta 过度招聘了吗？可能。但这就是他们裁员的原因吗？当然不是。Meta 有很多钱。这些公司都在赚钱。他们这样做是因为其他公司也在这样做，”费弗 告诉 斯坦福新闻记者梅丽莎·德·维特。 
了解科技裁员的全貌 
让我们深入了解一下多个大型科技公司就业情况究竟发生了什么。 
Meta（Facebook） 
 
图片来源：作者 
Meta 报告称 2022 年 11 月裁员 11,000 人，影响了 13%的员工。这些裁员几乎影响了公司的每个领域，尤其是招聘，因为 Meta 还计划在 2023 年第一季度冻结招聘，并在此后减缓招聘速度。 
大多数专家一致认为，这一现象的主要原因是 Meta 在其元宇宙上的巨大支出。此外，2022 年最后两个季度的广告收入下降。由于 Meta 的成本在第三季度同比增长了 19%，达到了 221 亿美元，收入无法跟上。 
然而，在 Q3 财报的分析师电话会议上，马克·扎克伯格宣布“某些团队将显著增长”，以试图“将[Meta]的投资集中在少数几个高优先级的增长领域”。 
那些领域可能是什么呢？ 
查阅 LinkedIn、Indeed 和 Meta 自己的 招聘页面，这些成千上万的新职位大多出现在科技行业。Meta 正在招聘数据科学家、研究员、分析师和工程师。 
微软 
 
图片来源：作者 
微软进行了非常小规模的裁员，据报道 这次裁员影响了不到 1% 的员工。 
这次裁员的原因很简单：2022 年第三季度的收入增长是过去五年来最慢的。 
我想稍微提一下，收入仍然 增长。只是增长的速度减慢了。 
LinkedIn 报道称微软仍在科技招聘网站上招聘数千个职位，包括研究、机器学习和数据科学领域。 Indeed 和微软的招聘 网站 也证实了这一点。 
Netflix 
 
图片来源：作者 
流媒体巨头在过去几年面临了独特的困难，科技泡沫暂且不提。在 2022 年 5 月，Netflix 报告了十年来的首次用户流失。因此，公司 进行了 450 人的裁员。 
尽管有这些裁员，Netflix 仍在其 内部招聘网站 上提供多个数据科学和工程职位。LinkedIn 显示 有 200 个职位空缺，其中一些在分析师/数据科学领域， Indeed 也显示了类似的职位。 
亚马逊 
 
图片来源：作者 
这家电商巨头 裁员 了 18,000 人。然而，裁员主要集中在公司员工。裁员影响了该部门大约 5% 的员工和其全部员工的 1.2%。公司将此归因于疫情期间大规模扩张带来的压力。 
目前的就业前景并没有那么严峻。 Indeed、LinkedIn 和亚马逊的 网站 显示了许多科技行业的职位机会。 
推特 
 
作者提供的图片 
Twitter 可能是报道最多的裁员来源，原因在于 Twitter 上的记者与新管理层之间的敌意。Twitter 因新任 CEO 埃隆·马斯克的任性裁员超过了其 7,500 人的一半员工。 
马斯克报道Twitter 每天损失超过 400 万美元，迫使其“别无选择”只能裁员约 3,700 名员工，包括信任与安全团队、内容审核团队以及 Twitter 总部的清洁工。 
与名单上的其他公司不同，Twitter 的独特之处在于公司内没有任何职位空缺。Twitter 的职业网站上只有 24 个职位空缺，全部仅供“未来考虑”。LinkedIn和Indeed也显示一样。 
其他科技公司 
这五个例子无疑是最大的和最知名的，但并非唯一的裁员科技公司。其他裁员的公司还包括 Snap（20%的员工）、Salesforce（裁员 9,090 人）、Zillow（25%的员工）以及更多公司。 
这是layoffs.fyi 的图表，展示了 2022 年和 2023 年科技行业的裁员情况。 
 
来源：layoffs.fyi 
我喜欢他们的表格，展示了自 COVID-19 开始以来裁员的 1,910 家公司。如果你想了解得更清楚，我推荐你查看一下。 
或者，更好的是，这里有可视化的数据。 
 
来源：layoffs.fyi 
该怎么做呢？ 
如果你仔细阅读了这篇文章，我希望你能了解到三件事： 
 裁员已经影响了许多科技公司。如果你被裁员了，这与您的技能水平或能力无关。只是现在的市场情况。裁员 并未 过分针对技术员工。也就是说，科技公司在裁员时要么对所有员工都采取行动，要么在非工程、分析、数据科学等领域进行裁员。大多数这些公司仍在招聘技术职位，Twitter 是一个显著的例外。
如果你在科技行业，由于科技公司在努力调整方向，你可能会被裁员，这并非完全由于你的过错。但仍然有很多机会。 
那么你该怎么做呢？ 
确保你不会长时间失业的最佳措施就是保持你的面试技巧敏锐。我建议查看像我们自己的StrataScratch这样的平台，它会帮助你练习常见的面试问题，包括编码和非编码问题。 
你也可以参加像Kaggle或Leetcode这样的网站上的编码竞赛，这是提高或保持你的数据科学技能的好方法。Hackerrank 还提供了一个很棒的面试准备工具包，如果你很久没参加技术面试了，这可能会很有用。它还提供 Python 和问题解决等有价值的数据科学技能的认证。 
最后，我建议花点时间审视一下你的网络。更新你的 LinkedIn，并向你在其他公司可能有的老联系人发送信息，提议见面。很多就业机会不仅依赖于你知道什么，而更在于你认识谁。 
最终想法 
在所有关于裁员的恐慌中，我觉得这篇文章来自 MarketWatch 非常有趣。你能猜到他们称为“美国最佳职位”的是什么吗？ 
这是一个软件工程师。 
所以说，很容易被关于裁员的坏消息所困扰并担心你的未来就业。我想重申，如果你发现自己被裁员，那不是因为你的问题，而是因为市场正在降温。 
与其担心是否会被裁员，不如采取行动提高从一个技术职位直接跳槽到另一个的机会。 
尽管头条新闻可能会这样说，但职位板证明许多公司仍然需要熟练的技术员工。 
Nate Rosidi 是一位数据科学家，专注于产品策略。他也是一名讲授分析学的兼职教授，并且是StrataScratch，一个帮助数据科学家准备面试的平台注册者。可以在Twitter: StrataScratch或LinkedIn上与他联系。 
了解更多主题 
 2024 年技术裁员激增：谁该负责？科技行业所有领域的热门谷歌认证Python 中的所有集合超级巴德：能够做所有事情且更出色的 AI今天所有市场营销分析和数据科学专业人士需要的 5 项技能如果你想精通生成式 AI，请忽略所有（除了两个）工具
KDnuggets™ 新闻 21:n48，12 月 22 日：使用管道编写干净的 Python 代码；成为伟大数据科学家所需的 5 项关键技能 
 
 原文：www.kdnuggets.com/2021/n48.html 
 
特性 | 产品 | 教程 | 观点 | 排行榜 | 职位 | 提交博客 | 本周图片 
本周 KDnuggets：使用管道编写干净的 Python 代码；成为伟大数据科学家所需的 5 项关键技能；将机器学习算法完整部署到实时生产环境；成功数据科学家的 5 个特征；学习数据科学统计的最佳资源；以及更多精彩内容。 
KDnuggets 顶级博客奖励计划 每月向顶级博客作者支付奖励。接受转载，但原创投稿的转载率是 3 倍。阅读我们的 指南 并先提交你的博客到 KDnuggets！ 
特性 
 **，作者：Khuyen Tran**，作者：Sharan Kumar Ravindran将机器学习算法完整部署到实时生产环境，作者：Graham Harrison成功数据科学家的 5 个特征，作者：Matthew Mayo学习数据科学统计的最佳资源，作者：Springboard
产品，服务 
利用 AI 和分析引擎更快地准备时间序列数据，作者：PI.EXCHANGE 
教程，概述 
 每位数据科学家都应了解的三个 R 库（即使你使用 Python），作者：Terence Shin如何加速 XGBoost 模型训练，作者：Michael Galarnyk云机器学习透视：2021 年的惊喜与 2022 年的预测，作者：George Vyshnya如何在没有合适学位的情况下进入数据分析领域，作者：Zulie Rane
观点 
 我如何在 14 年里将薪资提升 14 倍，作者：Leon Wei2022 年及以后 10 大 AI 与数据分析趋势，作者：David Pool聊天机器人转型：从失败到未来，作者：Lubo Smid为什么我们总是需要人类来训练 AI——有时是实时的，作者：Shoma Kimura
最新故事 
 2021 年十大故事：我们不需要数据科学家，我们需要数据工程师；成为数据科学家的指南（逐步方法）；我如何在 18 个月内将收入翻倍，作者：Gregory Piatetsky最新故事，12 月 13-19 日：使用管道编写清晰的 Python 代码，作者：KDnuggets
职位 
 查看我们最近的 AI、分析、数据科学、机器学习职位你可以在 KDnuggets 的职位页面上免费发布与 AI、大数据、数据科学或机器学习相关的行业或学术职位，详细信息请发送电子邮件至 kdnuggets.com/jobs
本周图片 
 
 ![成为伟大的数据科学家所需的 5 项关键技能 
 成为伟大的数据科学家所需的 5 项关键技能 
 
更多相关主题 
 使用管道编写清晰的 Python 代码成为伟大的数据科学家所需的 5 项关键技能低代码：开发人员仍然需要吗？KDnuggets™新闻 22:n09，3 月 2 日：讲述一个伟大的数据故事：A…KDnuggets™新闻 22:n01，1 月 5 日：跟踪和可视化的 3 个工具…利用数据科学让清洁能源更加公平
构建有效的数据分析团队和项目生态系统以实现成功 
 
 网址：www.kdnuggets.com/2021/04/build-effective-data-analytics-team-project-ecosystem-success.html 
 
评论 
作者：Randy Runtsch，数据分析师 
介绍 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速入门网络安全职业。 
 2. Google 数据分析专业证书 - 提升您的数据分析水平 
 3. Google IT 支持专业证书 - 支持您的组织在 IT 领域 
 
在长期的软件开发、信息安全和数据分析职业生涯中，我观察到大型、复杂和令人不知所措的项目有可能无法满足利益相关者的需求，除非它们得到极其出色的管理。相反，我参与的大多数成功的数据分析项目规模、团队规模和时间线都很小。它们通常在几天、几周或几个月内完成，并且通常能满足最终用户的需求。 
高效的管理、敏捷的实践、熟练的从业者、强大的工具、标准和指南可以结合起来创建一个数据分析生态系统，从而实现短项目周期和有用的解决方案。以下部分描述了我的团队开发和利用的一些组织、项目和数据分析特质与技术。虽然我们在大型企业的风险管理和内部审计职能中构建了数据分析程序，但你也许可以将这些技术应用于你的工作环境中。 
良好的管理与领导力 
在我 34 年的软件开发和数据分析工作经历中，我观察到许多管理和领导风格，无论好坏。我最近在与一位创建框架并指导我们团队在大型企业中建立成功的部门数据分析程序的经理合作时，取得了"
KDNuggets 博客中文翻译（九十六）,2024-10-28,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
数据科学诗 
 
 原文：www.kdnuggets.com/2019/05/data-science-poem.html 
 
 评论 
由 Igor Korolev, DO, PhD，Brainformatika 
 
图片来源。 
 
我们的前三名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业轨道。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织进行 IT 工作 
 
数据科学非常酷， 
它充满了许多有用的工具。 
R、Python 和 Excel， 
Tableau、SAS 和 SQL。 
定义问题，收集数据 
然后你会发现有用的见解。 
提取、转换、加载和清理 
在大屏幕上进行可视化。 
跟随领域专家的指导， 
创建数据所需的特征。 
选择要运行的机器学习算法 
回归、随机森林， 
神经网络带来终极乐趣。 
将数据分成训练集和测试集 
找到最有效的参数 
使用 AUC、召回率和精度进行评估 
提供有助于决策的见解 
数据科学需要批判性思维和学习 
但它可能会有深远的影响和回报。 
更多相关话题 
 停止学习数据科学以寻找目的，并找到目的以…数据科学基础：你需要知道的 10 项必备技能KDnuggets™ 新闻 22:n06, 2 月 9 日：数据科学编程…数据科学定义幽默：一组独特的名言…5 个数据科学项目以学习 5 项关键数据科学技能KDnuggets 新闻，11 月 30 日：什么是切比雪夫定理以及如何…
一个能让你找到工作的数据科学作品集 
 
 原文：www.kdnuggets.com/2021/09/data-science-portfolio-job.html 
 
评论 
 
照片由 Bram Naus 提供，来自 Unsplash。 
 
我们的三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 
 
当我获得第一个数据科学实习时，我还没有毕业。我正在攻读计算机科学学位，但在向雇主展示任何成绩单或学术记录之前，就已经获得了这份工作。我是在自学数据科学的四个月左右才得到了这个角色。 
现在，我已经过渡到同一公司的全职角色。 
我能获得这份工作的唯一途径就是借助我的数据科学作品集。 
仅在过去一年里，我就收到了来自世界各地组织的多个数据科学工作邀请。 
我目前正在做一些自由职业的工作，同时也全职工作，要是没有我为自己建立的作品集，我绝不会有这些机会。 
当我刚开始自学数据科学时，我在 Kaggle 上尝试了一些数据集——简单的，如波士顿房价数据集和泰坦尼克号数据集。 
随着时间的推移，厌倦感开始显现。这些项目有着非常相似的结构，话题也没有真正激发我的兴趣。 
我想从事一些我觉得有趣的工作，于是我上网寻找令我兴奋的话题的数据集。 
一段时间后，我想出了一个数据科学项目的想法。然而，我无法找到满足我开始构建项目所需的所有信息的现成数据集。 
我不得不通过 APIs 和网络爬虫工具自己收集数据，清理数据，然后构建数据框架。这整个过程帮助我了解了从网上收集数据的不同技术。 
这个项目证明比我想象的要困难得多，但我在过程中学到了很多。我开始更好地掌握 Python 及其库。 
一旦我完成了几个项目，我就想在我的 GitHub 仓库中分享它们。 
然而，我并不满足于仅仅分享项目的源代码。我想分享我的发现和我用来收集数据的技术。我想讲述我所做的所有工作的故事。 
我开始撰写解释创建项目步骤的文章。我将这些文章提交给数据科学出版物。 
在创建了大约 3 到 4 个项目后，我建立了一个作品集网站。这个网站包括了项目的基本描述，以及相关的源代码和我写的文章。 
这是一种将我所有工作的成果集中展示的方式，以便任何拥有我网站链接的人都能看到我所做的一切。 
我的作品集网站是我在学习数据科学的头几个月内获得实习机会的唯一原因。 
 
照片来自 Firmbee.com 在 Unsplash。 
在实习期间，我有一些闲暇时间。我利用这些时间学习工作之外的技能，构建项目，并撰写相关文章。 
随着我通过写作开始接触更广泛的受众，招聘人员和数据科学家们根据我写的项目联系了我。我开始自由职业，并为客户按合同基础构建数据科学项目。 
这对我来说是一个很好的方式来提升技能，获取超出日常工作范围的知识。 
我也获得了多次技术写作机会，并且偶尔以自由撰稿人的身份为出版物写作。 
我的作品集包含了我所做的项目、我的博客、推荐信和我合作过的客户。大多数这些机会都是因为我通过写作与世界分享我的工作。 
如果你是数据科学行业的初学者，我建议你学习新知识并写作。编写关于你刚刚学到的概念的教程。 
如果你有一个数据科学项目的想法，就将其付诸实践。这可能需要几天、几周甚至几个月的时间。 
一旦你成功建立了项目，将其分享给全世界。将其分解为通俗易懂的术语，解释你所做的一切。 
这将引起招聘人员的兴趣，当你申请数据科学职位时，它还将使你与那些简历上仅有在线证书的其他申请者区分开来。 
创建项目也是学习和提升数据科学技能的好方法。如果你上过一两门数据科学课程，那么现在是时候通过构建你热衷的项目来运用你的编程和机器学习技能了。 
相关： 
 适合初学者的十大数据科学项目数据科学初学者的投资组合指南如何获得数据科学面试：寻找工作、接触决策者和获取推荐
了解更多相关话题 
 一个能在 2022 年帮你找到工作的数据科学投资组合2022 年能帮你找到工作的数据科学项目3 个保证能帮你获得工作的数据科学项目7 个原因为什么你难以找到数据科学职位KDnuggets 新闻，6 月 1 日：数据科学的完整合集…成为优秀数据科学家所需的 5 项关键技能
一个能让你在 2022 年获得工作的数据科学作品集 
 
 原文：www.kdnuggets.com/2022/10/data-science-portfolio-land-job-2022.html 
 
 
图片来源：编辑 
如果你正在阅读这篇文章，很可能以下一项或多项陈述适用于你： 
 你没有数据科学硕士学位，也没有相关领域的经验。每个你遇到的数据科学职位招聘信息都要求至少 2-3 年的相关经验。但如果没有经验，没人愿意雇佣你，你怎么获得经验呢？你已经参加了无数的数据科学在线课程，它们都教给你类似的内容。你最终陷入了一个陷阱，你并没有真正学到新的东西，只是觉得自己在学习。你甚至创建了数据科学项目，并将其包含在你的作品集中，希望它们能为你在该领域找到工作，但这些项目没有成功。
 
我们的前三名课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业轨道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析能力 
 3. 谷歌 IT 支持专业证书 - 支持你所在组织的 IT 
 
如果以上任何陈述与你有关，那么你可能在以下四点上做错了： 
1. 你参加了太多的在线课程 
每次你参加一个新的针对初学者的在线课程并将其添加到简历上时，你是在告诉雇主你在这个领域还是新手。这使得你看起来缺乏经验，可能会对你的作品集造成比好处更多的伤害。 
我见过很多数据科学候选人在简历上列出超过十个类似的在线课程，却没有任何项目或现实应用来展示他们学到的技能。 
这里是你应该做的事情 
参加一到两个数据科学在线课程，并基于所学技能创建项目。如果你在构建这些项目时发现自己在特定领域缺乏专业知识，那么参加一个填补你知识空白的课程。 
这样，你不仅能更快地学习，还能在简历上展示广泛的技能，而不是列出十个教授相同内容的课程。 
2. 你没有从人群中脱颖而出 
大多数申请者的另一个错误是，他们在简历上列出相同类型的项目。 
像鸢尾花分类和泰坦尼克号生存预测这样的项目非常受欢迎。 
由于其简单性，这些是大多数人在学习数据科学时创建的第一个项目。如果你将这些项目添加到简历中，你是在告诉雇主你缺乏经验和创造力。 
其他数百名申请者已经创建了相同的项目并在他们的作品集中提到这些项目。既然你的简历与 80%的其他数据科学 aspirants 没有区别，为什么你会获得工作邀请？ 
相反，你应该这样做 
创建展示各种技能的数据科学项目，例如数据分析、机器学习和数据预处理。如果你需要灵感来创建独特的项目，可以查看我最好的 5 个数据科学作品集项目。 
现实世界的数据通常是脏的，需要从外部来源收集，而不像 Kaggle 数据集那样已经结构化和预处理。 
创建一个端到端的数据科学项目是个好主意，涉及通过 API 或网络抓取技术收集数据。 
这向雇主展示了你的编程技能足够强大，可以处理现实世界的数据科学用例。 
3. 你做的研究不够 
许多求职者倾向于向每个开放的数据科学职位投递简历，认为这会增加他们找到工作的机会。 
然而，这样做实际上会降低你的就业机会。你可能会收到一封通用的拒绝邮件，或者根本没有来自你申请公司的回复。 
这是因为你在申请时没有考虑公司实际在寻找什么。每个空缺职位都是独特的，不同公司雇佣数据科学家的原因各不相同。 
例如，一家电子商务公司可能会在其市场部门雇佣数据科学家，建立一个推荐系统，鼓励客户在网站上进行更多购买。 
另一方面，一家科技公司可能会雇佣数据科学家来帮助他们的产品团队引入新功能并衡量产品成功。 
尽管这两个职位的职位名称都是“数据科学家”，但它们的工作范围却不同。如果你用相同的简历申请每个职位而没有考虑公司的用例，这一点会表现出来。 
相反，你应该这样做 
选择几个你想申请的公司。阅读关于它们的信息，并对它们所在的行业进行一些研究。 
然后，尝试创建与该行业相关的项目。这告诉招聘经理你会为他们增值，因为你已经做过类似他们用例的项目。 
你甚至可以更进一步，联系那些已经在你想加入的公司工作的数据科学家。通过 LinkedIn 或电子邮件与他们联系，尝试了解他们参与的项目类型。 
然后，你可以创建一些相关的项目，以使你的简历在其他申请者中脱颖而出。 
4. 你没有发挥你的优势 
几个月前，一位有志成为数据科学家的求职者联系了我。她希望能在数据科学领域找到一份工作，但未能成功。 
我审阅了她的简历，并立即意识到问题所在。 
这位申请者来自市场营销背景，只参加过一次数据科学 Bootcamp。 
她在简历中突出的技能包括编程、机器学习和统计。 
任何阅读到那份简历的雇主都能看出她在上述科目上的知识有限，因为从三个月的 Bootcamp 中能学到的知识是有限的。 
她应该这样做 
这位申请者的优势在于她的市场营销领域知识。 
由于有市场营销背景，她应该创建一些与市场营销分析相关的项目。 
大多数市场营销专家无法处理大量数据。他们缺乏技术和分析技能。 
由于她已经拥有市场营销领域的专业知识，她只需掌握一些 Python、Excel 和 SQL 技能。然后，她应该使用这些工具创建一些市场营销数据分析项目，并将它们包含在她的作品集中。 
这本来可以很容易地为她在分析领域找到一份工作。 
然后，在该领域有一两年的经验后，她可以从数据分析转型为数据科学。 
数据科学是一个需要应用统计知识以及强大的分析和编程能力的领域。对大多数人来说，没有正式的资格认证，很难立即获得数据科学的职位。 
因此，首先进入门槛较低的领域，如数据工程或分析，然后逐渐过渡到数据科学，是有意义的。 
我的数据科学作品集如何让我在该领域找到工作 
我在没有硕士学位的情况下成功获得了多个数据科学领域的职位。 
在我的第一次数据科学实习之后，我成为了同一公司的高级顾问，现在在一家跨国公司担任预测分析师。 
我获得这些工作不是因为我是最符合条件的申请者，而是因为我展示了对这个领域的真正兴趣。雇主寻找的是对工作充满热情的申请者，因为技能可以随着时间的推移得到提升。 
我创建了自己感兴趣的主题的数据科学项目，并撰写了相关的博客文章。我将我的作品发布在我的数据科学博客和作品集网站上。 
然后，我在简历上包含了我的网站和博客链接。我在简历的“项目”部分突出了一些我最好的项目，并写了几行说明。 
在获得我的第一个实际数据科学职位后，我积累了一些该领域的经验，这帮助我创建了更大、更有趣的项目。随着我继续在线发布我的工作，雇主们开始联系我进行 自由项目。 
我被聘请来为公司收集大量数据，建立机器学习模型，进行市场研究，并参与自由写作项目。所有这些任务都在我的数据科学家日常工作之上。 
建立强大的在线存在感是获得工作机会的关键。许多有才华的人因为雇主不知道在哪里找到他们而未被发现。 
Natassha Selvaraj 是一位自学成才的数据科学家，对写作充满热情。你可以在 LinkedIn 上与她联系。 
更多相关话题 
 能让你在 2022 年获得工作的数据科学项目KDnuggets 新闻，6 月 1 日: 数据科学的完整收藏…KDnuggets™ 新闻 22:n05, 2 月 2 日: 掌握机器学习的 7 个步骤…3 个保证能让你获得工作的数据科学项目7 个你在找数据科学职位时挣扎的原因无法获得数据科学职位？这就是原因
数据科学作品集项目创意（可能让你找到工作，也可能不会） 
 
 原文：www.kdnuggets.com/2021/10/data-science-portfolio-project-ideas.html 
 
评论 
有哪些数据科学作品集项目创意可以帮助你找到工作？ 
当雇主招聘数据科学家时，他们通常寻找的是能够为他们的业务创造收入和机会的人。仅仅了解编程、机器学习、统计学等是不够的，获取数据科学工作也需要一个作品集来展示你的数据科学技能。一个全面的数据科学作品集可以展示你所有的技能，使你有资格获得该职位。一个解释得当的数据科学作品集应展示你沟通、协作、推理数据、主动性以及技术技能的能力。 
 
我们的三大课程推荐 
 1. Google 网络安全证书 - 快速开启网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你组织的 IT 需求 
 
 
数据科学作品集的重要性 
 数据科学作品集比简历更有价值，因为你可以利用作品集记录你的项目、代码和数据集。通过作品集，你可以展示你的数据科学技能。作品集让你能够与其他专业人士建立联系。它也会增加你获得数据科学工作的机会。
 
构建数据科学作品集最重要的部分是弄清楚要在作品集中添加什么。在你的数据科学作品集中，你需要在 GitHub、你的网站或博客上拥有一些项目。每个项目都应该结构清晰，以便招聘经理可以快速评估你的技能。在这篇博客中，我们将介绍一些应该出现在你作品集中的数据科学项目创意。 
数据科学作品集项目创意 
在将项目添加到你的作品集之前，你需要了解应该添加哪些数据科学项目以及需要避免哪些。这就是我们现在要讨论的数据科学作品集项目创意。 
你应该添加那些与你角色相匹配的项目。例如，如果你打算申请分析师职位，构建使用数据清洗和讲故事的项目可能对你有用。 
你的数据科学作品集应该包含 3-5 个项目，以展示你的能力： 
 沟通与利益相关者和团队成员协作拥有技术能力理解数据积极主动拥有领域专业知识
数据清洗项目 
你应该添加能够展示你数据清洗技能的项目。找到一个混乱的数据集，然后清理数据并进行基本分析。尝试寻找并处理一些非结构化数据，以提高你的技能。你还可以通过 API 或网页抓取收集自己的数据。 
数据可视化和讲故事的项目 
包含那些能够展示你技能的项目： 
 讲故事提供真实的见解说服行动
在这里你需要演示和解释你的代码在做什么，因此数据可视化和良好的沟通技巧非常有用。 
构建端到端项目 
构建端到端项目是向招聘经理展示你具备提取见解和向他人展示见解的技能的最佳方式。这表明你知道如何处理和处理数据，然后生成一些输出。 
真实数据和网页抓取 
你可以使用真实数据进行分析，而不是预先清理过的数据。数据收集、清洗、准备和转换是数据科学工作的真实部分。网页抓取也是获取有趣数据的好方法。 
尝试选择有趣的分析 
不论你发现什么，选择有趣的数据都是一个好主意。最好的作品集项目更注重处理有趣的数据，而不是展示复杂的建模。 
数据科学作品集中不应包括的项目 
建议你的作品集中不要包含常见项目。你需要避开最常见的项目创意来建立你的作品集。尝试提出一些真正能够使你与众不同的项目。 
以下是一些最常见的项目，如果你在数据科学作品集中包含它们，可能会对你不利： 
 在泰坦尼克号数据集上进行生存分类。MNIST 数据集上的数字分类。使用虹膜数据集进行花卉物种分类。
这些是最常见的项目，可能对你帮助不大。你无法通过这些数据集与他人区分开来。你必须确保列出新颖的项目，以便从众多项目中脱颖而出。 
具体数据来源的创意 
一些很棒的数据来源包括 - Reddit、Tumblr、体育、维基百科、非营利组织、大学网站等。还有一些难度较大的来源，因为它们有严格的 API 政策 - Facebook、Yelp、Foursquare、LinkedIn 和 Craigslist。 
一旦你有了一些有趣的项目可以添加到你的数据科学作品集中，你的下一步就是以最佳方式展示你的工作。为了增加你数据科学项目的权重，你可以使用 GitHub URL，撰写关于你成就的博客，并使用 BI 工具创建仪表盘。 
原文。转载已获许可。 
相关： 
 如何通过从事作品集项目来应对数据科学面试如何作为初学者建立强大的数据科学作品集一个能让你获得工作的数据科学作品集
更多相关话题 
 成为一名优秀数据科学家所需的 5 项关键技能每个初学者数据科学家应该掌握的 6 个预测模型2021 年最佳 ETL 工具每个数据科学家都应该了解的三个 R 库（即使你使用 Python）如何在 GPT-4 时代被聘为数据科学家初级 ML 工程师实际需要了解什么才能获得聘用？
使用数据科学预测和防止现实世界问题 
 
 原文：www.kdnuggets.com/2021/04/data-science-predict-prevent-real-world-problems.html 
 
评论 
各行各业的人们越来越感兴趣于利用数据科学来了解改善决策的重要趋势。数据科学在预测事件和防止不良后果方面也发挥了重要作用。以下是一些发人深省的例子。 
 
图片来自 Gerd Altmann Pixabay 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
1. 阻止网络攻击 
许多行业现在都有广泛的在线存在。然而，定向网络攻击可以迅速阻碍整个公司，迫使员工暂时转向纸质记录，或处理文件或门户的访问丧失。 
然而，数据科学可以帮助公司 IT 团队掌握并阻止威胁。例如，关联规则学习（ARL）基于某些特征来评估风险。它允许管理人员根据是否与过去的问题有共同点来预测可能的问题。 
当数据科学帮助发现异常值时，预防也变得可能。假设系统发现一个来自外国的未知 IP 地址的人在深夜试图访问公司网络。它可以自动拒绝该个人的访问，直到人工进行更仔细的检查。 
2. 改善执法政策 
对有色人种的持续警察暴力问题导致了对执法部门进行重大变革的呼声。然而，大多数人不知道如何推进。 
一个名为 Campaign Zero 的组织使用数据驱动的方法来提出未来警务的新政策框架。该组织的研究人员审查了 600 份警察工会合同。结果显示，84% 的文件包含至少一项条款，使得追究警察责任变得困难。 
数据还显示哪些地点的警察部门与黑人男性死亡的案件相关联，这些案件的比例高于全国谋杀率。应用数据分析的另一种方法是评估哪些警官的不当行为或投诉事件高于平均水平。警察局长可以据此做出部门决策，以预测和防止不必要的暴力。 
3. 保持输出水平高 
人们特别感兴趣的是数据科学如何帮助那些成功几乎完全依赖于实现一致输出的行业。在这些情况下，意外事件或误导性的判断可能导致利润骤降和客户不满。 
然而，数据科学也可以支持制造厂等设施中使用的预防性维护程序。研究表明，80% 的维护技术人员宁愿预防问题，也不愿意应对问题。当工厂领导使用连接传感器将数据传输到分析平台时，他们可以预测部件何时会故障或通过提醒相关人员关注不利趋势来防止问题发生。 
在澳大利亚，政府官员认为数据科学可能是改善采矿行业结果的关键。寻找新的高质量矿床的困难意味着公司通常会进行成本高昂的勘探工作，但结果却不尽如人意。数据科学可以在这方面提供帮助，同时也能监控已建立矿井的通风系统。工人的安全性将得到提升，而公司则能避免过高的成本。 
4. 帮助健身设施吸引客户 
许多人都能与加入健身房并兑现健身承诺但随着时间推移变得不再热衷的情境产生共鸣。数据科学可以预测导致动机丧失的原因，并防止其发生。它还可以帮助为会员提供个性化的推荐。 
一家名为 CIPIO 的风险投资科技公司帮助健身房老板将数据转化为可操作的策略。记录可能显示某位会员只参加了瑜伽课程，并且他们的出勤率逐渐变得不稳定。系统可以建议健身房工作人员通知该会员有关一个将瑜伽与短时间高强度心肺训练结合的课程。这个建议可能通过提供不同的机会来引起兴趣。 
这样的工具还可以显示更广泛的趋势，例如大多数会员在哪几个月结束订阅。如果数据显示这种情况通常发生在三月，俱乐部可以提供额外的激励措施，以减少会员流失的可能性。它还可以预测对新特定类型课程的可能兴趣，例如适合老年人或受伤者的椅子锻炼。 
5. 避免缺货情况 
当消费者打算购买某物却发现货架空空时，他们自然会感到沮丧。这就是为什么许多零售商现在利用大数据来预测需求并防止库存短缺。数据分析系统可以查看社会趋势、天气模式、区域偏好和其他细节，以评估人们购买的可能性。 
这种方法还可以将适当数量的商品发送到正确的位置，例如具有全国性网点的连锁店。数据可能显示在科罗拉多州对运动服的需求迅速增加，而在阿肯色州这种销售下降或保持平稳。零售商可以利用这些信息保持商店的适当库存。 
一些大型节日，如英国的格拉斯顿伯里，也依赖数据科学来预测结果和防止失望。研究人员分析数据，以了解某一年是否可能有比平常更多或更少的降雨。这些信息帮助零售商知道应该带什么样的商品。相同的方法使供应商能够确定最受欢迎的食品和饮料，以便更好地准备。 
数据科学具有丰富的潜力 
大多数商业领袖知道了解不良事件整体可能性的的重要性，并尽力阻止它们发生。数据科学可以使这些目标更可管理，无论是在这里的例子中还是在其他地方。 
虽然无法确定地预测未来，但数据科学可以追踪趋势并从中提取有意义的见解。无论行业或服务的受众如何，它都可以帮助商业决策者平稳运营行业，同时减少不良结果。 
简介：德文·帕提达是一位大数据和技术作家，同时也是ReHack.com的主编。 
相关内容： 
 预测分析在劳动行业的潜力数据科学家开发了更快的方式来减少污染，减少温室气体排放自动化如何改善数据科学家的角色
更多相关内容 
 大数据如何实时拯救生命：IoV 数据分析帮助…数据科学项目如何帮助解决现实世界问题Kaggle 竞赛对现实世界问题有用吗？今天 90% 的代码是为了防止失败，这才是问题所在利用基础和现代算法解决计算机科学问题如何使用 Python 和机器学习预测足球比赛赢家
数据科学预测未来 
 
 原文：www.kdnuggets.com/2018/06/data-science-predicting-future.html 
 
 评论 
数据科学中的预测分析依赖于解释性数据分析，这正是我们在前一篇文章中讨论的内容 - 数据科学的数据的什么、哪里和如何。我们讨论了数据科学中的数据，以及商业智能（BI）分析师如何利用它来解释过去。 
事实上，一切都是相互关联的。一旦 BI 报告和仪表板准备好，并且从中提取出洞察，这些信息就成为预测未来值的基础。而这些预测的准确性取决于所使用的方法。 
 
我们的前 3 个课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你组织的 IT 
 
回顾传统数据与大数据在数据科学中的区别，或参考我们关于数据科学的第一篇文章。 
我们可以对预测分析及其方法做出类似的区分：传统数据科学方法与机器学习。一者主要处理传统数据，另一者则处理大数据。 
 
数据科学中的传统预测方法：它们是什么？ 
传统的预测方法包括线性回归分析、逻辑回归分析、聚类分析、因子分析和时间序列等经典统计方法。这些方法的输出将输入到更复杂的机器学习分析中，但让我们先逐一回顾它们。 
一个快速的旁注。数据科学行业中的一些人也将这些方法称为机器学习，但在本文中，机器学习指的是更新、更智能、更好的方法，如深度学习。 
 
线性回归 
在数据科学中，线性回归模型用于量化分析中不同变量之间的因果关系。比如房价与房屋大小、邻里环境和建造年份之间的关系。该模型计算系数，通过这些系数你可以预测一栋新房的价格，只要你有相关的信息。 
逻辑回归 
由于不可能将所有变量之间的关系表达为线性关系，数据科学利用如逻辑回归等方法来创建非线性模型。逻辑回归处理的是 0 和 1。例如，公司在筛选过程中应用逻辑回归算法来过滤求职者。如果算法估计一个潜在候选人在公司一年内表现良好的概率超过 50%，它会预测 1，即成功申请。否则，它会预测 0。 
聚类分析 
当数据中的观察值根据某些标准形成组时，就会应用这种探索性数据科学技术。聚类分析考虑到一些观察值具有相似性，并促进了发现新的重要预测因素，这些因素可能并不在数据的最初概念化中。 
因子分析 
如果聚类是关于将观察值分组，那么因子分析则是关于将特征分组。数据科学利用因子分析来降低问题的维度。例如，如果在一个 100 项的问卷中，每 10 个问题属于一个通用态度，因子分析将识别这 10 个因子，这些因子可以用于回归，从而提供更可解释的预测。数据科学中的许多技术都是这样集成的。 
时间序列分析 
时间序列是一种流行的方法，用于跟踪特定值随时间的发展。经济学和金融领域的专家使用它，因为他们的主题是股票价格和销售量——这些变量通常是相对于时间进行绘制的。 
数据科学如何在传统预测方法中找到应用？ 
相应技术的应用范围极广；数据科学正在逐步进入越来越多的行业。尽管如此，有两个突出的领域值得讨论。 
用户体验（UX）与数据科学 
当公司推出新产品时，他们通常会设计测量客户对该产品态度的调查问卷。在 BI 团队生成仪表板后分析结果包括将观察值分组到不同的区域（例如地区），然后单独分析每个区域，以提取有意义的预测系数。这些操作的结果通常证实了结论，即产品在每个区域需要进行轻微但显著不同的调整，以最大化客户满意度。 
预测销售量 
这是时间序列分析发挥作用的类型。销售数据已被收集到某个日期，数据科学家希望了解下一销售周期或一年后可能发生的情况。他们应用数学和统计模型并进行多次模拟；这些模拟为分析师提供了未来的情景。这是数据科学的核心，因为基于这些情景，公司可以做出更好的预测并实施适当的策略。 
谁使用传统的预测方法？ 
数据科学家。但请记住，这个头衔也适用于使用机器学习技术进行分析的人。许多工作在不同的方法论之间流动。 
数据分析师则是准备高级分析类型的人，这些分析解释了已经出现的数据模式，并忽略了预测分析的基础部分。 
机器学习和数据科学 
机器学习是数据科学的最先进方法。这是完全正确的。 
机器学习相对于任何传统数据科学技术的主要优势在于其核心存在算法。这些是计算机用来寻找尽可能适合数据的模型的方向。机器学习和传统数据科学方法之间的区别在于我们不给计算机指示如何找到模型；它采用算法并利用其方向自主学习如何找到所述模型。与传统数据科学不同，机器学习需要较少的人为干预。事实上，机器学习，尤其是深度学习算法非常复杂，以至于人类无法真正理解“内部”发生了什么。 
明确来说，我们必须注意到，机器学习方法超越了传统方法。例如，监督学习有两个子类型——回归和分类（例如多项逻辑回归）。自然，许多传统方法也属于‘机器学习’这个总称。这是合乎逻辑的，因为线性回归是许多其他方法的基础，包括深度神经网络。 
聚类和主成分分析（PCA），另一方面，是无监督学习算法（对于 PCA 的争论甚至比回归更激烈）。 
无论如何，传统方法和机器学习的区别或多或少是主观的。有些人画了界限，有些人则没有。在我们的框架中，传统方法的简单性（某种程度上的优雅）是区分的主要原因。有关这一问题的有趣观点可以在这里探讨：www.kdnuggets.com/2017/06/regression-analysis-really-machine-learning.html 
最后，深度学习相比传统方法计算开销非常大。为了给你一些背景，我见过一些线性回归是在纸上手动计算的工作。 
对我来说，界限在于：你能否创建一个 CNN 并在合理的时间内在纸上解决它？实际上不能，所以这就是我所称之为机器学习的东西。 
在数据科学中，机器学习是什么？ 
机器学习算法类似于一个试错过程，但特别之处在于每次试验至少和之前一次一样好。但请记住，为了学得好，机器必须经历数十万次试错，每次错误的频率逐渐减少。 
 
一旦训练完成，机器将能够将其所学到的复杂计算模型应用于新数据，仍然可以进行高度可靠的预测。 
机器学习主要有三种类型：有监督学习、无监督学习和强化学习。 
 
有监督学习 
有监督学习依赖于使用标记数据。机器获得与正确答案相关的数据；如果机器的表现没有得到正确答案，优化算法会调整计算过程，计算机会进行另一次试验。请记住，通常机器一次处理 1000 个数据点。 
支持向量机、神经网络、深度学习、随机森林模型和贝叶斯网络都是有监督学习的实例。 
无监督学习 
当数据过大，或者数据科学家在标记数据的资源上面临过多压力，或者他们完全不知道标签是什么时，数据科学便会诉诸于无监督学习。这包括将未标记的数据提供给机器并要求其从中提取见解。这通常会导致数据根据其属性被划分为特定的方式。换句话说，它被聚类了。 
无监督学习在发现数据中的模式方面非常有效，特别是那些传统分析技术下人类可能会遗漏的东西。 
数据科学通常结合使用有监督学习和无监督学习，其中无监督学习对数据进行标记，而有监督学习找到最佳模型以适应数据。其中一个例子是半监督学习。 
强化学习 
这是一种机器学习类型，其重点在于性能（如走路、看、阅读），而不是准确性。每当机器的表现优于之前时，它都会获得奖励；但如果表现不佳，优化算法不会调整计算。想象一下小狗学习命令。如果它执行命令，它会得到奖励；如果不执行命令，就得不到奖励。因为奖励很美味，所以狗会逐渐改进执行命令的能力。也就是说，与其最小化错误，强化学习更注重最大化奖励。 
机器学习在数据科学和商业领域的应用在哪里？ 
欺诈检测 
通过机器学习，特别是监督学习，银行可以利用历史数据，将交易标记为合法或欺诈，并训练模型以检测欺诈活动。当这些模型检测到任何微小的盗窃概率时，它们会标记交易，并实时阻止欺诈行为。 
客户留存 
借助机器学习算法，企业组织可以预测哪些客户可能会购买他们的商品。这意味着商店可以高效地提供折扣和‘个性化服务’，从而减少营销成本并最大化利润。几个突出的名字浮现在脑海：谷歌和亚马逊。 
谁在数据科学中使用机器学习？ 
如上所述，数据科学家深度参与机器算法的设计，但舞台上还有另一位明星。 
机器学习工程师。这是一个专门寻找将最先进的计算模型应用于解决复杂问题（如商业任务、数据科学任务、计算机视觉、自动驾驶汽车、机器人等）的专家。 
数据科学中的编程语言和软件 
处理数据和数据科学时需要两大类工具：编程语言和软件。 
 
数据科学中的编程语言 
掌握编程语言使数据科学家能够设计可以执行特定操作的程序。编程语言最大的优势在于我们可以重用创建的程序，以多次执行相同的操作。 
R、Python 和 MATLAB，结合 SQL，覆盖了处理传统数据、商业智能和传统数据科学时使用的大部分工具。 
R 和 Python 是所有数据科学子学科中最受欢迎的工具。它们最大的优势在于能够处理数据，并且与多种数据和数据科学软件平台集成。它们不仅适用于数学和统计计算；它们还具有很强的适应性。 
SQL 是王者，尤其在处理关系型数据库管理系统时，因为它是为此目的专门创建的。SQL 在处理传统历史数据时最具优势，例如在准备 BI 分析时。 
MATLAB 是数据科学中第四个不可或缺的工具。它非常适合处理数学函数或矩阵操作。 
数据科学中的大数据当然借助 R 和 Python 来处理，但从事这一领域的人通常也精通其他语言，如 Java 或 Scala。这两种语言在合并来自多个来源的数据时非常有用。 
除了上述提到的语言外，JavaScript、C 和 C++ 通常在涉及机器学习的数据科学分支中被使用。它们比 R 和 Python 更快，并提供更大的自由度。 
数据科学中的软件 
在数据科学中，软件或软件解决方案是为特定业务需求调整的工具。 
Excel 是一个适用于多个类别的工具——传统数据、BI 和数据科学。类似地，SPSS 是一个非常著名的工具，用于处理传统数据和应用统计分析。 
另一方面，Apache Hadoop、Apache Hbase 和 Mongo DB 是用于处理大数据的软件。 
Power BI、SaS、Qlik 和尤其是 Tableau 是用于商业智能可视化的顶级软件示例。 
在预测分析方面，EViews 主要用于处理计量经济学时间序列模型，而 Stata 用于学术统计和计量经济学研究，其中回归、聚类和因子分析等技术被不断应用。 
这就是数据科学 
数据科学是一个含糊的术语，涵盖了从处理数据——传统数据或大数据——到解释模式和预测行为的所有内容。数据科学通过传统方法如回归和聚类分析或通过非传统的机器学习技术来进行。 
这是一个广阔的领域，我们希望你更接近理解它是多么包罗万象并与人类生活紧密交织。 
 
简历：Iliya Valchanov 是 365 Data Science 的联合创始人。 
相关内容： 
 使用机器学习进行销售预测向前选择特征：Python 实践示例如何组织一个较大的数据科学团队？
更多相关内容 
 使用回归模型预测加密货币价格为数据游戏做好未来准备：2023 年每位数据科学家需要的顶级技能AI 与数据分析师的对决：影响分析未来的 6 大限制聊天机器人变革：从失败到未来用智能分析未来成功的概率……边界框深度学习：视频标注的未来
数据科学、预测分析在 2016 年的主要发展及 2017 年的关键趋势 
 
 原文：www.kdnuggets.com/2016/12/data-science-predictive-analytics-main-developments-trends.html 
 
 评论我们最近向一些数据科学和预测分析领域的领先专家询问了他们对 2016 年最重要的发展以及他们对 2017 年关键趋势的期望。 
另见之前的帖子 大数据：2016 年的主要发展和 2017 年的关键趋势。AI & 机器学习主要发展和关键趋势的总结将于下周发布。 
一些主要的主题包括 2016 年美国总统选举中的民调失败、深度学习、物联网、对价值和投资回报率的更多关注，以及“大众”行业对预测分析的日益采纳。 
这是专家对数据科学、预测分析在 2016 年的主要发展及 2017 年的关键趋势的看法。 
 
Kirk D. Borne，BoozAllen 的首席数据科学家，天体物理学博士，顶级数据科学/大数据影响者。 
在 2016 年，我看到几个重要的数据科学相关发展，包括 
 公民数据科学家的出现，并伴随着自助分析和数据科学工具的增长；深度学习 被应用于各种用例（包括文本分析）在客户呼叫中心和客户服务接触点出现了由人工智能驱动的聊天机器人；组织对从大数据和数据科学中看到实际投资回报和效益的要求增加，更关注“价值证明”而不是“概念证明”；机器智能成为广泛用例中流程、产品和技术的重要组成部分：联网汽车、物联网、智慧城市、制造业、供应链、处方性机器维护等。
2017 年，我们预计边缘分析用例将得到更大扩展：将机器学习嵌入传感器或靠近数据采集点 – 机器学习可能通过 API 或在靠近数据采集器的处理器中调用，或集成到传感器芯片架构中。发现的模式、趋势、异常和新兴现象（BOI：行为兴趣）将促进许多领域更好更快的预测和处方分析应用：网络安全、数字营销、客户体验、医疗保健、紧急响应、发动机性能、自动驾驶车辆、制造业、供应链等。 
汤姆·达文波特，Babson College 杰出教授，国际分析研究所联合创始人，以及德勤分析的高级顾问。 
2016 年进展 
 分析团队的去中心化：经过一段时间的整合，组织开始将分析工作分散到业务部门和职能部门，在许多情况下尝试保留一定程度的企业协调。专有与开源技术的组合： 许多大型公司正在利用专有和开源的分析及大数据技术—通常是在一个应用程序中组合使用。认知技术的碎片化： 大型、单一的认知技术已被拆分成一系列单功能的 API，这些 API 可以组合成完整的系统。模糊的定量角色： 定量分析师、数据科学家和认知应用开发者之间的区别变得越来越模糊；明确的角色和职位已成过去。
2017 年趋势 
 操作认知应用：认知技术将从“科学项目”转向操作应用。模型假设的质疑： 2016 年总统选举中的民意调查失败将使更多管理者质疑分析模型背后的假设。认知工具的分类： 更多组织将理解和分类可用的不同认知工具，并将其应用于适当的业务问题。推动透明度： 战略性和受监管的机器学习应用的所有者将推动这些应用的更大透明度，并避免使用不够透明的算法。
塔玛拉·达尔，SAS 新兴技术总监。 
今年的“大数据”事件是美国选举周期，因为它将大数据/数据科学/预测分析的讨论带到了公共领域。虽然这些不是大多数人使用的术语，但在美国，我们近距离体验了数据：它的角色、用途和误用、解读和误解，以及正确与错误的洞察。 
随着数据继续渗透到我们职业和个人生活的每个方面，得益于物联网，私人和公共部门将受到压力，确保数据的收集、使用和分析是安全、可靠和伦理的。如果公司处理不当，他们将不复存在。 
约翰·埃尔德，Elder Research 的创始人兼主席，美国最大的分析咨询公司。 
一年前，《科学》杂志将“2015 年度最佳科学突破提名奖”授予了一项尝试复制 100 个顶级实验的研究。然而，研究人员只能复制 39 个实验。尽管如此，这也比流行病学领域的记录要好很多，后者中那些发表的医学“发现”仅有 5-35% 的准确率。我相信，大多数虚假相关的问题都源于糟糕的数据科学。用目标随机化等重采样程序替代过时的显著性公式，可以更好地校准随机结果出现的可能性，尤其是考虑到研究人员和挖掘软件所进行的大规模搜索。出版标准需要新的标准，但结果会更加可靠，节省大量资源，甚至挽救生命。 
安东尼·戈德布鲁姆，Kaggle 的联合创始人兼首席执行官，Kaggle 是领先的数据科学竞赛平台。 
像 Airbnb、Climate Corporation（现为孟山都）和 Opendoor 这样的公司是数据科学如何产生巨大影响的极佳例子。他们建立了强大的数据科学团队，影响了公司内部的决策。在 2017 年，我们将看到这些公司引领采用解决数据科学中一些重大痛点的工具和流程：特别是共享和协作数据科学工作流，并推动模型投入生产。2016 年，学术研究的热点话题从深度神经网络转向强化学习和生成模型。 
在 2017 年，我们应该开始看到这些技术用于实际的商业用例。强化学习的一些有前景的领域包括算法交易和广告定向。 
更多相关主题 
 人工智能、分析、机器学习、数据科学、深度学习等2021 年数据科学与分析行业的主要发展和关键预测2021 年主要发展和 2022 年 AI、数据科学等领域的关键趋势成为一名优秀数据科学家所需的 5 项关键技能每个初学者数据科学家应掌握的 6 个预测模型2021 年最佳 ETL 工具 向“我们如何通过分析改善业务”。这意味着要投资于业务理解的开发，例如使用决策建模，以及将分析模型部署到生产系统中（以及所有相关的组织变革）。
相关内容： 
 大数据：2016 年的主要发展及 2017 年的关键趋势2017 年数据科学趋势Kobielus 对 2017 年数据科学的预测
相关主题更多信息 
 AI、分析、机器学习、数据科学、深度学习…2021 年数据科学与分析行业的主要发展及关键…2021 年的主要发展和 2022 年的关键趋势：AI、数据科学…成为优秀数据科学家所需的 5 项关键技能每个初学者数据科学家应掌握的 6 种预测模型2021 年最佳 ETL 工具
数据科学入门：初学者的基本概念 
 
 原文：www.kdnuggets.com/2017/08/data-science-primer-basic-concepts-for-beginners.html 
 
 
数据科学究竟是什么？ 
 

 我们的前 3 个课程推荐 

1. Google 网络安全证书 - 快速进入网络安全职业轨道。 
2. Google 数据分析专业证书 - 提升你的数据分析技能 
3. Google IT 支持专业证书 - 支持你的组织的 IT 

 数据科学是一个多面向的学科，包括机器学习及其他分析过程、统计学及相关的数学分支，越来越多地借鉴高性能科学计算，最终目的是从数据中提取洞察，并利用这些新发现的信息讲述故事。 
 
对这个多面向的学科感到陌生？不确定从哪里开始？这是一个针对数据科学新手的简短且不太技术化的主题概述，从有监督学习与无监督学习等基础知识到幂律分布和认知偏见的重要性。 
数据科学基础：初学者的 3 个见解 
对于数据科学初学者，3 个基本问题进行了概述：有监督学习与无监督学习、决策树剪枝，以及训练集与测试集。 
数据科学基础：数据挖掘与统计学 
当我第一次接触数据挖掘和机器学习时，我承认：我认为这是一种魔法。能做出准确的重大预测？这是巫术！然而，好奇心很快让你发现一切都是光明正大的，可靠的科学和统计方法承担着责任。 
但这会导致短期内更多的问题。机器学习。数据挖掘。统计学。数据科学。 这些概念和术语有时重叠且显得重复。虽然有很多尝试去澄清这些（永远不安定的）不确定性，但这篇文章将探讨数据挖掘与统计学之间的关系。 
数据科学基础：从数据中可以挖掘出什么类型的模式？ 
数据挖掘功能可以分为 4 个主要“问题”，即：分类和回归（合称为预测分析）；聚类分析；频繁模式挖掘；以及异常值分析。我想你也可以用其他方式来拆分数据挖掘功能，比如关注算法、从监督学习与非监督学习开始等等。然而，这是一种合理且被接受的方法来识别数据挖掘可以完成的工作，因此这些问题每个都在下面进行讨论，重点是每个“问题”可以解决什么。 
数据科学基础：集成学习者介绍 
本文将概述 bagging、boosting 和 stacking，这些是最常用且最著名的基本集成方法。然而，它们并不是唯一的选择。随机森林是另一种集成学习器，它在一个预测模型中使用多个决策树，通常被忽视并被视为“常规”算法。还有其他选择有效算法的方法，下面也会讨论到。 
数据科学基础：幂律与分布 
也称为缩放律，幂律基本上意味着某些现象的少量发生是频繁或非常常见的，而相同现象的大量发生则是不频繁或非常罕见的；这些相对频率之间的确切关系在幂律分布中有所不同。幂律能够描述的自然发生和人为现象的广泛范围包括收入差距、某语言的词频、城市规模、网站规模、地震的震级、书籍销量排名和姓氏的受欢迎程度。 
数据科学基础：认知偏误四个关键点 
一些具体的认知偏误如何（并且确实会）在现实世界中干扰的例子包括： 
 不理解科学却以为自己理解的选民和政治家会因为冬天依然下雪而怀疑气候变化（邓宁-克鲁格效应）确认偏误 最近阻止了民调人员相信任何显示特朗普可能赢得美国总统选举的数据。
相关： 
 机器学习算法：简明技术概述 - 第一部分掌握 Python 机器学习的 7 个步骤数据科学难题，解释
更多相关主题 
 停止学习数据科学以寻找目的，并以目的来……学习数据科学统计的顶级资源建立一个稳固的数据团队成功数据科学家的 5 个特征成为优秀数据科学家所需的 5 项关键技能每位初学者数据科学家都应掌握的 6 种预测模型
数据科学过程生命周期 
 
 原文：www.kdnuggets.com/2021/09/data-science-process-lifecycle.html 
 
评论 
由莉莉安·皮尔森（Lillian Pierson, P.E.），世界级数据领导者和企业家的导师，Data-Mania 首席执行官 
 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速入门网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析水平 
 3. Google IT 支持专业证书 - 支持你组织的 IT 需求 
 
如果你在执行数据项目，确保你的项目顺利进行并为公司带来利润可能会感受到很大的压力。 
目前行业的数据项目失败率高达 80% (根据 Gartner 的数据)，难怪数据专业人员感到压力山大。 
这就是我认为数据行业如此错误的地方。 
问题 
数据和人工智能行业正在蓬勃发展。公司在数据项目上投资数百万 - 然而从上面的统计数据可以看出，通常无法从投入中获得真正的回报。 
为什么会这样？ 
好吧，我们可以整天制定战略 - 但我观察到的一个主要问题，我相信这可能显著导致低回报，是数据工作者缺乏战略和业务知识。 
你看，作为一名数据实施工作者，你可能对“如何做”非常着迷。你喜欢沉浸在细节中。你花时间构建复杂的模型，解决代码问题，开发技术解决方案 - 而且你热爱这份工作。 
但我有一个重要的问题要问你。 
你是否总是知道你为什么要这样做？ 
我曾经也是一名数据科学家。虽然现在我花时间在泰国苏梅岛发展我的数据创业公司 Data-Mania，但你曾经可以在我的办公隔间里找到我忙于构建模型。 
我知道陷入细节中的难处。 
当你过于专注于技术和编码时，很容易忽视实际的商业目标和愿景。你可能会开始原地踏步，跑题，并且整体上导致业务效率低下 - 通常却没有注意到。 
更不用说，在没有清晰了解自己在公司愿景中的位置和没有前进策略的情况下执行项目，可能会令人感到沮丧和低效。没有人喜欢在看不到成果的情况下连续工作数小时。 
数据专业人士感到迷茫并不意外。在我与一小组领导和企业主进行的2020 年数据调查中，87%的企业主报告说他们没有明确的、可重复的盈利数据项目的流程。 
数据专业人士如何在没有强有力的领导和指导框架的情况下出色地完成工作？ 
我们需要确保作为数据实施人员，我们保持对最终目标的关注。作为领导者，我们需要确保数据实施人员从一开始就被纳入整体战略中。 
如果你准备好确保你参与的数据项目始终保持在正轨上并且盈利，那么让我们深入探讨数据科学过程生命周期框架。 
数据科学过程生命周期 - 它是什么 
本质上，数据科学过程生命周期是一个可以用来管理数据实施的结构。 
这使数据实施人员能够看到他们的角色首次进入项目整体图景的地方，并确保有一个连贯的管理结构。 
为了开始，我首先想与大家分享微软的 Azure 数据科学过程生命周期，然后分享我认为我们如何改进它。 
微软将他们的过程生命周期分为四个类别： 
 商业理解数据获取和理解建模部署
这个框架在解决上述问题上迈出了巨大的一步：巨大的低效、离题项目、范围蔓延，等等。 
这个过程中的一个完整节点——商业理解——确保技术和数据实施人员在实际尝试解决问题之前理解商业问题。 
不过，我认为这个生命周期可以得到改进。 
我认为这个过程没有充分强调技术人员试图解决的商业问题。通过这个框架，四个节点中的只有一个教育数据实施人员商业敏锐度。 
在深入项目之前进行快速检查是一个好的起点，但还不够。 
为什么需要更深入 
数据实施人员需要对项目背后的大背景有透彻的理解。 
这样考虑一下。 
如果你正在执行数据项目，你就是那个密切关注现场的人。你能获得经理和高层可能无法得到的技术和数据解决方案的内部视角。 
当你完全了解业务愿景并获得了更高层次的洞察时，你将能够发现之前可能忽视的东西。无论是业务问题解决效率低下，还是可能更早实现目标的策略，你都会开始从新的视角审视你的工作。 
如何改进数据过程生命周期 
我建议在这个框架上增加一个第五个职能单元，称之为数据策略单元。 
如果你在心里想着，“等一下，我对数据策略一无所知。” 
不要害怕。 
我绝对不是建议数据实施人员要负责从零开始创建数据策略。相反，这将是一种自上而下的方法。 
这个单元将确保所有技术和数据实施团队对业务问题、目标和愿景有深入的理解——以确保他们每天进行的工作是推动这一愿景的。 
这个单元也将作为一个 sanity check。 
作为一名有多年经验的注册工程师，我深知在项目中陷入细节是多么容易。你需要有一个框架来回归，以便知道如何做出决策。 
通过实施这个过程，数据实施人员将不再在项目中感到迷茫，避免走岔路或将过多时间花费在对整体目标没有贡献的努力上。 
让我们全面审视一下我‘全新升级’的框架，加入数据策略单元： 
 商业理解数据策略数据获取与理解建模部署
将这一部分添加到框架中，确实会成为一种双赢的局面。 
数据实施人员将会从他们的项目中获得更大的成就感和满足感，因为他们保持正轨，并为业务创造更大的成功。 
商业领导者实际上将从他们投入大量精力的数据项目和员工中获得真实的投资回报。 
而整个数据行业将会繁荣发展！ 
简介：莉莲·皮尔森（Lillian Pierson, P.E.） 是一位首席执行官及数据领导者，致力于支持数据专业人士发展成为世界级的领导者和企业家。迄今为止，她已经帮助教育了超过 130 万名数据专业人士，涵盖人工智能和数据科学。莉莲与 Wiley & Sons 出版社合作出版了 6 本数据书籍，并与 LinkedIn Learning 合作开设了 5 门数据课程。她为全球各类组织提供支持，从联合国和国家地理到爱立信和沙特阿美，涵盖了所有领域。她是一位注册工程师，保持良好信誉。自 2007 年以来，她一直担任技术顾问，自 2018 年起成为数据业务导师。她偶尔在全球峰会和数据隐私与伦理论坛中志愿提供专业知识。 
相关： 
 如何几乎在一夜之间从数据自由职业者转型为数据企业家数据职业不是一刀切的！揭示你在数据领域理想角色的技巧数据分析师与数据科学家的 7 大区别
更多相关话题 
 机器学习生命周期优化和管理机器学习生命周期的十大 MLOps 工具如何在几秒钟内处理包含数百万行的 DataFrame机器学习过程的框架在 Python 中处理 CSV 文件的 3 种方法KDnuggets™ 新闻 22:n06，2 月 9 日：数据科学编程…
数据科学流程，重新发现 
 
 原文：www.kdnuggets.com/2016/03/data-science-process-rediscovered.html 
 
上周，KDnuggets 的 热门推文 是对 数据科学家的工作流程或过程是什么？ 的 Quora 回答。这个回答由自称为“神经科学家转数据科学家”的 Ryan Fox Squire 撰写，采用了数据科学流程来描述这种工作流程。 
数据科学流程 
 
我们的前 3 名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业轨道。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织 IT 
 
数据科学流程是一个用于处理数据科学任务的框架，由 哈佛大学 CS 109 的 Joe Blitzstein 和 Hanspeter Pfister 制定。CS 109 的目标，正如 Blitzstein 本人所述，是向学生介绍数据科学调查的整体过程，这一目标应能提供对框架本身的一些洞察。 
 
以下是 Blitzstein 和 Pfister 框架的一个示例应用，关于每个阶段的技能和工具，由 Ryan Fox Squire 在他的回答中提供： 
第 1 阶段：提出问题 
 技能：科学、领域专长、好奇心工具：你的大脑、与专家交流、经验
第 2 阶段：获取数据 
 技能：网络爬虫、数据清洗、查询数据库、计算机科学知识工具：python, pandas
第 3 阶段：探索数据 
 技能：了解数据，提出假设，模式？异常？工具：matplotlib, numpy, scipy, pandas, mrjob
第 4 阶段：建模数据 
 技能：回归、机器学习、验证、大数据工具：scikits learn, pandas, mrjob, mapreduce
第 5 阶段：传达数据 
 技能：演讲、口语、视觉效果、写作工具：matplotlib, adobe illustrator, powerpoint/keynote
Squire 然后（正确地）得出结论，数据科学工作流是一个非线性、迭代的过程，并且需要许多技能和工具来覆盖整个数据科学过程。Squire 还表示，他喜欢数据科学过程，因为它强调了提出问题以指导工作流的重要性，以及随着对数据的熟悉不断迭代问题和研究的重要性。 
数据科学框架是一个创新的框架，用于解决数据科学问题。不是吗？ 
接下来，我们看看 CRISP-DM。 
CRISP-DM 
与 Blitzstein & Pfister 提出的数据科学过程相比，并且由 Squire 进一步阐述，我们快速查看了事实上的官方（但无疑已过时）的数据挖掘框架（已扩展到数据科学问题），即跨行业数据挖掘标准过程（CRISP-DM）。尽管该标准不再积极维护，它仍然是一个受欢迎的框架，用于指导数据科学项目。 
 
CRISP-DM 由以下步骤组成： 
 业务理解数据理解数据准备建模评估部署
你可以在这些模型中看到相似之处：我们首先提出一个问题或寻求对某种特定现象的洞察，我们需要一些数据进行检查，这些数据必须以某种方式进行检查或准备，这些数据用于创建一个合适的模型，然后对生成的模型进行处理，无论是“部署”还是“传达”。尽管与 Blitzstein & Pfister 的数据科学过程相比，CRISP-DM 的工作流允许迭代性的问题解决，而且明显是非线性的。 
就像标准本身不再维护一样，它的网站也没有维护。然而，你可以在其维基百科页面上获取有关 CRISP-DM 的更多信息。对于那些不熟悉 CRISP-DM 的人，这个视觉指南是一个很好的起点。 
所以 CRISP-DM 显然是调查数据科学问题的基础框架，对吗？ 
KDD 过程 
与 CRISP-DM 出现的同时，KDD 过程已经完成了开发。KDD（知识发现数据库）过程，由 Fayyad、Piatetsky-Shapiro 和 Smyth 提出，是一个以“应用特定的数据挖掘方法进行模式发现和提取”为核心的框架。该框架包含以下步骤： 
 选择预处理转换数据挖掘解释
如果你认为“数据挖掘”一词类似于之前框架中的“建模”一词，那么 KDD 过程也相似。请注意此模型的迭代性质。 
 
讨论 
重要的是要注意，这些并不是此领域唯一的框架；SEMMA（用于样本、探索、修改、建模和评估），来自 SAS，以及面向敏捷的游击分析都值得提及。还有许多公司和行业中各种数据科学团队和个人无疑使用的内部流程。 
那么，数据科学过程是对 CRISP-DM 的全新解读，CRISP-DM 只是 KDD 的再加工，还是一个全新的独立框架？嗯，是的，也不是。 
就像数据科学可以被视为数据挖掘的现代版本一样，数据科学过程和 CRISP-DM 可能被视为 KDD 过程的更新。需要明确的是，即使如此，也并不意味着它们变得不必要；对这些过程呈现方式的更新可能对新一代从更新的语言和“新”的框架视角来看待这些过程有所帮助，因此值得关注。 
每一个JavaScript 库都值得吗？我不是这个领域的专家，但我会说可能不值得。虽然这并不是一个完美的类比，但基本观点是，在技术中，工具的使用经常会有所重叠。人们对新奇的事物和不同的东西很感兴趣，因此，即使新包装的术语与之前的术语完全相同或相对相似，也可以起到心理和实际的作用。 
数据科学过程及其前身 CRISP-DM 基本上是 KDD 过程的再加工。这并非出于恶意或阴暗的含义；这并不是指责或指指点点。这只是一个简单的事实陈述：前面的东西影响后面的东西。最终，我们用于进行数据科学的任何框架或过程或步骤，只要它对我们有效并提供准确的结果，就值得使用。即使这恰好是数据科学过程，或者 CRISP-DM，或者 KDD 过程，或当你进入 Kaggle 比赛，或者你的老板让你对小部件数据进行聚类，或你尝试最新的深度学习研究论文时所采取的步骤。 
马修·梅奥 (@mattmayo13) 是一位数据科学家，也是 KDnuggets 的主编，KDnuggets 是一个开创性的在线数据科学和机器学习资源。他的兴趣包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。马修拥有计算机科学硕士学位和数据挖掘研究生文凭。他的联系方式是 editor1 at kdnuggets[dot]com。 
更多相关主题 
 如何在几秒钟内处理拥有百万行的 DataFrame接近机器学习过程的框架Python 中处理 CSV 文件的 3 种方法KDnuggets™ 新闻 22:n06，2 月 9 日：数据科学编程…数据科学定义幽默：奇趣名言集…5 个数据科学项目以学习 5 个关键数据科学技能
数据科学过程 
 
 原文：www.kdnuggets.com/2016/03/data-science-process.html 
 
评论 
由 Springboard 提供。 
在 Springboard，我们的数据学生经常问我们诸如“数据科学家做什么？”或者“数据科学的一天是什么样的？”之类的问题。 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 加速进入网络安全职业的快车道。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析能力 
 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 
 
这些问题很棘手。答案可能因角色和公司而异。 
 
图 1: 数据科学过程，来源: 维基百科 
所以我们问了 Raj Bandyopadhyay，Springboard 的数据科学教育总监，看他是否有更好的答案。 
结果发现，Raj 使用了一个非常有用的框架，这不仅是理解数据科学家做什么的一种方式，也是一种拆解任何数据科学问题的备忘单。 
Raj 将其称为“数据科学过程”，他在短短的 5 天电子邮件课程中详细阐述了这个过程。以下是他的洞察总结。 
步骤 1: 确定问题 
在你解决问题之前，首先要做的是准确地定义问题是什么。你需要能够将数据问题转化为可操作的内容。 
你经常会从有问题的人那里得到模糊的信息。你需要培养将稀缺信息转化为可操作输出的直觉——并提出其他人没有问的问题。 
比如说你在为公司销售副总裁解决问题。你应该从了解他们的目标和数据问题背后的原因开始。在你开始考虑解决方案之前，你需要和他们一起明确问题。 
做这件事的一个好方法是提出正确的问题。 
然后，你应该弄清楚销售流程的样子以及客户是谁。你需要尽可能多的背景信息，这样你的数字才能转化为洞察。 
你应该提出以下问题： 
 谁是客户？他们为什么购买我们的产品？我们如何预测客户是否会购买我们的产品？表现良好的细分市场和表现不佳的细分市场有什么不同？如果我们不主动向这些群体销售产品，我们将损失多少资金？
针对你的问题，销售副总裁可能会透露他们想了解为什么某些客户群体的购买量低于预期。他们的最终目标可能是确定是否继续投资这些客户群体，或将其降级。你需要根据这个问题调整你的分析，并挖掘出能够支持任何结论的洞察。 
在这个阶段结束时，你需要拥有解决问题所需的所有信息和背景。 
步骤 2：收集解决问题所需的原始数据 
一旦你定义了问题，你需要数据来提供转变问题的解决方案所需的洞察。这个过程涉及思考你需要什么数据，并找到获取这些数据的方法，无论是查询内部数据库还是购买外部数据集。 
你可能会发现你的公司将所有销售数据存储在 CRM 或客户关系管理软件平台中。你可以将 CRM 数据导出为 CSV 文件以进行进一步分析。 
步骤 3：处理数据以进行分析 
现在你拥有了所有原始数据，你需要在进行任何分析之前对数据进行处理。数据往往会比较混乱，特别是当数据维护不善时。你可能会看到会破坏分析的错误：设为 null 的值实际上是零、重复值和缺失值。你需要仔细检查数据，以确保能够获得准确的洞察。 
你需要检查以下常见错误： 
 缺失值，例如没有初次联系日期的客户被破坏的值，例如无效条目时区差异，可能你的数据库没有考虑到用户的不同时区日期范围错误，例如会有不合理的日期，比如销售开始之前的注册数据
你需要查看文件行列的汇总，并抽取一些测试值以查看你的值是否合理。如果你发现有些值不合理，你需要删除这些数据或用默认值替换它们。你需要在这里运用直觉：如果客户没有初次联系日期，是否合理说没有初次联系日期？还是需要找销售副总裁询问是否有人有关于客户缺失的初次联系日期的数据？ 
一旦你完成了这些问题的处理和数据清理，你就准备好进行探索性数据分析（EDA）了。 
步骤 4：探索数据 
当你的数据清理完成后，你应该开始动手玩弄数据！ 
这里的难点不在于提出测试的想法，而在于提出可能转化为洞察的想法。你的数据科学项目有一个固定的截止日期（你的销售副总裁可能在急切等待你的分析结果！），因此你需要优先考虑你的问题。 
你需要查看一些有趣的模式，这些模式可以帮助解释为什么该群体的销售减少。你可能会注意到他们在社交媒体上活动不多，很少有人拥有 Twitter 或 Facebook 账户。你还可能会发现他们大多数年龄比你的普通受众要大。由此你可以开始追踪可以更深入分析的模式。 
第 5 步：进行深入分析 
这个步骤是你需要运用你的统计、数学和技术知识，并利用所有数据科学工具来分析数据并找出每一个见解的地方。 
在这种情况下，你可能需要创建一个预测模型，将你的表现不佳的群体与平均客户进行比较。你可能会发现年龄和社交媒体活动是预测谁会购买产品的重要因素。 
如果你在构建问题时提出了许多正确的问题，你可能会意识到公司一直在大力投入社交媒体营销，信息传递的目标是年轻受众。你会知道某些人口统计群体更喜欢通过电话而非社交媒体进行联系。你开始看到产品的市场推广方式显著影响了销售：也许这个问题群体并不是无药可救的！将战术从社交媒体营销转变为更多的面对面互动，可能会对一切产生积极的变化。这是你必须向销售副总裁提出的。 
你现在可以将所有这些定性见解与定量分析的数据结合起来，编织一个能促使人们行动的故事。 
第 6 步：沟通分析结果 
重要的是销售副总裁要理解你发现的见解为什么重要。最终，你被召唤来在整个数据科学过程中创造解决方案。有效沟通将是你提案能否转化为行动或无行动的关键。 
你需要在这里编写一个引人入胜的故事，将你的数据与他们的知识结合起来。你可以从解释年长人群表现不佳的原因开始。你将这些与销售副总裁给出的答案和从数据中发现的见解结合起来。然后你提出解决问题的具体方案：我们可以将一些资源从社交媒体转移到个人电话中。你将所有这些结合成一个叙述，解决销售副总裁的困境：她现在清楚如何恢复销售并实现她的目标。 
她现在准备根据你的提案采取行动。 
在数据科学过程中，你的日常工作会因你所在的位置而有很大变化，你肯定会接到超出标准流程的任务！你也会经常同时处理不同的项目。 
如果你希望系统地思考数据科学，了解这些步骤是很重要的，更何况如果你打算开始从事数据科学职业。 
如果是这样的话，你可能会想查看我们免费的 40 页《数据科学入门指南》！ 
即使你并不打算进入这个领域，通过回到基础并彻底理解它们，你在数据科学领域的职业生涯也将得到改善。我们很欢迎你对数据科学过程的任何反馈。 
相关内容： 
 数据科学与残疾数据科学如何预测和减少不良出生结果预测分析——一碗汤的故事
更多相关话题 
 如何在几秒钟内处理包含百万行的数据框处理机器学习过程的框架在 Python 中处理 CSV 文件的 3 种方法KDnuggets™ 新闻 22:n06，2 月 9 日：数据科学编程…数据科学定义幽默：一系列古怪的名言…5 个数据科学项目以学习 5 项关键的数据科学技能
数据科学即产品 – 为什么如此困难？ 
 
 原文：www.kdnuggets.com/2020/12/data-science-product-hard.html 
 
评论 
由 Tad Slaff，Picnic Technologies 数据科学产品负责人。 
 
 
我们的前三名课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业道路。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织的 IT 
 
在 Picnic，我们将自己视为科技对食品杂货的回应。从仅限应用程序的商店、20 分钟的配送窗口到及时供应链，技术和底层数据对 Picnic 的成长至关重要。 
作为 Picnic 内的数据科学团队，我们的任务是将数据驱动的决策提升到一个新水平。我们负责构建具有智能、上下文和授权的自动化系统，以实现每年数千万欧元的业务影响决策。 
然而，构建这些系统是困难的。而将它们投入生产并被业务使用则更为困难。让我们来看看在 Picnic 将我们的数据科学项目产品化需要什么，我们亲切地称之为‘数据科学即产品’。 
为什么这如此困难？ 
2019 年 7 月的一项研究发现，87% 的数据科学项目未能投入生产。有许多原因被提及，包括缺乏领导支持、数据源孤岛和缺乏协作。除了这些问题，还有一些固有的方面使得数据科学和机器学习项目与其他软件开发有所不同。 
首先，数据科学，尤其是机器学习，存在于概率和不确定性的世界中。一个基于机器学习的支付欺诈模型的典型输出可能是，‘该订单欺诈的概率为 73% +/- 5%，置信区间为 95%’。我们在业务方面的对口人员生活在确定的世界中，‘我们希望阻止所有欺诈订单’。在这两个世界之间进行转换并不是一项容易的任务。 
此外，数据科学项目中存在的非线性（通常）不会出现在“传统”软件开发中。我们在开始构建模型之前不知道它的表现如何。可能需要一周、三个月，或者可能无法达到令人满意的性能水平。这使得制定一个包含时间表和业务希望看到的交付物的漂亮项目计划变得非常困难。 
最后，将模型投入生产时，模型信任的重要性难以夸大。当与业务部门合作将模型生产化时，我们进入了他们是专家的领域。在许多情况下，我们希望自动化一个手动过程或替代一组精心设计的业务规则。虽然这些规则并不完美，但它们是由深刻理解业务的人所制定的。交付一个黑箱机器学习算法并告知业务它将取代他们当前的工作方式是一个具有挑战性的任务。最终，业务部门拥有模型试图自动化的任何过程的盈亏，我们作为数据科学家需要说服他们将生计交给我们的模型。 
根据我们的经验，成功生产化模型涉及多个领域，可以通过以下因素实现： 
 用例选择商业对齐敏捷（数据科学）开发
用例选择 
 
 “我想如果你只有一把锤子，确实很容易把所有问题都当成钉子来处理。” — 亚伯拉罕·马斯洛 
 
机器学习可以解决的问题宇宙是巨大的。你在客户成功、供应链、分销、金融等方面有无数的用例。考虑到 Picnic 美丽维护的数据仓库中高质量数据的获取如此简单，确定从哪里开始是很困难的。选择正确的用例对数据科学项目的成功至关重要。 
那你打算如何决定选择哪个用例？ 
 具有最大商业价值的那个？“低垂的果实”来实现快速胜利？与公司战略目标一致的那个？
在 Picnic，我们考虑了这些因素，但关键的决定因素归结为一件事： 
我们对机器学习是否是解决这个问题的最佳方法有多大的信心？ 
(还记得我说过我们数据科学家习惯于概率思维吗？) 
我们希望确保数据科学家的时间得到最有效的利用。假设有一个有吸引力的问题可以产生巨大的价值，但通过几个精心设计的业务规则可以获得 80%的价值。让数据科学团队花几个月时间来获得额外的 10%是否是资源的最佳利用？可能不是。 
使用我们的数据科学禅原则作为指导，我们可以将用例选择标准分解为几个组成部分： 
 我们是否有足够的干净、高质量的数据来建模这个问题？是否有一个明确的客观标准（或损失函数）供我们优化？商业部门是否准备好让这个过程自动化？它如何融入生产系统？该产品团队是否有足够的带宽来实施它？是否有关于成功实施机器学习以解决此类问题的案例研究、研究论文或其他资源？是否有任何偏见或伦理问题需要解决？
如果对这些问题有任何疑虑，我们将重新考虑是否这是我们团队应接手的最佳项目。 
无论你有多少资源可以投入到问题上，如果没有正确的使用案例，成功的机会也很低。 
业务对齐 
 
 如果首先记录下所有未知因素，完美的项目计划是可能实现的。——比尔·兰利 
 
确保项目目标的一致性看起来既简单又明显。商业部门希望更准确的预测。你相信你可以超越现有系统。问题出在哪里？ 
问题在于，这不仅仅是关于模型的性能。 
假设你构建了一个出色的模型，并设置了一个每日执行的任务。结果可能是商业部门需要能够在一天内更新他们的预测。突然之间，你需要一个实时服务。你的模型在大多数文章/段落/地区上表现良好，但这个季度有一个新产品推出。现在你的模型在没有历史数据的情况下做出预测（冷启动问题（cold start problem）说你好）。 
机器学习项目需要商业部门对系统的运作有一定的理解。他们需要了解机器学习模型的固有优势和劣势，如何处理边界情况，以及使用了哪些特征。 
除此之外，你需要知道模型将如何使用。预期的输出是什么？预测将如何被消耗？如果模型无法运行，会有备用机制吗？在开始开发之前知道这些问题的答案可以避免许多头疼的问题、紧张的讨论和熬夜的重做工作。 
模型信任的问题再次出现。如果商业部门不信任你的模型输出怎么办？ 
你可以展示所有的 ROC 曲线、F1 分数和测试集性能，但如果模型做出的前几次预测恰好是错误的，它会有机会恢复吗？之前存在的基本业务规则并不完美，但商业部门知道哪些情况它工作良好，哪些情况不工作，然后可以相应地进行干预。你的模型（希望）具有操作影响，如果商业部门不信任它们，那么它们将不会被使用。就是这么简单。 
关于模型信任的讨论虽然不舒服，但却至关重要。你需要事先了解业务需要什么条件才能在生产中使用你的模型。至少，双方需要决定并签署一个包含性能指标的评估期。 
数据科学家与业务部门之间期望的差异导致了许多数据科学项目的终结。对话需要在花费几个月的工作之前进行。你的模型的生命周期可能取决于此。 
敏捷数据科学开发或…… 
MVP 优于 POC。 
 
 “当你在筹集资金时，它是 AI。当你在招聘时，它是 ML。当你在实施时，它是线性回归。当你在调试时，它是 printf()。” — 巴伦·施瓦茨 
 
* * 
德雷克同意，MVP 优于 POC。 
敏捷开发已成为软件开发的实际标准，但尚未进入数据科学领域。如今的数据科学项目往往采取“建好了他们就会来”的心态。数据科学家与业务部门讨论问题，决定优化的指标，并询问如何获取数据。然后他们离开，花费几个月时间构建一个美观且稳健的模型，并展示最终产品。然后…… 
…它不会被使用。敏捷开发有效的核心原因同样适用于数据科学：它需要以客户为中心。 
有效的方法是跳过概念验证（POC），因为它往往停留在数据科学家的笔记本电脑上，转而专注于创建最小可行产品（MVP）。 
对于最小可行产品（MVP），目标是尽可能快地构建一个端到端的解决方案。你构建数据管道，从一个基本的基准模型（也称为线性回归或逻辑回归）开始，并向最终用户展示预测。一个实际的例子可以参见我们如何利用机器学习找到最佳投放时间。 
这成为软件开发实际标准的原因同样适用于机器学习。我们正在尽力遵循敏捷宣言的核心原则： 
专注于可工作的软件 
不要花时间对可能永远不会被使用的模型进行微调。将时间用于构建一个可行的 
客户协作 
大幅缩短市场时间，使你的‘客户’看到的输出与更复杂的系统相似。你可以从那里进行迭代和改进。 
响应变化 
在第二周而不是第二季度发现什么有效更为合适。也许你计划整合的内部系统没有办法暴露你需要的数据。对需求保持灵活，尽早并频繁地交付可工作的代码。 
数据科学项目的难点不在于建模。难点在于其他所有事情。通过专注于 MVP，你可以更快地将工作系统投入生产，输出预测。你会更早发现问题，并在几周内为客户提供一个崭新的模型，而不是几个月。 
最终，我们的目标不是为了构建模型而构建模型。我们在构建一个包含模型的产品。我们可以借鉴数十年的产品开发经验来实现这一目标。 
结论 
基于机器学习的产品开发并不容易。你需要处理所有的软件开发组件，同时核心还需应对机器学习的复杂性。这是一个新领域，没有现成的蓝图来指导如何做好它。通过确保你选择了正确的用例，与业务对齐，并遵循经过验证的敏捷软件开发实践，你可以为成功做好准备。 
在 Picnic，技术、数据和机器学习是我们的核心。我们拥有一支极具才华的数据科学家团队，一个可扩展的自助数据科学平台，以及业务部门的全力支持，以将我们的模型投入生产。 
原文。经许可转载。 
相关内容： 
 我关于敏捷数据科学研究的最佳建议机器学习与敏捷开发的注定失败的结合MLOps 正在改变机器学习模型的开发方式
更多相关内容 
 停止在数据科学项目中硬编码 - 改用配置文件数据科学中的 3 个难度 Python 编程面试问题想成为数据科学家？第一部分：你需要的 10 种硬技能进入 FAANG 公司有多难扩展你的网页数据驱动产品时需要知道的事项介绍 Objectiv：开源产品分析基础设施
数据科学编程语言及其使用时机 
 
 原文：www.kdnuggets.com/2022/02/data-science-programming-languages.html 
 
 
使用编程语言是数据科学不可或缺的。数据科学的广泛性和可用编程语言的数量使得决定使用哪种语言以及何时使用变得相当困难。 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织在 IT 方面 
 
我的办法是向你展示编程在数据科学中最常见的使用案例。从那里开始，我将介绍最适合特定使用案例的编程语言。当然，我不能分析所有的编程语言，我需要缩小范围。 
我之所以这样做，多亏了一项特定的调查。 
编程语言在数据科学中的使用时机 
当谈到编程语言的受欢迎程度时，我们将使用 Anaconda（Python 的发行版）2021 年的调查。 
 
来源：www.anaconda.com/state-of-data-science-2021 
调查显示了最受欢迎的数据科学编程语言的使用频率。样本量为 3,104，比较安全地得出结论，这些编程语言反映了数据科学中的受欢迎程度。一些其他的列表包含了其他语言，但我们将坚持这个列表来分析每一种语言。 
问题是，这些编程语言在数据科学中什么时候使用？如果你不知道什么时候使用它，告诉你‘使用这种语言’是没有意义的。 
数据科学家的工作通常包括以下阶段： 
 数据提取和处理统计分析数据可视化建模/机器学习（ML）模型部署自动化
数据提取和处理 
数据提取意味着从数据库或其他来源获取数据。 
一旦获取数据，你需要对其进行清理，以确保你拥有正确和合适的数据。通过清理，你可以去除错误、不一致、重复的数据，替换不完整的数据，并相应地格式化数据。 
数据清理之后是数据处理。这意味着你要修改数据，使其更易读和有条理。 
希望你不会认为应该手动完成这些任务。有一些编程语言非常适合执行这些任务。它们是： 
 SQLPythonR
这是它们擅长的概述： 
 
SQL 
优点： SQL 的主要目的是处理数据和数据库。因此，它在数据提取和操作（在一定程度上）方面非常理想，特别是在处理关系型数据库时。数据检索速度快，相对简单。语法也经过标准化。 
缺点： 数据操作相对有限 
Python 
优点： 由于内置的分析工具，非常适合处理数据。它是一个开源语言，因此其受欢迎程度和活跃的社区为你提供了大量的分析库。易于使用，复杂的数据操作可以用更少的代码完成。 
缺点： 不适合数据提取，但其库在一定程度上弥补了这一不足。 
R 
优点： 由数学家和数据科学家为数学家和数据科学家设计。再一次，开源语言为数据操作提供了大量的 R 包。它是为处理大量数据而设计的。 
缺点： 尽管可以在 R 中提取数据，但由于其复杂性和相对较慢，适用性不强。 
统计分析 
数据科学周期的下一阶段是统计分析。一旦你收集了数据并根据需要进行了操作，你需要对其进行分析。这样做的目的是在数据中发现模式，然后转化为洞察、预测和报告。 
最适合的编程语言是： 
 RPythonJavaJavaScript "
KDNuggets 博客中文翻译（一百二十一）,2024-10-27,"原文：KDNuggets 
 协议：CC BY-NC-SA 4.0 
 
事件处理：三大重要开放问题 
 
 原文：www.kdnuggets.com/2018/05/event-processing-important-open-problems.html 
 
 评论 
由 Miyuru Dayarathna，WSO2 
简介 
事件处理（EP）是一种分析事件流以提取现实世界事件有用见解的范式。如图 1 所示，我们可以将 EP 分为两个 主要领域，即 事件流处理 和 复杂事件处理（CEP）。第一个领域，流（即事件）处理 支持多种连续分析，例如过滤、聚合、丰富、分类、连接等。第二个领域，CEP 使用简单事件序列上的模式来检测和报告复合事件。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析水平 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
 
图 1：事件处理术语的区别 
基于最近的 调查 和对行业趋势的多项分析，我们识别出事件处理领域的三大重要开放问题，如下所示， 
 如何开发和调试事件处理应用程序？如何解决系统可扩展性问题？如何管理系统状态？
如何开发和调试事件处理应用程序？ 
首先，我们需要解决事件处理应用程序开发的关键问题。主要有两个关键子问题，如下所示， 
2.1 我们可以在不同的流处理器之间使用一种编程语言吗？ 
事件处理应用程序的开发遵循三种主要技术。首先，通过使用用标准编程语言（如 Java/C++/Python）编写的 API。此类事件处理平台的一些示例（全部为 Apache 项目）包括 Flink、Samza、Apex、Storm、Heron 等。图 2 展示了事件处理软件使用编程语言的情况。 
 
 
图 2：事件处理软件编程语言的使用情况。（a）汇总视图（b）不同事件处理系统中使用的编程语言的分类。 
第二，它们可以使用自定义（即领域特定）语言开发。例如，IBM Infosphere Stream 的流处理语言（SPL），Software AG 的 Apama 等。用自定义语言编写的程序语句会被翻译成中间语言，如 C/C++/Perl 等，然后被编译成应用程序可执行文件。 
第三，事件处理应用程序可以使用 SQL 或类似 SQL 的语言开发。例如，WSO2 的 Siddhi (WSO2 流处理器)，Confluent 的 KSQL，FlinkSQL，SparkSQL，BeamSQL，等等。类似 SQL 的语言（也称为流式 SQL）为具有关系数据库背景的应用开发者提供了熟悉的语法。流式 SQL 提供的核心操作覆盖了 90% 的用例。 
编程语言的多样性产生了需要遵循通用语法的需求，以解决在事件流处理器中普遍存在的可用性和移植性问题。在这样做的过程中，上述第三种方法（流式 SQL）有可能成为指定事件流处理应用程序的标准。理想的情况是创建一个符合 ANSI SQL 的流式 SQL 系统。目前，还存在一些缺失的部分，例如在流式 SQL 中支持集合操作（见）。 
2.2 查询编写与调试环境 
大多数流处理器使用编程语言代码来表达业务逻辑，例如 流式 SQL、Scala、Java 等。因此，拥有正确的工具集对于开发流处理应用程序非常有用。为了开发大型复杂的应用程序，大多数非程序员更倾向于使用基于图形用户界面（GUI）的环境，而不是命令行界面（CLI）。大量的流处理器用户是非程序员，例如数据科学家。因此，缺乏适当的查询编写和调试环境使非程序员的工作非常困难。数据分析笔记本正在成为流处理应用程序的查询编写环境。几乎所有的事件流处理器都有某种应用调试支持。然而，调试支持的深度差异很大。 
如何解决系统可扩展性问题？ 
系统可扩展性和性能是衡量流处理器处理大工作负载能力的重要指标。一个可扩展的流处理器可以扩展其操作规模。传统的扩展方法基于增加更多资源，称为垂直扩展（向同一服务器添加更多资源）和水平扩展（添加多个服务器）。然而，一些其他技术如 弹性扩展、近似计算 也可以用来解决系统可扩展性问题。 
如何有效管理系统状态？ 
事件处理器的状态是处理来自输入数据流的事件期间反复访问的值集合。状态有三种类型：应用程序状态、用户状态和系统状态。应用程序状态是指在运行应用程序时创建和维护的值。例如，检测模式或长度窗口内容所需的状态。应用程序状态驻留在短期存储器（见图 2）中，例如主内存，并定期刷新到长期存储器。用户状态是应用程序访问的用户数据，用于做出运行时决策。例如，像 RDBMS 这样的长期存储器可以包含用户的信用历史信息。系统状态指框架提供的其他所有内容，以确保如果流处理器崩溃，它可以恢复到正常操作状态。 
 
图 3：具有状态管理功能的典型数据流处理器 
可靠且容错的状态管理对流处理器提供高可用服务至关重要。传统上，流处理器的可靠性和容错性通过将其操作员状态检查点保存到分布式文件系统、关系数据库管理系统（RDBMS）或分布式消息队列（如 Apache Kafka）来实现。然而，检查点引入了延迟，因为我们需要在进行检查点之前停止流应用程序的执行。已经提出了几种技术来减少保存检查点所需的时间。例如，增量检查点保持对状态所做更改的日志，并使用日志进行恢复，而不是每次都对应用程序状态进行完整检查点。因此，该技术仅在快照恢复成本可以通过增量检查点带来的性能提升来弥补时才有用。另一种方法是将计算层和数据库层合并为一个层，将另一层作为流和快照的存储层 (详细信息请参见)。 
结论 
事件处理应用程序如何编程、如何扩展以及如何管理其状态是事件处理范式中需要解决的三个主要问题。找到有效的解决方案可能会引导我们进入下一代流处理器。 
简介： Miyuru Dayarathna 是 WSO2 的高级技术主管。他是一名计算机科学家，在流计算、图数据管理和挖掘、云计算、性能工程、物联网等方面有多个研究兴趣和贡献。 
相关： 
 数据科学与机器学习的执行指南数据科学与机器学习与数据分析与业务分析的区别可操作的洞察：消除 BI 和数据仓库的现状
相关主题 
 每位数据科学家都应了解的三个 R 库（即使你使用 Python）更多分类问题的性能评估指标常见数据问题（及解决方案）识别机器学习可解决问题的 4 个因素想用你的数据技能解决全球问题？这里有一些建议…解决 5 个复杂的 SQL 问题：棘手查询解析
你是否应该免费提供你的数据技能？ 
 
 原文：www.kdnuggets.com/2018/03/ever-volunteer-data-skills-free.html 
 
 评论 
 
作为数据专家，你属于相对较小的职业群体。你拥有的技能始终处于高需求状态，并且随着技术的进步，这些技能的需求只会变得更高。 
 
我们的前三名课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织 IT 工作 
 
然而，可能会出现这样一个问题：是否可以免费向个人或组织提供你的数据相关知识？采取这种方式是否对你有利？ 
你可以帮助非营利组织用数据回答关键问题 
你可能愿意无偿提供服务的一个潜在原因是，你正在与非营利组织短期合作，帮助慈善机构利用数据。这种情况被称为 DataDive，并在 48 小时或更短时间内完成。 
在参加该活动期间，你将成为一个最多由 20 名数据专家组成的团队的一部分，并且可以在过程中在各组之间跳转。这种设置意味着你有机会与领域内的其他人建立联系，并协助各种项目，同时帮助需要帮助的组织。 
许多非营利组织可能已经意识到收集和分析数据的潜力，但可能没有足够的财力开始。这种安排帮助你用你的专业知识直接协助他们。 
获得新编程语言经验的机会 
分析师表示，R 和 Python 是数据分析师的顶级编程语言。如果你还不熟悉其中一种，学习它将显著提升你的知识基础。 
尽管许多数据专业人士会自学这两种语言或参加在线课程以掌握基础知识，但你可能会发现，在现实环境中学习效果更佳。 
在这种情况下，如果你遇到一个无薪实习或其他类型的机会，可以教你另一种编程语言并让你获得相关经验，这值得关注。这不会立即带来经济收益，但可能帮助你以后获得更好的工作，因此值得考虑将你的技能免费贡献出来。 
除了各种硬技能之外，成功的数据科学家职业生涯需要具备正确的性格特质并意识到即使是小的创新也是重要的。你可能还需要采取试错的方法来克服挑战。 
在上述描述的情况中免费贡献你的技能是一种互利的经历。组织获得了你的知识，而你则可以尝试真实世界的技术。 
作为数据科学家进行志愿服务不会给你带来作品集或个人资料 
与数据科学相关的人员相比，其他技术领域的人可能更愿意主动贡献他们的时间和才能。例如，在网页设计领域，你所做的工作通常可以纳入你的作品集。 
此外，它可能会提升你的公众形象。如果你有权限这样做，很容易向朋友、家人、同学以及其他认识的人展示你在网上所做的贡献。 
数据科学有所不同。虽然你可能能够通过案例研究或统计数据来证明你的工作如何在实现目标中发挥了重要作用，但这些东西对大多数人，尤其是那些不从事你行业的人而言，视觉吸引力不如其他方式。 
愿意免费工作可能帮助你开启一段新职业生涯 
数据科学职位的潜力使得许多各个领域的人考虑转行并尝试从其他背景进入这个行业。如果这听起来像你，免费工作可能是有价值的。 
必须具备基本的数据技能，但如果你能展示出对工作的热情，这种性格特质可能会大有裨益，甚至可能比计算机科学或数学学位更有价值。 
记住你的生计 
你在这里读到的内容可能会促使你考虑免费工作——至少在某些情况下。如果你决定走这条路，务必要以实际的方式进行。例如，不要让志愿服务优先于能带来收入的工作。 
此外，要明确而坚定你能提供的帮助范围，无论是针对某个特定项目还是你整体的贡献。否则，组织可能会过度依赖你，从而导致倦怠，并让你意识到你把太多时间投入到某个原因上了。 
每当你有机会作为数据科学家免费工作时，务必确定你能从中获得什么经验，并且要考虑是否能在不被过度劳累和压倒的情况下实现预期。 
如果机会显而易见且不需要付出太多牺牲，那就去把握它吧！否则，仔细考虑所有利弊后再做决定。 
简历：凯拉·马修斯 讨论了诸如《周刊》、《数据中心期刊》和《风险投资》上的技术和大数据，已有五年以上的写作经验。欲阅读更多凯拉的文章，订阅她的博客 Productivity Bytes。 
相关： 
 2 年内提高数据科学技能的 8 种方法非营利组织如何从数据科学的力量中受益新 Enigma Public – 连接人们与数据的平台
更多相关话题 
 KDnuggets 新闻，9 月 14 日：免费的数据科学 Python 课程•…你想了解的机器学习一切在不断变化的世界中如何成长为数据科学家IMPACT：数据可观测性峰会将于 11 月 8 日回归…KDnuggets 新闻 3 月 30 日：最受欢迎的编程入门…如果你想成为数据分析师，应该考虑的 3 门课程
互联网上每个数据科学入门课程的排名 
 
 原文：www.kdnuggets.com/2017/03/every-intro-data-science-course-ranked.html 
 
作者：David Venturi，有志数据科学家。 
 
数据可视化由Alanah Ryding提供。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
一年前，我退出了加拿大最好的计算机科学项目之一。我开始利用在线资源创建自己的数据科学硕士课程。我意识到，通过 edX、Coursera 和 Udacity，我可以学到所需的一切。而且，我可以更快、更高效地学习，而且成本仅是原来的一个小部分。 
我现在快完成了。我已经参加了许多与数据科学相关的课程，并审核了更多课程的部分内容。我了解目前的选择，以及为数据分析师或数据科学家角色准备所需的技能。几个月前，我开始创建一份以评论为驱动的指南，推荐数据科学每个主题的最佳课程。 
在系列的第一份指南中，我推荐了一些编程课程给初学者数据科学家。接着是统计学和概率课程。 
现在开始介绍数据科学。 
(如果你不确定数据科学入门课程的内容，不用担心。我会很快解释的。) 
在这份指南中，我花费了 10 多个小时试图识别 2017 年 1 月提供的所有在线数据科学入门课程，从它们的课程大纲和评论中提取关键信息，并汇总它们的评分。为此，我依赖了开源的 Class Central 社区及其数千个课程评分和评论的数据库。 
自 2011 年以来，Class Central的创始人Dhawal Shah比世界上几乎任何人都更加关注在线课程。Dhawal 亲自帮助我整理了这份资源列表。 
我们如何挑选考虑的课程 
每门课程必须符合三个标准： 
 必须教授数据科学过程。 详细内容将在稍后提供。必须是按需提供或每隔几个月开设一次。必须是互动的在线课程，不接受书籍或只读的教程。虽然这些也是学习的可行方法，但本指南专注于课程。
我们相信我们涵盖了符合上述标准的所有显著课程。由于Udemy上似乎有数百门课程，我们选择仅考虑评价最多和评分最高的课程。不过，我们仍有可能遗漏了某些课程。如果我们遗漏了好的课程，请在评论区告诉我们。 
我们如何评估课程 
我们从 Class Central 和其他评论网站汇总了平均评分和评论数量，以计算每门课程的加权平均评分。我们阅读了文本评论，并利用这些反馈来补充数字评分。 
我们基于两个因素做出了主观的课程大纲判断： 
 数据科学过程的覆盖范围。 课程是否略过或跳过某些主题？是否对某些主题讲解过于详细？有关这一过程的具体内容请参见下一部分。常见数据科学工具的使用情况。 课程是否使用 Python 和/或 R 等流行编程语言教授？虽然这些不是必要的，但在大多数情况下很有帮助，因此对这些课程稍有偏好。
 
Python 和 R 是数据科学中最受欢迎的两种编程语言。 
数据科学过程是什么？ 
什么是数据科学？数据科学家做什么？ 这些是数据科学入门课程应回答的基本问题。以下来自哈佛教授 Joe Blitzstein 和 Hanspeter Pfister 的图表概述了典型的数据科学过程，这将帮助我们回答这些问题。 
 
来自Opera Solutions的可视化图。 
我们的目标是通过这门数据科学入门课程来熟悉数据科学过程。我们不希望深入覆盖过程的具体方面，因此标题中有“入门”一词。 
对于每个方面，理想的课程应在过程框架内解释关键概念，介绍常见工具，并提供一些示例（最好是动手操作）。 
我们只寻找一个入门课程。因此，本指南不会包括像约翰霍普金斯大学的数据科学专业化在 Coursera 上或 Udacity 的数据分析师纳米学位这样的完整专业化或项目。这些课程的汇编超出了本系列的目的：寻找每个学科的最佳单独课程，以组成数据科学教育。本系列文章中的最后三篇指南将详细介绍数据科学过程的各个方面。 
需要基本的编码、统计和概率经验。 
下列几个课程需要基本的编程、统计和概率经验。考虑到新内容相对高级，而这些学科通常有几门专门的课程，这一要求是可以理解的。 
可以通过我们在前两篇文章中的推荐（编程，统计）来获得这些经验，这些文章是《数据科学职业指南》的一部分。 
我们推荐的最佳数据科学入门课程是… 
数据科学 A-Z™：包括真实数据科学练习（Kirill Eremenko/Udemy） 
Kirill Eremenko 的数据科学 A-Z™在 Udemy 上在覆盖数据科学过程的广度和深度方面显然是胜出的。它在 3,071 条评论中获得了 4.5 星的加权平均评分，这使它在所有考虑的课程中排名最高。 
该课程概述了完整的过程，并提供了实际的例子。内容长达 21 小时，时长适中。评论者喜欢讲师的授课方式和内容的组织。价格根据 Udemy 的折扣而有所不同，折扣频繁，因此你可能只需$10 即可购买。 
尽管它没有检查我们“使用常见数据科学工具”的选项，但非 Python/R 工具选择（gretl、Tableau、Excel）在上下文中被有效使用。Eremenko 在解释 gretl 选择时提到（gretl 是一个统计软件包），尽管它适用于他使用的所有工具（重点是我的）： 
 
 在 gretl 中，我们可以像在 R 和 Python 中一样进行建模，但无需编写代码。这就是关键所在。你们中的一些人可能已经很熟悉 R，但有些人可能完全不知情。我的目标是展示如何建立一个强大的模型，并提供一个可以在任何你选择的工具中应用的框架。gretl 将帮助我们避免陷入编程的困境。 
 
一位著名的评论员指出： 
 
 Kirill 是我在网上找到的最好的老师。他使用现实生活中的例子，并解释常见问题，以便你能更深入地理解课程内容。他还提供了大量关于从处理不足的数据到向 C 级管理层展示工作的数据科学家的见解。我强烈推荐这门课程给初学者和中级数据分析师！ 
 
 
数据科学 A-Z™的预览视频。 
这是一个以 Python 为重点的绝佳入门课程。 
数据分析入门 (Udacity) 
Udacity 的 数据分析入门 是 Udacity 流行的 数据分析师纳米学位 的一部分，相对较新。它使用 Python 清晰而有条理地覆盖了数据科学过程，但在建模方面略有不足。预计时间为 36 小时（每周六小时，共六周），但根据我的经验时间较短。它在一条评论中获得了 5 星评价。课程免费。 
视频制作精良，讲师（Caroline Buckey）清晰而亲切。大量编程测验巩固了视频中学习的概念。学生将在课程结束后对他们的新技能或改进的 NumPy 和 Pandas 技能充满信心（这些是流行的 Python 库）。最终项目——在纳米学位中评分和评审，但在免费的单独课程中没有——可以为个人作品集增添亮点。 
 
Udacity 的讲师 Caroline Buckey 讲解了数据分析过程（也称为数据科学过程）。 
这是一项令人印象深刻的课程，虽然没有评论数据。 
数据科学基础 (Big Data University) 
数据科学基础是由 IBM 的 Big Data University 提供的四门课程系列。它包括名为数据科学 101、数据科学方法论、开源工具实践数据科学和R 101的课程。 
它涵盖了完整的数据科学过程，并介绍了 Python、R 以及其他几种开源工具。课程制作质量极高。预计需要 13 到 18 小时，取决于是否参加最后的“R 101”课程，这对本指南的目的并非必要。不幸的是，它在我们用于此分析的主要评价网站上没有评价数据，因此我们暂时不能推荐它作为上述两个选项的替代品。它是免费的。 
 
来自 Big Data University 的数据科学 101（这是数据科学基础系列的第一门课程）的一个视频。 
竞争情况 
我们的首选具有 3,068 条评论的加权平均评分 4.5 星。接下来我们来看看其他替代选项，按评分降序排列。下面你会找到一些以 R 为重点的课程，如果你坚定地想要在该语言中入门。 
 数据科学与机器学习 Python 全栈训练营（Jose Portilla/Udemy）：全面覆盖过程，侧重于工具（Python）。流程驱动较少，更详细地介绍了 Python。虽然这个课程很棒，但不适合本指南的范围。它和 Jose 的 R 课程一样，可以同时作为 Python/R 和数据科学的入门课程。内容长达 21.5 小时。它的4.7星加权平均评分来自 1,644 条评论。费用根据 Udemy 的折扣而异，折扣频繁。Data Science and Machine Learning Bootcamp with R（Jose Portilla/Udemy）：涵盖整个过程，重点是工具使用（R）。过程驱动较少，更侧重于 R 的详细介绍。尽管如此，这门课程非常出色，但对于本指南的范围可能不太适合。它和上面提到的 Jose 的 Python 课程一样，可以作为 Python/R 和数据科学的入门课程。内容长达 18 小时，基于 847 条评价，平均评分为 4.6 星。费用根据 Udemy 的折扣而异，折扣频繁。
 
Jose Portilla 在 Udemy 上有两个数据科学和机器学习的速成课程：一个是 Python，另一个是 R。 
 Data Science and Machine Learning with Python — Hands On!（Frank Kane/Udemy）：部分过程覆盖。重点在于统计和机器学习。内容适中（九小时）。使用 Python。基于 3,104 条评价，平均评分为 4.5 星。费用根据 Udemy 的折扣而异，折扣频繁。Introduction to Data Science（Data Hawk Tech/Udemy）：覆盖整个过程，但深度有限。内容较短（三小时）。简要覆盖 R 和 Python。基于 62 条评价，平均评分为 4.4 星。费用根据 Udemy 的折扣而异，折扣频繁。应用数据科学：入门（雪城大学/Blackboard 开放教育）：覆盖整个过程，但分布不均。重点关注基础统计学和 R 语言。过于应用，过程关注不够，无法满足本指南的目的。在线课程体验感到断断续续。具有4.33星的加权平均评分，共 6 条评论。免费。数据科学导论（Nina Zumel & John Mount/Udemy）：仅覆盖部分过程，但在数据准备和建模方面深度不错。长度适中（六小时内容）。使用 R 语言。具有4.3星的加权平均评分，共 101 条评论。费用因 Udemy 折扣而异，折扣频繁。使用 Python 的应用数据科学（V2 Maestros/Udemy）：全面覆盖过程，并且每个过程的深度都很好。长度适中（8.5 小时内容）。使用 Python。具有4.3星的加权平均评分，共 92 条评论。费用因 Udemy 折扣而异，折扣频繁。
 
V2 Maestros 的“应用数据科学”课程有两个版本：一个是针对Python，另一个是针对R。 
 想成为数据科学家吗？（V2 Maestros/Udemy）：覆盖整个过程，但深度有限。内容较短（3 小时）。工具覆盖有限。具有4.3星的加权平均评分，共 790 条评论。费用因 Udemy 折扣而异，折扣频繁。数据到洞察：数据分析简介（奥克兰大学/FutureLearn）：覆盖范围不明确。声称专注于数据探索、发现和可视化。不可随时提供。内容时长为 24 小时（每周三小时，共八周）。它有 4 星的加权平均评分，基于 2 条评论。免费，提供付费证书。数据科学导论（Microsoft/edX）：部分过程覆盖（缺乏建模方面）。使用 Excel，这一点很合理，因为这是微软品牌的课程。内容时长为 12-24 小时（每周两到四小时，共六周）。它有 3.95 星的加权平均评分，基于 40 条评论。免费，提供 $25 的认证证书。数据科学基础（Microsoft/edX）：全面的过程覆盖，每个方面的深度都很好。涵盖 R、Python 和 Azure ML（微软的机器学习平台）。有几个 1 星评论提到工具选择（Azure ML）和讲师的表现不佳。内容时长为 18-24 小时（每周三到四小时，共六周）。它有 3.81 星的加权平均评分，基于 67 条评论。免费，提供 $49 的认证证书。
 
上述两个课程来自 Microsoft 的 数据科学专业证书 在 edX 上。 
 应用数据科学与 R（V2 Maestros/Udemy）：V2 Maestros 的 Python 课程的 R 伴侣。全面的过程覆盖，每个方面的深度都很好。时长适中（11 小时内容）。使用 R。它有 3.8 星的加权平均评分，基于 212 条评论。价格因 Udemy 的折扣而异，折扣频繁。数据科学导论（Udacity）：部分过程覆盖，虽然对所涉及主题的深度很好。缺乏探索方面，尽管 Udacity 有一门完整的 课程 讲解探索性数据分析（EDA）。声称时长为 48 小时（每周六小时，共八周），但根据我的经验更短。一些评论认为高级内容的设置不足。感觉组织较差。使用 Python。它有 3.61 星的加权平均评分，基于 18 条评论。免费。Python 中的数据科学入门（密歇根大学/Coursera）：部分过程覆盖。不包括建模和可视化，尽管课程#2 和#3 在应用数据科学与 Python 专项中涵盖这些方面。完成这三门课程对于本指南而言过于深入。使用 Python。时长四周。其3.6星的加权平均评分基于 15 条评论。提供免费和付费选项。
 
密歇根大学在 Coursera 上教授应用数据科学与 Python 专项。 
 数据驱动的决策制定（普华永道/Coursera）：部分覆盖（缺乏建模），重点在商业应用。介绍了许多工具，包括 R、Python、Excel、SAS 和 Tableau。时长四周。其3.5星的加权平均评分基于 2 条评论。提供免费和付费选项。数据科学速成课程（约翰霍普金斯大学/Coursera）：对完整过程的极简概述。对本系列来说过于简略。时长两小时。其3.4星的加权平均评分基于 19 条评论。提供免费和付费选项。数据科学家的工具箱（约翰霍普金斯大学/Coursera）：对整个过程的极简概述。更多是约翰霍普金斯大学的数据科学专业化的准备课程。声称有 4-16 小时内容（每周一到四小时，持续四周），尽管一位评论者指出可以在两小时内完成。其3.22星的加权平均评分基于 182 条评论。提供免费和付费选项。数据管理与可视化（卫斯理大学/Coursera）：部分过程覆盖（缺乏建模）。为期四周。制作质量良好。使用 Python 和 SAS。其2.67星的加权平均评分基于 6 条评论。提供免费和付费选项。
以下课程在 2017 年 1 月时没有评论。 
CS109 数据科学（哈佛大学）：对整个过程进行了深入的全面覆盖（可能对于本系列来说过于深入）。一个完整的 12 周本科课程。由于课程未设计为在线学习，导航较为困难。实际的哈佛讲座被录制。上述数据科学过程信息图源自该课程。使用 Python。没有评论数据。免费。 
 
哈佛 CS109 的主页上的特色可视化。 
 面向商业的数据分析导论（科罗拉多大学博尔德分校/Coursera）：部分过程覆盖（缺乏建模和可视化方面），重点在于商业。数据科学过程在其讲座中伪装成“信息-行动价值链”。为期四周。描述了几种工具，但只有 SQL 有深入覆盖。没有评论数据。提供免费和付费选项。数据科学导论（Lynda）：全面覆盖过程，但覆盖深度有限。相当短（内容三小时）。介绍了 R 和 Python。没有评论数据。费用取决于 Lynda 订阅。
总结 
这是涵盖进入数据科学领域最佳在线课程的六篇系列文章中的第三篇。我们在第一篇文章中讨论了编程，在第二篇文章中讨论了统计学和概率。系列的其余部分将涵盖数据科学的其他核心能力：数据可视化和机器学习。 
 如果你想学习数据科学，从这些编程课程开始如果你想学习数据科学，先上几节这些统计学课程
最终的部分将总结这些文章，并推荐其他关键主题如数据处理、数据库甚至软件工程的最佳在线课程。 
如果你在寻找完整的数据科学在线课程列表，可以在 Class Central 的数据科学和大数据主题页面找到它们。 
如果你喜欢阅读这篇文章，看看Class Central的其他文章： 
 这里有 250 门你现在可以免费在线学习的常春藤联盟课程根据数据的 50 门最佳免费在线大学课程
这是我在**Class Central** 上发布的原始文章的精简版，其中包含了进一步的课程描述、大纲和多个评论。 
David Venturi 使用 MOOCs 为自己创建了一个个性化的数据科学硕士课程。他拥有化学工程和经济学的双学位。结合他对体育和数据的热爱，他喜欢阅读和写作关于最新的冰球和棒球分析的内容。 
作为在线课程和 MOOCs 的首选平台，Class Central 提供经过策划和学生评论的高质量课程，帮助学习者做出明智的学习决策。 
原文。经许可转载。 
相关内容： 
 顶级机器学习 MOOCs 和在线讲座：全面调查顶级 Coursera 数据科学专业：比较与独家见解15 门数据科学数学 MOOCs
更多相关话题 
 每个初学者数据科学家都应掌握的 6 种预测模型每个数据科学家都应了解的三个 R 库（即使你使用 Python）KDnuggets 新闻 3 月 30 日：最受欢迎的编程入门…停止学习数据科学以寻找目的，并通过找到目的…学习数据科学的顶级统计资源成为优秀数据科学家所需的 5 项关键技能
如何在计算机视觉中做一切 
 
 原文：www.kdnuggets.com/2019/02/everything-computer-vision.html 
 
 评论 
Mask-RCNN 进行对象检测和实例分割 
想做计算机视觉吗？如今深度学习是实现这一目标的途径。大规模的数据集加上深度卷积神经网络（CNN）的表现能力，构建了超级准确和稳健的模型。唯一仍然存在的挑战是：如何设计你的模型。 
 
我们的三大课程推荐 
 1. Google Cybersecurity Certificate - 快速进入网络安全职业生涯 
 2. Google Data Analytics Professional Certificate - 提升你的数据分析能力 
 3. Google IT Support Professional Certificate - 支持你的组织的 IT 需求 
 
在计算机视觉这样一个广泛而复杂的领域中，解决方案并不总是明确的。计算机视觉中的许多标准任务都需要特别考虑：分类、检测、分割、姿态估计、增强与修复以及动作识别。虽然用于每种任务的最先进网络都展示了共同的模式，但它们仍然需要独特的设计调整。 
那么我们如何为所有这些不同的任务构建模型呢？ 
让我向你展示如何用深度学习做计算机视觉中的一切！ 
分类 
最著名的！图像分类网络以固定大小的输入开始。输入图像可以有任意数量的通道，但 RGB 图像通常为 3。当你设计网络时，分辨率在技术上可以是任何大小，只要足够大以支持网络中所有的下采样。例如，如果你在网络中下采样 4 次，那么你的输入图像大小至少需要为 4² = 16 x 16 像素。 
随着你深入网络，空间分辨率会降低，因为我们试图将所有信息压缩成一个一维向量表示。为了确保网络始终有能力传递它提取的所有信息，我们会根据深度增加特征图的数量，以适应空间分辨率的降低。即，我们在下采样过程中丢失了空间信息，为了弥补这种损失，我们扩展特征图以增加语义信息。 
在选择的一定量下采样后，特征图被矢量化并输入到一系列全连接层中。最后一层的输出数量与数据集中类别的数量相同。 
 
目标检测 
目标检测有两种类型：一阶段和两阶段。它们都以“锚框”开始；这些是默认的边界框。我们的探测器将预测这些框与真实框之间的差异，而不是直接预测框。 
在一个两阶段探测器中，我们自然有两个网络：一个框提议网络和一个分类网络。框提议网络提出了边界框的坐标，这些坐标是基于它认为对象可能存在的高概率区域；这些坐标相对于锚框是相对的。分类网络则对每个边界框中的潜在对象进行分类。 
在一阶段探测器中，提议和分类网络被融合成一个单独的阶段。网络直接预测边界框坐标和框内的类别。由于两个阶段融合在一起，一阶段探测器通常比两阶段的更快。但由于任务的分离，两阶段探测器的准确性更高。 
 
Faster-RCNN 两阶段目标检测架构
SSD 一阶段目标检测架构 
分割 
分割是计算机视觉中较为独特的任务之一，因为网络需要学习低层次和高层次的信息。低层次信息用于准确地按像素分割图像中的每个区域和对象，而高层次信息则用于直接分类这些像素。这导致网络被设计为将早期层和高分辨率（低层次空间信息）的信息与更深层次和低分辨率（高层次语义信息）的信息结合起来。 
如下所示，我们首先通过标准分类网络处理图像。然后，我们从网络的每一阶段提取特征，从而利用从低到高的一系列信息。每个信息层级在合并之前会独立处理。当信息被合并时，我们上采样特征图，以最终达到完整的图像分辨率。 
想了解更多关于深度学习分割的细节，请查看 这篇文章。 
 
GCN 分割架构 
姿态估计 
姿态估计模型需要完成两个任务：（1）检测图像中每个身体部位的关键点（2）找出如何正确连接这些关键点。这分为三个阶段： 
(1) 使用标准分类网络从图像中提取特征 
(2) 给定这些特征，训练一个子网络来预测一组 2D 热图。每个热图与特定关键点相关，并包含关于每个图像像素是否可能存在关键点的置信度值。 
(3) 再次利用分类网络的特征，我们训练一个子网络来预测一组 2D 矢量场，每个矢量场编码了关键点之间的关联程度。高关联的关键点被认为是连接在一起的。 
以这种方式训练模型与子网络将共同优化关键点的检测和连接。 
 
OpenPose 姿态估计架构 
增强与恢复 
增强和恢复网络是它们自己独特的存在。我们不对这些进行任何下采样，因为我们真正关心的是高像素/空间准确性。下采样会降低空间准确性，因为它减少了用于空间准确性的像素数。相反，所有处理都在全图像分辨率下进行。 
我们首先将要增强/恢复的图像以全分辨率传递给我们的网络，不做任何修改。该网络仅由多个卷积和激活函数堆叠而成。这些模块通常受到启发，并且偶尔直接复制自最初为图像分类开发的模块，如Residual Blocks、Dense Blocks、Squeeze Excitation Blocks等。最后一层没有激活函数，甚至没有 sigmoid 或 softmax，因为我们希望直接预测图像像素，而不需要任何概率或分数。 
这就是这些类型网络的全部内容！在图像的全分辨率下进行大量处理，以实现高空间准确性，使用已被证明在其他任务中有效的相同卷积。 
 
EDSR 超分辨率架构 
动作识别 
动作识别是少数几个特定需要视频数据才能良好工作的应用之一。要分类一个动作，我们需要了解场景随时间发生的变化，这自然要求我们需要视频。我们的网络必须学会空间和时间信息，即空间和时间的变化。最适合这个任务的网络是3D-CNN。 
3D-CNN，顾名思义，是一种使用 3D 卷积的卷积网络！它们与常规 CNN 的不同之处在于卷积应用于 3 个维度：宽度、高度和时间。因此，每个输出像素的预测基于其周围像素和相同位置的前后帧中的像素的计算！ 
 
直接传递大批量图像 
视频帧可以通过几种方式传递： 
(1) 直接在大批量中，如第一张图所示。由于我们传递的是一系列帧，因此同时提供了空间和时间信息。 
 
单帧 + 光流（左）。视频 + 光流（右） 
(2) 我们也可以在一个流中传递单个图像帧（数据的空间信息）及其对应的视频光流表示（数据的时间信息）。我们会使用常规的 2D CNN 从这两者中提取特征，然后将它们结合后传递给我们的 3D CNN，后者结合了这两种信息。 
(3) 将我们的帧序列传递给一个 3D CNN，将视频的光流表示传递给另一个 3D CNN。这两个数据流都有空间和时间信息。这将是最慢的选项，但也可能是最准确的，因为我们对视频的两种不同表示进行特定处理，而这两种表示包含了所有信息。 
所有这些网络输出视频的动作分类。 
喜欢学习吗？ 
在 twitter上关注我，我会发布关于最新和最伟大的 AI、技术和科学的内容！ 
简介：George Seif 是一位认证的极客和 AI/机器学习工程师。 
原文。经许可转载。 
相关： 
 快速轻松解决任何图像分类问题Scikit Learn 简介：Python 机器学习的黄金标准如何为机器学习设置 Python 环境
更多相关话题 
 TensorFlow 在计算机视觉中的应用 - 转移学习轻松实现探索计算机视觉的世界：介绍 MLM 的最新……计算机视觉的 5 个应用关于数据管理你需要知道的 6 件事及其重要性…KDnuggets 新闻 2022 年 3 月 9 日：在 5 天内构建一个机器学习 Web 应用DINOv2：Meta AI 的自监督计算机视觉模型
关于数据湖仓你需要了解的一切 
 
 原文：www.kdnuggets.com/2022/09/everything-need-know-data-lakehouses.html 
 
 
来源：Dremio 
你是否可以将数据湖仓视为科技界的新流行词？看起来我们可以。我们最初使用的数据仓库，指的是一种可以分析以帮助决策的信息存储架构。 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速开启网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析能力 
 3. 谷歌 IT 支持专业证书 - 支持你组织的 IT 
 
数据仓库可以追溯到 20 世纪 80 年代，在商业世界的许多不同方面都发挥了作用。然而，大数据时代很快来临，当时非结构化原始数据占据了许多组织中可用数据和信息的 80-90%。 
正是因为数据仓库无法处理非结构化数据，其模型基于结构化数据，数据湖仓成为了下一个大趋势。 
作为这篇文章的一部分，我与Dremio，一个提供“适合熟悉和喜爱 SQL 的团队的湖仓平台”的组织合作。Dremio 成立于 2015 年，专注于数据湖仓。他们的技术已应用于包括联合利华、德勤、诺基亚在内的各种组织，并与 AWS 等多个云和技术合作伙伴合作。Dremio 已成为数据湖仓领域的领导者。 
 
 “数据湖仓结合了数据湖的可扩展性和灵活性，以及数据仓库的性能和功能。数据湖仓是两者的最佳结合，是分析目的的数据管理的未来。” 
 — 马克·莱昂斯，Dremio 
 
他们的团队很乐于助人，并回答了一系列我们提出的问题，以帮助你了解有关数据湖仓的所有信息。 
与 Dremio 的马克·莱昂斯的讨论 
编辑注：以下问题由马克·莱昂斯，Dremio的产品管理副总裁回答。 
什么是数据湖仓？ 
 
 数据湖屋在技术市场上相对较新，它是在 2010 年发明的，并迅速获得了主流采用。数据湖屋具有用于数据分析和处理非结构化数据的能力，这与数据仓库不同。 
 
数据湖屋的作用是什么？ 
 
 数据湖屋使公司能够在数据探索到关键业务智能仪表板的所有分析工作负载中，在一个数据副本上运行（通常为Apache Parquet格式，这是性能最好的），因为它存储在云对象存储中。 
 
数据湖屋如何工作？ 
 
 数据湖屋是 2.0 版本的数据湖，具有许多相同的目标，但改进了技术，解决了数据湖即“数据沼泽”的历史性不足。数据湖屋不仅仅是对象存储中的文件——它的意义更大。 
 数据湖屋具有添加表格格式的能力，这些格式支持仓库功能，例如一致的插入、更新和删除，以及对底层数据优化（如文件压缩，以防止著名的“小文件”问题）等。 
 
数据湖屋的基本特征是什么？ 
 
 重要特征包括： 
 多引擎支持（机器学习、SQL、流处理等）对行业标准表格格式（如Apache Iceberg）进行插入、更新和删除像传统数据库管理系统（DBMS）一样的原子事务和数据一致性保证元数据（Hive metastore、Project Nessie、AWS Glue Catalog等）供应商无关标准的客户端集成以支持使用笔记本或仪表板工具的数据消费者自动扩展用户可以选择最适合自己需求的软件或 SaaS。 
 
数据湖屋与数据仓库之间的关系是什么？ 
 
 数据湖屋与数据仓库有相似之处也有不同之处。数据湖屋使 SQL 引擎（如 Dremio、Hive、Athena、Spark SQL 等）能够对行业标准表格格式提供数据操作语言（DML）功能，并为事务和数据一致性提供数据库级别的保证。 
 这使得数据湖屋相当于数据仓库的能力，但当你查看其差异时，情况还远不止于此。数据湖屋支持其他引擎用于 SQL 之外的用例，如机器学习或流处理，同时保持供应商无关性，这意味着它们以及所有使用的数据都保持在遵循开放数据架构哲学的开放格式中。 
 
为什么数据湖屋变得如此受欢迎？ 
 
 通过数据湖仓架构，数据消费者可以立即使用他们喜欢的工具来分析数据——这是一个巨大的优势！数据工程师节省了大量将数据加载到仓库中供他人使用以及维护相关基础设施的时间和金钱。 
 此外，数据湖仓消除了云数据仓库臭名昭著的供应商锁定和锁定问题。云对象存储中的数据以开放的、与供应商无关的格式存储，如Apache Parquet和Apache Iceberg，因此没有供应商能够对数据施加影响。 
 在湖仓领域，公司自然受益于竞争和创新。它们在选择计算引擎以处理当前使用的数据类型方面具有灵活性，并且能够轻松尝试未来出现的新计算引擎。 
 
结论 
Dremio 的目标是打破一个持续了 30 年的范式，这个范式束缚了每个公司。他们希望消除某些障碍，帮助公司变得创新，加快洞察时间，并将控制权交还给用户。 
我希望这次与来自 Dremio 的 Mark Lyons 的访谈能帮助你更好地理解数据湖仓是什么，它们为何进入市场，它们的工作原理，它们的重要特性，以及它们与数据仓库的区别。 
我们要感谢 Mark Lyons 和Dremio参与本文的创作。 
Nisha Arya 是一位数据科学家和自由技术作家。她特别关注提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能如何（或能）促进人类寿命的不同方式。作为一名热心的学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。 
更多相关内容 
 KDnuggets 新闻，4 月 13 日: 数据科学家应了解的 Python 库…朴素贝叶斯算法: 你需要知道的一切你需要知道的张量一切你需要知道的 MLOps 一切: KDnuggets 技术简报ChatGPT: 你需要知道的一切GPT-4: 你需要知道的一切
你需要了解的 Cohere LLM 大学的一切 
 
 原文：www.kdnuggets.com/2023/07/everything-need-llm-university-cohere.html 
 
 
作者提供的图像 
你可能听到很多关于大型语言模型（LLMs）的信息。你们中的一些人对未来充满兴趣。也有一些人想知道“我怎么参与其中？”。无论你对 LLMs 的看法如何——最终的目标是想要更多了解它。如果你想学习 LLMs 以便转行到技术行业的其他职业——Cohere 的 LLM 大学可以帮助你实现这一目标！ 
 
我们的前三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全领域的职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能。 
 3. Google IT 支持专业证书 - 支持你的组织在 IT 方面。 
 
我们看到越来越多的开发者希望将他们的职业生涯提升到一个新的水平。自然语言处理（NLP）是一个很多开发者本来不打算深入的领域。但随着大型语言模型（LLMs）的发展以及像 Cohere 这样的组织提供教育内容——这使得过渡变得更加容易。 
什么是 LLM 大学？ 
Cohere旨在通过赋能开发者和企业，构建语言 AI 的未来，使他们能够利用语言 AI 捕获重要的商业价值。为了实现这一目标，他们为希望了解更多关于 NLP 和 LLMs 的开发者创建了LLM 大学。 
他们提供了一个全面的课程，旨在为学生和开发者提供扎实的 NLP 基础知识，并在此基础上开发自己的应用程序。 
听到这是为开发者准备的，不要感到紧张——因为他们面向各类背景的人群。你将学习 NLP 和 LLMs 的基础知识，并将你的知识提升到更高级的水平，比如构建和使用文本表示和文本生成模型。 
理论部分有明确的解释和例子来支持，而实践部分则有代码示例以巩固你的知识。一旦你对该领域有了较好的理解，你将通过实际操作来检验你的技能，然后你将能够构建和部署你自己的模型。 
学习路线 
那么这如何运作呢？初学者和中级者一起？不。所以有两种学习方式： 
*** 顺序** 
**如果你是新手机器学习工程师，你可能更愿意从 NLP 和 LLM 的基础开始。通过顺序路线，你将学习 NLP 和 LLM 的基础知识及其架构。 
尽管这条路线需要的背景知识很少，但你仍然可以通过以下资料来巩固你的机器学习和 NLP 知识：附录 1。 
*** 非顺序** 
**如果你对 NLP 和 LLM 的基础知识感到比较自信，你可能不想从基础开始。你可以跳过这些基础模块，转而学习适合你需求的特定模块，或帮助你完成某个特定项目的模块。你可以通过查看以下资料了解这包含了什么：附录 2。 
LLM 大学课程 
想知道你将学习什么吗？让我们深入了解… 
在接下来的主要模块中，你将学习 LLM 的工作原理，并进行实践实验室以构建自己的语言应用程序。第一个模块完全以理论为主，然后在模块 2、3 和 4 中，你将结合理论和带代码实验室的实践。 
这些是模块： 
*** 模块 1：大型语言模型是什么？** 
**在这个模块中，你将学习 LLM 的基础知识，还将学习更多关于嵌入、注意力、变换器模型架构、语义搜索的内容，以及实际示例和动手练习。 
*** 模块 2：使用 Cohere 端点进行文本表示** 
**在第二个模块中，你将学习理论和实际实验室，在那里你将学习如何使用 Cohere 的端点进行分类、嵌入和语义搜索。在本模块结束时，你将学习如何编写代码调用 Cohere API 的多个不同端点。 
*** 模块 3：使用 Cohere 端点进行文本生成** 
**在第三个模块中，你将学习如何使用生成式学习生成文本。你将从一个教你如何使用生成的端点的代码实验室开始，然后掌握提示工程。 
*** 模块 4：部署** 
**最后但同样重要的是，部署！当你构建应用程序时，你将学习如何使用平台和框架进行部署，如 AWS SageMaker、Streamlit 和 FastAPI。 
一旦完成这些模块，你将掌握 NLP 的领域，并开启语言技术不断增长的新机遇。 
总结 
为了获得所需的帮助，Cohere 正在招收第一批学习者，并一同指导他们完成课程材料。他们还设有阅读小组，并将举办独家活动。你可以注册他们的 Discord 社区：Cohere 的 Discord 社区，在那里你可以与其他学习者联系，互相帮助，分享想法，共同进步。 
妮莎·阿雅 是一名数据科学家、自由技术写作人以及 KDnuggets 的社区经理。她特别感兴趣于提供数据科学职业建议或教程，并围绕数据科学进行理论知识的传授。她还希望探索人工智能在延续人类生命方面的不同益处。作为一个热衷学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。 
相关主题 
 朴素贝叶斯算法：你需要知道的一切关于张量的所有你需要知道的事情关于数据湖屋的所有你需要知道的事情关于 MLOps 的所有你需要知道的事情：KDnuggets 技术简报KDnuggets 新闻，4 月 13 日：数据科学家应该关注的 Python 库…ChatGPT：你需要知道的一切************
你想知道的所有机器学习知识 
 
 原文：www.kdnuggets.com/2022/09/everything-youve-ever-wanted-to-know-about-machine-learning.html 
 
在基础知识中增加趣味！一系列短视频，旨在让初学者和专家都能享受 
 
我们的三大课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织的 IT 
 
寻找一门有趣且幽默的 AI 入门课程？看看《与机器学习交朋友》（MFML），这是一个受人喜爱的免费 YouTube 课程，旨在满足每个人的需求。是的，每个人。如果你在读这篇文章，这门课程就是为你准备的！ 
 
图片由 Randall Munroe 提供，xkcd.com CC。 
课程预告 
短视频： 下面的大多数视频长度为 1–5 分钟，这意味着你可以在小块、好，味道鲜美的知识中升级自己的知识！从一开始就深入学习，或者滚动找到你想了解的主题。 
长视频： 对于那些喜欢在 1–2 小时的长时间学习中学习的人，课程也以 4 个较长的部分提供这里。 
基本概念 
 基础知识：MFML 000 — 欢迎基础知识：MFML 001 — 什么是机器学习？基础知识：MFML 002 — 为什么使用机器学习？基础知识：MFML 003 — 机器学习是如何工作的？基础知识：MFML 004 — 如何测试机器学习基础知识：MFML 005 — 黑箱中有什么？基础知识：MFML 006 — 简单线性回归基础知识：MFML 007 — 多元线性回归基础知识：MFML 008 — 特征工程基础知识：MFML 009 — 什么是 AI？基础知识：MFML 010 — 为什么我们等了这么久才有 AI？基础知识：MFML 011 — 算法、数据和计算
 
实践中 
 实践中：MFML 012 — 实际应用实践中： MFML 013 — 如何找到好的 AI 用例实践中： MFML 014 — AI 中的人类创造力实践中： MFML 015 — GANs 如何工作？实践中： MFML 016 — 为什么信任 AI？实践中： MFML 017 — 可解释性与 AI实践中： MFML 018 — 训练、验证和测试简介实践中： MFML 019 — 如何避免机器学习陷阱实践中： MFML 020 — 决策智能实践中： MFML 021 — 为什么企业在机器学习中失败？实践中： MFML 022 — 熟练的决策者实践中： MFML 023 — 可靠还是不可靠？实践中： MFML 024 — 可预防的灾难实践中： MFML 025 — 负责任地许愿实践中： MFML 026 — AI 是一项团队运动！实践中： MFML 027 — 我们的 AI 未来
AI 的 12 个步骤 
 
 第 0 步： MFML 028 — AI 的 12 个步骤第 0 步： MFML 029 — 从哪里开始应用 AI？第 0 步： MFML 030 — 分类与回归第 0 步： MFML 031 — 实例、特征和目标第 0 步： MFML 032 — 监督学习第 0 步： MFML 033 — 无监督学习第 0 步： MFML 034 — 半监督学习第 0 步： MFML 035 — 强化学习第 0 步： MFML 036 — 数据科学究竟是什么？第 0 步： MFML 037 — 数据科学流程图第 0 步： MFML 038 — 不要忘记数据！
 
 
 第 1 步： MFML 039 — AI 的“良好行为”是什么？第 1 步： MFML 040 — 假阳性和真阴性第 1 步： MFML 041 — 混淆矩阵第 1 步： MFML 042 — 性能指标第 1 步： MFML 043 — 真实情况第 1 步： MFML 044 — 精确度与召回率第 1 步： MFML 045 — 什么是优化？第 1 步： MFML 046 — 损失函数第 1 步： MFML 047 — 设置启动标准
 
 
第 2 步： MFML 048 — 数据工程 
 
 
 第 3 步： MFML 049 — 过拟合的危险第 3 步： MFML 050 — 你需要关注欠拟合吗？第 3 步： MFML 051 — 数据拆分的重要性
 
 
第 4 步： MFML 052 — 探索性数据分析 (EDA) 
 
 
 第 5 步： MFML 053 — 如何选择 AI 算法第 5 步： MFML 第四部分 — AI 算法指南（不适合快速阅读！快速阅读版在本页的最后部分。）
 
 
 第 6 步： MFML 054 — 训练 AI 系统容易吗？第 6 步： MFML 055 — 数据集的理想形状第 6 步： MFML 056 — 如何加速你的 ML/AI 训练阶段第 6 步： MFML 057 — 统计学与“统计学”第 6 步： MFML 058 — 当你的机器学习项目耗时极长第 6 步： MFML 059 — 正则化第 6 步： MFML 060 — 你绝对不应该在 AI 中使用的特征第 6 步： MFML 061 — 你可以跳过 AI 训练阶段吗？
 
 
 第 7 步： MFML 062 — 调试你的机器学习模型第 7 步： MFML 063 — 超参数调优第 7 步： MFML 064 — 什么是保留集，你如何使用它？第 7 步： MFML 065 — 理解 k 折交叉验证第 7 步： MFML 066 — 高级 AI 调试第 7 步： MFML 067 — 如果跳过调试会怎样？
 
 
 第 8 步：MFML 068 — 模型验证失败时该怎么办第 8 步：MFML 069 — 正确的模型验证第 8 步：MFML 070 — 验证轮盘
 
 
 第 9 步：MFML 071 — 测试和验证的区别第 9 步：MFML 072 — 统计学的 12 个步骤第 9 步：MFML 073 — 解读 AI 测试输出第 9 步：MFML 074 — 理解 p 值第 9 步：MFML 075 — 统计显著性第 9 步：MFML 076 — 如果测试失败该怎么办第 9 步：MFML 077 — 测试的重要性
 
 
 第 10 步：MFML 078 — 生产化第 10 步：MFML 079 — 安全地重新利用数据第 10 步：MFML 080 — 解决 AI 延迟问题第 10 步：MFML 081 — 你应该多久重新训练一次 AI 系统？第 10 步：MFML 082 — 训练-服务偏差第 10 步：MFML 083 — 注意链式模型第 10 步：MFML 084 — 对 AI 代码进行微小改动第 10 步：MFML 085 — 当你的 AI 模型失败时的重测第 10 步：MFML 086 — AI 中的长尾风险第 10 步：MFML 087 — 如何捕捉异常值和 AI 失败第 10 步：MFML 088 — AI 安全和政策层
 
 
第 11 步：MFML 089 — 实时交通实验 
 
 
 第 12 步：MFML 090 — 监控你的 AI 系统第 12 步：MFML 091 — AI 系统维护
 
算法导论 
 
菜单 
MFML 092 — 揭开黑箱 
无监督学习 
MFML 093 — 聚类与 k 均值 
懒惰学习 
 MFML 094 — 懒惰学习与 k-NNMFML 095 — 维度诅咒
线性分类器 
 MFML 096 — 什么是感知机？MFML 097 — 最大边际分类器MFML 098 — 什么是支持向量？MFML 099 — 支持向量机与核技巧
决策树与随机森林 
 MFML 100 — 什么是决策树？MFML 101 — XAI 与可解释性辩论MFML 102 — 决策树与支持向量机比较MFML 103 — 提升聚合（又名 Bagging）MFML 104 — 什么是随机森林？
集成方法 
MFML 105 — 集成模型 
朴素贝叶斯 
 MFML 106 — 贝叶斯规则导论MFML 107 — 朴素贝叶斯到底有什么“朴素”？
回归模型 
 MFML 006 — 简单线性回归MFML 007 — 多重线性回归MFML 008 — 特征工程MFML 108 — 所有回归模型有什么共同之处？MFML 109 — 什么时候使用线性回归？MFML 110 — 逻辑回归MFML 111 — 那些 sigmoid 函数是干什么的？MFML 112 — 如何进行大规模排序
深度学习 
 MFML 113 — 深度学习导论MFML 114 — 神经网络内部是什么？MFML 115 — 使用人工智能进行自动特征提取MFML 116 — 理解神经网络的组成部分MFML 117 — 反向传播MFML 118 — 深度学习的陷阱、优点和缺点MFML 119 — 神经网络架构MFML 120 — 什么时候使用神经网络
课程介绍 
与机器学习交朋友 是谷歌内部专为启发初学者和取悦专家创建的课程。* 今天，它向所有人开放！ 
该课程旨在为你提供在解决商业问题和在日益以人工智能驱动的世界中成为合格公民所需的有效参与工具。MFML 适合所有人；它专注于概念理解（而非数学和编程细节），并引导你了解成功机器学习方法的基础理念。它适合每一个人！ 
完成本课程后，你将能够： 
 获得对核心机器学习概念的直观和正确理解。理解几种流行机器学习方法的特点。避免机器学习中的常见错误。了解机器学习如何帮助你的工作。了解从构思到发布及其后的机器学习项目涉及的步骤。提升你与机器学习专家和非专家沟通的能力。
这个课程适合你吗？看看人们对它的评价 
 
 “她的演讲质量让我完全惊叹。这是一个 6 小时的杰作；在每一分钟里，Cassie 都表现得清晰、幽默、充满活力、平易近人、富有洞察力和信息量十足。” — Hal Ableson，麻省理工学院计算机科学教授 
 “我无法过分强调本课程面向普通观众的价值。” — 人力资源专家 
 “精彩的课程，而且非常有趣！” — 软件工程师 
 “我现在对我对机器学习的理解更有信心了……非常喜欢。” — 公共关系经理 
 “比我在大学里学过的所有相关课程都更有用。” — 可靠性工程师 
 “我喜欢她如何组织课程，了解内容，并在一天的课程中保持我们的兴趣不让我们感到无聊。因此我在这节课中学到了两件事：1）机器学习，2）演讲技巧。” — 高管 
 “非常棒的课程：我会推荐它。” — 机器学习研究科学家 
 “…始终有趣并且保持我的注意力。” — 高级领导，工程部门 
 “…结构良好，内容清晰，针对像我这样的人的水平恰到好处，并且充满了有用的视觉效果和故事，帮助我理解和记忆。我学到了很多。” — 高级领导，销售部门 
 “MFML 课程非常出色。它为理解最佳实践奠定了基础，并提升了我尝试应用机器学习的信心。” — 产品技术经理 
 “我终于感觉到自己理解了 90%以上的内容。通常我总是迷失，觉得自己非常愚蠢，想要哭泣。” — 项目经理 
 “比其他课程高层次得多，但也更通用、更全面。提供了该主题的全面视角……填补了我所存在的许多“空白”。它确实解开了许多谜团。” — 软件工程师 
 “虽然我在过去（大约 8 年前）学习和使用过机器学习，但感到厌倦，因此对最近的趋势不太感兴趣…经过这次刷新，我觉得我可以重建这种关系，再次与机器学习成为朋友。” — 产品经理 
 “否则难以理解的材料由于 Cassie 的深厚专业知识、机智和幽默变得易于理解和愉快。” — 用户体验设计师 
 
如果你喜欢它，那么你应该分享一下… 
现在有很多垃圾信息，如果你认为这门课程比大多数替代方案要好，请帮助它脱颖而出，通过与你的社区分享来实现。为了让课程作者微笑，只需选择一个让你开心的视频并发布到任何地方。 
P.S. 你是否曾经尝试过在 Medium 上多次点击鼓掌按钮，看看会发生什么？ ??? 
Cassie Kozyrkov 是 Google 的数据科学家和领导者，她的使命是普及决策智能和安全可靠的人工智能。 
原文。经许可转载。 
更多相关话题 
 KDnuggets 新闻，9 月 14 日：免费数据科学 Python 课程 •…KDnuggets 新闻，4 月 13 日：数据科学家应该知道的 Python 库…StarCoder：你一直想要的编程助手关于张量的所有你需要知道的关于数据湖仓的所有你需要知道的关于 MLOps 的所有你需要知道的：KDnuggets 技术简报
Apache Druid 的演变 
 
 原文：www.kdnuggets.com/2022/07/evolution-apache-druid.html 
 
近年来，包括 Netflix、Confluent、Target 和 Salesforce 在内的数千家公司的软件开发人员都转向 Apache Druid 以推动他们的分析应用程序。Druid 因其能够提供交互式数据体验而成为首选数据库，且没有数据量或并发要求的限制。 
Apache Druid 在任何规模下对多维、高基数数据进行交互式切片和切块方面表现卓越。它被设计来支持快速移动、大量数据，适用于任何数量的用户，并且可以在灵活的分布式架构中从单个节点轻松扩展到数千个节点。最大的 Druid 安装可以扩展到数 PB 的数据，由数千台数据服务器提供服务，并能够在不到一秒的时间内返回数十亿行的数据查询结果。 
 
我们的前三个课程推荐 
 1. Google 网络安全证书 - 快速进入网络安全职业生涯。 
 2. Google 数据分析专业证书 - 提升你的数据分析技能 
 3. Google IT 支持专业证书 - 支持你的组织在 IT 领域 
 
但根据我的经验，尽管 Druid 非常适合交互式切片和切块，这并不是现代分析应用的全部故事。分析应用程序越来越需要其他面向用户的功能，如数据导出和报告，这些功能依赖于运行时间较长或更复杂的查询，这些查询对于 Druid 来说并不理想。今天，开发者通过在 Druid 旁边使用其他系统来处理这些工作负载。但这会增加成本和复杂性：相同的数据必须加载两次，并且必须管理两个独立的数据管道。 
贴合其名称起源的事实，Apache Druid 正在不断变革——新增了一个多阶段查询引擎。但在深入了解新引擎之前，让我们看看 Druid 的核心查询引擎如何执行查询，以比较其差异。 
Druid 查询执行现状 
性能是交互性的关键，在 Druid 中，“不要做”是性能的关键。这意味着要专注于效率，并减少计算机需要做的工作。 
Druid 擅长这一点，因为它从一开始就被设计为高效。Druid 拥有紧密集成的查询引擎和存储格式，二者协同设计，以最小化每台数据服务器需要执行的工作量。 
Druid 的查询引擎使用“散布/聚合”技术来执行查询：它迅速识别出哪些分段与查询相关，将计算推送到各个数据服务器，然后通过我们称之为 Broker 的组件收集过滤和聚合后的结果，Broker 然后执行最终合并并将结果返回给用户。 
每个数据服务器可能处理数十亿行数据，但由于下推的过滤器、聚合和限制，返回给 Broker 的部分结果集要小得多。因此，Broker 通常处理相对较少的数据。这种设计意味着单个 Broker 可以处理跨越数千个数据服务器和万亿行的数据的查询。 
 
散布/聚合对分析应用中使用的查询类型非常高效且性能卓越。但是，该技术有一个 Achilles’ heel：当查询结果非常庞大，或查询结构需要对数据进行多次访问时，Broker 可能成为瓶颈。 
新的多阶段查询引擎 
当我们重新考虑长期运行的查询在 Druid 中如何工作的时，我们知道保持所有好的方面是很重要的：即，与存储格式的紧密集成，以及出色的数据服务器性能。我们还知道，我们需要保留使用轻量级、高并发的散布/聚合方法的能力，用于在数据服务器上完成大部分处理的查询。但我们还需要支持数据服务器之间的数据交换，而不是要求每个查询都使用散布/聚合。 
为了实现这一点，我们正在构建一个多阶段查询引擎，它接入了 Druid 标准查询引擎的现有数据处理流程，因此它将拥有所有相同的查询能力和数据服务器性能。除此之外，我们还在其上添加了一个系统，将查询拆分成多个阶段，并使数据能够在阶段之间通过洗牌网格进行交换。每个阶段都被并行化，以便同时在多个数据服务器上运行。不需要任何调优：Druid 将能够使用抗偏斜的洗牌和协作调度自动运行这一过程。 
 
通过允许多阶段查询的每个阶段在整个集群上分布式运行，我们可以有效地处理所有阶段中的任何数量的数据，而不需要在底层处理大部分数据。 
分析数据库的新标准 
一旦我们开始深入思考，我们意识到我们可以做的远不止处理复杂查询。我们可以通过一个系统和一个 SQL 语言来实现查询和摄取的功能。我们可以摆脱对分开操作模型的需求。我们可以支持外部数据查询，并启用分离存储和计算的部署模式。 
通过这项工作，我们正在建立一个平台，将实时分析数据库的性能与传统 SQL 关系数据库管理系统相关的功能和能力相结合。我对这个项目的方向感到非常兴奋。如今，Druid 是最具吸引力的大规模实时分析数据库。随着 Druid 逐渐获得这些新功能，它将成为最具吸引力的分析数据库绝对。 
贾恩·梅尔利诺 是开源 Apache Druid 项目的联合作者，并且是 Imply 的共同创始人和 CTO。贾恩还担任 Apache Druid 委员会（PMC）主席。之前，贾恩曾在 Metamarkets 领导数据摄取团队，并在 Yahoo 担任高级工程职位。他拥有加州理工学院计算机科学学士学位。 
更多相关内容 
 从 Oracle 到 AI 数据库：数据存储的演变分析未来成功概率与智能…从人工智能到机器学习再到……语音识别指标的演变深入探讨 GPT 模型：演变与性能比较数据领域的演变
从人工智能到机器学习再到数据科学的演变 
 
 原文：www.kdnuggets.com/2022/08/evolution-artificial-intelligence-machine-learning-data-science.html 
 
 
图片由 DeepMind 提供，来源于 Unsplash 
近年来，在人工智能（AI）、机器学习（ML）和数据科学领域取得了许多突破和发现。这些领域交叉如此之多，以至于它们变得同义。不幸的是，这导致了一些模糊性。 
 
我们的前三大课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 需求 
 
本指南旨在通过定义术语并解释它们如何应用于商业和科学来澄清混乱。我们不会深入讨论这些内容；但是，到本文末尾时，你应该能够区分这些概念。 
定义人工智能 
作为一个领域，人工智能专注于创建灵活的自动化系统。人工智能的终极目标是建立能够像人类一样智能和独立运作的系统。因此，人工智能必须能够模仿人类的一些感官。 
它们必须至少能够听、看，有时还能感知触觉和嗅觉。然后，人工智能必须能够解释通过这些感官接收到的刺激并做出相应的反应。因此，人工智能领域及其分支致力于赋予机器和系统这些能力。 
人工智能的主要分支 
人工智能的主要分支包括： 
 机器学习（ML）深度学习（DL）自然语言处理（NLP）模糊逻辑专家系统神经网络
这些概念并不是人工智能的独立领域，而是使现代和未来的人工智能实现成为可能。 
人工智能的阶段 
人工智能的三个阶段如下： 
 人工狭义智能（ANI）是当前人工智能的发展阶段。它也被称为弱人工智能，描述了能够执行有限一组定义任务的人工智能系统。人工通用智能（AGI）：我们正在慢慢接近这一阶段，也称为强人工智能。它描述了能够像人类一样进行推理的人工智能。一些学者认为 AGI 标签应仅限于有感知的人工智能。人工超级智能（ASI）：这是一个假设的人工智能阶段，其中计算机的智能和能力超越了人类。目前，ASI 仍然存在于科幻领域之外。
上述信息对于商业用户来说可能显得有些术语化和深奥。这些在现实世界中如何转换，又是如何应用人工智能的？ 
人工智能的常见应用 
 图像处理在照片编辑软件中的功能客户参与服务社交媒体算法在线广告平台通过自然语言处理提供的翻译机器人流程自动化（RPA）营销和产品使用分析视频和文本游戏中的非可玩和敌对角色人工智能在增强现实（AR）中的提升销售和趋势预测自动驾驶汽车交通检测
定义机器学习 
机器学习（ML）这个术语通常与人工智能互换使用。虽然它们不是同一个概念，但它们紧密相关。 
应用程序和软件在代码上运行，这些代码大多是固定的。代码中包含的参数集有限，只能在程序员编辑或添加时进行更改。机器学习旨在使计算变得更加灵活，允许软件随意更改其源代码。这类似于人们学习新事物时，大脑结构会发生细微而剧烈的变化。 
机器学习的主要分支 
机器学习的四大主要分支是： 
 有监督学习半监督学习无监督学习强化学习
当然，还有一些子集和新范式，如强化学习、降维等。机器学习通常是通过模型来实现的。 
机器学习模型的类型 
 人工神经网络决策树支持向量机回归分析贝叶斯网络遗传算法联邦学习强化学习
深度学习是机器学习的最知名和最常用的子集之一。它本质上由一个多层神经网络组成。神经网络试图通过密切模仿人脑的结构来模拟认知。它们被认为是实现通用人工智能（AGI）的最可行路径。 
机器学习在商业中的应用 
机器学习在商业和消费者产品中的一些应用示例包括： 
产品推荐 
产品推荐无疑是机器学习和人工智能中最流行的应用之一——特别是在电子商务领域。在这一应用中，商家的网站或应用程序会跟踪你的行为，基于你的活动使用机器学习。这些活动可能包括你的以前购买记录、搜索模式、点击、购物车历史等。商家随后会使用算法生成个性化的产品推荐。 
欺诈检测 
在金融和银行领域应用机器学习后，金融机构能够发现隐藏的模式、检测可疑活动，并在为时已晚之前预见到文书错误。科技咨询公司 Capgemini 声称，经过良好训练的机器学习解决方案可以将所有欺诈事件减少70%，同时将交易准确率提高 90%。 
医疗保健 
机器学习提高了医疗诊断中异常检测的准确率，使医疗从业者能够做出更准确的诊断。最近，机器学习驱动的软件被证明能比经验丰富的医生更准确地诊断患者。它通过处理医疗记录和实时评估变化参数来实现这一点。其快速适应环境变化的能力是机器学习在医疗保健领域的最大优势之一。 
定义数据科学 
数据科学是一个广泛的术语，指的是数据管理的所有方面，包括收集、存储、分析等。因此，这是一个涉及多个学科的领域，包括： 
 统计学信息学数据分析计算机科学数学领域知识信息科学等
每天（全球范围内）生成的估计2.5 万亿字节的数据中，很多数据是非结构化且噪声很大。数据科学家的大量精力用于结构化、排序并从这些数据中获得洞见。 
因为数据科学是一门多学科的科学而非一个概念，所以它不能像人工智能和机器学习那样进行分类。然而，在我们探讨它在商业环境中的应用之前，让我们扩展一下数据科学中涉及的不同职业。 
数据科学中最重要的职业 
一些最常见的数据科学家类型包括： 
 机器学习科学家数据工程师软件工程师精算科学家统计学家数字分析师商业分析师空间数据科学家质量分析师
建议数据科学家必须能够开发软件（代码）、使用分析工具和软件、开发预测模型、分析数据完整性和质量，并能够优化数据收集流程。 
数据科学在商业中的应用 
数据科学对于企业来说是极其有用的工具。每天生成的大量数据都是潜在的消费者数据。例如，机器学习实现可以处理旧的医疗记录或观察并收集用户行为信息。这是一种数据挖掘。数据科学在商业中的其他应用包括： 
 定向广告：像 Google、Facebook 和百度这样的公司主要通过数字广告赚取收入。无论是经营博客还是在线商店，你都可以利用数据科学在发布定向广告活动之前进行客户细分或聚类。执行聚类和分组的最佳方式是通过无监督的机器学习模型。库存管理的销售预测：你可以利用数据科学预测模型来预测未来的销售情况。预测模型试图基于历史数据预测未来的销售。电商推荐引擎：你可以利用数据科学根据顾客的购买历史为忠实顾客构建定制的产品推荐。
数据科学（主要通过数据分析实现）也可以用于商业智能。公司可以从数据仓库中提取有价值的洞察，并利用这些洞察做出明智的商业决策。 
摘要 
上述指南作为一个简单的入门书，主要突出人工智能、机器学习和数据科学之间的区别，以及它们如何在商业环境中应用。要了解更多关于这些主题的信息，你可以访问 KDnuggets 网站上许多相关的指南和文章。 
Nahla Davies 是一位软件开发人员和技术作家。在全职从事技术写作之前，她曾管理——在许多有趣的工作中——担任过一家 Inc. 5,000 体验品牌组织的首席程序员，该组织的客户包括三星、时代华纳、Netflix 和索尼。 
更多相关话题 
 利用智能分析未来成功的概率…免费人工智能与深度学习速成课程人工智能如何变革数据集成人工智能系统中的不确定性量化2022 年最受欢迎的人工智能技能AI 指数报告概述：衡量人工智能趋势
数据领域的进化 
 
 原文：www.kdnuggets.com/2023/06/evolution-data-landscape.html 
 
共同作者 Srujan 和 Travis Thompson 
数据领域已经成熟，并从地下的厚重文件时代走过了漫长的道路。这段旅程既迷人又激动人心，不亚于软件革命。幸运的是，我们正处于数据革命的中心，并有机会亲身见证这一过程。 
 
我们的前三个课程推荐 
 1. 谷歌网络安全证书 - 快速进入网络安全职业生涯。 
 2. 谷歌数据分析专业证书 - 提升你的数据分析技能 
 3. 谷歌 IT 支持专业证书 - 支持你的组织的 IT 
 
我们五到十年前关注的是完全不同的问题，而今天我们面对的是全新的问题。有些是数据的寒武纪大爆炸的结果，有些则出乎意料地源于解决初始问题所制定的解决方案。 
这导致了大量数据堆栈和架构的过渡。然而，突出的却是三种简单但根本性的关键堆栈：传统数据堆栈、现代数据堆栈和数据优先堆栈。让我们看看这些是如何发展的。 
进化基础：触发数据领域变革的模式 
进化有两种广泛的模式：发散型和汇聚型。这些广泛的模式同样适用于数据领域。 
地球上的物种多样性源于发散型进化。类似地，发散型进化在数据行业中产生了种类繁多的工具和服务，今天被称为疯狂数据景观。汇聚型进化则随着时间的推移创造出具有共同特征的工具变种。例如，老鼠和老虎虽然是截然不同的动物，但它们都有类似的特征，如胡须、皮毛、四肢和尾巴。 
汇聚型进化导致工具解决方案中的共同特征，意味着用户为冗余的功能付费。发散型进化则导致更高的集成成本，需要专家理解和维护每个工具的独特理念。 
请注意，常见的共同点并不意味着这些点解决方案正在朝着统一的解决方案收敛。相反，每个点正在开发与其他点的解决方案相交的解决方案，这些交集是基于需求的。这些共同能力有各自的语言和哲学，需要专业的专家。 
例如，Immuta 和 Atlan 分别是数据治理和目录解决方案。然而，Immuta 也在开发数据目录，而 Atlan 正在添加治理功能。客户倾向于用专门的工具替代次要功能。这导致了： 
 投入时间去理解每个产品的语言和哲学引入两个具有类似功能的工具的冗余成本专业人才的高资源成本；尤其困难的是人才短缺
 
现在我们对演变模式有了高层次的理解，让我们来看一下它们在数据领域中的表现。为了简洁起见，我们不会回顾太远。 
回溯几年 
我们今天作为数据行业面临的问题与 5-6 年前截然不同。当时组织面临的主要挑战是从本地系统向云的巨大的过渡。本地大数据生态系统和 SQL 数据仓库（即传统数据堆栈或 TDS）不仅维护困难且正常运行时间极低，而且在数据到洞察的过程中极其缓慢。简而言之，规模和效率远远无法实现，尤其是由于以下障碍： 
数据工程师的数量 
任何数量的数据工程师都不足以维持内部系统。从数据仓库和 ETL 到仪表板和 BI 工作流程的所有内容都必须在内部工程化，导致组织的大部分资源用于建设和维护，而不是产生收入的活动。 
管道的压倒性 
数据管道复杂且互相交织，许多是为了应对新的业务需求而创建的。有时，一个新管道是为了回答一个问题或从较少的源表中创建大量的数据仓库表。这种复杂性可能让人感到不知所措，难以管理。 
零容错性 
数据没有备份、恢复或根本原因分析时既不安全也不可靠。数据质量和治理往往被忽视，有时甚至超出了工程师的工作描述，这些工程师在繁重的管道活动下辛勤工作。 
数据迁移成本 
传统系统间的大规模数据迁移是另一个警示，耗费了大量资源和时间。此外，数据还常常出现损坏和格式问题，这需要几个月时间解决，或者直接被抛弃。 
变更抵触 
本地系统中的数据管道非常脆弱，因此对频繁的变化或任何变化都很抵触，这对于动态且易变的数据操作来说是一场灾难，并使实验成本高昂。 
紧张的节奏 
部署新的管道以回答通用业务问题花费了数月和数年时间。动态的业务请求更是不在考虑之中。更不用说在频繁的停机期间业务损失的情况了。 
技能缺口 
高债务或陈旧的系统导致项目交接时的抵触，因为存在关键的依赖关系。市场上缺乏合适的技能并没有改善这种情况，往往导致关键管道的重复工作长达数月。 
当时的解决方案及其结果问题 
云计算的出现与成为云原生的义务 
十年前，数据并不像今天这样被视为资产。尤其是因为组织没有足够的数据来作为资产利用，并且必须解决无数问题才能生成一个可用的仪表盘。但随着时间的推移，随着流程和组织变得更加数字化和数据友好，数据生成和捕获出现了突然的指数增长。 
组织意识到，通过"
世界上速度最快的AI芯片，有多快？竟是它...,2023-10-27,"1 
 前言 
 这两天，IBM低调地发了一个新闻，推出了一款类脑芯片“北极”（NorthPole），对比4nm节点实现的Nvidia H100 GPU相比，NorthPole的能效提高了五倍，成为当之无愧是现在世界最强的AI芯片。 
 如此逆天的性能，但在国内，关于这款芯片的新闻却寥寥无几。那么，它究竟是何方神圣？ 
 付斌丨作者 
 电子工程世界（ID：EEworldbbs）丨出品 


 2 
 把脑子装进芯片，就行了？  
 首先，IBM的“北极”NorthPole是一种类脑芯片，我们需要先了解什么是类脑芯片。 
 所谓类脑芯片，顾名思义，就是一种高度模拟人脑计算原理的芯片，基于对现代神经科学的理解，反复思考如何从晶体管到架构设计，算法以及软件来模仿人脑的运算。如果把类脑芯片做得更像人脑，就会被赋予一个新的名字——神经形态计算（Neuromorphic Computing）。 
 人类的思考方法与现在传统的芯片存在许多区别，比如，人没有单独的存储器，没有动态随机存取存储器，没有哈希层级结构，没有共享存储器等等。 
 “存储”和“处理器”错综复杂地深绕在人脑里，在人脑的结构中有“神经元”的存在。在电脑中，以数字化核心相互交流基于事件的信息，叫做脉冲，这点和人脑传递信息的方式相似。 
 人类，便从人脑中，得到灵感，从而创造出这样的芯片，来提升性能，成为真正的“人工智能”，甚至无限接近人类，获得思考能力。 
 当然，人脑很复杂，而且人们所制造出来的IC器件，也存在优势，所以人们最终目标就是把这二者的优势融合起来。 
 总而言之，用人话来解释，类脑芯片，就是结合生物大脑和人造器件各自优点而设计制造出来的一种芯片，而它也会像人一样思考，自我学习。 

 想造一颗这样的芯片，可不是光是变变器件结构的就行的，而是从材料、器件、电路、架构带动算法和应用改变的。一言蔽之，就是集合各种最先进的技术，才能造出这样的芯片。 
 虽然实现路径很多，但奈何这种芯片技术难度太大了，且不说好不好造，设计出来就很难了，所以目前也在开拓阶段，都还无法达到商业化水平。 
 但笔者了解到，之所以类脑芯片还未形成大规模商业化，一是因为设计难题依存，就拿英特尔、IBM都看好的CMOS型，多块全数字异步设计的芯片互联、芯片连接的有效性和时效性以及软件层互连计算、分布式计算和灵活分区等问题都难以解决；二是制造、软件和生态都要完全推翻，虽然硅基晶体管路线部分可复用，但底层不可能完全照搬，这就进一步加剧大规模商业化难度。 
 那，我们为啥费大力气折腾类脑芯片？ 
 类脑芯片实在太香了，笔者了解到，某些情况下，完美的神经形态芯片可以用比传统解决方案低1000倍的能耗来解决问题，这意味着我们可以在固定的功耗预算下，打包更多的芯片来解决更大规模的问题。 
 类脑芯片的超高能效比足够让我们牟足力气研究和突破。拿一个最典型的例子来看，AlphaGo下棋打败了人类，但人类只用了20瓦的大脑能耗，而AlphaGo是2万瓦。 

 神经形态芯片涉及的领域和机会，图源丨Nature Computational Science 
 结构上，目前全世界的类脑芯片基本都一致，都是由神经元计算、突触权重存储、路由通信三部分构成，同时采用与脉冲神经网络（SNN）模型。 
 但依据材料、器件、电路，分为模拟电路主导的神经形态系统（数模混合CMOS型）、全数字电路神经系统（数字CMOS型）、基于新型器件的数模混合神经形态系统（忆阻器是候选技术）三种流派。 
 全球范围内，参与神经形态计算芯片开发的机构主要包括三类：英特尔、IBM、高通等为代表的科技巨头企业，斯坦福、清华为代表的高校/研究机构以及初创企业。 
 根据笔者之前与英特尔研究院对话中获悉，数字CMOS型是目前最易产业化的形式，一方面，技术和制造成熟度高，另一方面，不存在模拟电路的一些顾虑和限制。 
 当然，需要强调的是，数字CMOS型还只是最初阶的类脑芯片，还算不上完全模拟人脑的神经形态器件，只能算是一种借鉴神经形态理念的一种芯片。但光是借鉴人脑，这种芯片就能够碾压世界上任何一种芯片。IBM的NorthPole就是这样的数字CMOS型的类脑芯片。 

 类脑芯片主要类型和研发进度，制表丨电子工程世界 

 3 
 IBM的芯片，什么水平？ 
 先说结论，应该是迄今为止，人类水平最高的类脑芯片了，对于类脑芯片研究又上了一个台阶。 
 2008年，IBM就开始研究类脑计算了，2011年有了第一次突破性成果：IBM的第一代神经突触计算机芯片。研究人员制造出两个这样的芯片模型：一颗包含262,144个可编程突触，一颗则包含65,536个可学习突触，通过测试展示其可执行导航、机器视觉、模式识别、关联记忆和分类等简单功能。 
 直到2014年，IBM在《科学》杂志发表了一篇文章，向全世界展示了划时代的技术进展：一个符合DARPA SyNAPSE项目指标的、拥有100万神经元的类脑处理器，即TureNorth，在当时掀起了类脑芯片研究热潮。 

 计算资源方面，NorthPole每个单元都经过优化，可执行精度较低的计算，精度从 2 bit到8 bit 不等。为保证执行单元使用，它们不能根据变量值执行条件分支，也就是说，使用者代码不能包含if语句。这种简单的执行方式使每个计算单元都能进行大规模并行执行。 
 存储方面，所有的内存都被封装在一颗芯片内， 这意味着每个内核都可以轻松地访问芯片上的内存。设备外部来看，NorthPole看起来像是一个主动存储芯片，这有助于将NorthPole集成到系统中。 
 不止如此，这款NorthPole目前采用的是12nm纳米节点工艺制造，目前CPU最先进的技术是3nm，而IBM还在研发2nm纳米节点技术，如果用上2nm，可能IBM的类脑芯片性能还会再提升很大档次。 

 NorthPole的潜在应用主要包括图像和视频分析、语音识别以及Transformer神经网络，这些网络是为ChatGPT等聊天机器人提供支持的大型语言模型 (LLM) 。这些人工智能任务可能会用于自动驾驶汽车、机器人、数字助理和卫星观测等领域。 
 某些应用程序需要的神经网络太大，无法安装在单个NorthPole芯片上。在这种情况下，这些网络可以分解为更小的部分，并分布在多个NorthPole芯片上。 
 而NorthPole的超高能效比，意味着它不需要笨重的液体冷却系统来运行，风扇和散热器就足够了，而它也可以部署在更小的空间中。 
 4 
 国内开始研究脑类芯片了吗？ 
 如此强大的芯片，国内也早已有所布局。 
 国内研究则包括清华大学、浙江大学、复旦大学、中科院等顶级学府和机构，同时近两年不断涌现初创公司，如灵汐科技、时识科技、中科神经形态等。其中以清华大学的天机芯和浙江大学的达尔文芯片最具代表性。 

 具体而言，国内的主要成果包括： 
 清华大学2015年开发的第一代天机芯采用110nm工艺，2017年，第二代天机芯开始取得先进成果，基于28nm工艺制成，由156个功能核心FCore组成，包含约4万个神经元和1000万个突触。相比第一代，密度提升20%，速度提高至少10倍，带宽提高至少100倍，此外，清华大学还自主研发了软件工具链，支持从深度学习框架到天机芯的自动映射和编译。根据清华大学的计划，下一代天机芯将是14nm或更先进的工艺，且功能会强大更多；浙江大学联合之江实验室共同研制的类脑计算机，其神经元数量与小鼠大脑神经元数量规模相当。该计算机包含792颗达尔文2代芯片，支持1.2亿个脉冲神经元、720亿个神经突触，而其典型运行功耗仅为350W~500W；2020年10月，清华大学计算机系张悠慧团队精仪系施路平团队与合作者在《自然》（Nature）杂志发文首次提出“类脑计算完备性”以及软硬件去耦合的类脑计算系统层次结构；2023年9月，中国科学院计算技术研究所尤海航研究员和唐光明研究员带领的研究团队研制了超导神经形态处理器原型芯片“苏轼（SUSHI）”，它是一款基于超导单磁通量子（SFQ）电路的超导计算芯片；2023年10月，由中科南京智能技术研究院自主研发是目前国内规模最大、国际一流的类脑超级计算机服务正式启用，它已实现5亿神经元2500亿突触智能规模，较现有计算系统能效提升10倍以上，核心芯片自主可控。 
 IBM的成果代表着，这项布局未来的技术离我们又进了一步，而目前一些初创公司逐渐形成方案，开始应用。不难预见，在近几年，这项技术商业化将逐步展开，而彼时研究成果也将照进现实。 
 -END- 

 往期推荐：点击链接即可跳转阅读 
 这个技术方向！再不学就晚了... 
 看完这篇秒懂！步进电机知识 
 推荐10款常用的轻量型嵌入式GUI 

 ↓↓↓点击关注↓↓↓"
光子芯片领域研究重点和发展趋势分析,2024-03-10,"目录 
引言 
图1 
1 光子芯片的全球竞争态势 
1.1 美国 
1.2 欧洲 
1.3 日本 
1.4 中国 
2 光子芯片发展的重点方向 
2.1 光子芯片设计软件 
2.2 光子材料与器件 
2.3 计量测试 
2.4 异构集成与封装 
3 光子芯片主要发展趋势 
3.1 光通信与光互连为光子芯片的重要应用场景 
3.2 硅光子芯片迎来技术快速迭代与产业链高速发展 
图2 
3.3 光子芯片为人工智能、量子计算等新兴计算带来突破性发展 
图3 
3.4 新材料、新工艺持续提高元件集成度、缩小光子芯片尺寸 
4 结论与展望 
 
 
摘要 
 
【目的】在问题导向、应用导向驱动下，分析光子芯片领域的全球竞争态势、重点发展方向和主要发展趋势，旨在为光子芯片研究领域的管理决策人员和科研人员提供信息参考。【文献范围】对光子芯片领域的科技领先国家的政策文献、一流学术期刊、企业翘楚资讯进行了检索与分析。【方法】采用元分析方法和层次分析法，遴选光子芯片前沿学术与工业最新进展，对光子芯片领域的政策文献、科技文献和新闻资讯进行归纳和总结。【结果】分析了美欧日以及我国在光子芯片领域的资助政策，从芯片设计软件、材料与器件、计量测试、异构集成与封装4个研究方向梳理了光子芯片现阶段的关键发展问题与相关重要进展，并简要阐述了目前主要的发展趋势。【局限】光子芯片涉及的研究内容丰富，本文仅从宏观层面把握其在信息科技领域的重点研究方向和主要发展趋势，无法涵盖全部细分方向、微观研究内容及相关研究进展。【结论】光子芯片的优良特性使其成为全球集成电路产业“后摩尔时代”的潜在颠覆性技术之一，将迎来一个关键的产业爆发期，但我国仍面临着诸多基础研究问题和技术发展挑战。 
关键词： 光子芯片; 光子集成电路; 全球竞争态势; 研究重点; 发展趋势 
引言 
光子芯片（Photonic Chip），也称光电子芯片或光子集成电路（Photonic Integrated Circuit，PIC），采用光波（电磁波）作为信息传输或数据运算的载体，一般依托于光波导介质来传输光信号，将光信号和电信号的调制、传输、解调、处理等进行功能集成。光波长在百纳米到1微米量级，使光子芯片不必像电子芯片那样追求工艺尺寸的极限缩小，就能有更多的性能提升空间。1969年，美国诺基亚贝尔实验室（Nokia Bell Labs）Miller SE首次提出了集成光学的概念[1]。 
人工智能、5G、工业物联网以及自动驾驶等新兴领域的蓬勃发展，不仅带动了全球数据的爆炸式增长，算力需求急剧增加，也带来了显著的能耗问题（如图1所示）。随着集成电路产业逐步进入后摩尔时代，光子芯片采用全新的芯片设计思路，具有低功耗、低时延、高算力以及不易受到温度、电磁干扰和噪声变化的影响等优良特性，使其成为突破现有电子芯片设计和算力瓶颈的有效途径之一。从国家战略安全和战略需求的角度，光子芯片可以解决很多数据处理时间长、无法实时处理、功耗高等应用领域的关键问题，在光通信、光互连、光计算、激光雷达、生物传感和光量子等领域展现出诱人的发展前景[3]。 
图1 
 
 
图1   传统CPU的能耗[2] 
Fig.1   The energy efficiency of traditional CPU 
 
本文采用元分析方法（Meta-Analysis）和层次分析法，对光子芯片在信息科技领域的政策文献、科技文献和新闻资讯进行归纳和总结，分析其全球竞争态势、重点发展方向和主要发展趋势，旨在为光子芯片研究领域的管理决策人员和科研人员提供信息参考。 
1 光子芯片的全球竞争态势 
当前，光子芯片正处在一个关键的发展节点。随着摩尔定律濒临极限，美欧日科技发达国家围绕光电产业发展纷纷进行了系统的部署和行动，以抢占光子技术发展先机，布局高端芯片产业链。在中美贸易摩擦和中美科技脱钩等多重因素作用下，我国对光子芯片产业日渐重视并推出了多项国家政策和地方政策，光子芯片有望成为我国“换道超车”的一条路径。 
1.1 美国 
美国一直注重光电产业的发展，早在1991年就成立了美国光电子产业振兴会（Optoelectronics Industry Development Association, OIDA）。2013年，美国国家研究委员会（NRC）更新了《光学与光子：必要技术》（Optics and Photonics: Essential Technologies）报告[4]，提高了美国政府、产业界和学术界对光子技术的重视程度。2014年，美国国家科学技术委员会发布《用光学和光子学打造更加光明的未来》（Building a Brighter Future with Optics and Photonics）报告以促进光学与光子基础研究与早期应用研究，支持生物光子学、微弱光光学和单光子技术、复杂媒介成像、超低功耗纳米光电子、研究人员易获取的制造设施、奇异光子研究（如相干辐射、太赫兹、X射线）和关键光子材料的国内来源7方面研究[5]。2014年，美国由产学研组成建立了“国家光子计划”产业联盟，白宫还公布了“国家制造业创新网络（NNMI）计划”以确保美国在先进制造业领域的全球领先地位。NNMI于2015年成立美国集成光子制造研究所（AIM photonics），旨在打造美国端到端光子生态系统[6]。2022年，美国白宫发布的《国家先进制造业战略》中提出将通过集成光电、纳电子制造、异构集成研发设施建设等技术方案，加快微电子学和半导体制造业创新[7]。 
美国国防部将光子学视为关键技术之一，国防部高级研究计划局（DARPA）持续资助研发项目促进光子芯片技术的军事和商业应用：（1）从2008年开始先后资助了“超高效纳米光子芯片间通讯（Ultraperformance Nanophotonic Intrachip Communications, UNIC）”项目和“光学优化嵌入式微处理器（Optically Optimized Embedded Microprocessor, POEM）”项目以开发和CMOS兼容的光子技术用于高通量通讯网络；（2）2018年宣布了第二阶段电子复兴计划项目“通用微光学系统激光器（Lasers for Universal Microscale Optical Systems, LUMOS）”和“极端可扩展性光子学封装（Photonics in the Package for Extreme Scalability, PIPES）”，旨在通过三维异构集成开发高性能光子芯片以实现人工智能、相控阵、传感器和数据处理等领域的突破性发展；（3）2019年启动“未来计算系统”项目，旨在研究基于知识/推理的引擎，具备深度学习能力、高算力和低功耗的集成光子芯片；（4）2021年DARPA联合半导体研究联盟（SRC）及产业界财团启动联合大学微电子学计划（JUMP）2.0，先进单片和异构集成（新型光电互连结构和先进封装）和高性能节能器件（支持下一代数字和模拟应用的新型材料、器件和互连技术）为其拟解决的关键研究主题。 
1.2 欧洲 
光子学是欧盟的关键使能技术之一，欧盟一直关注并开发颠覆性光子学技术。为了发展硅光子芯片，2002年以来欧盟先后设立了“基于CMOS工艺的光子互连层（"
CSDN博客专家证书发放名单（已暂停）,2022-07-12,"目录

证书发放频次

6月（第一批）证书发放名单（80位）

7月（第二批）证书发放名单（50位）

8月（第三批）证书发放名单（54位）

9月（第四批）证书发放名单（51位）

10月（第五批）证书发放名单（53位）

11月（第五批）证书发放名单（54位）

12月（第六批）证书发放名单（59位）

2023年1月份（47位）

2023年2月份（54位）

2023年3月份（60位）

2023年4月份（60位）

2023年5月份（53位）

2023年6月份（60位） 

证书发放以及邮寄时间

问题&回答

其他

「博客专家」（博客专家申请规则）是CSDN给予影响力较大的技术类博客的荣誉称谓，代表了官方对其博客的肯定。要想成为博客专家首先得有影响力，其次是其博客技术内容质量必须得好，成为博客专家后会得到我们一系列的服务，如在头像上加象征荣誉的专家勋章、有专属专家社群，有专人对接提供服务，不仅如此，我们还将为专家博主发放实体证书，具体详情请往↓看。

证书发放频次

每个月根据当月专家发文/活跃情况进行发放，每月发放一次，每个博客专家只可获取一次专属证书

6月（第一批）证书发放名单（80位）
CSDN ID	昵称	博客主页
feiying0canglang	IT利刃出鞘	https://blog.csdn.net/feiying0canglang
i042416	汪子熙	https://blog.csdn.net/i042416
zhuoqingjoking97298	卓晴	https://blog.csdn.net/zhuoqingjoking97298
qq_27489007	芝麻粒儿	https://blog.csdn.net/qq_27489007
xixihahalelehehe	ghostwritten	https://blog.csdn.net/xixihahalelehehe
admans	JackieZhengChina	https://blog.csdn.net/admans
howard2005	howard2005	https://blog.csdn.net/howard2005
commshare	等风来不如迎风去	https://blog.csdn.net/commshare
u011068702	码莎拉蒂 .	https://blog.csdn.net/u011068702
jacke121	AI视觉网奇	https://blog.csdn.net/jacke121
frank2102	小兔子编程	https://blog.csdn.net/frank2102
weixin_42250835	全栈哈士奇	https://blog.csdn.net/weixin_42250835
ZhangRelay	zhangrelay	https://blog.csdn.net/ZhangRelay
weixin_40816738	gblfy	https://blog.csdn.net/weixin_40816738
xiaoweite1	Lansonli	https://blog.csdn.net/xiaoweite1
qq_22182989	南北极之间	https://blog.csdn.net/qq_22182989
qq_43762191	看，未来	https://blog.csdn.net/qq_43762191
WhereIsHeroFrom	英雄哪里出来	https://blog.csdn.net/WhereIsHeroFrom
qq_37865996	千里ZLP	https://blog.csdn.net/qq_37865996
qq_41185868	一个处女座的程序猿	https://blog.csdn.net/qq_41185868
wenyusuran	文宇肃然	https://blog.csdn.net/wenyusuran
qq_23934063	不太灵光的程序员	https://blog.csdn.net/qq_23934063
mouday	彭世瑜	https://blog.csdn.net/mouday
ImagineCode	前端修罗场	https://blog.csdn.net/ImagineCode
feixiaoxing	嵌入式-老费	https://blog.csdn.net/feixiaoxing
huangbangqing12	虚坏叔叔	https://blog.csdn.net/huangbangqing12
duan_zhihua	段智华	https://blog.csdn.net/duan_zhihua
MinggeQingchun	MinggeQingchun	https://blog.csdn.net/MinggeQingchun
yaoxin521123	yaoxin521123	https://blog.csdn.net/yaoxin521123
hiwoshixiaoyu	编程小鱼酱	https://blog.csdn.net/hiwoshixiaoyu
qq_40374604	JavaPub-rodert	https://blog.csdn.net/qq_40374604
uncle_ll	uncle_ll	https://blog.csdn.net/uncle_ll
weixin_41445387	Vuko-wxh	https://blog.csdn.net/weixin_41445387
Forever_wj	╰つ栺尖篴夢ゞ	https://blog.csdn.net/Forever_wj
jdsjlzx	一叶飘舟	https://blog.csdn.net/jdsjlzx
weixin_41605937	庄小焱	https://blog.csdn.net/weixin_41605937
chenlycly	dvlinker	https://blog.csdn.net/chenlycly
HandsomeHong	奇妙之二进制	https://blog.csdn.net/HandsomeHong
fyfugoyfa	叶庭云	https://blog.csdn.net/fyfugoyfa
qq21497936	长沙红胖子Qt	https://blog.csdn.net/qq21497936
syc000666	一一哥Sun	https://blog.csdn.net/syc000666
AA5279AA	失落夏天	https://blog.csdn.net/AA5279AA
qq_36686437	点云侠	https://blog.csdn.net/qq_36686437
A757291228	1_bit	https://blog.csdn.net/A757291228
liuhao9999	紫微前端	https://blog.csdn.net/liuhao9999
dog250	dog250	https://blog.csdn.net/dog250
weixin_46192679	一只特立独行的兔先森	https://blog.csdn.net/weixin_46192679
weixin_56128392	马晓东国云数据CEO	https://blog.csdn.net/weixin_56128392
fanjufei123456	嵌入式职场	https://blog.csdn.net/fanjufei123456
github_35631540	拿我格子衫来	https://blog.csdn.net/github_35631540
lfdfhl	谷哥的小弟	https://blog.csdn.net/lfdfhl
whirlwind526	编程指南针	https://blog.csdn.net/whirlwind526
weixin_38633659	瓜子三百克	https://blog.csdn.net/weixin_38633659
yellow_hill	Franklin	https://blog.csdn.net/yellow_hill
github_39655029	村雨遥	https://blog.csdn.net/github_39655029
stevensxiao	dingdingfish	https://blog.csdn.net/stevensxiao
weixin_42135087	代码改变世界ctw	https://blog.csdn.net/weixin_42135087
a486259	万里鹏程转瞬至	https://blog.csdn.net/a486259
hihell	梦想橡皮擦	https://blog.csdn.net/hihell
leyi520	二当家的白帽子	https://blog.csdn.net/leyi520
feiyangqingyun	feiyangqingyun	https://blog.csdn.net/feiyangqingyun
qq_38322527	专注写bug	https://blog.csdn.net/qq_38322527
shi_hong_fei_hei	小虚竹	https://blog.csdn.net/shi_hong_fei_hei
u010634066	石臻臻的杂货铺	https://blog.csdn.net/u010634066
u012562943	科技D人生	https://blog.csdn.net/u012562943
ITzhongzi	ITzhongzi	https://blog.csdn.net/ITzhongzi
m0_37422217	GoatGui	https://blog.csdn.net/m0_37422217
u014534808	码农飞哥	https://blog.csdn.net/u014534808
weixin_44953658	Jiangxl~	https://blog.csdn.net/weixin_44953658
xiangzhihong8	xiangzhihong8	https://blog.csdn.net/xiangzhihong8
yuanziok	Leo.yuan	https://blog.csdn.net/yuanziok
FENGQIYUNRAN	FeelTouch Labs	https://blog.csdn.net/FENGQIYUNRAN
m0_38106923	不脱发的程序猿	https://blog.csdn.net/m0_38106923
qq_34125999	响彻天堂丶	https://blog.csdn.net/qq_34125999
wangjiaweiwei	WangLanguager	https://blog.csdn.net/wangjiaweiwei
wylfengyujiancheng	Yeliang Wu	https://blog.csdn.net/wylfengyujiancheng
lidew521	脑瓜凉	https://blog.csdn.net/lidew521
oscar999	oscar999	https://blog.csdn.net/oscar999
arv002	三雷科技	https://blog.csdn.net/arv002
david_520042	厦门德仔	https://blog.csdn.net/david_520042
7月（第二批）证书发放名单（50位）
CSDN ID	用户昵称	主页链接
it_xiangqiang	全栈技术博客	https://blog.csdn.net/it_xiangqiang
qq_19309473	Roc-xb	https://blog.csdn.net/qq_19309473
iamonlyme	YoungerChina	https://blog.csdn.net/iamonlyme
toto1297488504	to.to	https://blog.csdn.net/toto1297488504
wizardforcel	布客飞龙	https://blog.csdn.net/wizardforcel
qq_34414530	蚂蚁小兵	https://blog.csdn.net/qq_34414530
heihaozi	万猫学社	https://blog.csdn.net/heihaozi
meiqi0538	科皮子菊	https://blog.csdn.net/meiqi0538
youanyyou	Java技术栈	https://blog.csdn.net/youanyyou
inthat	西京刀客	https://blog.csdn.net/inthat
weixin_39541632	Jia ming	https://blog.csdn.net/weixin_39541632
f641385712	方向盘(YourBatman)	https://blog.csdn.net/f641385712
jiangguilong2000	流子	https://blog.csdn.net/jiangguilong2000
lj2048	静静喜欢大白	https://blog.csdn.net/lj2048
weixin_44225182	海轰Pro	https://blog.csdn.net/weixin_44225182
qq_33589510	JavaEdge.	https://blog.csdn.net/qq_33589510
han1202012	韩曙亮	https://blog.csdn.net/han1202012
weixin_43510203	Sumarua	https://blog.csdn.net/weixin_43510203
w605283073	明明如月学长	https://blog.csdn.net/w605283073
qq_38880380	worthsen	https://blog.csdn.net/qq_38880380
u011018979	iOS逆向	https://blog.csdn.net/u011018979
shaoyezhangliwei	再见孙悟空_	https://blog.csdn.net/shaoyezhangliwei
u013468614	windSeS	https://blog.csdn.net/u013468614
qq_35427589	共饮一杯无	https://blog.csdn.net/qq_35427589
wangyaninglm	shiter	https://blog.csdn.net/wangyaninglm
xiaolong1126626497	DS小龙哥	https://blog.csdn.net/xiaolong1126626497
zhangchen124	teayear	https://blog.csdn.net/zhangchen124
cui_yonghua	cui_yonghua	https://blog.csdn.net/cui_yonghua
weixin_43135178	马鹏森	https://blog.csdn.net/weixin_43135178
qq_41684621	兮动人	https://blog.csdn.net/qq_41684621
lovely_yoshino	Hermit_Rabbit	https://blog.csdn.net/lovely_yoshino
qq_24312945	记帖	https://blog.csdn.net/qq_24312945
wodeyijia911	多则惑少则明	https://blog.csdn.net/wodeyijia911
qq_33487044	内核笔记	https://blog.csdn.net/qq_33487044
weixin_43178406	爱编程的喵喵	https://blog.csdn.net/weixin_43178406
caimouse	caimouse	https://blog.csdn.net/caimouse
guorui_java	哪 吒	https://blog.csdn.net/guorui_java
suiyingy	Coding的叶子	https://blog.csdn.net/suiyingy
qq_35387940	小目标青年	https://blog.csdn.net/qq_35387940
w1014074794	斗者_2013	https://blog.csdn.net/w1014074794
lixiaogang_theanswer	内核之道	https://blog.csdn.net/lixiaogang_theanswer
m0_37602827	little_fat_sheep	https://blog.csdn.net/m0_37602827
wl1780852311	顾道长生'	https://blog.csdn.net/wl1780852311
weixin_35770067	安替-AnTi	https://blog.csdn.net/weixin_35770067
weixin_39709134	java李杨勇	https://blog.csdn.net/weixin_39709134
u022812849	morris131	https://blog.csdn.net/u022812849
qq_35812205	山顶夕景	https://blog.csdn.net/qq_35812205
u012410733	carl-zhao	https://blog.csdn.net/u012410733
superfjj	flydean程序那些事	https://blog.csdn.net/superfjj
hzw2312	BUG胡汉三	https://blog.csdn.net/hzw2312
8月（第三批）证书发放名单（54位）
CSDN ID	用户昵称	主页链接
lvoelife	念兮为美	念兮为美的博客_CSDN博客-java,mysql,springboot领域博主
qq_42229253	听雪闻歌	听雪闻歌的博客_CSDN博客-三维建模(3dmax/SketchUp)初级、进阶与高级,Java基础与进阶,Creo初级与进阶领域博主
weixin_38239050	小星星丨U3D开发支持	小星星丨U3D开发支持的博客_CSDN博客-C#,Unity功能点,⭐️U3D实用插件集合⭐️领域博主
weixin_47723732	王小王-123	王小王-123的博客_CSDN博客-数据库原理及其应用,《50篇》机器学习之Python开源教程,数据炫酷可视化之模板大全领域博主
yetaodiao		yetaodiao的博客_CSDN博客-php,java,web领域博主
YuDBL	华哥折腾历险记	华哥折腾历险记的博客_CSDN博客-web前端（H5）,错误累积（错题本）,安卓开发领域博主
Jmilk	范桂飓	https://blog.csdn.net/Jmilk
u012486840	somnus_小凯	somnus_小凯的博客_CSDN博客-linux,java,nginx领域博主
weixin_43949535	孤傲小二~阿沐	孤傲小二~阿沐的博客_CSDN博客-PostgreSQL的学习心得和知识总结,数据结构与算法,C++的学习心得和知识总结领域博主
huyuchengus	HoneyMoose	HoneyMoose的博客_CSDN博客-Confluence,CWIKIUS,Java领域博主
qq_38403590	我要用代码向我喜欢的女孩表白	我要用代码向我喜欢的女孩表白的博客_CSDN博客-BUG,bigdata-大数据专栏,java领域博主
Together_CZ	Together_CZ	Together_CZ的博客_CSDN博客-编程技术,python实践,机器学习领域博主
qq_29235677	奇点_97	https://blog.csdn.net/qq_29235677
ReCclay	ReCclay	https://blog.csdn.net/ReCclay
kangweijian	小康师兄	小康师兄的博客_CSDN博客-Android,ESP32-C3入门教程,杂七杂八领域博主
promsing	小小张自由—>张有博	https://blog.csdn.net/promsing
A_art_xiang	秃了也弱了。	秃了也弱了。的博客_CSDN博客-java,springboot,jvm领域博主
jingerppp	私房菜	https://blog.csdn.net/jingerppp
hy592070616	von Neumann	von Neumann的博客_CSDN博客-机器学习中的数学,算法设计与分析,系统学习Python领域博主
SunnyYoona	@SmartSi	https://blog.csdn.net/SunnyYoona
qq449245884	@大迁世界	@大迁世界的博客_CSDN博客-前端,javascript,Vue领域博主
liangklfang	liangklfang	https://blog.csdn.net/liangklfang
u012515223	SpikeKing	https://blog.csdn.net/u012515223
lyndon_li	Li-Yongjun	https://blog.csdn.net/lyndon_li
hhhhhhhhhhwwwwwwwwww	AI浩	AI浩的博客_CSDN博客-人工智能,C#和WinFrom,图像分类领域博主
Octopus21	ZWZhangYu	ZWZhangYu的博客_CSDN博客-分布式和微服务理论与实践,数据库,开发工具和组件领域博主
shanglianlm	mingo_敏	https://blog.csdn.net/shanglianlm
qq_36478920	Swift社区	Swift社区的博客_CSDN博客-Swift,iOS集,python集领域博主
weixin_43013761	江南才尽，年少无知！	江南才尽，年少无知！的博客_CSDN博客-RK3399移植,姿态估计,(01)ORB-SLAM2源码无死角解析-免费领域博主
qq_36380426	pingan8787	pingan8787的博客_CSDN博客-Cute-JavaScript,3-JavaScript,4-Vuejs2.0领域博主
s_lisheng	chirpyli	chirpyli的博客_CSDN博客-数据库,Rust,区块链领域博主
mydo	大熊猫侯佩	大熊猫侯佩的博客_CSDN博客-Cocos2D苹果游戏开发,iOS开发之旅,Apple开发入门领域博主
wirelessqa	毕小烦	https://blog.csdn.net/wirelessqa
qq_41895747	沉迷单车的追风少年	沉迷单车的追风少年的博客_CSDN博客-经验问题汇总,LeetCode,Qt开发领域博主
Ciellee	CielleeX	CielleeX的博客_CSDN博客-Linux内核,数据结构与算法,Qualcomm经验总结领域博主
java_wxid	java_wxid	java_wxid的博客_CSDN博客-中级开发进阶,高级开发进阶,Redis领域博主
fengyuwuzu0519	【星星之火】	【星星之火】的博客_CSDN博客-S3C2440,S3C2440学习之旅,Linux子系统领域博主
universsky2015	禅与计算机程序设计艺术	禅与计算机程序设计艺术的博客_CSDN博客-Kotlin 开发者社区,Kotlin,架构设计领域博主
l848168	技术武器库	https://blog.csdn.net/l848168
Dr_Guo	光于前裕于后	光于前裕于后的博客_CSDN博客-大数据动物园,大数据基础知识,Spark领域博主
qq_39591494	延瓒@yankerp	延瓒@yankerp的博客_CSDN博客-自动化运维脚本开发,Python全栈开发【持续更新中....】,解决方案领域博主
qq8864	特立独行的猫a	https://blog.csdn.net/qq8864
u013253075	咸鱼弟	咸鱼弟的博客_CSDN博客-单片机设计,Linux,计算机类设计领域博主
qq_22473611	四月天03	四月天03的博客_CSDN博客-数据治理DAMA,数仓&数据中台,Flink领域博主
BIT_666	BIT_666	https://blog.csdn.net/BIT_666
leftfist	左直拳	左直拳的博客_CSDN博客-.NET,学习笔记,JavaScript领域博主
qq_34137397	穆雄雄	穆雄雄的博客_CSDN博客-java技术,JavaScript,数据库领域博主
u011344545	MJ卡尔曼的鱼	MJ卡尔曼的鱼的博客_CSDN博客-卫星导航,程序语言设计（C语言、C++、Matlab、Python等）,MATLAB之计算机视觉与深度学习领域博主
qq_21950671	小秋SLAM实战教程	小秋SLAM实战教程的博客_CSDN博客-从零开始学习SLAM,ORB_SLAM2代码实战教程,OpenCV代码实战教程领域博主
weixin_44259720	Java Punk	https://blog.csdn.net/weixin_44259720
dpjcn1990	单片机菜鸟哥	https://blog.csdn.net/dpjcn1990
xw1680	Amo Xiang	https://blog.csdn.net/xw1680
zong596568821xp	宗而研之	宗而研之的博客_CSDN博客-Linux,Python,NVIDIA Jetson领域博主
zhouzhiwengang	在奋斗的大道	在奋斗的大道的博客_CSDN博客-python 学习笔记,java 基础知识,微服务springboot领域博主
9月（第四批）证书发放名单（51位）
CSDN ID	昵称	博客主页
LSGO_MYP	青少年编程竞赛交流	青少年编程竞赛交流的博客_CSDN博客-Scratch四级真题解析【电子学会】,Scratch三级真题解析【电子学会】,Scratch一级真题解析【电子学会】领域博主
jiaoyangwm	呆呆的猫	呆呆的猫的博客_CSDN博客-目标检测,Android,python学习笔记领域博主
zhengzaifeidelushang	勇敢羊羊在飞奔	勇敢羊羊在飞奔的博客_CSDN博客-大数据,日常分享专栏,软考高级系统架构设计师考试领域博主
weixin_42981560	前端技术栈	前端技术栈的博客_CSDN博客-前端面试题,js,vue领域博主
inthirties	inthirties	inthirties的博客_CSDN博客-跟Inthirties学Oracle,Oracle 每日一帖,Oracle数据库管理专栏领域博主
walykyy	与自己作战	与自己作战的博客_CSDN博客-Linux,Java基础,MySQL领域博主
TZ845195485	所得皆惊喜	所得皆惊喜的博客_CSDN博客-JUC并发编程,JVM虚拟机,商城项目 - Mall Project领域博主
qq_38526573	波波烤鸭	波波烤鸭的博客_CSDN博客-SPRING-CLOUD系列,面试题汇总,SPRINGBOOT系列领域博主
uniquewonderq	Think_Bigger	Think_Bigger的博客_CSDN博客-Java专栏,算法,ElasticSearch 从入门到实战领域博主
qq_41112170	乔卿	乔卿的博客_CSDN博客-程序设计实验,程序设计与算法,数据库系统领域博主
hebtu666	兔老大RabbitMQ	兔老大RabbitMQ的博客_CSDN博客-leetcode,os/设计模式/耳机/测试/导图/资源 杂记,数据结构与算法领域博主
slandarer	slandarer	slandarer的博客_CSDN博客-MATLAB 笔记,MATLAB 科学绘图,MATLAB 游戏领域博主
sinat_33087001	存在morning	存在morning的博客_CSDN博客-高频算法题训练,C#技术合集,软件设计模式领域博主
Xin_101	天然玩家	天然玩家的博客_CSDN博客-Java ABC,Spring,架构设计领域博主
Mculover666	Mculover666	Mculover666的博客_CSDN博客-STM32全链路开发实战教程,i.MX6ULL开发笔记,linux领域博主
qq_22695001	白鹿第一帅	白鹿第一帅的博客_CSDN博客-Java 基础与高级开发应用,集成开发环境应用与开发（IDE）,开源社区与技术测评（开源布道师）领域博主
hacker_long	言有三	言有三的博客_CSDN博客-deep learning,AI工程师修行之路,开源框架领域博主
YangCheney	OxYGC	OxYGC的博客_CSDN博客-Java / C++ / C# ....,Install / Deploy / Release,Unix / Linux / Windows / OS领域博主
qq_38723677	莫逸风	莫逸风的博客_CSDN博客-E__随笔,B5__算法刷题,MYF_牛客面试笔刷101题（Java版）领域博主
lucky51222	刘一哥GIS	刘一哥GIS的博客_CSDN博客-《GIS/ArcGIS风暴》,《VS/C/C++/C#》,《GIS程序设计》领域博主
weixin_43838785	李响Superb	李响Superb的博客_CSDN博客-算法,深度学习入门到精通系列讲解,Android开发领域博主
WannaHaha	林锋Space	林锋Space的博客_CSDN博客-windows,网络,路由交换配置领域博主
qq446282412	字节卷动	字节卷动的博客_CSDN博客-我的Android进阶之旅,Android常见错误解决之道,渲染技术：OpenGL ES、Filament领域博主
chehec2010	月疯	月疯的博客_CSDN博客-【健康医疗】,【HTML5】,【web前端基础知识】领域博主
weixin_44936889	BIT可达鸭	BIT可达鸭的博客_CSDN博客-▶ 深度学习-计算机视觉,深度学习-模型压缩（蒸馏剪枝量化）,多模态Vision+Language领域博主
qq_36119192	谢公子	谢公子的博客_CSDN博客-域环境渗透,网络安全工具使用集锦,linux系统安全领域博主
Naisu_kun	Naisu Xu	Naisu Xu的博客_CSDN博客-ESP32与ESP8266相关,Web与JS相关,嵌入式Linux与设备相关领域博主
weixin_43734095	萌宅鹿同学	萌宅鹿同学的博客_CSDN博客-《Java 从 0 到架构师》,《恋上数据结构》,《黑马设计模式详解》领域博主
JiShuiSanQianLi	击水三千里	击水三千里的博客_CSDN博客-算法,大数据,面试领域博主
CC1991_	三掌柜666	三掌柜666的博客_CSDN博客-iOS开发--付费,萌新程序猿疑问解答--付费,常识领域博主
weixin_38316697	栗子~~	栗子~~的博客_CSDN博客-JAVA,多线程/高并发,dos领域博主
bisal	bisal(Chen Liu)	bisal(Chen Liu)的博客_CSDN博客-技术之我见,Oracle,个人心得领域博主
weixin_43606158	吴迪98	吴迪98的博客_CSDN博客-吴小迪专栏之原生JS,React的专栏,笔者个人小笔记领域博主
weixin_43823808	宋子浩	宋子浩的博客_CSDN博客-【Java刷题】,Python刷题（轻工业OJ）,【JavaSE核心内容】领域博主
CodingFire	CodingFire	CodingFire的博客_CSDN博客-iOS日常开发,Core Animation,Flutter实用开发技巧合集领域博主
Jun-53	小羊子说	小羊子说的博客_CSDN博客-Android,Android 架构之路,Android 性能优化领域博主
david_lv	david_lv	david_lv的博客_CSDN博客-领域博主
K346K346	恋喵大鲤鱼	恋喵大鲤鱼的博客_CSDN博客-Linux 命令,C++,算法与数据结构领域博主
u013818205	黄菊华老师	黄菊华老师的博客_CSDN博客-毕业设计,视频教程,uni-app领域博主
qq_34623621	C君莫笑	C君莫笑的博客_CSDN博客-Qt 学习之路,人生苦短-我用Python,OpenCV图像处理领域博主
fengbingchun	fengbingchun	fengbingchun的博客_CSDN博客-C/C++/C++11,OpenCV,Deep Learning领域博主
wuyoudeyuer	Carl_奕然	Carl_奕然的博客_CSDN博客-Python开发实战及代码示例讲解,Selenium(从入门到框架设计),全链路压测(从理论到流量平台选型搭建再到真实系统改造预演)领域博主
vbirdbest	风流 少年	风流 少年的博客_CSDN博客-Spring Boot从入门到实战,iOS,Axure RP 从入门到精通领域博主
FUTEROX	Huterox	Huterox的博客_CSDN博客-突发奇想,笔记,web学习领域博主
u012702547	_江南一点雨	_江南一点雨的博客_CSDN博客-关于Spring Boot,管中窥豹,java领域博主
weixin_42782150	Yale曼陀罗	Yale曼陀罗的博客_CSDN博客-数据分析-pandas,Mac系统Python环境配置,Linux系统领域博主
qq_45034708	吾仄lo咚锵	吾仄lo咚锵的博客_CSDN博客-算法,Java,安装配置教程领域博主
diandianxiyu	小雨青年	小雨青年的博客_CSDN博客-PHP实践问题集锦,琐事杂谈,PMP新考纲备考笔记领域博主
elim168	elim168	elim168的博客_CSDN博客-Spring,Spring核心之bean,SpringMVC简介领域博主
qq_38131812	漫游嵌入式	漫游嵌入式的博客_CSDN博客-嵌入式Linux实战开发教程,Linux图形驱动与桌面,《ARMv8架构初学者指南》领域博主
melody157398	布道师Peter	布道师Peter的博客_CSDN博客-Android,linux,Multimedia领域博主
10月（第五批）证书发放名单（53位）
CSDN ID	昵称	博客主页
qq_21137441	林中静月下仙	林中静月下仙的博客_CSDN博客-C++,Linux,奇怪的知识领域博主
m0_37816922	微小冷	微小冷的博客_CSDN博客-一些过时的内容,Python,Python标准库领域博主
yangshangwei	小小工匠	小小工匠的博客_CSDN博客-【小工匠聊架構】,【ES-Elasticsearch】,【Oracle基础】领域博主
super828	CodeAllen嵌入式编程	CodeAllen嵌入式编程的博客_CSDN博客-②-STM32深入实践教程,CodeAllen （嵌入式编程）,高级C语言领域博主
lifetragedy	TGITCIC	TGITCIC的博客_CSDN博客-架构师修练之道,架构师之路,flink领域博主
FMC_WBL	小明java问道之路	小明java问道之路的博客_CSDN博客-《计算机底层原理》[更新中],精讲笔试面试企业题库,程序人生领域博主
abu935009066	lakernote	lakernote的博客_CSDN博客-《从零搭建开发脚手架》,《Spring Cloud - 实战》,《你悟了没 - 架构之路》领域博主
qq_22795513	我去热饭	我去热饭的博客_CSDN博客-有手就会写的接口测试平台,饭谈,测试开发日记领域博主
weixin_44593822	深海鱼肝油ya	深海鱼肝油ya的博客_CSDN博客-PyQt5,Blue_Bridge,CV领域博主
qq_15719613	TwcatL_tree	TwcatL_tree的博客_CSDN博客-JAVA_Web,我和我的C语言,Java领域博主
Leytton	李乾文	李乾文的博客_CSDN博客-【电子嵌入式】,【机器学习】,【前端开发】领域博主
qq_19734597	张志翔 ?	张志翔 ̮的博客_CSDN博客-Linux,Java语言,Ubuntu领域博主
m0_37605642	花花少年	花花少年的博客_CSDN博客-编程工具,深度学习,运维领域博主
qq_32761549	月照银海似蛟龙	月照银海似蛟龙的博客_CSDN博客-SLAM,ros,无人机领域博主
m0_38082783	Rattenking	Rattenking的博客_CSDN博客-Rattenking 的前端笔记,微信小程序,【JavaScript】领域博主
qq_36434219	韧小钊	韧小钊的博客_CSDN博客-Daily Record,dolphinscheduler,Linux领域博主
qq_24950043	wu@55555	wu@55555的博客_CSDN博客-Elastic Stack,微服务,软件安装领域博主
wuxintdrh	宝哥大数据	宝哥大数据的博客_CSDN博客-spark,python3,算法与数据结构领域博主
TiaoZhanJi_Xian	黄啊码	黄啊码的博客_CSDN博客-PHP从入门到拆门,前端,Linux之rm -rf领域博主
kobeyu652453	总裁余(余登武)	总裁余(余登武)的博客_CSDN博客-cv计算机视觉,学习杂记和工具包讲解,最优化实战例子领域博主
freewebsys	freewebsys	freewebsys的博客_CSDN博客-Java,Python,docker领域博主
zhubaitian	天地会珠海分舵	天地会珠海分舵的博客_CSDN博客-虎嗅36氪供稿译文,创业论,产品论领域博主
scgaliguodong123_	李国冬	李国冬的博客_CSDN博客-大数据实践,Java后端技术实践,人工智能工程化（MLOps）领域博主
zhonglunshun	疯人院的院长大人	疯人院的院长大人的博客_CSDN博客-Android,系统定制,架构/管理领域博主
lys_828	百木从森	百木从森的博客_CSDN博客-数据分析师,python办公自动化,python数据分析领域博主
u013810234	Heartsuit	Heartsuit的博客_CSDN博客-运维,SpringBoot,Database领域博主
superdangbo	Python-Basketball	Python-Basketball的博客_CSDN博客-Spring Cloud实战,Python 实战,mysql领域博主
NoamaNelson	虫无涯	虫无涯的博客_CSDN博客-Python,💖API接口自动化测试框架从0到1（完整源码）,软件测试体系领域博主
weiqifa0	嵌入式Linux,	嵌入式Linux,的博客_CSDN博客-Linux,Android,C领域博主
z609932088	DreamLife.	DreamLife.的博客_CSDN博客-Qt、QML,Qt 汽车 HMI 仪表,Linux领域博主
zuiyishihefang	当交通遇上机器学习	当交通遇上机器学习的博客_CSDN博客-领域博主
zyb228107	脑机接口社区	脑机接口社区的博客_CSDN博客-脑机接口社区,脑机论文和资讯,脑电信号处理领域博主
starrow	starrow	starrow的博客_CSDN博客-IBM Domino开发菁华,编程与IT,Thinking in JavaScript领域博主
zl18603543572	早起的年轻人	早起的年轻人的博客_CSDN博客-Flutter项目开发全套教程,Android开发中的点点滴滴,java点滴积累领域博主
bdmh	bdmh	bdmh的博客_CSDN博客-Java、Android源码解读,Android,Delphi领域博主
zhangphil	zhangphil	zhangphil的博客_CSDN博客-Android,Python,Java领域博主
Follow_24	程序大视界	程序大视界的博客_CSDN博客-《程序员经验之谈 》,《Java基础 》,《Java 语言篇》领域博主
ShuSheng0007	ShuSheng007	ShuSheng007的博客_CSDN博客-Android,设计模式,Java领域博主
fly910905	琦彦	琦彦的博客_CSDN博客-琦彦の百宝箱,kubernetes-云原生的掌舵人,微服务-SpringCloud和Istio领域博主
qq_25580555	喜欢猪猪	喜欢猪猪的博客_CSDN博客-Java开发问题,Java面试整理,系统架构师领域博主
u011115875	xiaokcehui	xiaokcehui的博客_CSDN博客-经验总结,软件开发,多波束领域博主
googdev	stormzhangV	stormzhangV的博客_CSDN博客-杂谈,Android,GitHub领域博主
ximenjianxue	羌俊恩	羌俊恩的博客_CSDN博客-虚拟化实践及FAQ,Horizon Daas cloud实践及FAQ,Linux知识点领域博主
qq_42259469	勤奋的树懒	勤奋的树懒的博客_CSDN博客-运维,编程,Elastic领域博主
gc_2299	gc_2299	gc_2299的博客_CSDN博客-UX Movement网站文章翻译,ui-patterns网站文章翻译,程序设计领域博主
u011074149	Linux猿	Linux猿的博客_CSDN博客-ACM算法,Linux 技术,C/C++面试通关集锦领域博主
hyj_king	郝少	郝少的博客_CSDN博客-大数据,Spark技术经验,Java及框架领域博主
weixin_42350212	罗四强	罗四强的博客_CSDN博客-Linux,Windows,python黑科技领域博主
u012247418	hello_courage	hello_courage的博客_CSDN博客-Linux,网络驱动及协议栈开发,USB开发领域博主
xianpanjia4616	JasonLee实时计算	JasonLee实时计算的博客_CSDN博客-Flink实战系列,Spark实战系列,leetcode领域博主
qq_41782425	cdtaogang	cdtaogang的博客_CSDN博客-Testing,功能测试,Python领域博主
feng19870412	Linux编程学堂	Linux编程学堂的博客_CSDN博客-Linux C 程序设计课程,Linux C++程序设计课程,Linux 系统编程课程领域博主
woshisangsang	程序员大阳	程序员大阳的博客_CSDN博客-Java开发技术,技术周边,Java SSM框架篇领域博主
11月（第五批）证书发放名单（54位）
CSDN ID	用户昵称	博客主页
baidu_41666295	安卓兼职framework应用工程师	安卓兼职framework应用工程师的博客_CSDN博客-andoid10.0Rom定制化开发系列讲解,Android高级进阶,android领域博主
qq_42055933	你若不离不弃，我必生死相依	你若不离不弃，我必生死相依的博客_CSDN博客-后端,spring,高级数据库领域博主
liangzc1124	Leon_George	Leon_George的博客_CSDN博客-嵌入式开发,c,Linux3.4.2驱动开发领域博主
phmatthaus	蓝天居士	蓝天居士的博客_CSDN博客-Linux,Linux内核,AI领域博主
weixin_42753193	嘟嘟的程序员铲屎官	嘟嘟的程序员铲屎官的博客_CSDN博客-Java,安卓开发(AndroidStudio),软考高级系统架构师考试领域博主
qq_44867340	胖胖的懒羊羊	胖胖的懒羊羊的博客_CSDN博客-Java,LeetCode,Android领域博主
dawei_yang000000	Davieyang.D.Y	Davieyang.D.Y的博客_CSDN博客-Java基础即高端,APP自动化测试-JavaAppium,Web自动化测试-PythonSelenium领域博主
ityouknow	微笑很纯洁	微笑很纯洁的博客_CSDN博客-精通 Spring Boot 42 讲,springboot,快速学习 Spring Boot 技术栈领域博主
m0_38039437	Bruce小鬼	Bruce小鬼的博客_CSDN博客-最美前端, 前端【H5+Css3】,VUE【vue核心】领域博主
wanggao_1990	wanggao90	wanggao90的博客_CSDN博客-音视频编解码,OpenCV,c/c++领域博主
Auspicious_air	auspicious航	auspicious航的博客_CSDN博客-Oracle数据库学习资料,大数据相关学习资料,Debezium实战资料领域博主
weixin_43598687	1 + 1=王	1 + 1=王的博客_CSDN博客-LeetCode每日一题,笔记,课程设计实验报告领域博主
yerenyuan_pku	李阿昀	李阿昀的博客_CSDN博客-项目实战之淘淘商城,Java Web基础入门,史上最详细的Spring注解驱动开发系列教程领域博主
perfect2011	香菜+	香菜+的博客_CSDN博客-香菜聊游戏,Java高级知识点,算法领域博主
yelangkingwuzuhu	夜郎king	夜郎king的博客_CSDN博客-java,工作拾遗,中间件领域博主
whik1194	whik1194	whik1194的博客_CSDN博客-Microsemi Libero SoC系列教程,SmartFusion从FPGA到ARM系列教程,Qt领域博主
gschen_cn	算法与编程之美	算法与编程之美的博客_CSDN博客-编程之美,算法之美,前端开发领域博主
tugouxp	papaofdoudou	papaofdoudou的博客_CSDN博客-嵌入式系统,Linux,人工智能领域博主
weixin_42172261	H4ppyD0g	H4ppyD0g的博客_CSDN博客-算法基础,经验&代码,渗透测试领域博主
dkl12	董可伦	董可伦的博客_CSDN博客-Spark,Hudi,大数据领域博主
y601500359	SunkingYang	SunkingYang的博客_CSDN博客-翻译谷歌官方Native Client开发手册,谷歌Google Native Client,QT开发领域博主
qq_27248989	行知天下	行知天下的博客_CSDN博客-Android随记,Java随记,Android项目总结领域博主
weixin_41960890	见见大魔王	见见大魔王的博客_CSDN博客-小见读文献,简简单单强化学习,数据结构与算法领域博主
qq_40065776	Asurplus	Asurplus的博客_CSDN博客-SpringBoot系列,Java杂记,第三方平台领域博主
ju_362204801	嗑嗑嗑瓜子的猫	嗑嗑嗑瓜子的猫的博客_CSDN博客-Java开发总结,笔记,工具使用领域博主
weixin_43767015	刘Java	刘Java的博客_CSDN博客-Java JUC源码,Spring 5.x 源码,Java 数据结构与算法领域博主
ZCShouCSDN	ZC·Shou	ZC·Shou的博客_CSDN博客-STM32,ARM,C/C++领域博主
junxuezheng	郑同学的笔记	郑同学的笔记的博客_CSDN博客-C++,linux,ELK领域博主
kupe87826	kupeThinkPoem	kupeThinkPoem的博客_CSDN博客-c++,人工智能-人脸识别,算法领域博主
qq_31985307	从善若水	从善若水的博客_CSDN博客-5G/6G系列,高级C语言,GTK领域博主
u013700502	-小马快跑-	-小马快跑-的博客_CSDN博客-Android View相关,数据结构与算法,Android知识点总结领域博主
u010164190	Android系统攻城狮	Android系统攻城狮的博客_CSDN博客-Ubuntu高级用法,Android系统调试方法,C++学习领域博主
sunhuaqiang1	No Silver Bullet	No Silver Bullet的博客_CSDN博客-Vue2,剑指Offer,网站开发进阶领域博主
weixin_38815609	Gabriel.Tian	Gabriel.Tian的博客_CSDN博客-Linux驱动/应用开发,QT实战开发,C/C++领域博主
a486259	万里鹏程转瞬至	万里鹏程转瞬至的博客_CSDN博客-paddlepaddle,python工具方法,pytorch工程实践领域博主
qq_41800366	zhangpaopao0609	zhangpaopao0609的博客_CSDN博客-Python,JS,C领域博主
cuichongxin	/*守护她的笑容	/*守护她的笑容的博客_CSDN博客-玩转linux,shell,windows技巧领域博主
shihengzhen101	AlbertS	AlbertS的博客_CSDN博客-C++,Practical,Ubuntu|Linux领域博主
sxf1061700625	小锋学长生活大爆炸	小锋学长生活大爆炸的博客_CSDN博客-学习之旅,干货,技术文章领域博主
u011466469	我先测了	我先测了的博客_CSDN博客-JMeter轻量级性能测试工具,【JAVA版Web】UI自动化测试实战,Linux运维之环境管理领域博主
qq_28513801	抛物线.	抛物线.的博客_CSDN博客-Linux,云计算,openstack领域博主
sanhewuyang	山河已无恙	山河已无恙的博客_CSDN博客-K8S&CKA&云原生相关笔记,Linux笔记,JAVA笔记领域博主
qq_43061290	拉格朗日(Lagrange)	拉格朗日(Lagrange)的博客_CSDN博客-java面试知识点,javaweb,python领域博主
weixin_43151418	xcLeigh	xcLeigh的博客_CSDN博客-html,资源源码,C#使用领域博主
liuzehn	Data-Mining	Data-Mining的博客_CSDN博客-Electron实战,音视频开发,Android实战领域博主
u012039040	一头小山猪	一头小山猪的博客_CSDN博客-萌新科普,学习路上,征文联动领域博主
a29562268	坤昱	坤昱的博客_CSDN博客-一起学习C语言,linux内核源码分析,C语言领域博主
hbblzjy	hbblzjy	hbblzjy的博客_CSDN博客-iOS开发等常用知识点,推荐网站、书籍、文章、第三方,Swift语言基础学习到开发领域博主
loongggdroid	非著名程序员	非著名程序员的博客_CSDN博客-android编程笔记,Android--UI设计,Android--View领域博主
sonbai	fulinux	fulinux的博客_CSDN博客-Linux移植篇,Yocto开发篇,Linux综合篇领域博主
weixin_42445727	Overboom	Overboom的博客_CSDN博客-C++,C,嵌入式Linux开发环境搭建领域博主
weibo1230123	魏波.	魏波.的博客_CSDN博客-物联网、智能家居,嵌入式、旋翼无人机,windows桌面软件开发领域博主
ctwy291314	gmHappy	gmHappy的博客_CSDN博客-java web,大数据学习,Java领域博主
wireless_com	半吊子全栈工匠	半吊子全栈工匠的博客_CSDN博客-架构,云计算,IoT领域博主
12月（第六批）证书发放名单（59位）
用户ID	昵称	博客地址
ai_lx	AI_LX	https://aipro.blog.csdn.net/
u014096024	猿小鱼	猿小鱼的博客_CSDN博客-Python基础学习,软件开发工程师,Oracle从入门到精通领域博主
feit2417	hinzer	https://hinzer.blog.csdn.net/
zag666	若年封尘	https://zag666.blog.csdn.net/
bluishglc	bluishglc	https://laurence.blog.csdn.net/
ubuntutouch	Elastic 中国社区官方博客	https://elasticstack.blog.csdn.net/
xingyu_qie	经海路大白狗	经海路大白狗的博客_CSDN博客-前端开发 前端日常工作 前端面试题,大学毕业解疑,React领域博主
henjay724	痞子衡	痞子衡的博客_CSDN博客-痞子衡嵌入式半月刊,i.MXRT跨界处理器开发经验,Perl领域博主
qq_35082030	刘炫320	https://fjiang.blog.csdn.net/
szullc	架构师李肯	https://recan.blog.csdn.net/
u010986776	搬砖的乔布梭	https://steveouyang.blog.csdn.net/
u013254183	珍妮玛?黛金	https://laowanggaobug.blog.csdn.net/
haojiagou	朱 江	https://zhujiang.blog.csdn.net/
csdnnews	CSDN资讯	https://csdnnews.blog.csdn.net/
misayaaaaa	MISAYAONE	https://renjie.blog.csdn.net/
qq_25305833	BenjaminQA	https://qatester.blog.csdn.net/
qq_37236745	数学家是我理想	https://mathor.blog.csdn.net/
augustme	机器不学习我学习	机器不学习我学习的博客_CSDN博客-python,CV面试,目标检测领域博主
nanke_yh	nanke_yh	nanke_yh的博客_CSDN博客-小技术,嵌入式软件设计,java领域博主
qq_32099833	程序员小潘	https://javap.blog.csdn.net/
qq_44868502	叫我阿呆就好了	https://foolman.blog.csdn.net/
weixin_42990464	点PY	https://py1995.blog.csdn.net/
yangwohenmai1	量化交易领域专家YangZongxian	https://forecast.blog.csdn.net/
baimafujinji	白马负金羁	https://baimafujinji.blog.csdn.net/
guyuealian	AI吃大瓜	https://panjinquan.blog.csdn.net/
la_vie_est_belle	la_vie_est_belle	https://pyqt5.blog.csdn.net/
weixin_42072280	安安csdn	安安csdn的博客_CSDN博客-C语言,Linux,LeetCode领域博主
weixin_43883917	牛哄哄的柯南	https://keafmd.blog.csdn.net/
zhuocailing3390	一恍过去	https://lhz1219.blog.csdn.net/
abc13526222160	布衣小张	https://zhangkaifang.blog.csdn.net/
eastmount	Eastmount	Eastmount的博客_CSDN博客-网络安全自学篇,Python学习系列,知识图谱、web数据挖掘及NLP领域博主
ljfphp	铁柱同学	铁柱同学的博客_CSDN博客-php日常bug,linux,mysql领域博主
lovechris00	伊织code	伊织code的博客_CSDN博客-Apple 开发,软件工具/使用技巧,ML/DL领域博主
mao_hui_fei	翟羽嚄	https://qitiandasheng.blog.csdn.net/
n5	n5	https://happyfire.blog.csdn.net/
nowshut	NOWSHUT	NOWSHUT的博客_CSDN博客-Windows Server 2019 配置与管理实战,VMware vSphere,云原生 | 容器和容器编排工具Kubernetes领域博主
qq_38251616	K同学啊	https://mtyjkh.blog.csdn.net/
red_stone1	红色石头Will	https://redstonewill.blog.csdn.net/
sunbaigui	AI记忆	AI记忆的博客_CSDN博客-LeetCode,面试编程训练,浙大PAT领域博主
u012206617	墨痕诉清风	https://mohen.blog.csdn.net/
weixin_43233219	Lotay.	https://lotay.blog.csdn.net/
xiaoxiaopengbo	Wireless_Link	https://wlink.blog.csdn.net/
xinshuzhan	互联网老辛	https://zmedu.blog.csdn.net/
yong_qi2015	3Ｄ视觉工坊	https://yongqi.blog.csdn.net/
zhangcongyi420	小码农叔叔	小码农叔叔的博客_CSDN博客-技术总结,springboot相关,java领域博主
caoshangpa	草上爬	草上爬的博客_CSDN博客-Qt/QML,资料,流媒体领域博主
jack_chen0309	天才程序YUAN	https://alexanderchen.blog.csdn.net/
laoyuanpython	LaoYuanPython	LaoYuanPython的博客_CSDN博客-老猿Python,PyQt+moviepy音视频剪辑实战,OpenCV-Python图形图像处理领域博主
linxingliang	悟纤	https://wuqian.blog.csdn.net/
linzhefeng89	笨_鸟_不_会_飞	https://linzhefeng23.blog.csdn.net/
petterp	petterp	https://petterp.blog.csdn.net/
qq_19707521	_yuki_	https://yuki-ho.blog.csdn.net/
qq_32618327	氢键H-H	https://joveh-h.blog.csdn.net/
qq_33154343	偕臧x	https://xmuli.blog.csdn.net/
qq_37334150	长臂人猿	https://liaoxg.blog.csdn.net/
qq_41467882	ty_xiumud	https://guodongm.blog.csdn.net/
sinat_28442665	墨理学AI	https://positive.blog.csdn.net/
u013171226	陈 洪 伟	https://cumtchw.blog.csdn.net/
zgpeace	AI架构师易筋	https://zgpeace.blog.csdn.net/

2023年1月份（47位）
CSDN ID	昵称	博客地址
weixin_43896318	星拱北辰	https://blankspace.blog.csdn.net/
qq_33957603	小哈里	https://gwj1314.blog.csdn.net/
qq_34417408	魏小言	魏小言的博客_CSDN博客-LeetCode&剑指Offer 面试编程题锦集,JAVA,服务端领域博主
cl939974883	长路?? ??	https://changlu.blog.csdn.net/
poisonchry	打码的老程	https://seagochen.blog.csdn.net/
softwareteacher	SoftwareTeacher	SoftwareTeacher的博客_CSDN博客-软件工程,AI,人工智能实战领域博主
weixin_45525272	杨 戬	https://yangyongli.blog.csdn.net/
powertoolsteam	葡萄城技术团队	https://grapecity.blog.csdn.net/
czhenya	陈言必行	https://czhenya.blog.csdn.net/
hu_zhenghui	胡争辉	https://huzhenghui.blog.csdn.net/
hursing	hursing	https://hursing.blog.csdn.net/
weixin_44427181	叶 秋	https://yeqiu.blog.csdn.net/
bb15070047748	緑水長流*z	緑水長流*z的博客_CSDN博客-《SSM框架系列》,《RabbitMQ系列》,《设计模式系列》领域博主
gangzhucoll	喵叔哟	https://programercat.blog.csdn.net/
gzy0506	十年老鸟	https://myfpga.blog.csdn.net/
herosunly	herosunly	https://herosunly.blog.csdn.net/
jhxl_	几何心凉	几何心凉的博客_CSDN博客-Vue,javaScript,PHP领域博主
lmy_520	小5聊	小5聊的博客_CSDN博客-C#,基础知识,前端领域博主
m_buddy	m_buddy	m_buddy的博客_CSDN博客-General Object Detection,图像&视频分割,程序设计领域博主
qq_41603102	小牛呼噜噜	https://xiaoniuhululu.blog.csdn.net/
qq_42257666	振华OPPO	振华OPPO的博客_CSDN博客-Android经验,Android项目,Ubuntu领域博主
zuoyueliang	恋猫de小郭	https://carguo.blog.csdn.net/
boling_cavalry	程序员欣宸	https://xinchen.blog.csdn.net/
btnode	机智的程序员小熊	https://coding3min.blog.csdn.net/
google19890102	zhiyong_will	https://felix.blog.csdn.net/
indexman	罗汉爷	罗汉爷的博客_CSDN博客-Java-Core,Java项目实战,Spring-Boot领域博主
nokiaguy	蒙娜丽宁	https://unitymarvel.blog.csdn.net/
qq_33375598	繁星蓝雨	https://fanxinglanyu.blog.csdn.net/
qq_41938259	TIM33470348	https://timtian.blog.csdn.net/
qq_44159028	山山而川'	https://chenchena.blog.csdn.net/
aa518189	阿华田512	阿华田512的博客_CSDN博客-Flink学习必读系列,java知识库,大数据开发领域博主
cplvfx		https://cplvfx.blog.csdn.net/
glorydx	可缺不可滥	https://dengxi.blog.csdn.net/
jones2000	HQChart	https://jones2000.blog.csdn.net/
l1028386804	冰 河	https://binghe.blog.csdn.net/
myhes	时空琴弦	https://troll.blog.csdn.net/
mystonelxj		mystonelxj的博客_CSDN博客-开发工具,数据库使用,Python使用领域博主
qfmzhu	汽车电子助手	汽车电子助手的博客_CSDN博客-Autosar,UDS/OBD诊断CDD,Autosar从入门到精通-实战篇领域博主
qq_36393978	Imagine Miracle	Imagine Miracle的博客_CSDN博客-Linux,学习过程中遇到的小问题,Rust小小白领域博主
qq_36816848	GoAI	GoAI的博客_CSDN博客-深度学习,机器学习,大数据笔记领域博主
qq_40772692	阿阿阿安	阿阿阿安的博客_CSDN博客-====数据结构学习====,====数学物理====,-----JavaWeb-----领域博主
qq_43592352	征途黯然.	https://comdy.blog.csdn.net/
qq_44824148	莫余	莫余的博客_CSDN博客-凌晨3:40的Vue,设计模式,数学建模领域博主
u012932409	cczhengv	https://cczheng.blog.csdn.net/
why_still_confused	若明天不见	https://tillend.blog.csdn.net/
wo541075754	程序新视界	https://hello.blog.csdn.net/
zhangxin09	sp42a	https://zhangxin.blog.csdn.net/
2023年2月份（54位）
用户名	用户昵称	博客地址
qq_39132095	坚果的博客	https://jianguo.blog.csdn.net/
feng8403000	红目香薰	https://laoshifu.blog.csdn.net/
u012877217	低代码布道师	https://easycode.blog.csdn.net/
qq563129582	PassionY	https://passion.blog.csdn.net/
weixin_42137700	喜欢打酱油的老鸟	喜欢打酱油的老鸟的博客_CSDN博客-人工智能,SAP文章,管理领域博主
knight_zhou	叱咤少帅（少帅）	https://knight.blog.csdn.net/
zgl_cyy	赵广陆	赵广陆的博客_CSDN博客-java,tool,architect领域博主
wgl307293845	wellnw	wellnw的博客_CSDN博客-qsdk遇到问题及解决方案,WiFi,网络领域博主
flyfish1986	西西弗Sisyphus	https://flyfish.blog.csdn.net/
u011046042	道亦无名	https://jackzhang.blog.csdn.net/
cpongo2	名名名名名名名名名名名名名名名名名名名名	https://cpongo.blog.csdn.net/
qq_42897427	刘架构	https://spiritmark.blog.csdn.net/
m0_37609579	十步杀一人_千里不留行	十步杀一人_千里不留行的博客_CSDN博客-java,算法高级,程序人生领域博主
liwei16611	产品人卫朋	https://pm-wsir.blog.csdn.net/
u010194271	黄毛火烧雪下	黄毛火烧雪下的博客_CSDN博客-----------Android----------,计算机网络,Kotlin领域博主
csd11311	软件质量保障	https://qualityassurance.blog.csdn.net/
catoop	catoop	https://shanhy.blog.csdn.net/
xuelang532777032	雪狼之夜	雪狼之夜的博客_CSDN博客-vue,各种報錯解決方案,eggjs领域博主
chengyq116	Yongqiang Cheng	https://yongqiang.blog.csdn.net/
lilongsy	小龙在山东	https://lilongsy.blog.csdn.net/
zmlovelx	帅得不敢出门	帅得不敢出门的博客_CSDN博客-LINUX,Android平台,ARM嵌入式领域博主
a332324956	程序员入门进阶	https://codegg.blog.csdn.net/
update7	code_kd	https://update.blog.csdn.net/
qq_15969343	任博啥时候能毕业？	https://geo-ai.blog.csdn.net/
automoblie0	信必诺	信必诺的博客_CSDN博客-Qt,FFmpeg,Android Studio领域博主
weixin_32393347	Yan Errol	https://errol.blog.csdn.net/
lamp_yang_3533	lamp_yang_3533	lamp_yang_3533的博客_CSDN博客-Linux,Linux 基础知识,PHP领域博主
qq_42139931	CoderZ1010	https://coderz.blog.csdn.net/
qq_15071263	简简单单OnlineZuozuo	https://wretchant.blog.csdn.net/
jackgo73	高铭杰	https://mingjie.blog.csdn.net/
xishining	IT技术分享社区	IT技术分享社区的博客_CSDN博客-电脑技巧,数据库,数据库软考领域博主
qq_35366269	郭宝	https://guobao.blog.csdn.net/
qq_43290288	梁同学与Android	梁同学与Android的博客_CSDN博客-Android 笔记,Scratch 青少年蓝桥杯考试真题,Python 青少年蓝桥杯考试真题领域博主
weixin_41552975	Christo3	https://christo3.blog.csdn.net/
shangyanaf	搞前端的半夏	https://picker.blog.csdn.net/
q764424567	恬静的小魔龙	https://itmonon.blog.csdn.net/
yinxing2008	蓝不蓝编程	https://cxyxy.blog.csdn.net/
breaksoftware	breaksoftware	https://fangliang.blog.csdn.net/
qq_43753724	别团等shy哥发育	https://codeleader.blog.csdn.net/
weixin_40639095	史一试	https://shixuebin.blog.csdn.net/
u010882234	坚持是一种态度	坚持是一种态度的博客_CSDN博客-java,异常报错,数据库及存储技术领域博主
aikongmeng	aikongmeng	https://aikongmeng.blog.csdn.net/
qq_39313596	hlvy	hlvy的博客_CSDN博客-java,vue,vue领域博主
whq12789	草宝虫	https://wanghq.blog.csdn.net/
qq_33876553	桥路丶	桥路丶的博客_CSDN博客-大数据,数据仓库原理与实战精讲,问题解决领域博主
zy1281539626	优小U	优小U的博客_CSDN博客-Web前端,ES6-11快速入门,移动开发领域博主
bjbz_cxy	17岁boy想当攻城狮	https://jrhar.blog.csdn.net/
zzu_seu	编程芝士	编程芝士的博客_CSDN博客-Java基础学习,数据结构与算法,ROS领域博主
jiazhen	路漫漫其修远.	https://jiazhen.blog.csdn.net/
shifang07	沐雨07	沐雨07的博客_CSDN博客-iOS开发笔记,Flutter开发教程,Swift 5.0领域博主
vip099	NIO4444	https://xdoctorx.blog.csdn.net/
wangzhae	极梦网络无忧	https://jimeng.blog.csdn.net/
xuezhiwu001	stark张宇	https://stark.blog.csdn.net/
yilovexing	极客点儿	极客点儿的博客_CSDN博客-Python,Linux,Ubuntu领域博主
2023年3月份（60位）
CSDN ID	用户昵称	博客地址
qq_32892383	思绪无限	https://wuxian.blog.csdn.net/
py8105	py_free-物联智能	https://pyfree.blog.csdn.net/
wangwei021933	王卫——David	https://liuyuying.blog.csdn.net/
bemavery	码海拾贝2023	https://bemavery.blog.csdn.net/
weixin_43970743	bug菌¹	https://luoyong.blog.csdn.net/
weixin_42429718	一百个Chocolate	https://chocolate.blog.csdn.net/
u010665216	小虎AI实验室	https://xiaosheng.blog.csdn.net/
w464960660	Winter_world	https://winter.blog.csdn.net/
weixin_44893902	明金同学	https://ymjin.blog.csdn.net/
high2011	high2011	https://jifei-yang.blog.csdn.net/
qq_31635851	猫巳	https://moonce.blog.csdn.net/
jackfrued	骆昊的技术专栏	骆昊的技术专栏的博客_CSDN博客-程序语言,Web开发,Python全栈开发领域博主
qq_41552508	Gene_INNOCENT	https://gene-liu.blog.csdn.net/
weixin_44904816	Apache Flink	Apache Flink的博客_CSDN博客-Flink,大数据,实时计算领域博主
qq_36720848	麒麟子MrKylin	https://qilinzi.blog.csdn.net/
qq_38998213	ZhangJiQun&MXP	https://zhangjq.blog.csdn.net/
qq_44850489	黑白极客	黑白极客的博客_CSDN博客-java,操作系统,springboot领域博主
weixin_41114301	ST小智	ST小智的博客_CSDN博客-鸿蒙万物互联人工智能之卓越,Linux操作系统嵌入式开发之经典,单片机项目实战操作之优秀领域博主
weixin_45052363	JOJO数据科学	https://jojo-ds-nlp.blog.csdn.net/
qq_32370913	总是幸福的老豌豆	https://zhaocunwei.blog.csdn.net/
qq_34988304	笑小枫	https://zhangfz.blog.csdn.net/
gou_hailong	流浪猪头拯救地球	流浪猪头拯救地球的博客_CSDN博客-Surveying And Mapping,Math,Python领域博主
u011240877	拭心	https://shixin.blog.csdn.net/
u014038143	GamebabyRockSun_QQ	https://gamebaby.blog.csdn.net/
viatorsun	ViatorSun	https://viatorsun.blog.csdn.net/
zcxey2911	刘悦的技术分享	https://v3ucn.blog.csdn.net/
kaitiren	Kaitiren	https://kaitiren.blog.csdn.net/
luolaihua2018	SOC罗三炮	SOC罗三炮的博客_CSDN博客-CPP,PCL点云处理,Linux_C领域博主
qq_20288327	Mr数据杨	h"
类脑运算--脉冲神经网络（Spiking Neural Network）发展现状,2020-08-24,"类脑运算–脉冲神经网络（Spiking Neural Network）发展现状 
前一段时间忙于博士论文的攥写和答辩， 抱歉拖更 
继上一章： 类脑运算–脉冲神经网络（Spiking Neural Network）叙述 
SNN是第三代人工神经网络（ANN），是一类event驱动的神经形态算法，可能具有广泛的应用领域，并适用于各种极低功耗的神经形态硬件。现阶段SNN 的应用落地和算法发展方兴未艾，工业界和学术界近几年来对其的探索和研发也从未停止。 这篇文章会从以下几个方面讨论类脑运算技术的发展概况。 
神经形态运算平台（Neuromorphic Computing Platform）神经形态传感器（Neuromorphic Sensor）SNN仿真软件（SNN simulators）类脑运算的应用 
神经形态运算平台（Neuromorphic Computing Platform） 
类脑运算平台或者类脑芯片， 是受到生物学脑工作机制启发开发的专用于为SNN提供计算的硬件系统。制作类脑芯片最具有挑战性的是如何把不计其数spiking 神经元和突触放进一个小小的芯片里并同时让他们的链接结构是可调整的。 
最初，类脑芯片仅由科研学术机构进行探索。 由于研究人员已经展示出这些出色的类似于大脑的计算模型的巨大潜力，因此许多大公司已开始参与类脑芯片的开发。 
IBM在2014年开发了TrueNorth芯片，它是美国国防高级研究计划局SyNAPSE开发计划的一部分。 单个TruNorth芯片包含4096个计算核心，可以实现神经突触和神经元排列的动态映射。 每个内核最多可将1024个轴突电路用于输入连接，从而实现256个IF神经元，这些神经元组织为静态随机存取存储器。 IBM TrueNorth系统的一个吸引人的功能是，单个芯片由54亿个晶体管组成，仅消耗70mW的功率密度，仅占传统计算单元的1/10000。 
 图：TrueNorth芯片 
SpiNNaker NM平台是由曼彻斯特大学的研究人员开发的，曼彻斯特大学的研究人员是由欧盟资助的“人脑计划” （Eupropean HBP）的一部分。 SpiNNaker为SNN的硬件实现提供ASIC解决方案。它利用多个ARM内核和FPGA来配置硬件和PyNN软件API，以实现平台的可扩展性。 ARM处理器使该平台能够以仅1毫秒的仿真时间步长，以生物逼真的连通性配置数十亿个脉冲神经元。此外，第二代平台SpiNNaker2仍在开发中，它可以使用1000万个处理器来模拟更大，更复杂的SNN。除SpiNNaker之外，BrainScaleS 也是HBP项目的另一个类脑计算平台。 BrainScaleS是使用晶圆级集成技术开发的混合信号神经形态芯片，该芯片允许利用4000万个突触和多达18万个神经元。正在设计下一代BarianScaleS，并将其命名为BrianScaleS-2，它能够使用更复杂的神经元模型，同时支持非线性突触和定制结构的神经元。图： SpiNNaker中央机 
SpiNNaker 平台提供SNN的云仿真和计算平台， 这就意味着如果你想试试自己的SNN在硬件上效果如何的话。你可以去SpiNNaker 云平台上 上传自己的代码，结果会由云平台返回， 前提是你的SNN需要是用PyNN构建的。 
斯坦福大学在类脑领域贡献了两个类脑硬件，分别是Neurogrid和Braindrop。 Neurogrid 中的神经核由256x256制成的CMOS阵列构成，该阵列可实现SNN的混合模数实现。 Neurogrid能够以数百万个神经元和数十亿个突触的能力提供生物学上合理的计算。 像Neurogrid一样的Braindrop是一个混合信号NM处理器，但抽象程度很高。 Braindrop采用28纳米FDSOI工艺进行设计，并将4096个尖峰神经元集成在单个芯片上，该芯片的神经元容量有限，无法大规模实施SNN。 
 图 ：斯坦福Neurogrid 
英特尔Loihi 类脑芯片是英特尔最近宣布的数字神经计算平台。 Loihi最吸引人的特点是芯片在线学习的潜力。 Loihi拥有一个特殊的可编程微代码引擎，可以即时进行SNN培训。 Loihi具有3个独特的Lakemont NM核心，专门设计用于协助高级学习规则。 一个Loihi芯片中总共有128个NM内核，能够实现130K LIF神经元和130M突触。 Loihi系统的最大大小可支持多达16个芯片的4096个片上内核。 
 图 ：英特尔Loihi 芯片 
Brainchip公司开发了名为Akida神经形态计算平台，该平台可以使用一个NSoC有效地实现120万个神经元和100亿个突触。 该平台具有多个板载处理器，包括基于事件的处理，数字处理，存储器，输入/输出接口和多芯片扩展的功能。图 ：Brain Chip Akida芯 
除了这些类脑芯片外，仍有许多新兴的类脑芯片在SNN计算中显示出巨大的潜力。 例如浙江大学的达尔文芯片，其目标是嵌入式低功耗应用。 苏黎世大学研究人员开发的DYNAP-SEL结合了异步数字逻辑和模拟电路，以实现模拟SNN实现。 清华大学的研究人员成功设计了混合型天机芯片 ，该混合型既可以实现常规神经网络又可以实现SNN。 如下原创表展示了各类芯片的汇总。 
 图： 类脑运算平台汇总[禁止转载] 
神经形态传感器（Neuromorphic Sensor） 
动态视觉传感器（Dynamic Vison Sensor） 
传统的视觉传感器的经典例子是数码相机，它以预定的帧频重复刷新其整个像素值阵列。但是，使用数码相机具有动态运动识别的三个缺点。首先，数码相机通常以预定义的帧采样速率（通常范围为每秒25-50帧）运行，这限制了观察到的活动的时间分辨率。其次，连续的帧和每个帧中的冗余像素浪费了大量的存储资源和计算。第三，传统图像传感器的动态范围受到其曝光时间和集成能力的限制。大多数相机会遭受饱和线性响应，动态范围限制在60-70dB，其中自然场景的光线可以达到动态范围的约140dB。动态视觉传感器（DVS）提供了解决这些问题的方法。使用AER数据形式（Address evnet representation）的DVS是基于人类视觉系统的事件驱动技术。基于事件的传感器在动态场景识别任务中的优势在于，当场景发生很大变化时，它可以提供非常高的时间分辨率，这只能通过高速数码相机来匹配，而这通常需要大功率和大量电量，和计算资源。 
 图： DVS原理展示（https://gfycat.com/incredibleimpoliteblackmamba） 
图： DVS视觉输出对比（https://www.ibm.com/blogs/research/2018/06/stereo-vision/） 
上展示了DVS输出的​​示例，并与常规传感器进行了比较，其中DVS可以为快速移动的物体产生更清晰的边缘。这在对象识别和节省内存的过程中都非常重要。在DVS中，信息被编码并作为电脉冲传输，这类似于生物传感系统中的处理机制。通过将人造视网膜像素的每个活动与某个阈值进行比较，可以异步生成DVS的输出。DVS的出现证明了在超快速节能计算应用中的巨大潜力。与传统的视觉传感器相比，DVS返回非同步事件，而不是基于采样时间的帧序列。对于给定的实际输入，DVS仅记录像素强度值的变化，并输出有关极性变化的ON / OFF离散事件流。这种基于事件的获取机制具有许多优势，例如低功耗，较少的冗余信息，低延迟和高动态范围。 
随着DVS的成功设计，为类脑传感器应处理的特性建立了基准，并为基于事件的视觉传感的进一步类脑运算的未来研究提供了指导。 提出了许多增强版的DVS，以改善设备的空间分辨率或动态范围。 例如Brandli等人提出了一种混合方法来解决基于帧和无帧的视觉传感问题。 DAVIS（动态和主动像素视觉传感器）在单个摄像机中集成了基于帧的主动像素感应和异步DVS感应。 
动态音频传感器（Dynamic Audio Sensor） 
就像DVS模仿人类视觉系统一样，动态音频传感器的工作机制也受到人类听觉系统中的感觉器官的启发。 DAS是异步事件驱动的人工耳蜗，可接收立体声音频输入。 DAS使用麦克风前置放大器和64个双耳声道，为神经形态音频感应设定了基准。 DAS集成了本地数模转换器（DAC），以允许修改每个通道中的质量因数。 通过级联的二阶模拟部分来模拟耳蜗的功能，这些模拟部分包括半波整流器，频率调制器，数模转换器，服务器放大器和缓冲器。 
下图中演示了双耳DAS语音输出的示例。 图展示了DAS对语音信号的响应，其中绿色和红色分别对应于左采样通道和右采样通道，每个点都是一个采集事件。 DAS的线性调频响应如图所示，输入信号的动态频率变化范围为30Hz至10kHz。 
 图： DAS语音信号相应输出（https://inilabs.com/ products/dynamic-audio-sensor/） 
 图： DAS 线性调频信号响应（https://inilabs.com/ products/dynamic-audio-sensor/） 
SNN仿真软件（SNN simulators） 
尽管SNN具有许多优势，但是在模拟尖峰神经元方面的计算问题还是比较大的。 在某些情况下，像IZ神经元模型一样，需要对生物物理峰值神经元进行详细的差分表示。 另一方面，在实际应用方面，不需要现实地重建生物突波产生机理的简化神经元模型（比如IF神经元模型）。 SNN的仿真策略可以分为两个系列：同步或异步。 同步算法会在每个时间步更新所有神经元，这比异步或“事件驱动”算法会导致更高的计算资源。 异步方法仅在神经元接收或发出脉冲时更新神经元状态，就像DVS传感器的工作范式一样。 
与ANN中的统一神经网络框架（例如Tensorflow 和Pytorch）不同，SNN模型和SNN的训练方法没有得到广泛一致的统一化。 模拟SNN的方法仍然是多种多样且客观的。 当前，设计SNN的过程不仅考虑了网络本身的可行性，而且还可以扩展到诸如生物学上的合理性，计算成本和学习机制之类的功能。 为了全面回顾SNN的软件实现，下表总结了noticeable的SNN框架，其中包括这些框架的特点。图： SNN软件框架汇总[禁止转载] 
类脑运算的应用 
在目前， 类脑运算虽然仍处于科研阶段， 但各界大牛的应用示例层出不穷。 SNN的优势在个人看法而言， 体现在 如果一个系统同时具有传感器，芯片，和强大的SNN算法，它的功耗和运行速度是传统ANN,DNN所不能匹敌的。 但就现有的学习算法而言， SNN在和DNN对比上对各类任务的表现（比如识别准确率）还差强人意。 
Diehl 在2015年paper “Unsupervised learning of digit recognition using spike-timing-dependent plasticity” 中实现了STDP和侧面抑制结合的WTA学习方法， SNN通过非监督式的学习方法可以在MNIST手写数字识别的数据库上达到95% 的准确率。 
2017年的CVPR上， TrueNorth团队在TrueNorth芯片上实现了实时的动态手势识别。虽然此方法是基于传统深度学习的模型，但整个系统的功耗大概只有200mW和105ms的延迟（1秒10帧）。 
清华大学天机芯片团队，2019年Nature上发表了使用混合神经网络驱动自动驾驶自行车的例子。这里包涵了传统ANN和SNN的信号处理融合。 https://www.nature.com/articles/s41586-019-1424-8 
在非监督式学习没有重大的突破的情况下， 研究人员们开始转向使用ANN-SNN转换或者直接将Backpropagation 塞进SNN里用于提高各项任务的准确率。这样的算法比如有SLAYER在语音识别TIDIGIT上取得99.09%的识别准确率。 
总结一下， 现阶段SNN的应用方向应总结为两类。 1， 是以神经形态传感器为驱动的比如利用DVS,DAS在目标传感上的优势来构建应用，因为SNN对于处理他们相应的AER输出有天生的优势。 2.，是以低功耗应用落地为驱动的。因为类脑芯片和SNN最大的优势在于此， 尽管可能在具体任务上SNN的表现不如传统嵌入式系统+DNN的模式， 但SNN在一些物联网， 功率有限的设备应用上将展现巨大的优势。 
国内外类脑机构/公司总结 
这里概括一下作者已经发现的或者感兴趣的类脑团队供读者们进一步了解，先后顺序不代表排名。 
国内： 
清华大学类脑计算研究中心（天机芯片团队）华为中央研究院之江实验室浙江大学达尔文芯片团队鹏城实验室中国科学院自动化研究所类脑智能研究中心灵汐科技（天机芯片）西井科技时识科技Synsense（苏黎世DYNAP芯团队） 
国外： 
曼彻斯特SpiNNaker团队欧盟HBP计划团队苏黎世神经信息研究所（DYNAP,DVS,DAS团队）Inilab（DVS 团队）滑铁卢大学（Nengo团队）Intel （Loihi芯片团队）IBM （TrueNorth团队）斯坦福大学（Braindrop和Neurogrid团队）德黑兰大学 (Spyketorch框架团队)"
人工智能GPU/FPGA/ASIC/类脑芯片优势分析,2018-01-18,"人工智能如果按照架构进行划分，有三个重要元素：数据、算法和算例，其中算例就是芯片，算例是基础，算法是核心，数据是保障。先看人工智能的定义，从广义上讲只要能够运行人工智能算法的芯片都叫人工智能芯片。嵌入式开发 
 为了进行大数据处理，当前的方案一般采用高性能的处理器辅助MCU进行计算，但是随着摩尔定律时间周期拉长，处理器上可集成的器件数量会达到极限，而数据量还在不断增加，因此我们需要通过架构的变化来满足数据量的增长，这就是人工智能芯片推出的背景。 
 目前来看，人工智能芯片有四类架构：GPU、FPGA、ASIC和类脑芯片。 
 GPU：是单指令、多数据处理，采用数量众多的计算单元和超长的流水线，如名字一样，图形处理器，GPU善于处理图像领域的运算加速。但GPU无法单独工作，必须由CPU进行控制调用才能工作。CPU可单独作用，处理复杂的逻辑运算和不同的数据类型，但当需要大量的处理类型统一的数据时，则可调用GPU进行并行计算。 
 FPGA：和GPU相反，FPGA适用于多指令，单数据流的分析，因此常用于预测阶段，如云端。FPGA是用硬件实现软件算法，因此在实现复杂算法方面有一定的难度，缺点是价格比较高。将FPGA和GPU对比发现，一是缺少内存和控制所带来的存储和读取部分，速度更快。二是因为缺少读取的作用，所以功耗低，劣势是运算量并不是很大。结合CPU和GPU各自的优势，有一种解决方案就是异构。 
 ASIC芯片：是专用定制芯片，为实现特定要求而定制的芯片。除了不能扩展以外，在功耗、可靠性、体积方面都有优势，尤其在高性能、低功耗的移动端。谷歌的TPU、寒武纪的GPU，地平线的BPU都属于ASIC芯片。谷歌的TPU比CPU和GPU的方案快30-80倍，与CPU和GPU相比，TPU把控制缩小了，因此减少了芯片的面积，降低了功耗。 
 类脑计算是真正模拟人脑进行设计，人脑的特点就是神经元进行传输数据。当我们用硬件去模拟人脑时，在硬件环节里还有许多多余的元素，而类脑芯片就拟合人脑的作用。要做类脑芯片非常难，IBM、高通、英特尔等公司的芯片策略都是用硬件来模仿人脑的神经突触。 
 将以上四种架构对比，GPU未来的主攻方向是高级复杂算法和通用性人工智能平台，其发展路线分两条走：一是主攻高端算法的实现，对于指令的逻辑性控制要更复杂一些，在面向需求通用的AI计算方面具有优势；二是主攻通用性人工智能平台，GPU的通用性强，所以应用于大型人工智能平台可高效完成不同的需求。FPGA更适用于各种细分的行业，人工智能会应用到各个细分领域。 
 ASIC芯片是全定制芯片，长远看适用于人工智能。现在很多做AI算法的企业也是从这个点切入。因为算法复杂度越强，越需要一套专用的芯片架构与其进行对应，ASIC基于人工智能算法进行定制，其发展前景看好。类脑芯片是人工智能最终的发展模式，但是离产业化还很遥远。"
1_华为是怎样开发硬件的,2023-05-19,"目录 
 
 
 文章目录 
 目录前言1.文档，评审，设计2.华为的硬件领域的人员构成3.华为的流程4.归一化器件归一化单板归一化平台归一化网络架构归一化 
 5.硬件工程师的核心竞争力-专题分析6.器件选型关于“器件选型规范”器件选型需要考虑的因素1.可供应性2.可靠性3.可生产性4.环保5.考虑归一化6.行业管7.器件部门8.成本 

 7.白板讲解8.问题攻关1.网上事故2.网上问题3.单板返还 
 9.会议1.华为的会议的特点2.《罗伯特议事法则》3.减少无效会议 
 10.兄弟文化无兄弟，不研发——共同奋斗的快乐团队作战，人尽其用管理，重要的是理，然后才是管做刘邦，不要做项羽 
 11.测试华为测试经历的阶段1.单元测试(UT)2.集成测试(IT)3.系统测试(ST)4.模块间接口测试(BBIT)5.系统设计验证(SDV)6.系统集成测试(SIT)7.系统确认测试(SVT)不可思议的小米5% 
 12. 硬件的知识管理与分类 

 
 
前言 
此文章转自微信公众号硬件十万个为什么 
最近很多朋友咨询的一些硬件问题，发现朋友们没有仔细的看datasheet，也没有好好的做电路分析。我讲一讲华为是怎么做硬件开发的，给正在做硬件开发的朋友一些启示。说的不对的地方，大家批评指正。 
曾经2007年，刚工作2年的时候去一家小公司去面试。当时考题，我感觉我做得很好，面试的时候，对方对我也很认可。但是他当时说：“我需要招一个，在大公司待过的，最好知道硬件开发流程和规范的。虽然你题答得不错，但是我们需要一个有丰富经验的，最好在华为待过的。” 
当时，我就在想“华为的规范和流程是啥样的”，就一直想去看看。之前对华为的面试一直都不是很感兴趣。之后，就很想有机会去华为看看。2008到了华为。 
我能想到的华为硬件开发的几个不一样的点，跟大家分享一下，想到哪写到哪，欢迎大家批评指正。 
1.文档，评审，设计 
当时刚入职时，三个人做一个电路板。虽然电路复杂一些，还是有一些人力过剩的。所以，我就被安排去写一个PCI转UART的逻辑。 
我当时是新员工，也急于表现自己，利用周末的时间，估计用了一周的时间，就写完代码，开始仿真了。我以为我的导师兼主管会表扬一下，结果没有，他说：“你为什么没有召集大家讨论？然后再写方案，评审？然后再动手写代码？”我当时是没有理解的，觉得我一个人就搞定的事情，为啥要这样劳师动众？ 
现在反思： 
第一、 从主管的角度，不知道新员工的个人能力，你能把做的事情讲清楚了，他才放心。 
第二、 从公司的角度，有一套流程来保证项目的交付。那么则不再太依赖某个人的个人能力，任何一个人的离职，都不会影响项目的交付。这也是华为最了不起的地方，把复杂的项目拆得非常细碎，这样不需要特别牛的人来交付项目。这是为什么华为的工程师的收入是思科的N分之一。 
第三、 从效果角度，毕竟一个人的想法是有限的，把想法文档化的过程，就是整理思路的过程；讨论的过程，就是收集你自己没有想到的过程。正式的评审，是大家达成意见的过程。提前讨论，让相关的人都参与到你的设计中，总比你设计完了，被别人指出一个致命的问题要强得多。 
就是因为华为把一项工作拆散了，所以沟通，文档，评审，讨论，变得非常重要。 
这个工作模式的缺点，也是显而易见，沟通成本高，工作效率低。 
2.华为的硬件领域的人员构成 
在华为内部里面，人员角色非常多。硬件的人是对产品开发阶段，端到端负责的。 
做单板硬件工程师，可以涉猎最多的领域，同时也是工作内容最杂，接触人最多，扯皮的最多的工种。 
 
但是也因为有人专门负责画PCB、EMC、电源、逻辑，原本硬件工程师应该做的领域。那么硬件工程师就武功尽废，变成“连连线”。 
其实不然，正是由于每个人都是一个小的领域，没有人统领，所以一个好的硬件经理的作用非常的重要，是贯穿所有领域和全部流程的关键角色。 
正如原来华为内部论坛上有一个人比喻的，硬件工程师更像是处理器里面的“Cache”，是所有环节的中转站。 
大公司把人的分工分的这么细，也是防止某一拨掌握了太多公司的核心技术，出去单搞了。 
3.华为的流程 
其实华为的流程，很多人都知道IPD流程是从IBM来的，同时华为也去咨询过爱立信，爱立信的硬件开发，完全没有流程一说。 
我个人理解：IPD流程已经在华为变种，结合了中国人的特点，华为的企业特点进行了变通和优化。如果华为僵硬的套用IBM的这套流程，也必定不会这么成功。 
那么概括一下华为的硬件开发流程： 
需求分析→总体设计→专题分析→详细设计→逻辑详设→原理图→PCB→检视→粘合逻辑→投板→生产试制→回板调试→单元测试→专业实验→系统联调→小批量试制→硬件稳定→维护。 
流程的根本在于，这个环节做好了，再进入下一个环节。所有的环节其实跟其他公司并没有太大的区别，只不过严格把握了进入下一个环节的考核条件。令硬件工程师最纠结的是“没有个节点跟’投板’对应”。 
华为支撑IPD流程的系统是PDM（又名爬的慢） 
PDM的中文名称为产品数据管理（Product DataManagement）。PDM是一门用来管理所有与产品相关信息（包括零件信息、配置、文档、CAD文件、结构、权限信息等）和所有与产品相关过程（包括过程定义和管理）的技术。 
华为所有的器件资料，产品部件，工具，文档，原理图，PCB，逻辑代码等都存在这个系统上。 
但是系统过于庞杂，其实比较难使用，跟服务器归档、SVN归档、也容易搞混淆。 
4.归一化 
器件归一化 
硬件工程师一般都能够理解，在一个板子上面的，尽可能的选择成本更低的器件，选择更少种类的器件，便于集中采购，同时也便于加工。但是其他公司可能没有对器件归一化的工作做得那么细致和严格。 
第一， 由于华为整个公司使用的器件种类非常的多，所以如果减小一个器件编码，带来的收益是十万人民币到几百万，而其他公司可能达不到这个高的收益。所以如果能减少一个编码，宁愿选择可能成本更高的器件。但是这个也需要按照每年的器件直接成本收益*器件发货数量，与编码成本+加工成本差异，进行对比的。不过器件归一化之后，器件的价格又可以跟供应商重新谈价格，这个收益是迭代的。所以，有时即使是成本占优，也会倾向去器件归一化的结论。例如，逐步去除了5%精度的电阻，归一化到1%。 
第二， 器件归一化，都是需要进行专题分析的。因为也有工程师为了归一化，对电路原理没有充分分析，导致的归一化带来“问题引入”。所以，当时我的部门当时有一个表格，“器件归一化分析.xls”的excel表格，把每个器件，原来选型，归一化的选型，更改的原因，都做好记录和原因分析。一是让每个做归一化的员工都充分考虑分析，二是问题都有记录，便于评审，三是出了问题，好打板子。 
单板归一化 
除了器件归一化，更高一个层次的归一化，就是单板归一化。（单板这个概念，我稍微澄清一下，我刚到华为的时候，也觉得这个词很奇怪。因为通信设备，都是机框，背板，加各个功能模块的电路板，各个功能模块的电路就叫做“单板”，硬件工程师，一般也叫做“单板硬件”） 
单板归一化带来的好处，首先是电路的种类少，电路的种类少的好处有两个：一是生产成本降低，二是硬件维护成本降低，三是软件开发和维护的成本降低。 
第一、单板归一化的先决条件首先是处理器归一化。其实，华为的有的产品这点做得其实不好，X86、MIPS、ARM、PPC全部都用个遍，所以一个硬件平台，需要配备各种软件人员，操作系统搞N套，VxWorks和Linux，BIOS各种配套。 
第二、单板的归一化，要注意产品的衍生。第一个版本的机框上的单板所实现的功能，如果后续的产品可以使用，应该直接可以用，不需要再开发。如果不注意这点，第一个版本的单板，到第二版本时，发现不能相互借用。反过来，再修改第一个版本的电路板，来适应新版本。有时问题更糟糕，就是完全不能兼容，只好重新开发。单板的规划显得非常重要。 
第三、单板归一化时，虽然电路部分兼容了，但是结构件不兼容。对于市场人员的配置来说，仍然是两种配置。一样是失败的。 
平台归一化 
那么如果发现不同的硬件平台的架构雷同，功能类似。那么机框也可以归一化。只需要制作不同的电路功能模块，就可以实现不同的功能需求。 
但是不同的硬件形态都是有他存在的意义的，如果强行归一，市场未必会接受这种事情的发生。例如用一个运营商的平台去归一一个企业应用或者家庭应用的产品，可能就未必能够成功。 
网络架构归一化 
这个说法是我自己想的，早在08年的时候，华为就在讨论“云管端战略”了，当时不是很理解。当我们一个运营商平台部门，跟“服务器”的部门合并的时候，似乎理解了点什么。 
当X86处理器足够强大的时候，所有的运算，不管是否性价比最高，都送到云端进行处理，那么所有中间的存储和计算都显得不重要了。那么整个网络的结构，就是终端+管道+云存储和云计算。 
 
既然计算和存储设备都是一样的，那作为运算和存储的设备，也就不需要那么多样化了。这时网络存储设备，和服务器就显得尤为重要。 
这也是华为成立IT产品线，做重点战略投资的重要原因。 
所以现在也就不需要那么多网络节点和网络平台了，只需要超强的处理和存储能力和宽广的通道，多样的终端。 
5.硬件工程师的核心竞争力-专题分析 
早期，我大中华自研的潜艇，都是海蓝色的，跟军舰一样颜色“蓝灰色”。后来我大海军去参观前苏联的军事演习，发现俄国人的潜艇不是蓝色的，是黑色的。于是回来大讨论，为啥俄国人的潜艇是黑色的。猜想：一定是黑色在夜里面不容易被发现，所以油漆成黑色的。于是全国油漆大运动。 后来才知道，原来俄国人的黑色不是油漆，是黑色的橡胶，消声瓦。于是我们也贴橡胶，可是我们贴了橡胶之后就潜艇跑不动了，因为我们的潜艇的动力不如别人。（以上故事纯属虚构，如有雷同，请把发生时间改为清朝。） 
为啥在这里说这个照葫芦画瓢失败的故事呢。我觉得很多硬件工程师有个误区，觉得自己的核心竞争力是在于会使用几个软件（cadence、Protel），画画原理图，画画PCB。我早期的一份工作就这样，最大的本事就是照葫芦画瓢，抄Demo板，抄以前成熟的电路，如果碰到了新的电路设计，一般是按照参考电路先画出电路，再通过调试，去尝试，碰到问题，再去解决问题。 
那么现在的观念是，硬件工程师最值钱的地方是在于懂硬件原理，懂得电路分析，模电数电原理，电磁场理论，而不是会使用画图软件。 
那么华为是怎样做电路设计的呢？为什么会有专题分析的说法呢？为什么电路设计的时候要做专题分析？ 
第一、 例行的，每个电路一般都会做几个必选的专题：电源、时钟、小系统；把每个管脚怎么用，怎么接，对接的管脚的电平是否满足要求，都需要文档化，分析清楚。在选用新器件的话，对应硬件工程师的工作量还是比较大的。但是如果是其他公司，直接按照推荐电路设计就完事了。电源专题，需要分析电源需求，每种电源的电压范围，电流需求，动态响应，上电时序；时钟专题，针对每个时钟的输入的电平标准，频率，抖动等参数，时钟时序，并按照各种时钟解决方案进行优化； 
第二、 当电路设计过程中，碰到一些新的问题，之前团队中没有接触过的问题，或者认为是重点，难点的内容，会专门做这个问题点的专题分析：例如我们做过的一些双BIOS启动，摄像头的红外LED的驱动，主备倒换啊，之类的，就会把一个问题点分析透，然后再动手做画原理图。 
第三、 那么在开发硬件的时候，Demo只是作为参考，每一个依据都是来自于datasheet，除了看芯片的数据手册之外，还要仔细查看数据手册的勘误表errata，核对datasheet与Demo的差一点，如果器件有checklist还得核对checklist。曾经开发AMD的时候，datasheet、Demo、checklist，三个文档对不上的情况。也出现过，一个比较难复现的问题，后来查看了Errata，发现是厂家芯片升级了，修正了bug，而我们还在采购老版本的芯片。 
第四、 由于项目本身有交付时间要求，那么在有限时间内其实不可能做到每个问题点都做得深入透彻。那么问题来了： 
是怎么做到的呢？首先，每个项目都有《问题跟踪表》，而硬件团队由于事情非常的杂，所以把这个表要用的非常好，不然丢东拉西很正常。我曾经把这个表应用到家里装修。这个表的原理很简单，就是记录，问题内容，责任人，完成状态，完成时间。但是只要你坚持用，你会发现，你问题不会跟踪丢，做事情会比较有条理，而且会有成就感。用了这个表以后，发现问题之后，先记录下来，即使现在不解决，那么也会识别他要不要解决，什么时候解决。其次、问题分优先级，任何项目都是带着风险前进的，那么识别出高风险的问题，优先解决高风险的问题，带着低风险的问题继续走。这也是华为电路设计中“0欧姆”电阻用的比较多的有一个原因，识别出风险之后，但是又分析不清楚，或者来不及分析，只好做兼容设计。这里不得不感慨一句，在你的设计过程中，你马虎对待，没有分析清楚的问题，最后一定会暴露出来。 
所以，在“菊花厂”做硬件工程师，“专题分析”是设计硬件最核心的工作，而不是画原理图。 
通过这个方法，用12个月做电路分析，而用12周时间画原理图，取代了，画图，调试，改版，再调试，在改版的形式。 
多快好省，是不可能同时实现的，那么硬件工程师有责任做很好的折衷和权衡。 
6.器件选型 
关于“器件选型规范” 
在进入华为的时候，当时整个公司都在“规范”运动，什么都写规范，人人都写规范，什么任职、绩效、技术等级都看规范。（大公司用KPI来引导，容易搞成“运动”）。 
所以当时，按照器件种类，很多人写了各种器件选型规范。当时，原理图评审的时候，听得最多的就是“规范就是这样写的”，这里面有一些问题： 
1、写规范的人不一定水平高，或者写得不细致，如果出现错误那就更是害人了。 
2、规范有时抑制了开发人的思维，什么都按照规范来，不一定适合实际的设计场景；例如我需要低成本设计，但是规范强调的是高质量，就不一定适用。 
3、有了规范之后，也会导致部分开发人员不思考，例如晶振要求在50MHz以上，放pF级的电容进行电源滤波，而低于50MHz的不用。大家都不想为什么，自然也不知道为什么；再例如网口变压器防护，室内室外，按照各种EMC标准的设计要求，直接照着画就可以；但是很少有人想为什么，也不知道测试的结果怎样，等实际碰到困难时就抓瞎了。的确在有的时候提高了工作效率和产品质量，但是工具也发达，人也就越退化，这是必然。 
4、有些器件的选型，不适合写规范，因为器件发展太快，有可能等你规范写好，器件都淘汰了。例如：在X86处理器进入通信领域了之后，处理器选型规范就显得多余。 
规范确实能带来好处。但是，并不是所有工作都适合用规范来约束。硬件工程师要能跳出“参考电路”、跳出“规范”，从原理思考问题和设计。 
当然规范还是非常有用的一个手段，是大量的理论分析+经验积累+实践数据的精华。我觉得当时我看得最多的规范，是《器件选型的降额规范》，这是基于大量试验，实际案例，总结出来的器件选型的时候，需要考虑的内容。 
例如：规定选用铝电解电容的时候，需要考虑稳态的工作电压低于额定耐压90%；而钽电容，稳态的降额要求在50%；而陶瓷电容，稳态的降额要求在85%；因为这里考虑了一些器件的实效模式、最恶劣环境（高温、低温、最大功耗），稳态功率和瞬态功率的差异……等等因素。 
器件选型需要考虑的因素 
在华为的PDM系统上，器件都有一个优选等级“优选”“非优选”“禁选”“终端专用”等几个等级。 
工程师可以根据这个优选等级来直观的感受到器件是否优选。 
那么器件的优选等级，是考虑了哪些因素呢？ 
1.可供应性 
特别是华为这样厂家，有大量发货的产品。慎选生命周期处于衰落的器件，禁止选用停产的器件。我2005年时曾设计过一个电路，设计的时候就是拷贝别人的电路，结果加工的时候发现器件根本买不着，由于器件停产了，只能在电子市场买翻新的器件。 
对于关键器件，至少有两个品牌的型号可以互相替代，有的还要考虑方案级替代。这点很重要，如果是独家供货的产品，是需要层层汇报，决策，评估风险的。 
2.可靠性 
散热：功率器件优先选用RjA热阻小,Tj结温更大的封装型号；处理器选型，在性能满足的情况下，尽量选择功耗更小的器件。但是如果是Intel这样垄断的器件，你也只有忍受，加散热器，加风扇。 
ESD：所选元器件抗静电能力至少达到250V。对于特殊的器件如：射频器件，抗ESD能力至少100V，并要求设计做防静电措施。（注：华为是严格要求，禁止裸手拿板的。我本来也不理解，后来我带团队之后，发现兄弟们花大量的时间在维修单板；我们的团队就非常严格要求这一点，看似降低效率，其实还是提高效率的。至少不用总怀疑器件被静电打坏了。） 
所选元器件考虑更高的湿敏等级。 
安全：使用的材料要求满足抗静电、阻燃、防锈蚀、抗氧化以及安规等要求。 
失效率：避免失效率高的器件，例如标贴的拨码开关。尽量不要选择裸Die的器件，容易开裂。不要选择玻璃封装的器件。大封装的陶瓷电容不要选择。 
失效模式：需要考虑一些器件的失效模式是，开路还是断路，会造成什么后果，都需要评估。这也是钽电容慎选的一个重要原因。 
3.可生产性 
不选用封装尺寸小于0402的器件。尽量选择表贴器件，只做一次回流焊，就完成焊接，不需要进行波峰焊。部分插件器件不可避免选用的话，需要考虑，能否采用通孔回流焊的工艺完成焊接。减少焊接的工序和成本。 
4.环保 
由于华为大量的产品是发往欧洲的，所以环保的要求也比较严格。由于欧盟提出无铅化要求，曾经整个公司的几乎所有的硬件工程师都在做无铅化的整改。 
5.考虑归一化 
例如某产品已经选用了这个器件，并且在大量出货的时候，往往有时这个器件的选型并不是很适合，也会选择，因为不但可以通过数量的增多来重新谈成本，还可以放心的选用，因为经过了大批量的验证。这也是为什么倾向于选用成熟期的器件，而慎选导入期和衰落期的原因。 
6.行业管 
某一个大类，例如：电源、时钟、处理器、内存、Flash等等都是有专门的人做整个公司的使用的规划和协调，提前进行市场调研，分析，编写规范。他们会参与到新器件的选型上来。 
7.器件部门 
专门有器件部门的同事，会分析器件的失效原因，可靠性分析，拍摄器件的X光，评估器件寿命等等工作。 
8.成本 
如果在上述因素都不是致命的情况下——上述的因素都是浮云，紧盯第八条。 
百度文库上面有很多文档关于《电子元器件选型规范》，有兴趣的同学可以看一下. 
7.白板讲解 
团队开发文化，那是华为中央硬件部的老大最自鸣得意的管理方法。团队开发文化，在多人协作的开发项目中还是非常有效的管理方法。 个人觉得“白板讲解”是团队开发文化中最精华的内容。 
把一个电路原理讲清楚，一般是在其他企业或者开发团队中，很少做的事情。但是有一个原则，如果你不能够把道理讲清楚，那么你一定自己没有搞清楚，或者没有理解到位，或者，其中，一定有什么内容是你忽略的内容。那么最后一定，出问题的地方就在这。也学这有点墨菲定律的意思。但是，讲清楚，一定可以帮助你成长。如果你掌握了某一个知识点，拿出来讲解给大家听，那么你一定会是掌握的最清楚的那一个。 
白板讲解的好处之一：深刻理解细节，当多人讨论的时候一定把原理讨论得更透彻，一是确保设计是正确的，同时也保证达到整个团队的最高水平。 
我在10年的时候，因为那时候项目停滞，我就专门把开关电源那个部分的每个细节，都拿出来讲解，一共讲了10次左右，后来把Buck电路的每个细节都讲一遍之后，我觉得对开关电源的原理才有了稍微透彻一点的理解。然后再把10次讲解的内容整理出来，就成了一个《单板电源是怎样炼成的》的教材。同时增加了电源调试经验丰富的老魏同学的经典案例，组成一个比较完整的电源教材，在公司内部广为传播。 
白板讲解的好处之二:很多很多的讲解，组成一次培训，很多的培训就是一套教材。整个团队讲解越多，技术积累就越深厚。 
曾经有一段时间搞PCI协议的逻辑，同时也有另外一个同事同时在看。我由于已经上手开始调试了，同时也做了各种仿真，所以对整个协议的理解还是比较清楚的。而另外一个同事的主要手段就是看代码，和协议原文，所以他并不是理解代码写的原因（因为逻辑写作的时候，有一些技巧性的内容在里面，例如：如何利用基地址寄存器，确定存储空间的大小）。 
当然，他开始讲解的时候，我就没有作声，因为当时我们都是新员工，主管都看着，别人组织的讲解，也不好抢了别人风头。后来由于他讲的内容有太多的错误，我实在看不下去，就指出他的错误。他当然不服气，表示他是正确的。 
但是事后他又向大家表示，他原先的理解是错误的。 
这件事情之后，我的项目经理（PM），跟我说：白板讲解，最厉害的地方其实不在于大家把问题搞清楚。而在于，“白板讲解”是一场比武，它能让团队里面的每个人做技术攀比，促进大家不断的提高技术。同时，也是在主管面前，谁水平高水平低，一目了然。 
*白板讲解的好处之三：在团队内部是最有效的技术比试，是骡子是马拉出来溜溜，别整天文人相轻，考评时相互不服气。有本事的，没本事的，一拿出来讲，全部都清清楚楚。 
一个团队，甚至一个公司，一个国家，它的成功或者失败都是由这个国家的绩效考评体系，人才选拔体系决定。白板讲解给团队的技术排名提供了最有利的数据支撑。 
研发团队大都气氛沉闷，状态不好的时候疲疲沓沓，开发周期拖延，效率不高，好像这是绝大多数企业的研发现状。 
为什么这样呢？因为相互之间不交流，人是社会人。整天埋头写代码的团队，肯定是问题很大的团队。如果坐在一起，面对面，或者背对背，都需要qq、或者espace这样交流，一天一个团队不说一句话。那自然大家人情冷漠。 
虽说白板讲解是技术比试，但是大家都心态open的话，其实这样的比试也是相互促进感情的一个重要手段。 
白板讲解的好处之四：有效改善组织气氛的重要方法，增进团队成员之间的技术认可度，只有愿意表达自己观点的团队才是有战斗力的团队。 
我现在自己创业，其实发现华为的那一套，讲解，培训，例会，跟踪，其实还是最有效的。 
毕竟华为是根据中国人的特点，长时间，多人，多团队，多项目实践出来的非常成熟的一套研发管理办法。自然华为的办法适合大公司，也有其一定的自身问题，但是，再没有更好的办法之前，这些手段不失为很好的方法。特别是白板讲解，去美国硅谷一些大公司、小公司看过的话，一定发现这些公司的工程师办公桌旁边都放着一块白板。只要一讨论问题，就是“来画一下”。 
白板讲解的好处之五：白板讲解的重要特点就是“用白板”，用白板的好处，就是避免口头表达的传达一次的误差；把讲的内容一条条记录下来，便于梳理思路；通过大面积的白板展示要讨论的内容，便于更多人都参与到讨论中来。 
另外我对白板讲解还有自己的几条建议： 
 当你的团队还没有白板讲解的，你可以勤于找别人讨论问题，达到白板讲解的效果。如果你带团队，还没形成白板讲解的氛围。可以先僵化，再优化。先强制大家养成习惯，体会到其中的好处，再让大家自发自动的进行讲解。在华为的朋友（或者其他大公司的朋友），如果是技术屌丝，那还得多在主管面前多讲解；如果你已经有机会给领导汇报了，那还是多联系PPT。因为PPT的本质还是白板。当然讲的内容要是主管感兴趣的内容，认可的内容，“以客户为中心”——你懂的。一开始，你得克服自己的心理，有可能这个内容是你还不懂的，一定要敢于问，敢于讲。不能因为技术羞耻心阻碍自己的技术进步。一方面，多看资料，勤学习新内容，功夫要实在；另一方面，要勤讨论，只有讨论才能知道自己的技术不足，理解错误或者不到位的地方。跟不同的人交流多了，你就是这群人中，最懂的那一个了。另外还是多利用互联网，多在QQ群，论坛里面问问题。也许有人嘲笑你，说这是低级问题，可是你问多了，自然就进步了，因为每个人都是从低级来的。在华为，有主管强制每一个项目组成员，是不是讲解了。在其他公司可能没有这样的环境和氛围，就靠你自己勤于讨论。
我计划我的孩子以后上学了，他到学校学到的东西，都讲给我听，这样才保证他是理解了。 
白板讲解，看似简单，其实里面的哲学还是挺深刻的，看各位理解到什么程度了。 
8.问题攻关 
因为世界上没有完美的东西，所以就算再高的水平开发出来的产品也不可能像蒙娜丽莎一样完美无缺。所以不管大问题，还是小问题，都可能有问题。 
第一部分、网上问题造成的三种后果： 
网上事故网上问题单板返还 
1.网上事故 
最严重的当然是“网上事故”，网上事故一般是造成“安全事故”、“客户损失”、“客户投诉”。等等情况。 
最严重的网上问题，自然是“安全事故”，危机客户人身安全。 
例如曾经有一个海量级发货的设备，曾经因为修改背板时，动了一条电源线的走线。这个电源线，被修改后，隔着绿油与机框的金属件，碰在一起。由于绿油本身有一些绝缘的作用，所以在研发测试和生产测试的过程中并没有暴露这个问题。 
但是由于在运输过程中，震动等原因，造成绿油在此过程中被磨损。在客户出上电后，有的设备出现的了短路，发生了烧板的情况。 
 
液态光致阻焊剂（俗称绿油）是一种保护层，涂覆在印制电路板不需焊接的线路和基材上。目的是长期保护所形成的线路图形。 
这是非常严重的情况，如果着火，发生火灾，在运营商的机房，那是非常严重的事故。 
但是，这种问题发生的时候，已经各种机框和单板发往五大洲，上百个国家。去解救这个问题，付出了非常惨重的代价。 
网上事故的另外一种情况，是造成运营商的业务中断；按照话费一分钟0.6元计算，一个省的运营商的用户都是千万级，甚至亿级的。如果造成客户的一分钟的业务中断，带来的损失，如何计算？ 
正式由于这个原因，所以大多数运营商的设备，都有备份机制。例如核心侧设备的内部交换模块，一定是1+1冗余备份的；如果是DSP资源，一些信令处理单元一般都是N+1备份的。这样如果出现单点故障，既不影响用户业务，也不影响设备的容量规格。 
第三种情况，就是客户投诉。有可能虽然没有造成什么严重的后果，如果客户投诉了，这个问题也会比较严重。例如，新机框和新单板邮寄到运营商处。这是出现了，电路板插不进去的情况，自然客户会非常恼火，觉得非常影响公司的品牌形象。那这个事情就会非常大。或者很早以前，任老板在现场的时候，某四川移动的领导，说“你们的设备还不如大唐好看”。于是，结构部的人就倒霉了。 
2.网上问题 
如果网上出了问题，那么一定通过一些手段，例如原先设计好的一些“可维护性”、“可测试性”的软硬件设计，尽量的去定位问题。 
当然这些措施都不能影响客户的正常业务。 
另外，会有一些寄存器，或者一些日志，去查看设备异常的记录。还可以查看一些设备的“临终遗言”。临终遗言，会利用处理器复位前，向存储区域存储的关键信息，便于后续去发现和解决问题。 
3.单板返还 
一线交付的人员一般都会抱怨：“你们研发都是三招：复位，下电，换单板”。 
其实网上问题分析，如果已经用上这三招了，那说明这个问题已经比较严重了，说着基本上是硬件问题了。 
可是“单板返还率”是非常重要的KPI，决定着大家的“考评”。所以维护人员都希望单板不要返还，或者不要记入指标。如果真的硬件已经不能正常工作了，那么一定会操作这个单板返还到实验室，进行失效分析，找失效原因。 
以上不管是哪个级别的问题，哪怕是实验室发现的一些问题，都非常重视。因为如果任何一个问题，都可能造成不可预见的效果。所以对每个问题都刨根问题，分析彻底。 
另外就是在做一些试验（EMC、环境），或者在测试的过程中，发现和暴露的问题，都会当做网上问题一样重视，进行一些问题的攻关。为什么呢？ 
因为有一个理论，问题越早解决，所付出的代价越小。 
问题攻关的三个信条： 
凡是“实验室”问题，如果不解决的话，一定会在网上出现。凡是出现过的问题，一定可以被复现。凡是不能复现的问题，一定是没有找到复现的规律。 
案例1、当时有一款NetLogic的处理器（NetLogic的网络处理器来自RMI。RMI收购了处理器创业公司Sandcraft，它本身又被NetLogic购买。后来NetLogic被博通收购），出现了器件失效的情况，但是网上还没有出现类似的情况。 
但是，有没有找到规律，是如何让器件失效的。于是双方进入了扯皮阶段。但是通过X光照射，发现失效的器件是焊盘开裂。但是是什么让焊盘开裂呢？当时怀疑了应力，高低温。试了各种措施，但是始终没有答案。 
后来大家讨论和试验的过程中，就有同事发现，单纯的低温和高温，都不足以引起器件失效。但是当高低温经历次数过多之后，器件失效的概率明显提高。后来这个同事通过多次试验，反复地使用热风枪和液氮，加速器件的老化。就非常容易出现焊盘开裂的情况。 
当拿着这个结论再去找Netlogic时，对方只能投降，承认问题，同意修改器件的工艺。 
非常说明问题的两件事情： 
第一， 后来实验室出现故障的单板，基本都是厂家改进工艺前的问题。 
第二， 另一个发货量很大的产品，在2年后，网上出现大规模这个问题的单板。 
案例二、如果在试验中发现问题，一定会把问题分析清楚，或者把问题解决掉。也许这个问题解决很难，经历时间很长。但是这个问题一定把记录下来，根据优先级把问题最后解决掉。 
例如曾经一个同事在做试验的时候，发现三极管有漏电流。 
理论分析之后，由于三极管作为开关管使用，所以理论分析不可能产生这么大的电流，导致电压变化；把三极管更换成MOS管，也无济于事。 
由于这个漏电流是在低温的时候才会出现的。所以当时就用液氮，让三极管处于极其低温的状态（-10度以下），试验中温度情况也差不多在这个范围（-40度到0度）出现问题。 
但是经过两周的试验，都没有找到规律，偶尔会复现一下问题，完全没有规律。 
我跟那个同事觉得非常费解，当时就观察天气，觉得这个三极管的漏电流感觉与天气有关。如果阴天，就容易复现，如果晴天就完全不复现。 
通过这个规律，我们开始怀疑“湿度”作祟。 
后来，我们通过增加器件的湿度，果然非常容易复现问题。 
把我们的结论去找厂家，厂家确认SOT封装的器件，在高湿度低温的前提下确实会有漏电流的现象。这个漏电流不是通过PN节流走的，所以跟PN节的漏电流的规律完全不符合。 
而是从SOT32的塑料封装上漏走的电流。 
后来通过调整电路参数，规避了这个问题。 
所以整个分析和试验的过程，哪怕是极端的环境条件下的问题，也绝不放过。 
其实产品的问题攻关，就是这样的，扎扎实实的解决每一个问题之后，产品质量才有试制性的提升。 
形式： 
 攻关组：任何问题攻关，为了表示重视，一般都会成立个什么问题攻关组。就是把相关的人，还有有经验的人走组织起来，一起参与讨论，这样可以拓宽思路，同时丰富经验。避免钻牛角尖，或者无头苍蝇。例会：重大的问题攻关，一定是每天例会，把前期讨论的问题汇总跟踪，把每项措施对应的结论记录下来，明确下一步的措施。日报：这种问题攻关，一定是领导重视的，所以每天都会发布进展。当然领导也会看，偶尔也会发现很久没有进展，之后会调配资源，协调兵力。总结：问题解决之后，一定把中间的九九八十一难，整理成案例、培训，给大家分享。这样所有的同事，虽然没有亲身经历这个攻关过程。可以通过分享，学习相关专业知识，和问题解决的思路。得到提升。
问题攻关是痛苦的，问题突破了也是非常有成就感的，痛并快乐着。 
最后两句话： 
 越是不舒适区，其实就是你成长的机会。越是困难的时候，越是要咬牙顶住；只要你坚持，你离成功永远都只有一步之遥。
9.会议 
1.华为的会议的特点 
对“华为的会议”的特点的一些阐述。 
 首先大公司就是“会多”，因为公司大，部门多，人的职责划分的细，所以一件事情，需要很多人参与。容易出现扯皮的事情。我刚到华为时，非常不适应，什么都写文档，什么都评审，什么都开会；所以不适应这么多会议，开会时就会无聊，所有的贪食蛇的最高纪录都是那段时间破的。任何事情还是有主要负责人的，华为给予负责人足够的权利，所以能够推动事情的发展，协调到资源。例如行销有足够的强势去推动研发实现客户的需求。产品经理、客户经理的能量还是很大的，能够跟研发的部长直接进行对话，推动研发干这干那。所有问题最终都是会记录，跟踪，保证完成的。这就是为什么哪怕有些设备的质量，性能并不能让客户足够满意的时候，客户还愿意用华为的设备。就是这个原因，运营商都喜欢用华为的设备。一个问题出来了，还没确定是哪家的问题，华为的兄弟就冲上去了。联通2个人参加会议，华为6个人来参加会议，通过试验举证，证明是Juniper设备的问题。然后给出充分的报告告诉客户，这不是我们的问题，这是XXX厂商的问题。林子大了，什么鸟都会有。所以推、拖、赖的事情自然总是有发生。这就需要强大而明确的绩效评价体系，去引导员工去主动承担任务，而不是去划清界限。这种“划清责任”的事情也不可避免。否则就是三个和尚没水喝。（注：华为的这种凡事充分讨论的做法，在电信运营商的领域是适用的，放在消费者领域、甚至企业IT领域往往会不适用的，因为没有足够的利润率去支撑这么做。所以说的一些华为的一些优点）在开会的过程中，经常人们容易进入误区，或者过于发散，或者过于保守。在产品定义阶段的会议，往往都有人提醒，发散的时候不要收敛；在问题解决的会中，往往会提醒，不要过去发散，聚焦问题。这个能够提醒大家的人往往就非常重要。当然有时也会流于形式，各位朋友可以点击原文链接看案例《华为帮孙杨调整游泳姿势》，会议中不断有人提醒聚焦，但是大家还是比较发散。
2.《罗伯特议事法则》 
什么是《罗伯特议事法则》 
一百年前有个好小伙子,名叫享利.马丁.罗伯特,二十五岁,中国人叫愣头青。他毕业于西点军校在南北战争期间奉命主持一个地方教会的会议。结果呢——搞砸了。人们争个不亦乐乎，什么结论都没有。总之一塌糊涂。这个会开了比不开还要糟糕。这个小伙子呢，有点一根筋。说我要研究一下，弄个规则，否则我就再也不开会了。他研究上下几千年的开会讨论，有一个结论：人大概是特别爱争论的一个动物，最难被道理说服的动物，分歧一旦出现。很难在短时间内靠语言交流说服对方。否则吵个几天几夜都不会有结果。而且越吵越觉得自己有道理，对方是个笨蛋。所以双方找到共同点达成一个结论一定要有一个机制。他把这个研究当作一个战争一样。把人的争论本性当作敌人。最后这个小伙子打赢了。 
打赢的结果是1876年罗伯特议事规则。他自费出版买了一千本到处送人。1915愣头青罗伯特成了将军，他修订了这规则。一开始人家不重视，嘴上没毛说话不牢的小家伙行吗。唉，没想到，真行，他们一实行这个规则，吵架没了，会开下去了。墨水瓶，板凳也不乱飞了。结果罗伯特议事规则成了世界上最通行的议事规则。 
会议常见问题。 
 跑题：一开头，我给你们讲个故事，这一讲，就讲到中饭了。一言堂：这一个一言堂呢，是领导者爱讲话，一讲就全他讲了。第二个呢，农村有一些特别爱讲话的。也有从来不讲话的。爱讲话的只要一讲话，基本就是他再讲，不会给别人讲话的机会。野蛮争论：一讨论问题，就说你上次多报了五元钱，你不是好孩子，怀疑别人的品德。一百句话中抓住人家一个词不放。甚至打起来。会议就没法子开了。打断：不得打断别人的正当发言。
 
罗伯特议事法则的一条就是：主持人来解决以上问题。 
但是一般的企业往往，领导出现的时候，主持人是不会去提醒领导，“你跑题了”，“你一言堂了”，“你不应该打断别人的正常发言”，这就是国外的科学的一些理论和方法到了中国往往不适应中国的土壤，不能生搬硬套的典型案例。 
其实在华为，已经能够在大多数会议中，做到发生“跑题、一言堂、打断、不文明”时，有主持人去提醒，并拉回到正轨上。但是一些会议也做不到，比如：领导比较强势，领导自己是主持人，主持人是个马屁精，一些政治敏感问题，就不能去破坏和谐。此处不展开细说。 
那么华为是怎么去解决这些问题的呢？ 
 公司定了大的基调“以客户为中心”，所以领导再大，大不过客户，客户需求一律允诺，一律搞定。所以大家都是为了搞定客户，当大家在原则性的问题上不会有大的分歧。绩效导向，一切是按照结果去评价绩效的。所以在一些问题上，如果领导提出了某个方案，但是可能存在重大隐患时，底下人是有责任去提醒和反对的。否则造成重大严重后果后，领导跑不掉，一样会修理底下的人。都是拴在一条绳子上的蚂蚱。当某个同事提出跟领导不同的意见时，并有价值时，会从绩效结果上去认可这个兄弟。这就是教育员工，鼓励提出反对意见，鼓励纠正领导的错误。教育主管。华为提倡狼文化，所有的主管能够被提拔上去，一般都是狼性十足，能讲会说，精力旺盛，在开会时balabala一顿，与员工沟通时也是balabala一顿自己说得爽。那么就会容易造成一言堂，或者跑题。那么在主管培训的时候，都会教育带团队的人，要会倾听，会交流，沟通时要把握节奏和分寸。
3.减少无效会议 
我曾经支持过CCB的网络建设一段时间，当时刚去的时候，跟他们的IT规划部，开了一个会。 
当时，开会时就是典型的“一言堂”，他们一个领导过来，一顿狂骂：“你们华为的设备怎么怎么不行，你们思科的设备也是狗屎，你们西门子服务太差。。。。。。”，建行的人，还有设备厂商的人都被骂蒙了，就听他一顿牢骚，骂完设备厂商，开始骂自己的员工“balabala”。然后所有人都不知道这哥们想干嘛，这哥们也讲不出自己想要什么样的设备，性能和服务。然后气愤愤就走了。 
一言堂、跑题、不文明，这些都不是致命的，最致命的就是“无效会议”。当这位领导走了之后，大家继续按照自己的思路，方法，继续讨论，然后花2分钟讨论一下，怎么应付这位领导。 
所以我们开会时需要的，但是如何开的有效是有套路的。 
那么如何做到呢？ 
 例行会议，有议题。例如周会，一周例会的议题做事先的安排，不是很随意的说一下。订好议题，订好每个议题的时间，保证不跑题。会议要有纪要，每次开会的会议主持人，会议纪要人都明确。会议纪要是很重要的一件事情，也需要很高的技巧，即需要有效参与会议讨论，有需要记录下关键要点，不记流水账。会议纪要，要分为：结论（会议结论不随意更改）；遗留问题（要符合SMART原则），要有责任人，要求完成的时间等等。纪要有模板，提醒大家纪要要符合SMART原则。勤跟踪，要闭环。所有的遗留问题，在下次会议的时候都会回顾，看看是不是完成了，有没有拖延，直到有个交代。当然，如果返现任务安排有问题，根据评估也会进行问题的关闭和挂起。所有的决议都是需要有理有据的，不能是拍脑袋。因为事前拍脑袋，事后就会拍大腿。然后就有人拍屁股走人了。这样就不会决议是下级服从上级，少数服从多数。
当然，这样的话就会存在效率问题，因为有些问题就会因为短时间研究不清楚，决策不下来。这是就有了CCB(这个CCB不是建设银行的意思，CCB(Change Control Board) 在CMMI(Capability Maturity Model Integration)中，是“变更控制委员会”的含义，CCB可以由一个小组担任，也可以由多个不同的组担任，负责做出决定究竟将哪些已建议需求变更或新产品特性付诸应用。典型的变更控制委员会会同样决定在哪一些版本中纠正哪些错误。CCB是系统集成项目的所有者权益代表，负载裁定接受那些变更。CCB由项目所涉及的多方成员共同组成，通常包括用户和实施方的决策人员。CCB是决策机构，不是作业机构，通常CCB的工作是通过评审手段来决定项目是否能变更，但不提出变更方案。至少会保证，决策的决议是集体的智慧。) 
10.兄弟文化 
无兄弟，不研发——共同奋斗的快乐 
2014年11月份，我从南京招来了一批实习生，全部是90后，其中有些朋友家境也还不错。这段时间，我跟他们一起赶进度的时间，基本上没有周末，基本上每天晚上都很晚。我原以为小伙子们会抱怨太辛苦，但是今年的年度总结会上面，看到小伙子们反馈的都是“收获”、“进步”，是我有史以来最具“正能量”的年度总结会。 
大家之所以这么拼，是为什么呢？因为兄弟们一起奋斗，在奋斗的路上有人陪伴不孤单，大家相互之间配合顺畅，相互鼓励，相互帮助，相互激励，同时，也相互竞赛。 
我曾经在一家研究所工作的时候，虽然在一家国有企业工作，工作非常稳定，收入非常稳定，但是我当时还是挺拼的，有时搞得也很晚。虽然也得到了组织和领导的认可，最后我还是选择了离开。 
“干好干差，收入差异不大”，这是我离开的一个原因。我离开的另一个重要原因就是——孤独，当你工作到很晚，看到别人都早早回家，或者去玩了，其实你在坚持的路上，走不了多久。独自努力坚持的时间不是靠毅力，因为这就是人性。 
当你在夜里搞定某个问题时，你最大的快乐，其实就是有人可以告诉。当你发现，只有你一个人在乎结果，或者别人并不在乎你是否在乎结果的时候，你还是会沮丧的。 
在华为的时候，如果有兄弟还在加班，会要求主管不要提前走。因为奋斗的路上，不能孤独。主管留下，一定不是简单的陪伴，而是要在具体的问题上给予帮助，或者看一下问题是否需要加班，是否在做正确的事情，是否正确的做事（在方向、和方法上给予指导和帮助），避免做无用功。同时让愿意付出努力的兄弟看到，有人支持你，关注你。 
一个好的主管，一定不能在你回家睡觉去了，而你的兄弟在做无意义的努力，这是最伤士气的。 
当然，我不是倡导加班，是倡导把事情做好，按时间把事情做；倡导大家共同努力把事情做好，而不是一部分人努力贡献，一部分人混日子，拿着相同的收入，甚至倒挂。 
曾经我华为的主管跟我说过：“任正非认为自己做得最自我认可的一件事，就是把钱分好了，分的公平”。所以，现在我们轻松行的责任也是要把钱分好，分的公平。 
团队作战，人尽其用 
华为有句话是：“知识密集型企业”。由于文革影响，导致中国40岁~60岁人才匮乏，工程技术没有足够的积累；由于大多数大学生在大学期间没有找到自己的人生方向，迷失在星际争霸、魔兽争霸、传奇、英雄联盟这些游戏之中，中国的工程师的技术能力相对美国的工程师，往往会觉得起点低，起步晚，工程知识积累不足。所以我们的单兵作战能力其实远远不及硅谷的工程师。那么我们可以通过拆解，可以把一个相对复杂的工程，分配给多个工程师协同作战，所以“知识密集”这个词，类比于“劳动密集”应运而生。 
相互之间的配合能力，项目管理的分配，就显得尤为重要。但是再好的分配和管理，都不可能完美，每个人的责任都不可能划分的那么精确。这就需要兄弟文化，每个人在团队中都找到自己对应的位置和责任，当责任不清晰时，能主动分担，主动担当。如何能让团队中的成员做到这点呢？绩效导向就显得尤为重要。（这个问题最后又归结到把钱分好） 
“败则拼死相救，胜则举杯相庆”， 有了这样的兄弟情怀，才能有最终的项目成功。 
管理，重要的是理，然后才是管 
有的项目管理者，抱怨“管不动，或者不好管”。其实原因在于你是否深入到团队中去。 
有一些华为很高层的领导，可以给一些项目具体的指导，而且意见高明，所以才能让下属信服。“宰相必起于州府，猛将必发于卒伍。”，这也是有些外企到了中国，仍然玩职业经理人这套，往往行不通。 
作为一个好的主管，不是只是去要求进度、质量的结果，必定是能够帮助你的下属达成你下达的目标，这样你的下属才能信服你，愿意跟着你干。 
不要只是下达“攻下那个山头”的命令，而是下达命令之后，能够帮助你的下属，分析敌情、提供情报、指导战略，最终达成“攻下那个山头”目标任务，然后给予奖赏。 
如此成功几次之后，你的下属已经具备攻下山头的能力之后，你可以放心交给他一些任务之后，再帮助他挑战更高的目标；当他能够挑战更高的目标之后，他自身也就具备了更高的价值，其实这个过程中他会感激你，信服你，愿意跟着你干。所以兄弟文化，不只是吃几顿饭，喝几顿酒（吃饭喝酒自然还是必须的），而是切实的工作上的帮助和关怀。 
所以帮你的兄弟“理”清楚了，自然也就不需要“管”了。如此长久之后，大家之间就是兄弟情怀，而不是劳资矛盾了。 
做刘邦，不要做项羽 
无论你自身有多强，你都需要大家一起作战。 
任正非说过：“我一不懂技术、二不懂外语、三不懂法律”，但是人家做到了世界500强，一定不只是靠他自己一个人能力强，靠的是一个精英的智囊团队，靠的是这个智囊团齐心合力，大家共同的理想和目标，共同的核心价值观，靠的是向心力，靠的是一个又一个的成功团队的复制。 
君子性非异也，善假于物也。 
刘邦之所以战胜项羽，就是因为他有兄弟们。 
11.测试 
从进度的角度对比华为和小米的测试 
 
上图是小米UI的一周进度图。按照小米UI每周发布的进度，周四一天的内测。我按照华为的流程怎么套都套不出来。 
疑惑点在于： 
内测是指开发人员自测试，还是测试人员的测试？如果是指开发人员自测试，那么测试人员在哪里测试？如果是测试人员测试，那么开发人员的自测试呢？开发转测试的点在哪里？ 
华为背景的朋友一定会问：测试人员怎么可能用一天的时间完成测试？ 
也许有人说，小米的效率就是高。那么我们来看一下华为的测试流程，你就知道是否可以压缩到一天完成相关的测试。 
首先说明一点，华为的软件部门，包括UI、或者网站的开发团队也是按照小步迭代进行开发的，在产品稳定后，新增需求会拆分成细小的版本，进行最短周期的开发测试。也可能华为的拆解需求的能力弱于小米，但是这里我们单纯谈测试流程。 
华为测试经历的阶段 
测试是产品开发过程中必不少的环节,在华为的研发人员中,有近三分之一的人员是测试人员,华为的测试体系在国内算是起步较早,大概经历了这样几个阶段: 
 青铜器时代: 手工作坊式测试 1996年研发测试团队成立 手工作坊方式的研发过程和测试铁器时代：IPD和CMM阶段 1998年华为与IBM合作，开始引进IPD流程 1999年左右引入CMM理念 产生IPD-CMMI流程
 
 火器时代：PTM阶段 2004年在IPD基础上开发PTM流程，自动化测试规模开展 2006~2007年左右PTM趋于完善
 
 集团军时代：IPD-RD-I&V阶段 2008年左右开始推广敏捷，研发组织演变为PDU方式 引进迭代开发模式，形成IPD-RD-I&V流程 系统集成与验证流程：IPD-RD-I&V (I&V：Integrationand Verification)
 
项目经理编写《项目计划》，开发人员产出《SRS》，这时测试组长就要根据SOW开始编写《测试计划》，其中包括人员，软件硬件资源，测试点，集成顺序，进度安排和风险识别等内容。 项目管理论坛 
《测试计划》编写完成后需要进行评审，参与人员有项目经理，测试经理和系统工程师，测试组长需要根据评审意见修改《测试计划》，并上传到VSS上，由配置管理员管理。 项目管理者联盟 
项目管理者联盟. 
待开发人员把《SRS》归纳好并打了基线，测试组长开始组织测试成员编写《测试方案》，测试方案要求根据《SRS》上的每个需求点设计出包括需求点简介，测试思路和详细测试方法三部分的方案。《测试方案》编写完成后也需要进行评审，评审人员包括项目经理，开发人员，测试经理，测试组长，测试成员和系统工程师，返回评审结果。测试组长组织测试成员修改测试方案，直到评审通过后才进入下个阶段――编写测试用例。 
测试用例是根据《测试方案》来编写的，通过《测试方案》阶段，测试人员对整个系统需求有了详细的理解。这时开始编写用例才能保证用例的可执行和对需求的覆盖。测试用例需要包括测试项，用例级别，预置条件，操作步骤和预期结果。其中操作步骤和预期结果需要编写详细和明确。测试用例应该覆盖测试方案，而测试方案又覆盖了测试需求点，这样才能保证客户需求不遗漏。同样，测试用例也需要通过开发人员，测试人员，系统工程师的评审，测试组长也需要组织测试人员对测试用例进行修改，直到评审通过。 
在我们编写测试用例的阶段，开发人员基本完成代码的编写，同时完成单元测试。转测试部后直接进行系统测试。测试部对刚转过来的测试版本进行预测试，如果软件未实现CheckList清单上的10％，测试部会把该版本打回。否则，软件转测试部进行系统测试。根据《测试计划》进度安排，测试组长进行多轮次的测试，每轮测试完成后测试组长需要编写测试报告，其中包括用例执行通过情况，缺陷分布情况，缺陷产生原因，测试中的风险等等，这时测试人员就修改增加测试用例。待到开发修改完bug并转来新的测试版本，测试部开始进行第二轮的系统测试，首先回归完问题单，再继续进行测试，编写第二轮的测试报告，如此循环下去，直到系统测试结束。在系统测试期间，测试人员还需要编写验收手册，验收用例和资料测试用例等。 
修改问题单，直到满足规定的缺陷密度，才能够通过相关TR点。 
如果验收发现的缺陷率在SOW规定的范围内，那么验收成功。如果超过规定的缺陷率，需要质量回溯。 
SRS：需求分析文档； 
HLD：概要设计文档； 
LLD：详细设计文档； 
1.单元测试(UT) 
单元测试的对象是LLD中所划分定义的程序单元或模块，它也是单元测试用例设计中可测试的最大单元。该测试对象可能由一个或多个函数或者类组成，测试设计就是对测试对象进行测试用例设计。 
UT的目的，是通过函数运行来检查模块代码对于LLD文档的顺从性，验证每个函数的输入输出响应，与它在详细设计文档中预先定义的是否一致。函数是产品开发实现的最基本单位，下一个实现单位是模块，从测试的角度看，希望UT完成后，每个函数都牢固可靠，下一步的IT测试将聚焦在函数之间配合能否实现分配需求，而不用担心函数本身的输入输出响应问题。 
单元测试比较适合开发人员做。 
2.集成测试(IT) 
集成测试是指把若干个经过单元测试的单元组装到一起而进行的测试，集成测试应依据HLD，主要发现接口、依赖中的错误或不完善的地方。集成测试的对象为若干个单元测试对象的组合，至少为两个。 
IT的目的，是根据模块设计对模块的分解，从已验证的函数开始，逐层向上集成，得到一个可运行的模块。 
IT可以由开发人员做，也可以由测试人员做。 
不难看出，UT是面向每一个单元的测试，IT是测试单元之间的接口，可以把UT/IT归为“单元级”测试。 
3.系统测试(ST) 
CMM定义的系统测试：系统测试是针对软件项目组所承担开发的软件系统进行的整体测试，将软件系统作为整体运行或实施明确定义的软件行为子集的测试。主要采用的测试方法是黑盒测试，即不管程序内部的实现逻辑，以检验输入输出信息是否符合规格说明书中有关需求规定的测试方法。可见ST的测试对象是规格说明书，更确切的说，是模块需求规格说明书，所以一般也称为MST。模块SRS文档给出了模块的输入输出的相应要求。MST后，每个模块是牢固可用的。 
4.模块间接口测试(BBIT) 
BBIT为模块间接口测试，验证模块之间的接口能不能配合，有时和联调混在一起，其实目的并不相同。BBIT的目的，是根据系统设计对系统的分解，从已通过验证的模块开始，逐层向上集成，得到一个可运行的系统。而联调一般涉及软件、硬件或者不同产品间的配合测试。MST和BBIT可以归到“模块级” 的测试，一个验证模块，一个验证模块间的接口。 
以上UT/IT/MST/BBIT一般由开发人员完成，系统基本可以运行起来了，测试人员可以开展SDV、SIT、SVT了。 
5.系统设计验证(SDV) 
SDV虽然属于测试人员开展的系统测试，但是有点偏灰盒测试，因为SDV验证各子系统的配合是否满足设计需求（DR），对内部的实现还是关注的，验证多个模块集成以后是否满足设计需求。 
6.系统集成测试(SIT) 
SIT也是验证设计需求是否得以满足，与SDV不同的是，SIT完全把系统当作一个黑盒来测试，不关心内部具体的实现。实际应用中，SDV和SIT 虽然都属于系统一级的测试，往往由不同项目组（子系统）的测试人员分别测试，他们只关注各自的子系统，所以还是把SDV和SIT归为“子系统级”的测试比较好。 
7.系统确认测试(SVT) 
SVT是验收测试，其测试对象是产品包需求OR。产品包需求给出了产品的范围，从产品可能的应用环境的角度刻画系统，SVT的目的就是确认（或验收）产品包需求给出的各种应用场景产品均能满足。 
即使是网页开发项目，外包项目，终端的项目，华为的测试仍然会经历以下几个测试阶段： 
SIV：System Integration Verify 系统集成验证. 
SDV：System design Verify 系统设计验证. 
SIT：System Integration Test 系统集成测试. 
SVT：System Verification Test 系统确认测试（系统模拟测试）. 
迭代结束后，在正式对外发布前，会将历次迭代实现的所有Story再做一次测试，测试 的主体在测试人员，包括功能、非功能，并要给出测试报告。这个活动就称为SIT或发布测试。 
如果Story 测试、迭代SDV测试都自动化了，则本次测试主要是执行自动化用例、如前 面有测试不充分，则补充测试，以及详细性能测试。如果用例自动化程度不高，则本次测试会 刷选部分用来进行测试。测试结束后需要给出测试报告。 SIT测试重点： 所有迭代开发完成后，由迭代开发团队中的测试人员完成对全系统进行回归测试，达到TR4A的质量标准。遗留问题要满足TR5的DI（缺陷密度）目标。 
不可思议的小米5% 
雷军说： 
在改革开放之后，强大的硬件的生产、制造、设计、研发的能力给了中国公司巨大的机会。再加上全流程优化,在小米过去的创业时间里,研发、制造、维修、服务、市场、渠道,全部加起来只占了小米营业额的5%，或者商品零售价的5%。 
首先华为工程师的平均收入应该是低于小米的，器件采购的成本也应该是低于小米的，制造的产线也应该是低于小米的，因为有自己的产线。 
那么华为的成本花到哪里去了呢？ 
那么我们看华为的硬件测试过程，就知道成本出在哪里了。 
1. 全程测试参与的流程： 
 
在项目开始的阶段，测试人员就开始参与需求分析，对产品的可测试性、测试方案等等因素进行评估。 
2. 多层级的测试与试验. 
对于电路的设计，会进行单元测试、整机测试、小批量试制、HALT试验、环境试验、EMC试验、热测试、进入生产环节之后会进行HASS试验。特殊的设备还会进行盐雾试验、硫化试验。整机结构还会进行：跌落试验、挤压、扭曲等等。 
HALT（Highly accelerated life test）高加速寿命试验。HALT是一种发现缺陷的工序，它通过设置逐级递增的加严的环境应力，来加速暴露试验样品的缺陷和薄弱点，而后对暴露的缺陷和故障从设计、工艺和用料等诸方面进行分析和改进，从而达到提升可靠性的目的，最大的特点是设置高于样品设计运行限的环境应力，从而使暴露故障的时间大大短于正常可靠性应力条件下的所需时间。 
环境试验是为了保证产品在规定的寿命期间，在预期的使用，运输或贮存的所有环境下，保持功能可靠性而进行的活动。是将产品暴露在自然的或人工的环境条件下经受其作用，以评价产品在实际使用，运输和贮存的环境条件下的性能，并分析研究环境因素的影响程度及其作用机理。 
HASS应用于产品的生产阶段，以确保所有在HALT中找到的改进措施能够得已实施。HASS还能够确保不会由于生产工艺和元器件的改动而引入新的缺陷。 
硬件工程师最怕HALT试验，因为会超越器件的限制范围去进行测试。但是为什么要这么做呢，其实是找到整个设备的最薄弱点，然后对最薄弱点进行改进。但是由于超出了器件的允许的工作范围，异常的情况特别多，原因也复杂。但是按照规范必须分析清楚，并给出优化措施。这是非常烧脑的意见事情，很多经典的问题都是HALT试验过程中产生的。 
12. 硬件的知识管理与分类 
电子硬件知识是极其庞杂的，每一个细分领域都可以钻研很深，可以成为某一个人的一辈子的工作，例如EMC工程师，互连工程师，电源工程师，可编程逻辑工程师……电源工程师又可以细分：一次电源、二次电源……分别作为职业。 
在硬件领域，由于大量的知识是隐形知识，如果只知道书上写的那些东西，是不能成为一个合格的硬件工程师的。就是因为知识体系的庞大，加上隐形知识众多，所以硬件工程师的知识体系是庞大的。是需要管理的。 
曾经有一位网友利用电影在硬盘的管理，来阐述他知识管理的观点； 
观点：分类有重叠不如不分类。网友的观点：电影的分类，大家一开始都按照演员进行分类，比如成龙，甄子丹……；但是分类之后，发现多人的怎么办？如果有一部片是由多人合演的，如何分类呢？后来随着收藏的演员目录增加，只通过演员的名字进行分类，会很难找到你需要找到的内容，后来又在演员文件夹的外面增加一个公司：北京制片厂……；公司太多了，又想分类：动作冒险；…… 
这位网友的观点：“既然无法分类，那么就不要分类，我们依赖搜索。” 
知识多了还是要整理的，靠搜索不行； 
如果你收录到本地的一些知识，如果不整理的话，那么不能建立起你的知识体系，也就是说你对哪一块，需要重点掌握，哪块只需要了解，没有概念；而你常用的工具或知识跟你不常用的工具和知识，查找需要的时间是相同的；当你学习某一项知识时，这个知识点与其他的知识点之间是没有关联的，因为你没有把知识系列化，或者没有做整合。 
如果你的硬盘一样，经常看的影片，你应该需要有快速找到它的途径；而不常看的，你是不是只需要保留个种子；而从来不看的，或者只会看一遍的，变态的、难看的、不清晰的，你其实都可以删除。 
 
上图是我整理的硬件工程师所处的一个地位，同时也是一个硬件工程师所需要的知识领域； 
有句话说：“硬件就是连连线，软件就是敲敲键”。可就是连连线，所需要考虑的因素非常多，在大公司的话需要打交道的人也非常多。 
上图画了一个车的形状，硬件工程师是这个车的主体，电源、逻辑、互连是硬件工程师密不可分的组成部分，是硬件这辆车前进的必备条件（一些小项目，或者一些公司并不做这样拆分。） 
最下面一条是支撑合格车前进的道路，是支撑这辆车能够前进的必备条件。这些领域的知识是硬件工程师必备的，例如：生产跟线，器件sourcing、器件失效分析，这些事情即使是在人员拆分很细的大公司，硬件工程师仍然是这些事务的责任主体；而一些公司，没有相关领域的人，就是硬件工程师自己去做的。 
车头是系统设计和产品设计，这些引领硬件方向的思考也是硬件工程师必须要考虑的，因为你不理解业务和产品的场景，又如何选一款适合的处理器？如何知道内存需要多少带宽和速率？ 
以前我经常说，硬件工程师，如果一个装修工程的木工兼包工头，是整个装修的灵魂，决定了项目的水平高低。所以硬件工程在自身电子领域的知识积累之外，还需要积累一些其他领域的知识，所以这块外围的知识掌握的程度是需要跟核心知识体系进行区分的。因为一个人的时间和精力有限，不可能掌握所有的知识，但也不能完全不懂。 
知识管理不只是知识分类和整理； 
首先，知识管理不只是罗列目录，他必须是有内容的，也就是大家常常说的，必须是有“干货”；一个再完善和再完整的知识框架的目录，都是没有用的，因为你只需要找到你最需要的那部。如果文件夹都是空的，那就更没用了。 
 
而且我也觉得知识管理，必须是目标为导向。不能为了整理而整理。所以这个知识体系的目录，我倡导是先有问题点或者知识点，积累到一定数量之后，形成你最需要的一个知识体系。 
如同，你先有N部你经常观赏的电影，然后再进行分类整理。而不是你先去建立一堆空的文件夹。 
例如上图中，进入了一个学习，利用，创新，积累，共享的一个有效循环。 
那么，我说说《硬件十万个为什么》的由来，我当年刚进入华为时，在华为的导师是一个项目经理，也就是我主管的主管。他很忙，但是他也很有责任心。没太多时间辅导我，他想出了一个办法：让我每周给他提3个问题，他一定会安排在周五给我解答；我有时为了给他提问也比较费尽脑筋，因为不可能提一些太低级的问题；因为我提问也不是那么好回答，他也要花一些时间去学习，并仔细的答复。我觉得试用期间，这个“每周三问”对我深入掌握一些单点知识非常有效。等我带新员工的时候，正值华为大发展，我一个项目组里面17个人，有4个老员工，其他都是新员工，根本没法一对一辅导。我当时就用了“每周三问”这个办法，这个办法很有效： 
强迫新员工思考问题；新员工碰到障碍，有地方可以提问；面对新员工各式各样的问题，老员工为了回答也需要提高和学习。 
慢慢的，新员工成长起来了，问题也没有那么好回答；我就不再区分新员工和老员工，大家都可以问问题，轮流来回答问题，自然轮到回答问题那位同学非常的痛苦，工作上进行调整，预留一些时间。 
后来我发现：为什么不能把我们的问题都整理到一起？当我把每周的问题整理到一起17个人，一周3问题，总共是51个问题；我们坚持了半年（后来我调到市场部门去锻炼，所以后来没有坚持下来）；总共大约1000个问题积累下来，知识领域涵盖硬件的各种领域：电源、时钟、处理器、逻辑、电平标准、接口协议…… 
当我把问题分门别类整理出来时，反向生成目录，一个大惊喜：一个完整的硬件工程师的知识体系完整的呈现在我的面前。 
但是一个很有意思的情况，就是往往在华为内部的技术论坛搜索硬件问题，往往就会搜索到我们整理的这个文档，因为大家碰到的问题往往都是类似的。 
所以一句广告词说明了知识管理的真谛：“重要的不是拥有一切，而是需要的就在手边”。我们不只是需要一个完整的知识框架的概念，我们最需要能解决我们现实的问题。 
《硬件十万个为什么》虽然积累了相当多的问题，但是这些问题哪怕再全，仍然是碎片化的。它虽然能解决一些具体的问题，但是不能形成知识框架。 
经典教材和原理知识，需要系统化； 
还在华为时，我曾经尝试过，整理出《硬件的十八般武艺》，涵盖硬件工程师必备技能，分别由大家一起参与，完成基础知识的一些系统化的培训材料：电源、时钟、处理器、高速互连、分立器件、JTAG、内存………基础知识的系统化，完备的掌握，是必须的，否则你无法完成你的工作，更无法说出色完成工作了。 
如同AV有句名言：“为人不识武藤兰，看遍毛片也枉然”。所以一些基础知识是必备的，否则你连“骑兵”“步兵”的含义都不懂，又如何去准确的下片？培训和教材，要分“入门”和“提高”；要分别针对不同的层次的需求。"
美国政府敦促开发者：停止使用 C、C++,2024-03-01,"整理 | 屠敏 
 出品 | CSDN（ID：CSDNnews） 
 “C、C++ 不安全，新应用开发时就别用了，旧应用应该采取迁移行动”，近日，美国白宫国家网络主任办公室 (ONCD)在一份主题为《回到基础构件：通往安全软件之路》的 19 页 PDF 报告中强烈呼吁道。 
 其直言，C 和 C++ 这几种编程语言既缺乏与内存安全相关的特性，又在关键系统中大量使用，可能会带来极大的安全风险，希望开发者抓紧使用“内存安全编程语言”。 
 这是继美国国家安全局、网络安全和基础设施局 (CISA)等之后，又一政府机构发起呼吁，而且这一次直接与“保护国家安全”挂钩。根据报告指出，该建议是美国总统拜登网络安全战略的一部分，是“确保网络空间基石安全”的举措。 


 ONCD 呼吁：用内存安全的语言、让软件变得可测量 
 这份报告指出，“软件和硬件产品的用户始终被置于对网络紧急情况作出反应时立于站不住脚的位置。在多个危机的基础上作出反应往往会使他们陷入困境，而那些在第一线确保系统安全的国家不应承担这一负担的全部重量。” 
 所以，ONCD 要求各大公司、工程师、开发者从两个维度切入： 
 从软硬件方面减少内存安全漏洞的存在，如采用内存安全的编程语言和内存安全的硬件；为了建立更好的网络安全质量度量，研究界可以解决软件可测量性这一困难而复杂的研究问题。 
 最终减少恶意行为者可以攻击的攻击面。 

 一提不安全的语言，必定被点名的 C、C++ 
 内存安全是指防止与内存访问相关的错误和漏洞，其中包括缓冲区溢出和悬空指针等问题。这类问题可能导致程序崩溃、未定义的行为，甚至是安全漏洞，被恶意利用。 
 这些内存安全漏洞主要分为两类：空间和时间。 
 空间内存安全问题是由于在为内存中的变量和对象设定的“正确”边界之外进行执行内存访问造成的。时间内存安全问题源于在时间或状态之外访问内存，例如在对象被释放后访问对象数据，或内存访问意外交错时，就会出现问题。 
 在过去几十年里，行业中发生的许多重大网络安全漏洞都是由内存安全漏洞促成的，包括 1988 年的 Morris Worm、2003 年的 Slammer Worm 拒绝服务攻击、2014 年的 Heartbleed 漏洞和 2023 年的 BLASTPASS 漏洞链。 
 聚焦到企业维度，此前我们也曾报道过一些关键数字： 
 2019 年微软安全工程师透露，从 2006 到 2018 年，微软每年 CVE 的漏洞中约 70% 是内存安全问题； 

 2020 年，在 Google Chromium 项目发现的漏洞中，约 70% 严重性安全错误是内存不安全问题（即 C/C++ 指针错误）；根据 Google Project Zero 团队的分析，2021 年有 67% 的零日漏洞是内存安全漏洞；... 
 “由于许多网络安全问题始于一行代码，解决这些问题的最有效方法之一是检查程序设计语言本身”，ONCD 说道。 
 至于哪些是不安全的语言，ONCD 发布的报告中直接引用一些专家的观点——“专家们已经确定了一些编程语言，它们既缺乏与内存安全相关的特性，又在关键系统中具有高扩散性，例如 C 和 C++。” 

 无论是此前 CISA，还是今天的 ONCD，一提到内存不安全的编程语言，C、C++ 总是“榜上有名”，至于其中原因，主要还是和这些语言的功能特性有关： 
 C、C++ 支持指针，这意味着程序员可以直接访问和修改内存地址。虽然这种灵活性使得一些底层操作更容易，但也增加了因为误用指针而导致的内存安全问题的风险，比如野指针、内存泄漏和越界访问。在 C 和 C++ 中，开发者需要手动分配和释放内存。这意味着程序员负责确保适当的内存分配和释放，而出现错误可能导致内存泄漏或释放后的悬挂指针问题。C 和 C++中的数组和指针没有边界检查。最终可能发生数组越界访问的错误，从而导致程序崩溃或者未定义的行为。缺乏自动垃圾回收： 相对于其他一些现代编程语言，C 和 C++ 不提供自动垃圾回收机制。这意味着程序员需要手动管理内存，容易出现错误。 
 由于这些特性，一些组织默认将 C、C++ 归为不安全的语言。 

 NSA 主推的七大安全语言：Java、Rust、C#... 
 有趣的是，报告并没有建议用一种特定的语言来代替它们。ONCD 倒是直接推荐了自己的兄弟单位，让大家去看看美国国家安全局（NSA）此前发布的网络安全信息表，并表示，“有几十种内存安全的编程语言可以使用。虽然在一些特殊情况下，使用内存安全语言可能不可行。但是在大多数情况下，使用内存安全的程序设计语言是显著改善软件安全性的最有效方法。” 
 而根据 NSA 的建议，内存安全编程语言有： 
 RustGoC#JavaSwiftJavaScriptRuby 
 在衡量编程语言受欢迎程度的 TIOBE 指数中，NSA 推荐这些安全的编程语言：C# 排在第五位，Java 排在第四位，JavaScript 排在第六位，Go 排在第八位。而 Swift（16 位）和各大厂都在推荐的 Rust（18 位）、Ruby（20 位）排名稍微靠后。 
 从流行度上来看，这 7 种语言中只有 4 种使用率比较高，而其中热度最高的 Java，之所以被认为是内存安全语言的一部分原因是它在运行时具有强大的错误检测机制。Java 虚拟机（JVM）会在运行时检查数组边界，防止缓冲区溢出。此外，Java 中的垃圾回收机制有助于防止内存泄漏。这种自动化的错误检测和内存管理机制有助于减少程序员犯错的机会。 

 对此，ONCD 也建议道，“对于新产品，选择构建内存安全程序设计语言是一个早期的架构决策，可以提供显着的安全优势。对于现有的代码库来说，完全重写代码更具有挑战性，但可以通过混合方法采用内存安全编程语言。例如，软件开发人员可以根据风险标准识别关键功能或库，并优先考虑重写这些功能或库的工作。” 

 硬件也要用上内存安全的语言 
 除了在软件上要用安全语言之外，ONCD 认为硬件上也必须用上。 
 在报告中，ONCD 还以“阿波罗13号登月失败”为例： 
 在 1970 年 4 月，阿波罗 13 号上的一次爆炸打乱了它的登月计划。 
 宇航员航行两天后，一根暴露在外的电线引燃了一场大火，导致有一个液氧罐发生了爆炸。宇航员生存的唯一希望就是火箭科学家们做出巧妙而快速的反应。 
 运用物理定律和数学规则，火箭科学家们计算出登月舱燃烧的发动机，给出的建议是，宇航员们必须继续前进，如果飞船靠近月球就有可能借助月球引力返航，最终成功地将阿波罗 13 号送上返回地球的轨道。 
 ONCD 称，五十多年过去了，航空航天工程师和决策者都没有把太空安全的未来听天由命。在很大程度上，由于现代计算和软件工程的技术进步，数字自动化已经将人为错误的风险降到了最低。 
 不过，太空生态系统也难免存在内存安全漏洞，但太空系统在语言使用方面存在一些限制。 
 第一，语言必须允许代码接近内核，这样才能与软件和硬件紧密交互； 
 第二，语言必须支持确定性，这样输出的时序才能保持一致； 
 第三，语言必须没有或无法覆盖“垃圾回收器”，该功能可自动回收计算机程序分配的不再使用的内存。 
 这些要求有助于确保太空系统所必需的可靠和可预测的结果。 
 ONCD 表示，“内存安全和内存不安全的编程语言都符合这些要求。目前，同时满足这三个属性的最广泛使用的语言是 C 和 C++，它们都不是内存安全的编程语言。Rust 是内存安全程序设计语言的一个例子，具有上述三个必要的属性，但尚未在太空系统中得到证明。需要在开发工具链、员工教育和实地案例研究方面取得进一步进展，才能证明在这些用例中存储安全语言的可行性。” 
 此外，该报告还呼吁业界要使用硬件芯片支持内存保护，其实行业中已有人展开行动，如美国 SRI International 和剑桥大学联合研究项目——硬件增强型 RISC 指令的能力(Capability Hardware Enhanced RISC Instructions，简称 CHERI)，为现有的芯片架构增加了新的功能；Arm 公司也探讨了其实验性 Morello Program 及 CHERI 架构实现原理，希望借此解决系统攻击中常被利用的一系列内存访问漏洞。 
 同时，ONCD 认为软件的可测量性也非常重要。不过与物理工程产品不同，大多数软件缺乏统一的结构或组成。这种设计和架构的不同性导致建立通用的衡量指标异常复杂化。 
 正因此，ONCD 也无法给出具体的建议，只是希望未来更多的大型组织、科技公司以及不同的政府部门协力参与进来。 

 盲目弃用并不可取，有人认为：“C++ 受到了不公正的对待” 
 值得注意的是，不同于其他部门，ONCD 在早些时候就对这些提议还征集了一波业界的一些看法。据 DevClass 报告，各大科技公司、基金会也都纷纷表态： 
 Rust 基金会呼吁“要求公共资助机构及其承包商默认使用 Rust 等内存安全编程语言”。微软将重点放在了软件供应链以及对开放源代码缺乏足够的投资等维度上。IBM 表示，重写软件可能过于昂贵，并提倡“保护现有软件免受内存安全漏洞影响 ”的方法。Google 表示，同意政府倡导的“向内存安全语言和框架过渡”。AWS 则“完全支持使用内存安全语言编写新项目的做法”，但也表示这“只是提高开源软件安全性的整体努力中的一个小因素”，并强调有些开发人员可能会直接禁用掉 Rust 内存安全的功能，同时逻辑错误可能会成为“比内存安全更大的安全问题”。 
 当然，除了支持声音之外，也有人认为 C++ 语言遭受了冤屈。 
 一位自称是“在 ISO C++（ISO/IEC SC22/WG21）中拥有数十年经验的 C++ 资深成员”发表了一篇论文做出回应。 
 他们的论文指出，“内存安全只是安全性中很小的一部分”，并且“C++ 的优势在于它有正式、完整规范的内存模型以及活跃的用户和实施者社区”。 
 相比之下，他们认为，一些被视为安全的语言却缺乏正式的规范。“C++ 受到了不公平对待。” 
 他们表示，“对 C++ 的许多批评都是基于用传统语言甚至 C 语言编写的代码，这些代码没有使用旨在提高类型和资源安全性的现代设施”。 
 同时，该团队也认为，还有很多其他方式会导致编程错误，包括逻辑错误、资源泄露、并发错误、类型错误、定时错误、终止错误等等。他们赞成加强对 C++ 程序员的教育，“从一开始学习阶段就解决安全问题”。 
 现实来看，作为 TIOBE 榜单上的热门语言，类 UNIX 编程都在用 C，而系统级编程、游戏开发、乃至人工智能和机器学习的底层实现中，都离不开 C++ 语言。短时间内在各个应用程序上谈用内存安全语言替代，似乎并不现实。 
 来源： 
 https://devclass.com/2024/02/27/white-house-demands-memory-safe-programming-languages-but-iso-c-group-says-its-only-part-of-solution/ 
 https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/memory-safety-statements-of-support/ 
 https://www.whitehouse.gov/wp-content/uploads/2024/02/Final-ONCD-Technical-Report.pdf 
 推荐阅读： 
 ▶Google Gemini 翻车，马斯克接连怒喷，皮查伊内部反思：这样错误完全不可接受！ 
 ▶马斯克搞脑机得“开瓢”？MIT 早在研究「挂耳式耳机」，戴上=“把整个互联网装进脑子”！ 
 ▶雷军、李想、何小鹏谈苹果放弃造车；字节跳动正秘密研发多个AI产品；微软GitHub Copilot企业版正式上线 | 极客头条"
cpu和gpu已过时，npu和apu的时代开始,2022-06-29,"🌎CPU是中央处理器。其实就是机器的“大脑”，也是布局谋略、发号施令、控制行动的“总司令官”。CPU的结构主要包括运算器（ALU，Arithmetic and Logic Unit）、控制单元（CU, Control Unit）、寄存器（Register）、高速缓存器（Cache）和它们之间通讯的数据、控制及状态的总线（BUS）。🌏GPU一般指图形处理器，又称为显示芯片、视觉处理器，是一种专门在个人电脑和一些移动设备上做图像和图形相关运算工作的微处理器。 
 
🌈现在笔记本基本上都是Intel处理器+NVIDIA独立显卡，很常见。
都说硬件是下一个风口，再加上近几年深度学习和计算机视觉的飞速发展，
迎来了新的处理器的时代，下面就来一一介绍。
123 
1、NPU 
⛲️嵌入式神经网络处理器（NPU），英文全称Neural-network Processing Units。采用“数据驱动并行计算”的架构，特别擅长处理视频、图像类的海量多媒体数据。比如实验室的ITX3588J 8K人工智能开发板，它的规格说明书上写的NPU介绍如下： 
 
⛺️这里科普下TOPS（处理器运算能力单位），OPS是Tera Operations Per Second的缩写，1TOPS代表处理器每秒钟可进行一万亿次（10^12）操作。 
与此对应的还有GOPS（Giga Operations Per Second），MOPS（Million Operation Per Second）算力单位。1GOPS代表处理器每秒钟可进行十亿次（10^9） 操作，1MOPS代表处理器每秒钟可进行一百万次（10^6）操作。TOPS同GOPS与MOPS可以换算，都代表每秒钟能处理的次数，单位不同而已。 
在某些情况下，还使用 TOPS/W 来作为评价处理器运算能力的一个性能指标，TOPS/W 用于度量在1W功耗的情况下，处理器能进行多少万亿次操作。 
 
🌃在业内，单位功耗的计算性能，也就是性能功耗比，被用来衡量处理器架构的优劣。据该实验室执行主任、中星微首席技术官张韵东介绍，“星光智能一号”的性能功耗比在传统的冯诺依曼架构上“至少提高了两三个数量级”，也就是几百倍。 
高功耗是很多顶尖人工智能技术被诟病的。IBM20世纪的“深蓝”和谷歌2016的AlphaGo因其需要由巨大的数据计算支撑，前者使用超级计算机，后者使用服务器集群，无法脱离恒定温度和湿度的机房。AlphaGo下一盘棋光电费就需要3000美元。张韵东将它们称之为“一场科学实验”，离技术落地、投入应用还有较远的距离。 
这凸显了嵌入式NPU的小型化、低功耗和低成本优势，加快人工智能技术落地应用。例如无人机对摄像头的重量和功耗有很高的要求，否则会影响起飞和续航能力。而“星光智能一号”只有普通邮票大小，重量仅几十克，它的诞生让诸多监控摄像头等小型设备有了人工智能化的可能，迈出了人工智能从神秘的机房，跨向生活应用的一步。 
2、VPU 
VPU(Video Processing Unit，视频处理单元）是一种全新的视频处理平台核心引擎，具有硬解码功能以及减少CPU负荷的能力。另外，VPU可以减少服务器负载和网络带宽的消耗。VPU由ATI提出，用于区别于传统GPU（Graph Process Unit，图形处理单元）。图形处理单元又包括视频处理单元、外视频模块和后处理模块这三个主要模块。简单来说，VPU是属于GPU的一个模块，可以单独拿出来。 
🌁VPU的基础特性： 
支持的视频编解码标准类别丰富，图像高清。支持多种抗误码工具、多解码和全双工多方通话同时进行。提供了可编程性、柔初性，以及易于升级的解码和编码或主机接口，因为在编解码处理和主接口都实现为可编程微处理中的固件。 
🎡VPU的编程特性： 
VPU编程特性体现在有一个称为BIT处理器的内部的DSP。该BIT处理器的操作是由称为位固件的专用微代码确定。拥有完整组位的固件代码，以及一套完整的VPU控制功能称为VPU的API。因此，应用程序开发人员并不需要管理的主机处理器的编解码器的具体问题。VPU提供给开发人员的编程实现接口具有如下特性： 
 🏅基于帧的处理。BIT处理器完成解码操作在一帧一帧的基础上，在VPU解码器操作没有负担的主机处理器发出图象处理命令之后，主机应用程序执行它自己的操作，直到它已准备好为下一次图像处理操作或直到它收到一个来自VPU的中断通知完成主处理器图像处理。🎽程序内存管理。它有自己的程序的内存来加载位固件用于支持应用程序特定的操作。为了使用这个内部存储器效率地进行，BIT固件具有动态重新装载计划，使VPU具有小量的程序存储器。🏆多实例。VPU支持多实例，这对于多声道解码器的应用程序十分有用。当创建一个新的时实例和启动的图像处理操作时，VPU创建和内自动更新一组上下文参数。这种内部上下文管理方案允许主机处理器上运行的不同解码器的任务，以控制VPU独立经营自己的实例编号。当创建一个新的实例，应用程序的任务接收到一个新的句柄指定一个实例。应用程序可以很容易地通过调用一个参数终止VPU单个任务关闭某个实例。
🌉VPU特色及应用： 
VPU针对视觉处理应用而设计，在性能、功耗和功能性方面都有特别的强化，使之更贴近于实际的应用需求，其设计兼顾到多种用途，专门为视觉处理进行硬件系统的优化。 
其实VPU也是一个SoC，内部集成有多个主控RISC的CPU、许多硬件加速器单元和矢量处理器阵列，专门为视觉海量像素设计的高性能影像信号处理器(ISP)，以及丰富的高速外围接口。在保证强大性能和功能的前提下，VPU采用更先进的集成电路工艺，大大缩小了芯片的尺寸，并结合有效的技术手段，降低各个运算单元的功耗。 
目前最知名的应用当属英特尔® Movidius™ ，这款视觉处理器能够高效完成要求苛刻的计算机视觉和边缘人工智能工作负载。通过在将数据移动最小化的独特架构中，将高度并行的可编程计算与面向特定工作负载的硬件加速相结合，Movidius 视觉处理器可以在电源效率和计算性能之间实现平衡。在视觉零售、安全、工业自动化等领域，VPU 技术借助深度神经网络和基于计算机视觉的应用程序为智能摄像头、边缘服务器和人工智能设备提供支持。 
 
3、APU 
⛺️APU，英文全称是“Accelerated Processing Units”，加速处理器，它是融聚了CPU与GPU功能的产品。电脑上两个最重要的处理器融合，相互补足，发挥最大性能。APU的中文名字叫加速处理器，是AMD融聚理念的产品，它第一次将处理器和独显核心做在一个晶片上，它同时具有高性能处理器和最新独立显卡的处理性能，支持DX11游戏和最新应用的“加速运算”，大幅提升电脑运行效率，实现了CPU与GPU真正的融合。APU性能强悍的秘密在于其革新的核心架构，最新的视频解码引擎，超小芯片和超低功耗设计，强悍的显示性能。 
🌌APU出现背景： 
英伟达、英特尔和AMD都想将自己的芯片做成CPU+GPU的结合体，而且早已推出此类产品，可以说CPU+GPU的形式已经成为未来芯片设计的趋势，这种结合体就是APU。 
自2006年Opteron CPU的鼎盛时期以来，AMD一直梦想着使用APU，并于2010年开始推出第一款用于PC的APU。随后在索尼Play Station4和5以及微软Xbox XS中推出了定制APU系列游戏机，也推出了一些Opteron APU——2013年的X2100和2017年的X3000。 
🏩APU发展趋势： 
最近，AMD公布的路线图中显示，其将在2023年推出Instinct MI300芯片，这是AMD推出的第一款百亿亿次APU，AMD将其称为“世界上第一个数据中心APU”。 
而这个APU是一种将CPU和GPU内核组合到一个封装中的芯片，仔细来说是将基于Zen4的Epyc CPU与使用其全新CDNA3架构的GPU相结合。 
 
🗽APU性能提升： 
AMD表示Instinct MI300预计将比其Instinct MI250X提供超过8倍的AI训练性能提升，与支持Instinct MI200系列的CDNA2 GPU架构相比，用于Instinct MI300的CDNA3架构将为AI工作负载提供超过5倍的性能功耗比提升。 
AMD表示，与使用分立CPU和GPU的实现相比，该架构的设计将允许APU使用更低的功耗；
英特尔同样表示，其Falcon Shores芯片将显着提高带宽、每瓦性能、计算密度和内存容量。
12 
4、NVR 
🏡NVR，全称Network Video Recorder，即网络视频录像机，是网络视频监控系统的存储转发部分，NVR与视频编码器或网络摄像机协同工作，完成视频的录像、存储及转发功能。 
智能NVR是当前最新一代NVR，采用嵌入式设计，集成高性能NPU模块，内嵌成熟的视频结构化识别算法，对实时视频进行人体、车辆、人脸目标的检出、跟踪、抓拍并识别，充分挖掘监控视频的价值，服务安防大数据时代。 
智能NVR使传统的网络摄像机具有聪慧的大脑，让系统具有视频内容理解能力，进而可根据视频内容进行基于目标的预警、布控，基于内容的搜索、大数据挖掘，把普通网络摄像机变成智慧型摄像机。 
🗽主要特性： 
 在传统NVR以视频存储为主要目的基础上，集成人体、车辆和人脸识别算法；支持8路视频结构化，或4路带人脸识别的视频结构化；支持视频检索，可通过结构化和非结构化信息进行视频检索；支持视频、图片、结构化和非结构化数据统一存储，方便管理；支持千万级动态库检索，百万级动态库检索秒级出结果；支持视频存储、智能处理、相机管理、平台管理、智能检索、布控预警、统计分析、事件处理、实时预览、录像回放等功能；支持4/8/16盘位存储，单盘位最大支持8T；支持ONVIF协议，可接驳第三方设备；支持HDMI视频输出，最高分辨率4K；支持H.265、H.264 HP/MP/BP解码；
 
🌁典型应用： 
 
 
 广泛应用于ARM PC、边缘计算、云服务器、智能NVR、智慧大屏、AR/VR智能汽车等领域；双千兆以太网RJ45、2.4GHz/5GHz双频WIFi6/蓝牙5.0支持5G/4G LTE。"
什么是GPU？跟CPU有什么区别？终于有人讲明白了,2023-09-19,"目录 
一、CPU和GPU是什么？ 
1.CPU 
2.GPU 
​二、GPU与CPU有什么区别？ 
三、训练神经网络GPU优于CPU 
 
 
 
一、CPU和GPU是什么？ 
1.CPU 
在学习深度学习时，电脑如果没有GPU，训练的时候需要用的是CPU，那么两者之间在训练数据集有什么差别？当你在一次训练过程中，着重看一下训练过程，就会发现，训练时间是真的差距大。 
这两个具体有什么差别呢？ 
CPU:中央处理器（英文Central Processing Unit）是一台计算机的运算核心和控制核心。CPU、内部存储器和输入/输出设备是电子计算机三大核心部件。其功能主要是解释计算机指令以及处理计算机软件中的数据。 
 
2.GPU 
GPU:英文全称Graphic Processing Unit，中文翻译为“图形处理器”。一个专门的图形核心处理器。GPU是显示卡的“大脑”，决定了该显卡的档次和大部分性能，同时也是2D显示卡和3D显示卡的区别依据。2D显示芯片在处理3D图像和特效时主要依赖CPU的处理能力，称为“软加速”。3D显示芯片是将三维图像和特效处理功能集中在显示芯片内，也即所谓的“硬件加速”功能。 
 二、GPU与CPU有什么区别？ 
CPU和GPU之所以大不相同，是由于其设计目标的不同，它们分别针对了两种不同的应用场景。主要区别如下。 
CPU需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断又会引入大量的分支跳转和中断的处理。这些都使得CPU的内部结构异常复杂。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境。 
GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了Cache。而CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是CPU很小的一部分。 
CPU 基于低延时的设计 
CPU有强大的ALU（算术运算单元）,它可以在很少的时钟周期内完成算术计算。当今的CPU可以达到64bit 双精度。执行双精度浮点源算的加法和乘法只需要1～3个时钟周期。CPU的时钟周期的频率是非常高的，达到1.532～3gigahertz(千兆HZ, 10的9次方).大的缓存也可以降低延时。保存很多的数据放在缓存里面，当需要访问的这些数据，只要在之前访问过的，如今直接在缓存里面取即可。 
复杂的逻辑控制单元。当程序含有多个分支的时候，它通过提供分支预测的能力来降低延时。数据转发。当一些指令依赖前面的指令结果时，数据转发的逻辑控制单元决定这些指令在pipeline中的位置并且尽可能快的转发一个指令的结果给后续的指令。这些动作需要很多的对比电路单元和转发电路单元。 
GPU是基于大的吞吐量设计 
GPU的特点是有很多的ALU和很少的cache. 缓存的目的不是保存后面需要访问的数据的，这点和CPU不同，而是为thread提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问dram（因为需要访问的数据保存在dram中而不是cache里面），获取数据后cache会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问dram，自然会带来延时的问题。 
GPU的控制单元（左边黄色区域块）可以把多个的访问合并成少的访问。 
GPU的虽然有dram延时，却有非常多的ALU和非常多的thread. 为啦平衡内存延时的问题，我们可以中充分利用多的ALU的特性达到一个非常大的吞吐量的效果。尽可能多的分配多的Threads.通常来看GPU ALU会有非常重的pipeline就是因为这样。所以与CPU擅长逻辑控制，串行的运算。和通用类型数据运算不同，GPU擅长的是大规模并发计算，这也正是密码破解等所需要的。所以GPU除了图像处理，也越来越多的参与到计算当中来。 
上面说的有点专业化，接下来就简单说一下，他们两个到底差在哪？ 
CPU（Central Processing Unit）是电脑最主要的部件，他的主要功能是解释计算机指令以及处理计算机软件中的数据，说白了就是做指挥工作，统筹各方面。CPU相当于整个电脑的心脏，而GPU相当于显卡的心脏。 
普通的处理器CPU差不多双核心四线程，目前市面上最高端的桌面处理器i9-7980XE（RMB1.5万）不过十八核心三十六线程。GPU则不同，就拿普通的2000块的游戏显卡RX 480来说，RX480的GPU芯片计算单元划分为36个CU计算核心，每个CU核心又包含了64个流处理器计算核心，所以总共就是36X64=2304个流处理器计算核心。 
CPU相对于GPU就像老教授和小学生，拿i9-7980XE和RX480举个例子，出一套小学数学试卷，老教授刚做一道题，两千多名学生一人一题早就交卷子了。如果套高数卷子，老教授做完学生们一道也不会做。 
 三、训练神经网络GPU优于CPU 
很多个简单的工作，交给GPU显然更适合。 
其实在早期，神经网络都是用CPU训练的。即使现在，像TensorFlow这样的流行框架也支持在CPU上运行。那么，既然CPU和GPU都可以训练神经网络，为什么一般用GPU训练神经网络？很简单，因为GPU比CPU快很多。比如，Victor Dibia（IBM研究院Research Staff Member）基于Tensorflow框架训练了一个实时检测手部的网络。 
训练这样一个神经网络要花多久？CPU上大约要12天，而GPU上则是5小时。（我们这里不讨论具体的型号，关键是两者的比例。CPU和GPU的差距太大了）。 
而且，实际上这个神经网络的训练时间已经通过迁移学习大大缩短了。迁移学习指直接拿训练好的图像分类的模型来用，只是重新训练最后一层或几层网络以检测手部，所以能快很多。那如果从头开始训练的话（有的时候没有现成的模型可供迁移），可能要几周甚至几个月。这还是GPU的情况。 
按照上面的比例，换算成CPU，那基本上就太慢太慢，慢到不现实的程度。GPU之所以比CPU快好多，主要是因为，从运算的角度来看，神经网络主要是由大量的浮点矩阵构成的。而现代的神经网络，可能有几千到几百万的浮点矩阵（所谓深度学习），因此需要很大的内存带宽来访问这些海量的浮点矩阵。而GPU的内存带宽比CPU高很多。比如Intel的Core i9-7980XE内存带宽约为57GB/s，而NVIDIA的Tesla P100带宽高达900GB/s。 
使用神经网络训练，一个最大的问题就是训练速度的问题，特别是对于深度学习而言，过多的参数会消耗很多的时间，在神经网络训练过程中，运算最多的是关于矩阵的运算，这个时候就正好用到了GPU，GPU本来是用来处理图形的，但是因为其处理矩阵计算的高效性就运用到了深度学习之中。"
窥探当今AI芯片中的类脑模型,2019-08-27,"来源：脑人言 
 撰文丨邓 磊（加州大学-圣塔芭芭拉分校UCSB 博士后） 
 责编丨高茂森 Soma 
 排版丨夏獭 





 科学研究的动机，并非仅仅是出于能看到多酷炫的应用，而是因为我们尚有诸多未知世界需要探寻，这是一切应用的前提。 








 AI芯片如火如荼 

 AI芯片是当今人工智能热潮中不可或缺的一个名词，且受国际局势影响愈加受到重视。摩尔定律难以为继，通用处理器的性能提升速度大为减缓，在晶体管物理微缩和计算体系架构改进带来的收益之争中，后者渐处上风。 
 在此背景下，学术界和工业界纷纷将视野转向针对特定领域设计高性能的专用芯片，拟将架构设计发挥到极致。 
 AI芯片作为最为典型的领域专用芯片代表，受到国内外研发单位的高度关注。国外有谷歌、苹果、英特尔、IBM、英伟达等巨头领衔，国内也有寒武纪、地平线、深鉴、华为、阿里等公司响应，最近清华大学的天机芯更是登上了Nature杂志的封面[1]，其形势可谓如日中天。 
   
 然而，AI芯片中所谓的“智能”究竟源自于哪些模型，又和我们的大脑有何关系，他们的过去和未来将会怎样，本文带你一探究竟。 








 人工神经网络 






 基本模型 
 深度学习又是当今机器学习的宠儿，其以人工神经网络（Artificial Neural Network, ANN）为主要模型。 
 ANN由大量神经元（Neuron）通过突触（synapse）连接而成，从输入到输出呈现层级结构，当层数较多时则被称为深度神经网络（Deep Neural Networks）。 
 图1 人工神经元示意图 
 每个神经元的基本结构如图1所示，而基本的计算原理为，其中x和y分别为输入和输出激活值、w为突触连接的权重值、b为偏置值、φ为非线性激活函数。 
  (a) 全连接神经网络；(b) 反馈神经网络；(c)卷积神经网络。 
 根据神经元的连接拓扑可以分为全连接神经网络、卷积神经网络、反馈神经网络等，如图2所示。 
 相比于全连接神经网络，卷积神经网络通过引入二维特征图与突触核的卷积操作获得了强大的局部特征提取能力，被广泛用于图像处理领域。 
 而反馈神经网络通过引入反馈连接，建立时序动力学模型，被广泛用于处理语音文本等时序数据。 
 其实，ANNs模型的建立也是在不同时期中分别借鉴了神经科学的知识： 






 人工神经网络的概念以及基本神经元模型于1943年就已提出[2]，这正是试图模拟脑皮层以神经元网络形式进行信息处理的体现。 
 卷积神经网络的局部感受野是受到大脑视觉系统的启发。 
 深度神经网络的层级构建是源于脑皮层的分层通路。 

 只不过在深度学习的后续发展中，研究者更加偏重把神经网络视为一个黑匣，用于拟合从输入到输出的复杂映射关系： 





 只需要给网络的输出定义一个收敛目标（目标函数，比如每张图像的输出对应到正确的类别）并描述为一个优化问题，然后用梯度下降的方式去更新系统参数主要是突触权重,使得输出逐渐逼近想要的结果。 
 原则上网络越大，特征提取的能力就会越强，也就需要越多的数据来训练网络更新参数使网络收敛，因此计算量也大幅增加。 
 故而，深度学习也被称为数据和算力驱动的智能。虽然深度学习以解决实际应用为目标而与神经科学渐行渐远，但近两年也有科学家试图在大脑中找到梯度下降的证据和吸收新的脑科学成果[3-8]。 



 深度学习的腾飞之路 
 20世纪末到21世纪初很长一段时间内神经网络方法不被多数人看好，其表现也不如传统机器学习模型，多亏图灵奖得主Hinton、LeCun和Bengio三位前辈的坚持，才有了今天的成就。 
 深度学习的飞速发展主要归功于大型数据库的诞生（如ImageNet）、高性能计算平台（如GPUs）的出现、友好开发软件的耕耘（如Tensorflow/Pytorch等）和神经网络模型的改进（如卷积神经网中的VGG/Inception/ResNet/DenseNet），缺一不可。 
 由于成熟的算法、工具和应用场景，深度学习受到学术界和工业界的一致认可。深度学习加速器也现身各大学术顶会，并受到巨头公司和创业公司的青睐，这其中就包括研发TPU的谷歌、手机内置神经引擎的苹果以及国内的寒武纪、地平线、深鉴科技、华为、阿里等，当前主要热门方向为研制运行ANNs模型的高性能云计算平台和低功耗终端器件。 







 脉冲神经网络 





 基本模型 

 与深度学习不同，神经形态计算（neuromorphic computing）是目前智能模型中的又一大主要分支，它更加注重模拟大脑回路的行为，这里给大家介绍两类神经形态模型：脉冲神经网络（Spiking Neural Network, SNN）和神经动力学网络（Neural Dynamic Network）。 
 SNN与ANN主要有两大不同，其一是采用脉冲编码—spike code(0/1)，其二是具有丰富的时间动力学。SNN神经元如图3所示，其模型可简单描述为： 

 其中，(t)为时间步，t是时间常数，u为神经元膜电位，s为神经元的脉冲输出，和分别为神经元静息电位和重置电位，为脉冲发放阈值，表示每个神经元会累加对应突触输入脉冲序列中一定时间窗内的脉冲信号，用于刻画时间衰减效应(距离当前时刻越近的脉冲输入对膜电位影响越大)。 

 图3 脉冲神经元示意图 




 135编辑器 
 虽然上述以微分方程为基础的SNN神经元模型看起来比ANN模型更为复杂，但它已经是最为简化的SNN模型了，被称为泄漏积分发放模型(Leaky Integrate and Fire, LIF)[9]，其它更为复杂的模型如Izhikevich[10]和Hodgkin & Huxley[11]仅凭借现有计算机还难以仿真大规模网络。 
 原理上，SNN的时间积分效应可以使得其能够处理时序问题，尤其是稀疏数据（事实上大脑神经元发放频率也是非常稀疏的）；其膜电位泄漏和阈值发放效应，进一步使得其能够具有一定的去噪功能（事实上大脑脉冲信号也具有很多噪音，但大脑仍能正常工作）。 
 这两个特性，目前的ANN模型都不具备(反馈神经网络尽管具备时域处理能力，但与SNN很不相同)。虽然理论上有较大潜力，但由于SNN神经元的多变量时空动力学比较复杂，而脉冲发放活动又不可导，导致其无法使用具有全局性的梯度下降算法进行学习。 




 135编辑器 
 在相当长一段时间里，其主要的学习规则都是非监督的脑启发STDP(spike timing dependent plasticity)规则[12]，仅利用突触两端神经元的局部脉冲响应来更新突触权重以满足学习目标。  
 虽然STDP具有一定的生物基础，但它太过于局部化，难以获得类似梯度下降算法的全局优化能力，导致其在诸多应用中表现差于ANN模型。 
 直到最近三年，才有研究者通过将预先采用梯度下降算法训练好的ANN模型转换为其SNN版本[13,14]，或者通过对脉冲发放函数进行导数逼近，进而直接计算每个时刻膜电位和脉冲活动的梯度以实现基于梯度下降的有监督算法学习[15,16]，这些努力逐渐使得SNN的应用性能接近目前的ANN模型。 



 争议丛生但未来可期 





 支持SNN的器件通常称为神经形态器件（如欧洲的SpiNNaker[17]和DYNAP[18]，IBM的TrueNorth[19]，Intel的Loihi[20]），其研究的动机主要是因为SNN的类脑特性使其更有希望通过构建大规模系统获得类脑智能。 

 不过从应用角度出发，目前的神经形态器件大多只能体现低功耗的特性，这是由于SNN神经元的输入输出信号为二值脉冲的缘故，可以去除输入和突触权重运算中的笨重乘法，再加之脉冲活动非常稀疏且允许事件驱动的异步电路设计。 
 而在功能方面，SNN所表现出的性能尚不能与深度学习匹敌，这也是工业界对神经形态方案的热情有所消退甚至持怀疑和观望态度的原因。 




 135编辑器 
 但笔者认为，目前的神经形态计算还有很大的提升空间，未来仍有可期： 
 一方面，深度学习的进步是数据、算力、软件和模型共同进步的结果，非一日之功，而神经形态计算在上述优化上还处于起步阶段。期待研究者们继续从各个子领域努力前行，逐步迭代发展。 







 神经动力学网络 





 基本模型 

 这类模型通常用于研究脑回路的响应特性，成果相对分散。这里给大家介绍比较典型的连续吸引子网络（Continuous Attractor Neural Network, CANN）[21,22]，其神经元模型可以简单总结为如下： 
   

  其中大多数变量含义与SNN类似。此外，x为神经元在二维空间中的坐标位置（如图4所示，以二维CANN为例），为两个对应位置神经元之间的突触连接权重，是神经元发放频率，为外部刺激，β和k为常数。 
 可以看到，CANN把SNN中“从膜电位到脉冲信号”的转换调整为了“从膜电位到发放频率”的转换，公式中的除法归一化体现了大脑中普遍存在的神经元抑制作用，使得发放频率不会无限增长，整个系统趋于稳定。 




 135编辑器 
 在CANN中，突触权重通常配置为高斯峰形状，即，相距越近的神经元连接越强，反之越弱，而α控制着高斯峰的半径。如此，在没有外部刺激的时候，CANN模型的神经元发放频率响应也呈现稳定的高斯函数形状，称为响应波包（Bump），表示网络的一个吸引子状态。 
 CANN不仅有简洁的形式和特殊的动力学特性，在大脑中也具备生物学基础： 






 大脑中头朝向神经元在空间上沿着一个环分布构成一维CANN，神经元群活动可以预测头转动的趋势。 
 海马中的空间神经元（Place Cell）互相连接构成一个二维CANN，神经元活动的波包可以编码动物的空间位置，支持大脑的空间导航。 
 少数动物如蝙蝠的空间定位系统甚至会用到三维CANN。 

 关于CANN的计算原理、生物基础和应用场景的更多信息，感兴趣的读者可以参考北京大学吴思教授课题组微信公众号“吴思Lab计算神经科学及类脑计算”最近发布的CANN专帖【学术思想】连续吸引子神经网络：神经信息表达的正则化网络模型。 

 图4 二维CANN模型示意图 




 135编辑器 
 CANN的应用场景有很多，这里主要给大家详细描述目标追踪的实现[23]。当图4中二维神经元阵列对应到视频中的像素阵列、每个神经元的接收对应位置像素强度的前后帧差分信号刺激时，网络便可实现目标追踪。  
 详细而言，网络初始时对目标所在的方框区域初始化为高响应值，其他区域为0；开始运行后，网络所有神经元按照CANN规则进行计算，而目标所在区域的强差分输入会牵引着神经元响应波包平滑移动，波包位置对目标进行实时追踪。突触权重高斯峰的半径决定了追踪的性能：太宽则容易受邻近目标干扰，太窄则容易跟丢目标。 
 这种吸引子跟随外部输入的牵引而移动是CANN的特有动力学特性，对目标的追踪也非常平滑自然。除目标追踪的应用外，波包编码本身也是一种神经元群编码的体现，相比于单神经元编码，更具有鲁棒性。 
 另一方面，一簇CANN神经元可以处理某种模态信息的不同任务，也可以处理不同模态的信息，多簇CANN可以用长程连接进行信息交互，因此CANN也有潜力为大脑提供多模态信息处理的统一框架。 



 实际应用任重道远 





 前面讲述的深度学习和SNN都有对应的专用处理芯片，而神经动力学网络由于本身研究比较分散，应用也不成体系，在硬件中的实现仍较为少见。  
 MIT曾实现了十多个神经元的简单CANN模型[24]，但并未引起重视。最近，清华大学的类脑芯片—天机芯通过对CANN模型突触连接的局部约束、数据的低精度化等硬件友好改造，实现了约800帧每秒的超快目标追踪[1,23]。 
 目前多数神经动力学网络还是以复现神经回路响应模式居多，距离实际应用还任重道远，需要持续不断地进行挖掘。 







 天机芯 





 跨域融合思想 

 上述模型各有特点和优缺点，孰优孰劣尚无定论。与现有深度学习加速器和神经形态器件分别支持ANN和SNN模型不同，天机芯[1]的目标是促进通用人工智能的发展，所以在保持专用芯片高效能的同时需要尽可能提高对上述模型支持的通用性。 
 由于不同模型的迥异计算原理、信息编码方式与应用场景，导致其所需的计算与存储架构以及优化目标相差较大，这点从现有深度学习加速器和神经形态器件的独立设计和应用体系可以看出来。这里需要注意的是，分别设计不同模型的专用模块再简单放置到一起是行不通的，原因如下。第一，很难确定各自的配置比例，因为现实应用中的工作负载往往是多变的；第二，面积和功耗都不高效，处理单一同构模型时，会导致总体利用率很低；第三，处理混合异构模型时，需要专门的信号转换单元，增加额外成本降低效率。 




 135编辑器 
 在天机芯的设计中，实现异构融合有两个关键点。首先，深入研究大多数目前的主流神经网络模型，包括人工神经网络、脉冲神经网络与神经动力学网络等，建立一个通用的模型描述框架，并把这些模型进行合理拆解后映射至包括轴突、树突、突触、胞体以及互连网络等基本模块上，归纳每个模块应具备的算子功能。每个模块都最大程度复用不同模型工作模式的存储和计算资源，所以最终面积只比单一模式高3%。  
 然后，基于现有神经形态芯片的众核可扩展架构（每个芯片中有许多个互相连接的基本功能核），并仔细设计上述几个基本模块，包括模式可独立配置的轴突和胞体（输入/输出模块），模式共享的树突和突触（运算模块），以及统一的路由协议和路由网络（连接模块）。当轴突和胞体工作在相同模式下，整个网络可支持典型单一同构模型；当轴突和胞体工作在不同模式下，整个网络可支持目前尚缺乏研究的混合异构模型，有望推动神经网络新模型的探索。  



 无人自行车应用演示 

 选择什么样的平台来演示天机芯的基本功能并非易事。首先，这应该是一个类似大脑的多模态系统，覆盖感知、决策和执行的完整链路，并能够为异构融合的多种模型提供任务支撑，这与目前很多AI系统演示的单一任务不同。其次，这应该是一个能够与现实环境交互的真实系统，而不是停留在机房实验。再者，这个系统最好对处理芯片有功耗和实时性要求，以体现专用芯片的优势。最后，这个系统必须是安全可控的，能够方便实验。综上所述，无人智能自行车平台应运而生，其具有语音识别、目标探测和追踪、运动控制、障碍躲避以及自主决策等功能，是一个运行在户外场景同时对功耗和实时性具有需求的一个嵌入式机器人，算得上是一个五脏俱全的小型类脑平台。  

 其中，目标探测采用的是ANN中的卷积神经网络，在相机采集的图像中探测目标位置；车身平衡控制采用的是ANN中的全连接神经网络，根据陀螺仪采集的车身姿态实现对转向电机的PID控制，保持自行车平衡；语音命令识别采用的是SNN模型，将语音信号转换为spike脉冲信号特征后进行命令分类；目标追踪便是前面介绍过的CANN模型；自主决策是一个有限状态机，实现上述不同模型的融合通信，因此是一个异构的混合模型。 




 135编辑器 





 总结 



 纵观当今的AI芯片，从计算机科学和神经科学角度寻求动机，采用了不同的神经网络模型，呈现了不同的硬件架构设计，应用和性能也各有所长。  
 笔者认为，在现阶段没有必要急着对各类途径分出胜负。 
 一方面应该继续保持研究的多样性，使各个领域迭代发展，毕竟每一条路线目前看来都不能保证成为终极方案。  
 另一方面，面对目前已知的各类神经网络模型，从数学根本上去探寻他们的表达能力差异从而寻求可控的大一统模型，也是很好的着力点。 
 科学研究的动机，并非仅仅是出于能看到多酷炫的应用，而是因为我们尚有诸多未知世界需要探寻，这是一切应用的前提。 



 参考文献 
 [1] Pei, Jing, Lei Deng, Sen Song, Mingguo Zhao, Youhui Zhang, Shuang Wu, Guanrui Wang et al. ""Towards artificial general intelligence with hybrid Tianjic chip architecture."" Nature 572, no. 7767 (2019): 106. 
 [2] McCulloch W S, Pitts W. A logical calculus of the ideas immanent in nervous activity. Bull Math Biophys, 1943, 5(4): 115–133. 
 [3] Sacramento, Joao, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. ""Dendritic error backpropagation in deep cortical microcircuits."" arXiv preprint arXiv:1801.00062 (2017). 
 [4] Guerguiev, Jordan, Timothy P. Lillicrap, and Blake A. Richards. ""Towards deep learning with segregated dendrites."" ELife 6 (2017): e22901. 
 [5] Sabour, Sara, Nicholas Frosst, and Geoffrey E. Hinton. ""Dynamic routing between capsules."" In Advances in neural information processing systems, pp. 3856-3866. 2017. 
 [6] Roelfsema, Pieter R., and Anthony Holtmaat. ""Control of synaptic plasticity in deep cortical networks."" Nature Reviews Neuroscience 19, no. 3 (2018): 166. 
 [7] Marblestone, Adam H., Greg Wayne, and Konrad P. Kording. ""Toward an integration of deep learning and neuroscience."" Frontiers in computational neuroscience 10 (2016): 94. 
 [8] Ullman, Shimon. ""Using neuroscience to develop artificial intelligence."" Science 363, no. 6428 (2019): 692-693. 
 [9] Gerstner, Wulfram, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal dynamics: From single neurons to networks and models of cognition. Cambridge University Press, 2014. 
 [10] Izhikevich, Eugene M. ""Simple model of spiking neurons."" IEEE Transactions on neural networks 14, no. 6 (2003): 1569-1572. 
 [11] Hodgkin, Alan L., and Andrew F. Huxley. ""A quantitative description of membrane current and its application to conduction and excitation in nerve."" The Journal of physiology117, no. 4 (1952): 500-544. 
 [12] Song, Sen, Kenneth D. Miller, and Larry F. Abbott. ""Competitive Hebbian learning through spike-timing-dependent synaptic plasticity."" Nature neuroscience 3, no. 9 (2000): 919. 
 [13] Diehl, Peter U., Daniel Neil, Jonathan Binas, Matthew Cook, Shih-Chii Liu, and Michael Pfeiffer. ""Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing."" In 2015 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2015. 
 [14] Sengupta, Abhronil, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. ""Going deeper in spiking neural networks: VGG and residual architectures."" Frontiers in neuroscience 13 (2019). 
 [15] Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. ""Spatio-temporal backpropagation for training high-performance spiking neural networks."" Frontiers in neuroscience 12 (2018). 
 [16] Wu, Yujie, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. ""Direct training for spiking neural networks: Faster, larger, better."" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp. 1311-1318. 2019. 
 [17] Furber, Steve B., Francesco Galluppi, Steve Temple, and Luis A. Plana. ""The spinnaker project."" Proceedings of the IEEE102, no. 5 (2014): 652-665. 
 [18] Moradi, Saber, Ning Qiao, Fabio Stefanini, and Giacomo Indiveri. ""A scalable multicore architecture with heterogeneous memory structures for dynamic neuromorphic asynchronous processors (dynaps)."" IEEE transactions on biomedical circuits and systems 12, no. 1 (2017): 106-122. 
 [19] Merolla, Paul A., John V. Arthur, Rodrigo Alvarez-Icaza, Andrew S. Cassidy, Jun Sawada, Filipp Akopyan, Bryan L. Jackson et al. ""A million spiking-neuron integrated circuit with a scalable communication network and interface."" Science345, no. 6197 (2014): 668-673. 
 [20] Davies, Mike, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou et al. ""Loihi: A neuromorphic manycore processor with on-chip learning."" IEEE Micro 38, no. 1 (2018): 82-99. 
 [21] Wu, Si, Kosuke Hamaguchi, and Shun-ichi Amari. ""Dynamics and computation of continuous attractors."" Neural computation20, no. 4 (2008): 994-1025. 
 [22] Fung, CC Alan, KY Michael Wong, and Si Wu. ""A moving bump in a continuous manifold: a comprehensive study of the tracking dynamics of continuous attractor neural networks."" Neural Computation 22, no. 3 (2010): 752-792. 
 [23] Deng, Lei, Zhe Zou, Xin Ma, Ling Liang, Guanrui Wang, Xing Hu, Liu Liu, Jing Pei, Guoqi Li, and Yuan Xie. ""Fast Object Tracking on a Many-Core Neural Network Chip."" Frontiers in neuroscience 12 (2018). 







 脑人言，与大脑对话 











 张亚勤、刘慈欣、周鸿祎、王飞跃、约翰.翰兹联合推荐 

 这是一部力图破解21世纪前沿科技大爆发背后的规律与秘密，深度解读数十亿群体智能与数百亿机器智能如何经过50年形成互联网大脑模型，详细阐述互联网大脑为代表的超级智能如何深刻影响人类社会、产业与科技未来的最新著作。 

 《崛起的超级智能;互联网大脑如何影响科技未来》2019年7月中信出版社出版。刘锋著。了解详情请点击：【新书】崛起的超级智能：互联网大脑如何影响科技未来 





 未来智能实验室是人工智能学家与科学院相关机构联合成立的人工智能，互联网和脑科学交叉研究机构。 

 未来智能实验室的主要工作包括：建立AI智能系统智商评测体系，开展世界人工智能智商评测；开展互联网（城市）云脑研究计划，构建互联网（城市）云脑技术和企业图谱，为提升企业，行业与城市的智能水平服务。 

   如果您对实验室的研究感兴趣，欢迎加入未来智能实验室线上平台。扫描以下二维码或点击本文左下角“阅读原文”"
脑科学和类脑智能技术综述学习笔记,2020-05-14,"文章目录 
 Part1 脑科学1、脑科学与类脑研究概述*摘要引言：1 国际脑科学和类脑研究的回顾与前瞻1.1 脑科学的回顾:现代神经科学的起点是**神经解剖学和组织学**对神经系统结构的认识和分析。1.2 脑科学领域的重大问题：从图谱制作到机制解析1.3 脑健康领域的重大问题：脑疾病的早期诊断和干预1.4 人工智能发展的回顾1.5 类脑人工智能发展的重大问题：具有自主学习能力的智能系统 
2 国际大型脑研究计划概述3 中国脑计划4 中国脑科学和类脑智能技术的优势领域5 中科院脑科学与智能技术卓越创新中心5个部门：5 个领域：6中科大类脑智能技术及应用国家工程实验室 

 2、神经科学和类脑人工智能发展*摘　要 ：1　我国未来经济社会发展对神经科学和类脑人工智能的需求：面向2020—2050年1.1　神经科学、人工智能技术演进与基本应用形态总体预测 
3　面向我国2020—2050年的重大科技布局初步建议：“3+2+2”模式3.1　基础神经生物学3.3　类脑人工智能3.3.1　充分利用神经科学研究成果，开发新理论、新算法3.3.2　重点部署深度学习，探索和模拟脑信息处理机制3.3.4　开发类脑芯片、类脑智能器械、脑机接口等智能产品3.3.5　推动类脑智能机器人的研发和应用 
 

 
Part2 类脑智能技术3、类脑智能研究的回顾与展望*摘要：1 引言：2 类脑智能的历史2.1 认知科学中类脑智能研究的萌芽2.2 计算神经科学中类脑智能研究的萌芽2.3 人工智能中的神经网络研究 
3 类脑智能的现状3.1 类脑模型与类脑信息处理3.2 类脑芯片与计算平台 
4 类脑智能的未来4.1认知脑计算模型的构建4.2 类脑信息处理4.3 类脑芯片与类脑计算体系结构4.4 类脑智能机器人与人机协同4.5 类脑智能的应用 
Q&A 
 4、类脑计算芯片与类脑智能机器人发展现状与思考 *摘要：引言：1、类脑芯片2、类脑智能机器人的仿生结构与感知控制3、类脑机器人的智能生长4、中科院B类先导专项下的类脑芯片与机器人研究工作5、基于类脑芯片与类脑智能机器人研究的进一步思考 
 5、类脑智能与深度学习的几个问题与思考＊1、深度学习的优缺2、未来类脑智能发展思考 
 6、多模态学习方法综述*摘要引言1、多模态学习的定义、基本任务与发展过程2 多模态统计学习方法2.1 核学习方法与多核学习2.2 典型相关性分析2.3 共享子空间学习2.4 协同训练方法 
3 多模态深度学习方法4 多模态对抗学习方法 
 
Part3 与类脑智能技术相关的学科1 **生物脑科学、计算机科学**2 **神经科学neuroscience**3 **认知科学cognitive science**4 **认知神经科学cognitive neuroscience**5 **计算神经科学**6 **认知心理学cognitive pyschology**7计算神经科学在脑认知与脑疾病中的研究 
 

 
 
Part1 脑科学 
1、脑科学与类脑研究概述* 
摘要 
脑科学和类脑智能技术(Q1)是两个重要的前沿科技领域 脑科学：脑科学研究对大脑认知神经原理的认识。 类脑智能技术：提升了人类对自身的理解和脑重大疾病的诊治水平，也为发展类脑计算系统和器件、突破传统计算机架构的束缚提供了重要的依据。 两者如何结合（Q2） 
引言： 
理解大脑的结构与功能是 21 世纪最具挑战性的前沿科学问题，理解认知、思维、意识和语言（脑功能）的神经基础，是人类认识自然与自身的终极挑战。 脑科学的意义：脑科学对各种脑功能神经基础的解析，对有效诊断和治疗脑疾病有重要的临床意义；脑科学所启发的类脑研究可推动新一代人工智能技术和新型信息产业的发展。 脑科学与认知科学研究的战略规划：脑功能的细胞和分子机理（生物），脑重大疾病的发生发展机理（医疗），脑发育、可塑性与人类智力的关系，学习记忆和思维等脑高级认知功能的过程及其神经基础，脑信息表达与脑式信息处理系统，人脑与计算机对话（脑科学与认知科学）等。 我国将启动“脑科学与类脑研究”国家科技重大专项：侧重以探索大脑认知原理的基础研究为主体，以发展类脑人工智能的计算技术和器件及研发脑重大疾病的诊断干预手段为应用导向。 
1 国际脑科学和类脑研究的回顾与前瞻 
1.1 脑科学的回顾:现代神经科学的起点是神经解剖学和组织学对神经系统结构的认识和分析。 
要理解神经系统处理信息的工作原理，必须先具有神经元层面的神经联结结构和电活动信息。 20 世纪在神经元层面从下而上的研究也有了一些标志性的突破：经信息以动作电位的频率来编码信息的幅度。从动作电位的离子机制，发现各种神经递质及其功能。各种视觉神经元从简单到复杂的感受野特性，发现突触的长期强化和弱化现象，发现对特定空间定位有反应的神经元。 但是这些神经元的特性是如何通过局部环路和长程环路产生的，我们的理解还十分有限。问题的关键是，我们对脑功能相关的神经环路结构和神经信息处理机制的解析仍极不清楚。 
1.2 脑科学领域的重大问题：从图谱制作到机制解析 
全脑图谱的制作已成为脑科学必须攻克的关口。核磁共振等脑成像技术大大推动了人们在无创条件下对大脑宏观结构和电活动的理解。但是，由于这些宏观成像技术的低时空分辨率，目前急需有介观层面（Q3）细胞级分辨率（微米级）神经网络图谱和高时间分辨率（毫秒级）的****载体神经元集群电活动图谱。 完整的全脑图谱制作的过程中，对每个脑区神经元种类的鉴定是必要的一步-单细胞深度RNA测序技术。利用在这些神经元亚型特异表达的分子作为标记，可以绘制各脑区各种类型神经元的输入和输出联接图谱。对一个神经元亚型的最好的定义是连接和功能的定义：接受相同神经元的输入并对相同脑区的相同神经元有输出的一群神经元。 描叙各个神经联接在进行脑功能时的电活动图谱，这就需要有对神经元集群在体内的观测手段。 
 
 上述三类脑图谱——神经元种类图谱、介观神经联接图谱、介观神经元电活动图谱的制作将是脑科学界长期的工作。脑结构图谱制作的进展速度很大程度上依赖于介观层面观测新技术的研发。 
 
值得注意的是，在全脑神经联接图谱未完成前，神经科学家针对特定脑功能的已知神经环路(Q4)，对其工作机制(Q5)已作出了许多有意义的解析。 基本脑认知功能：感觉和知觉、学习和记忆、情绪和情感、注意和抉择。 高等脑认知功能：共情心、思维、意识、语言。 
1.3 脑健康领域的重大问题：脑疾病的早期诊断和干预 
神经发育疾病是一类由脑部生长和发育缺陷导致的疾病，表现为情绪、认知等功能的异常。三类重大脑疾病——幼年期自闭症和智障、中年期抑郁症和成瘾、老年期的退行性脑疾病。早期诊断和早期干预将是对脑疾病最有效的医疗方式。在完全理解机理之前，急需研发出有效的脑重大疾病预警和早期诊断的各种指标。 
1.4 人工智能发展的回顾 
应用于心智建模的认知体系结构 SOAR 与 ACT-R 都是在 Newell 直接领导下或受其启发而发展起来的，并以此为基石实现了对人类各种认知功能的建模：动作电位模型、神经元之间的突触计算模型、微观（Q6）神经元及其微环路建模、特定脑区内皮质柱的计算模拟，以上工作要真正实现认知功能的模拟还有很大鸿沟需要跨越。 人工智能符号主义研究的出发点是对人类思维、行为的符号化高层抽象描述。 以人工神经网络为代表的联接主义的出发点正是对脑神经系统结构及其计算机制的初步模拟。 人工神经网络：感知器（Perceptron）是浅层人工神经网络的代表，由于其权值自学习能力引起了巨大关注。反向传播算法的提出解决了多层感知机学习的难题。误差反向传播算法、深度学习算法、卷积神经网络、随着 GPU 并行计算的推广和大数据的出现，在大规模数据上训练多层神经网络成为可能，从而大大提升了神经网络的学习和泛化能力。 在人工神经网络的研究中，大多数学者主要关心提升网络学习的性能，Poggio 及其合作者的工作是人工神经网络向更类脑方向发展的典范，Bengio 及其合作者融合了脑的基底神经节与前额叶的信息处理机制，提出了类脑强化学习，由 Hawkins 提出的分层时序记忆（Hierarchical Temporal Memory）模型更为深度地借鉴了脑信息处理机制。 从问题求解目标讲，几乎所有的人工智能系统都需要首先进行人工形式化建模，转化为一类特定的计算问题（如搜索、自动推理、机器学习等）进行处理，因而人工智能系统都被称为专用人工智能系统。 
1.5 类脑人工智能发展的重大问题：具有自主学习能力的智能系统 
智能机器人 （1）目前的人工智能技术缺乏通用性：语音识别、图像处理、自然语言处理、机器翻译等采用不同的模型和不同的学习数据，两种不同的任务无法采用同一套系统进行求解，不同任务之间知识也无法共享。而人脑却采用同一个信息处理系统进行自动感知、问题分析与求解、决策控制等。 （2）目前基于深度学习方法已经在人工智能领域取得了很大的成功，然而其代价是高耗能： 因此，受脑信息处理机制启发，借鉴脑神经机制和认知行为机制发展类脑智能已成为近年来人工智能与计算科学领域的研究热点。新一代人工智能需要在上述脑研究启发下，以多脑区、多模态和多任务协同为核心，研究神经网络微观刺激调控和宏观动态演化、视听触感认知通道及协同、长 时短时记忆与决策、运动视觉与规划控制等，建立具有生物和数学基础的计算模型与学习方法，实现具有自主学习能力的智能系统。 
2 国际大型脑研究计划概述 
2013美国-通过推动创新型神经技术开展大脑研究计划（Brain Research through Advancing Innovative Neurotechnologies，BRAIN） 9 项优先发展的领域和目标-1、鉴定神经细胞的类型并达成共识；2、绘制大脑结构图谱；3、研发新的大规模神经网络电活动记录技术；4、研发一套调控神经环路电活动的工具集；5、建立神经元电活动与行为的联系；6、整合理论、模型和统计方法；7、解析人脑成像技术的基本机制；8、建立人脑数据采集的机制；9、脑科学知识的传播与人员培训 2013欧盟-人脑计划 （Human Brain Project，HBP） 2014日本-“脑智（Brain/MIND）计划”，国家脑计划的目标是“使用整合性神经技术制作有助于脑疾病研究的大脑图谱” 
3 中国脑计划 
以理解脑认知功能的神经基础为研究主体，以脑机智能技术和脑重大疾病诊治手段研发为两翼，在未来 15 年内使我国的脑认知的神经基础研究、类脑研究与脑机智能技术和脑重大疾病研究达到国际先进水平，并在部分领域起到引领作用。 一些未来研究的重点内容： 在脑认知的神经基础原理领域： 1、基本脑认知功能（感觉和知觉、学习和记忆、情绪和情感、注意和抉择等）以及高等脑认知功能（同理心、思维、自我意识、语言等）的神经环路和工作机理。2、人脑宏观神经网络和模式。3、动物介观神经网络的结构性及功能性全景式图谱的绘制。 在类脑计算与脑机智能技术领域： 1、类脑计算理论。2、新一代人工神经网络计算模型。3、类神经形态的处理器和类脑计算机。4、脑计算系统所需要的软件环境和应用平台。5、可自我学习和能适应环境而成长的机器人。6、脑机接口和脑机融合的新模型和新方法。7、脑活动（电、磁、超声）调控技术等。 脑重大疾病与健康领域：~ 
4 中国脑科学和类脑智能技术的优势领域 
我国在计算机视觉、语音识别、机器翻译和自然语言理解等领域已达国际先进水平。同时我国也在深度学习芯片体系架构方面有所创新，并在类脑智能机器人方面率先进行了布局。 
5 中科院脑科学与智能技术卓越创新中心 
5个部门： 
脑认知神经基础研究部：感觉输入对本能行为的调控 神经振荡与多感整合 适应性行为核心环路的运行原理 记忆储存提取的功能和机构可塑性 脑研究新技术研究部：单神经细胞分子生物学技术 神经环路示踪与成像技术 神经电信号和化学信号检测技术 生物光子学技术 宏观脑成像技术 类脑模型与智能信息处理研究部：全脑网络数据分析 类脑多尺度神经网络计算模型 多模态感知计算模型 语言与知识处理计算模型 类脑器件与系统研究部：类神经计算芯片 新型神经计算器件 类脑智能机器人 人机协同和智能生长 
5 个领域： 
（1）在“脑认知功能的环路基础”领域，中心将研究感觉输入是如何启动 和调节动物的本能行为、神经元震荡活动在多感觉信息处理和整合中的作用、记忆储存与提取的神经机制、适应性行为和高级认知功能（如自我意识、共情心和语言等）的神经环路基础。 （3）在“脑研究新技术”研究方向，中心正在完善鉴别神经元类型的单细胞基因分析方法、病毒感染示踪标记神经环路的方法、记录电信号和化学信号的微电级阵列技术，以及各种观测脑结构和功能的光学、磁共振影像新技术。（4）在“类脑模型与智能信息处理”研究方向，中心在介观和宏观水平对光学 和磁共振成像数据进行全脑联结组分析，研发多感觉模态感知和能准确辨认图像、语音并理解语义的信息计算模型。 （5）在“类脑器件与系统”研究方向，中心正在研发类神经元计算芯片、新一代的神经网络计算器件、类脑智能机器人以及人机协同的智能训练和生长环境。 
6中科大类脑智能技术及应用国家工程实验室 
支持开展类脑认知与神经计算、类脑多模态感知与信息处理、类脑芯片与系统、量子人工智能、智能机器人等技术的研发与工程化。 
2、神经科学和类脑人工智能发展* 
摘　要 ： 
对 2020—2050 年我国经济社会发展面临的战略需求、实施相关重大科技研发计划的必要性和可能组织形式、神经科学和类脑人工智能布局和发展步骤以及具体目标。 建议我国神经科学和类脑人工智能未来规划布局可在国家重大科技项目“脑科学与类脑研究”和“人工智能 2.0”主题的基础上，细分为 3 大重点方向 ( 基础神经生物学、神经精神性疾病、类脑人工智能 ) 和 2 大支撑性领域 ( 变革性神经科学技术、支撑平台 ) 
1　我国未来经济社会发展对神经科学和类脑人工智能的需求：面向2020—2050年 
1.1　神经科学、人工智能技术演进与基本应用形态总体预测 
到2020年，神经科学蓄积变革基础，现有人工智能技术独领风骚，人类社会全面进入**“弱”人工智能时代**。目前，人工智能技术和神经科学基本上还属于两个独立的学科领域，在相关领域的融合应用也处于初级阶段。 到2030年，神经科学和类脑人工智能将迎来第一轮重大突破，革新原有人工智能的算法基础，人类社会初步进入**“强”人工智能时代**。在神经感知和神经认知理解方面出现颠覆性成果。这一时期，类脑人工智能和神经科学将相互促进。 到2050年，神经科学和类脑人工智能将迎来第二轮重大突破、类脑人工智能进入升级版，人类社会将全面进入强人工智能时代。在情感、意识理解方面出现颠覆性成果，神经科学和类脑人工智能融为一体。 
3　面向我国2020—2050年的重大科技布局初步建议：“3+2+2”模式 
以 3 大重点方向 ( 基础神经生物学、神经精神性疾病、类脑人工智能 )、2 大支撑性领域 ( 变革性神经科学技术、支撑平台 ) 为核心内容，2 大产业 ( 神经生物医药及生物工程、人工智能产业 )。 
3.1　基础神经生物学 
3.1.1　脑图谱绘制 3.1.2　神经发育研究 3.1.3　神经环路解析 3.1.4　认知的神经基础 
多模态感知整合研究2) 学习与记忆机3) 人类抉择行为的神经机制4) 意识的神经机制 
3.3　类脑人工智能 
一方面充分利用神经科学领域的丰硕研究成果，促进神经科学与人工智能的深度融合，产生原始创新的理论和方法 ； 另一方面进一步加强人工智能技术与产品的研发、应用和产业化发展，反过来助力神经科学研究与创新。 
3.3.1　充分利用神经科学研究成果，开发新理论、新算法 
中短期发展目标 ： 1) 进一步开发基于深度学习和生物识别的新型类脑算法，初步构建与之相应的新一代人工神经网络模型，对脑信息传递、自我学习、记忆等高级功能进行简单模拟 ； 2) 突破深度学习的理论和技术局限，结合脑认知机制，构建统计关联与特征关联相结合的新型学习理论，实现“知识驱动”与“语义驱动”关联统一。 2030—2050 年的发展目标 ： 1) 构建融合深度学习与强化学习、演化计算、主动学习、毕生学习等仿生和自然计算理论的新型理论框架 ； 2) 实 现大规模并行神经网络、进化算法和其他复杂理论计算 ； 3) 对大脑原始能力 ( 即理解和物种生存相关联的生物行为等 ) 有了深入理解，从而实现高级的机器逻辑能力。 
3.3.2　重点部署深度学习，探索和模拟脑信息处理机制 
中短期发展目标 ： 1) 对视觉、听觉、躯体觉等感知觉的神经机制进行模拟，实现模式识别的优化与创新，获得进一步优化和通用的虚拟现实技术、 人机交互技术等 ； 2) 神经网络、认知计算模型和智能系统的自适应能力大幅度提升，通用性人工智能技术获得突破 ； 3) 开发出能够同时完成语音识别、图像处理、自然语言处理、机器翻译等任务的通用性的人工智能系统。 2030—2050 年的发展目标 ： 1) 突破智能系统在感知、认知、控制等方面的巨大瓶颈，开发出具有自主学习能力的智能系统 ； 2) 建立类人学习 机制的认知结构，大幅度提高机器学习鲁棒性，提高机器人的认知能力和主动学习能力 ； 3) 建立在材料、物理等相关技术突破的基础上，开发出新型供能系统，智能系统耗能降低至通用水平。 3.3.3　重视大数据技术和新一代互联网技术等基础技术支撑 
3.3.4　开发类脑芯片、类脑智能器械、脑机接口等智能产品 
中短期发展目标 ： 1) 神经计算电路模块的通用性进一步提高，脑芯片等类脑元器件和硬件系统获得技术突破 ； 2) 研发出综合性类脑人工智能软硬件系统 ； 3) 脑电信号检测、传感、编译、转换等技术有所突破，高性能、高传输速度、高兼容性脑机接口得以推向应用。 面向 2030—2050 年的发展目标 ： 1) 类脑人工智能软硬件系统耗能和成本大幅度降低，通用性显著提高 ； 2) 将人类识别能力与计算机高速运算能力相结合的“人 - 机”协作系统在安全、医疗、工业等领域广泛应用 。 
3.3.5　推动类脑智能机器人的研发和应用 
面向 2030—2050 年的发展目标 ： 1) 机器人智能性大幅度提高 ； 2) 开发出应用于民生、减轻社会负担和家庭负担的民用机器人，如具有情绪感知和抚慰能力的老年服务机器人等 ； 3) 工业领域的基本操作全面“机器人化” ，民用、军用领域简单工种50% 被机器人替代。 
Part2 类脑智能技术 
3、类脑智能研究的回顾与展望* 
摘要： 
实现人类水平的智能系统（协同多种不同的认知能力、对复杂环境具备极强的自适应性、对新事物新环境具备自主学习能力）是人工智能学科探索的长期目标。随着脑与神经科学、认知科学的发展，在不同尺度下观测各种认知任务的脑神经网络的部分活动并获取相关数据已成为可能，人脑信息处理过程不再仅凭猜测，人脑工作机制也更具可靠性。因此发展类脑智能成为了人工智能和计算科学领域的热点。类脑智能是受脑神经机制和认知行为机制启发并通过软硬件协同实现的机器智能（可理解成根据人脑认知功能的神经机制，利用计算机模拟人脑的认知功能）。文中将从脑科学、认知科学、人工智能研究交叉的视角回顾类脑智能研究的历史、现状与研究焦点，展望该领域的发展方向、可能的应用领域及其深远影响。 
1 引言： 
经过近 6O 年的发展 ，人工智能学科已经奠定了若干重要的理论基础 ，并取得了诸多进展。 但在多认知功能协同和通用性智能方面，机器还与人类有明显差距。现有人工智能系统通用性较差与其计算理论基础和系统设计原理有密不可分的关系。 从计算基础角度（计算方式）讲，受制于图灵机与冯诺依曼体系结构，目前的智能系统在感知、认知、控制等多方面都存在巨大瓶颈。未来急需突破这种输入与处理形式相对固定的计算方式，取而代之的将是更为灵活的、更类人的信息处理与计算模式。 从问题求解原理角度讲，目前的人工智能系统都需要首先进行人工形式化建模，转化为一类特定的计算问题（如搜索、自动推理、机器学习等）进行处理。而人脑采用同一个信息处理系统进行自动感知、问题分析与求解、决策控制。 深度学习算法的提出，推动了很多特定领域机器智能水平的快速发展。但是深度学习的优越性仍然限于特定领域，依赖大量的标记样本，主要是离线学习，环境迁移和自适应能力较差。 大数据的兴起，给信息处理和智能技术提供了创新空间和发展前景。因为数据往往是非结构化的，目前机器对于这些数据的分析、理解和有效利用的能力仍然较弱，所以说大数据给智能技术带来了巨大的机遇和新的挑战。 人脑是如此的美妙：“除了人脑以外，没有任何一个系统具有对新环境新挑战的自适应能力、新信息与新技能的自动获取能力、在复杂环境下进行有效决策并稳定工作的能力。没有任何系统能够在多处损伤的情况下保持像人脑一样的鲁棒性，在处理同样复杂的任务时，没有任何人工系统能够媲美人脑的低能耗性。”因此，借鉴人脑的信息处理原理，发展类脑智能计算技术是实现人工智能创新的重要源泉。 
2 类脑智能的历史 
20世纪50年代，有学者初步论述了计算机与人脑信息处理机制的异同。 从脑信息处理机制出发推动人工智能研究最优的途径应当是受脑启发、借鉴其工作机制，而不是完全的模仿。 从借鉴脑信息处理机制的角度，下述工作可以看做是类脑智能的前期探索： 
2.1 认知科学中类脑智能研究的萌芽 
Newell 的具体探索是如何在计算机系统上重现人类思维。其探索被认为是以认知心理学为核心，对类脑智能的早期探索，其思想和成果汇聚为认知体系结构。两个重要的认知体系结构-SOAR 、ACT-R： SOAR-产生式系统，以认知统一理论为基石实现对人类各种认知功能的建模。 ACT-R-加入了认知神经科学研究关于脑信息处理机制的成果，基于脑影像技术的发展，ACT-R能够对不同认知功能的脑区环路进行计算建模，能够支持特定任务下脑区活动的预测。 
2.2 计算神经科学中类脑智能研究的萌芽 
Marr 不仅仅是计算机视觉的开拓者，他还在计算神经科学领域，奠定了神经元群之间存储、处理、传递信息的计算基础，特别是对学习与记忆、视觉相关环路的计算建模做出了重要贡献。 Hodgkin&Huxley创建了第一个精细的动作电位模型，Tsodysks构建了神经元之间的突触计算模型，是神经网络信息传递的计算基础。传统的计算神经科学更为关注神经系统表现出来的物理现象和微观尺度的建模，BBP计划试图通过计算模拟的方法在计算机上重建完整的脑模型，主要专注于极为精细的微观神经元及其微环路建模，目前较为完整地完成了特定区域内皮质柱的计算模拟。 
2.3 人工智能中的神经网络研究 
以人工神经网络为代表的连接主义的出发点正是对脑神经系统结构及其计算机制的初步模拟，早期的人工神经网络主要借鉴神经元、突触连接这些基本概念，而具体神经元的工作原理、突触的形成原理、网络结构则与脑神经网络存在巨大差异。 在人工神经网络的研究中，大多数学者主要关心提升网络学习的性能，部分学者使人工神经网络向更类脑的方向发展。过去在智能系统的实现机理上接近脑神经机理的重要成果并不多。 
3 类脑智能的现状 
3.1 类脑模型与类脑信息处理 
人工神经网络 深度神经网络的优缺-CNN RNN LSTM SPAUN语义指针架构统一网络-该脑模拟器是多脑区协同计算领域的标志性工作，实现了基于多脑区协同的能够实现多个特定任务的神经网络。 分层时序记忆模型-更为深度的借鉴了脑信息处理机制，该模型适用于处理带有时序信息的问题。 
3.2 类脑芯片与计算平台 
借鉴神经元信息处理机制（脑信息处理机制）发展类脑芯片与计算平台实在硬件层面发挥类脑智能优势的一大趋势。动机是通过借鉴脑神经系统的工作原理，实现高性能、低功耗的计算系统，终极目标还是达到高智能。 IBM的TrueNorth芯片 完全基于忆阻器的神经网络芯片 高通的神经处理器NPU 国内的DianNao深度学习系列芯片 
4 类脑智能的未来 
理想的智能系统应该具备：多模态协同感知与信息处理的能力、对复杂环境的自适应能力、对新事物新环境的自主学习能力。信息处理机制类脑、认知能力全面类人。 
4.1认知脑计算模型的构建 
传统人工智能系统的设计与实现思路是：从待解决问题相关数据的特点与问题目标的角度出发，从计算的视角设计算法。这使得所实现的智能系统只适用于解决某一类问题。 类脑智能研究的长期目标是实现通用智能系统，首要任务是集成科学界对于人脑多尺度结构及其信息处理机制的重要认识，构建模拟脑认知功能的认知脑计算模型。特别需要关注人脑如何协同不同尺度的计算组件，进行动态环路的组织，完成不同的认知任务。 在未来认知脑计算模型的研究中，需要基于 多尺度脑神经系统数据分析结果 对 脑信息处理系统 进行计算建模，构建类脑多尺度神经网络计算模型。 下述研究内容将对脑认知计算模型的发展至关重要： 1）多尺度、多脑区协同的认知脑计算模型：认知脑计算模型的多尺度计算组件、多脑区协同模型 2）认知行为的类脑学习机制：多模态协同与联想的自主学习机制，概念形成、交互式学习、环境自适应的机制。 3）基于不同认知功能协同实现复杂智能行为的类脑计算模型：通过计算建模实现哺乳动物脑模拟系统，实现具备感知、学习和记忆、知识表示、注意、推理、决策与判断、联想、语言等认知功能及其协同的类脑计算模型。 在认知脑计算模型的研究中处于最核心位置的是学习与记忆的计算模型。 脑神经网络的突触形成与信息传递有特定的生物工作机理支撑，所有认知任务的相关脑区中，学习与记忆遵循相同的法则：赫布学习法则、脉冲时序依赖的突触可塑性（STDP）。以往的人工神经网络模型大多没有采纳这些机制或者过于简化。 未来认知脑计算模型的研究应依据这些学习与记忆环路结构及相关学习理论构建多尺度的学习与记忆框架。 
4.2 类脑信息处理 
需要在认知脑计算模型的基础上进一步抽象，选取最优的策略和信息处理机制，建立类脑信息处理理论与算法。 类脑信息处理的研究目标：构建高度协同视觉、听觉、触觉、语言处理、知识推理等认知能力的多模态认知机。 类脑信息处理的关键研究内容： 1）类脑信息特征表达与语义识别模型：针对视觉（图像和视频）、听觉（语音和语言）、触觉等感知数据的分析和理解，借鉴脑神经机理和认知机理研究结果，研究感知信息的基本特征单元表示与提取方法、基于多层次特征单元的感知信息语义识别模型与学习方法、感知中的注意机制计算模型、结合特征驱动和模型驱动的感知信息语义识别方法。 2）多模态协同自主学习理论与方法：人脑的环境感知是多模态交互协同的过程，同时感知特征表示和语义识别模型在环境感知过程中不断地在线学习和进化。实现这种多模态协同的自主学习计算模型，对提高机器的多模态感知能力具有重要意义。实现的一种途径是结合多种特征表示和学习方法进行动态自适应的在线学习，同时对多种特征表示和语义识别模型进行适应。 3）多模态感知大数据处理与理解的高效计算方法：基于类脑信息特征表达与识别模型，研究面向感知大数据处理的新型计算模式与方法。如多层次特征抽取和识别方法，结合特征和先验知识、注意力机制的高层次高效学习、识别与理解等。 4）类脑语言处理模型与算法：借鉴人脑语言神经环路的结构和计算特点，实现类脑语言处理神经网络模型与算法。 
4.3 类脑芯片与类脑计算体系结构 
基于纳米等新型材料研制类脑忆阻器、忆容器、忆感器等神经计算元器件，从而支持更为复杂的类脑计算体系结构的构建。目前已经研发的神经芯片只是借鉴了脑信息处理最基本单元的最基本计算机之，而不同尺度的信息处理单元之间的机制尚未融入到类脑计算体系结构的研究中。 
4.4 类脑智能机器人与人机协同 
未来发展的趋势是基于认知脑计算模型、类脑信息处理的研究来构建机器脑，发展中枢神经系统和外周神经系统高度协同的、具有多模态感知、类人思维、自主学习与决策能力的类脑智能机器人。 类脑智能机器人的研究不但要机理上使其多尺度地接近人类，还要构建机器人自主学习与人机交互平台，使机器人在与人及环境自主交互的基础上实现智力水平的不断发育，最终甚至能够通过语言、动作、行为等与人类协同工作。 
4.5 类脑智能的应用 
适合于人类相对计算机更具优势的信息处理任务，如多模态感知信息处理、语言理解、知识推理、类人机器人与人机协同等。具体而言，类脑智能可用于机器的环境感知、交互、自主决策、控制等。 
Q&A 
1、三个领域如何交叉研究，如何利用计算科学、神经科学、认知科学发展类脑智能，又是如何相互促进的？ 2、类脑智能技术的发展现状，将来的发展方向、可能的应用领域？ 3、以图灵机和冯诺依曼体系为基础的计算方式是什么样的，更类人的计算方式又是什么样的？ 4、人脑是如何用同一个系统进行多种模态感知以及对这些信息进行处理的？ 5、深度学习算法地出现对于类脑智能发展有什么具体的影响，以及其不足之处？ 6、什么是离线学习？ 离线学习是指训练模型时的batch data都必须是可用的，先利用已有数据集训练好模型之后才能使用模型。缺点是不能利用实时数据更新模型。 在线学习按照顺序处理数据。先产生一个模型，并把这个模型放入实际操作中，而不需要在一开始就提供完整的的训练数据集。随着更多的实时数据到达，模型会在操作中不断地更新。 7、什么是认知心理学、计算神经科学？ 8、神经元信息处理机制具体是什么？ 9、什么是人脑不同尺度的计算组件？尺度如何定义？什么是动态环路？ 多尺度（神经元、突触、神经微环路、皮质柱、脑区） 10、什么是感知信息的基本特征单元？什么是感知信息的语义-（如视觉中的场景、文字、物体、行为等） 
4、类脑计算芯片与类脑智能机器人发展现状与思考 * 
摘要： 
首先分析了类脑器件与计算系统中的类脑芯片和类脑机器人的发展现状和应用前景，重点探讨了类脑芯片在模拟人脑神经元模型和认知计算方面、类脑机器人在感知控制和智能生长方面的研究内容。未来研究在仿人运动模型、类人神经运动控制、人机协同的智能机器人控制等方面有望取得重大突破。 
引言： 
忆阻器的出现使得类脑计算芯片的研究有了突飞猛进的发展，它可有效实现可调节突触强度的生物神经突触和神经元之间的互联，为类脑计算芯片的快速发展奠定基础。 
1、类脑芯片 
广义上讲，“类脑芯片”指模仿人脑神经元结构和人脑感知认知方式来设计的芯片。 类脑芯片的一个方向是“神经形态芯片”，侧重于模仿神经元模型及其组织结构来设计芯片结构。例如IBM开发的TrueNorth芯片 类脑芯片的另一个方向是“参考人脑感知认知模型，设计能支持认知计算算法的芯片”，如神经网络算法。例如“寒武纪”芯片 类脑芯片可以同时参考神经元组织结构并支撑成熟的认知计算算法，这并不矛盾。 
2、类脑智能机器人的仿生结构与感知控制 
仿肌肉驱动器是其中的重要部分，可以分为材料类、机械类和生物类。除了行为上类人，在认知上也要类脑，使机器人能够以类脑的方式实现对外界的感知和自身的决策控制。与类脑智能机器人相关的技术，如脑机接口、神经假体等，近几年也取得了积极地进展。脑机接口关键在于神经解码，把神经信号转化为对外部设备的控制信号，又分为侵入式脑机接口（在脑中植入电极阵列）、非侵入式脑机接口（用紧贴头皮的电极采集脑电图信号）。神经假体通过向脑区植入芯片，以达到恢复或者改变脑认知功能的目的，如视觉神经假体、运动神经假体等。 
3、类脑机器人的智能生长 
发展历程：离线状态下的人机交互->实时交互->类脑交互（认知合作）。 智能机器人行业将是未来“脑科学”和“脑认知和类脑计算”研究成果的重要产出方向。建立基于交互的学习及智力发育认知模型，使计算机能够在与人和环境的交互过程中实现智能增长。MIT人工智能实验室增量人机协同研究组采用增强学习让人与机器人在未知环境自由协作，在共同决策完成任务的同时，机器人也通过交互过程自主学习，实现智力发育。DeepMind提出了neural turing machine方法，利用深度增强学习，实现了靠不断试错学习获得提高的游戏人工智能。 
4、中科院B类先导专项下的类脑芯片与机器人研究工作 
机器人智力发育方面取得的积极进展： 1）模仿大脑在单样本或极少量样本条件下的基于交互学习的目标分类能力，构建新型类脑机器学习和认知模型，采用特征分析、无监督聚类、合并归纳等推理方法，实现单样本或极少量样本条件下同一物体的再次准确识别。 2）在机器人从零学习模型的基础上，采用语音交互和手势交互的模式教授计算机目标与环境知识，使得计算机具有从错误中学习新知识的能力。 3）在机器人基于交互的错误辨识纠正与记忆模型基础上，构建面向视觉、听觉、言语感知的多通道信息融合智能模仿模型，实现计算机借助视听觉方式对外界环境的智能增长学习，并具有一定模仿能力。 
5、基于类脑芯片与类脑智能机器人研究的进一步思考 
1）类脑计算芯片方面：发展类脑神经元计算模型，通过改变控制参数，使相同神经元电路模块能完成不同的神经元功能，增强神经计算电路模块的通用性。还需要迫切解决类脑芯片的功耗问题，需要研究建立神经网络处理器相关的功耗模型。 2）类脑智能机器人方面：需要研究心得机器人感知、交互和动作计算模型，从根本上提高机器人的智能型。尤其需要解决类人运动机构带来的类脑运动神经控制、人机融合环境带来的机器人多模态信息融合、交互式学习控制和双目可动摄像头带来的摄像头高速在线校准问题。 主要是类脑智能机器人比较火 
5、类脑智能与深度学习的几个问题与思考＊ 
1、深度学习的优缺 
在一些特定任务中取得了很大的飞跃，达到甚至超越人类水平，相关技术也已逐渐成熟落地到生活的多个方面，是现阶段人工智能的主流技术。其工作机理为依赖人类筛选出的训练数据，基于多层非线性变换来准确刻画和记忆数据，在封闭的环境下为特定的任务找到有用的数据模型。缺点是： 1）模型阐释困难：获得的每个特征与总体决策之间的关系如何，以及什么是提升深度学习系统的核心，这些问题仍然不具备可解释性。 2）模型认知缺陷：在知识抽象、自主学习、渐进学习、直觉与遗忘等认知智能层面的能力较弱。 3）模型学习瓶颈：学习算法的一致性收敛、稳定性、经验风险的泛化、梯度和驻点的属性，以及网络深度、宽度、大小等架构参数和网络权重参数等对算法收敛的影响等一系列问题，至今仍不明确。由于其深层的结构与庞大的参数，导致模型学习过程非常微妙，并且需要精细、耗时、耗力的超参数搜索、调试。这些问题都是模型学习中的瓶颈。 4）模型资源限制：大量资源消耗、大规模的标记数据，其训练和应用需要耗费大量的存储资源和计算资源。此外，现有的深度模型全局更新机制导致其配置与计算成本非常昂贵，无法在开放和泛在环境下获得实时的处理结果。 
2、未来类脑智能发展思考 
1）认知建模 2）模块装配 3）意识先验：有意识状态-可以用一个低维向量表示，无意识状态-可看作是一个高维但稀疏的向量。意识先验能够从无意识状态中提取出意识状态的少量要素，因此在认知建模过程中可以使用意识先验。 4）自主演化：自主学习以适应不断变化的环境，该能力来源于人脑中除了基本的兴奋与抑制性神经递质外，还有众多的神经调质。脑网络可能处于一个临界转态附近，通过对这一状态的微调，可以迅速调解网络功能，以适应变化的环境。 5）协同学习：大脑的协同机制为更高效实用的类脑模型提供了生物基础，深入分析微观、介观、宏观多尺度脑结构，实现协同学习的类脑智能计算模型。实现跨尺度机制的融合、多功能多模块的协同学习。 
6、多模态学习方法综述* 
摘要 
大数据是多源异构的，多模态数据已经成为近来数据资源的主要形式。研究多模态学习方法，赋予计算机理解多源异构数据的能力具有重要的价值。本文归纳了多模态的定义与多模态学习的基本任务，介绍了多模态学习的认知机理和发展过程。在此基础上，重点综述了多模态统计学习方法与多模态深度学习方法。此外，系统归纳了基于对抗学习的跨模态匹配与生成技术（应用）。并对多模态学习未来可能的研究方向进行思考与展望。 
引言 
本文结合大数据形态，重点综述了多模态统计学习方法、深度学习方法与对抗学习方法。 
1、多模态学习的定义、基本任务与发展过程 
1.1 多模态学习的定义 相较于图像、语音、文本等多媒体数据划分形式，模态是一个更为细粒度的概念，同一媒介下（比如说都是图像数据）可存在不同的模态。（该定义从模态和多媒体数据的定义的区别出发）“多模态”有三种形式： 1）描述同一对象的多媒体数据 如互联网环境下描述某一特定对象的多媒体数据，例如描述下雪场景（特定对象）的视频、图像、语音、文本等数据，这就是典型的多模态信息形式。 2）来自不同传感器的同一类媒体数据 如医学影像学中不同的检查设备对于同一对象所产生的不同的图像数据（都属于图像数据，属于同一媒体），包括B超、CT、核磁共振等。 3）描述同一对象的具有不同的数据结构特点、表示形式的表意符号与信息 如描述同一对象的结构化、非结构化的数据单元；描述同一数学概念的公式、逻辑符号、函数图及解释性文本等；描述同一语义的词向量、词袋、知识图谱等。 数据类型有图片、数值、文本、符号、音频、时间序列，或者集合、树、图等不同数据结构所组成的复合数据形式。 多模态学习（mutimodal machine learning）可以被理解为对于多源异构数据的分析挖掘。 1.2 多模态学习的基本任务 多源数据分类： 多模态情感分析：本质也是分类问题，情感分类问题所提取的特征往往带有明确的情绪信号。 多模态语义计算：语义分析是对数据更为高层次的处理，理想状态下，计算机能够处理一个特定场景下不同数据的概念关系、逻辑结构，进而理解不同数据中隐含的高层语义。 跨模态样本匹配：现阶段，最常见的跨模态样本匹配即为图像、文本的匹配。 跨模态检索：除了实现匹配外，还要求快速的响应速度以及正确的排序。多模态信息检索通过对异构数据进行加工，在有效特征匹配的情况下对图像采用基于内容的自动检索形式。哈希方法：将不同模态的高维数据映射到低维的海明空间，有效减小了数据存储空间。 跨模态样本生成：目前由图像到文本、图像到图像的生成较为成熟，由文本到图像的生成任务较为新颖。 多模态人机对话：在基本对话（文本模态）生成任务的基础上，进一步对人的表情、语调、姿势等多模态信息采集，采用模态融合的方法对多模态信号进行分析处理。 多模态信息融合：常见的信息融合方式有物理层融合、特征层融合、决策层融合几个类型。物理层融合指在传感器层级对采集到的数据进行融合处理，即多传感器信息融合，在工业生产场景中最常见的信息融合方法。特征层融合指在特征抽取和表达的层级对信息进行融合，例如对不同摄像头采集到的图像数据采用相同的特征表达形式，进而进行相应的叠加计算。决策层融合指对不同模态的感知模型所输出的结果进行融合，这种融合方式对传感器性能和种类要求相对不高，但具有较大的信息损耗。 1.3 多模态机器学习的发展-从符号计算到深度学习 
2 多模态统计学习方法 
广义的统计学习方法即采用统计学的理论，赋予计算机处理数据能力的机器学习方法。数据建模-预设数据符合某种分布形式，进而进行参数估计和假设推断；算法建模-通过算法去直接寻找映射函数以达到预测输出的目的。统计学习方法处理的样本往往是中小规模的数据集，需要人为的提取特征。本节将结合多模态数据的特点，对相应的统计学习方法进行介绍： 
2.1 核学习方法与多核学习 
核学习方法是将低维不可分样本通过核映射的方式映射到高维非线性空间，实现对样本有效分类的方法，任何半正定函数都可以作为核函数。 多核学习方法为不同模态的数据和属性选取不同的核函数，进而采用特定方法对不同核函数进行融合。多核学习方法可以较好地处理异构数据的分类和识别问题，早期的多核数据融合方法多采用对不同核进行线性叠加组合的形式，还可以采用核组合的方式解决该问题。 
2.2 典型相关性分析 
CCA被广泛地应用于度量两种模态信息之间的相关特征，并在计算中尽可能保持这种相关性，其本质是一种线性映射。 
2.3 共享子空间学习 
对于底层的特征表示，不同来源的数据往往具有较大差别，共享子空间学习对多源数据的相关关系进行挖掘，得到多模态特征的一致性表示。共享子空间学习可通过投影的方式实现，最常见的投影方法即CCA方法及其改进形式。对不同模态的特征进行映射、表示和整合。绝大多数深度学习多源信息处理方法将不同模态的数据通过深度学习网络特征学习映射到了同一个共享子空间，因此深度学习方法也被视为共享子空间学习。 
2.4 协同训练方法 
大致思想为分别采用两个模态的有标签数据训练两个分类器，进而用这两个分类器对各自模态内的无标签数据进行处理。在此基础上，将分类结果中达到一定置信度的样本作为训练集的补充，扩大训练集规模，进一步对分类器进行训练。在满足一定停止条件之后，将两个分类器的训练数据进行交换，即采用X1模态中的数据对分类器2进行训练，用X2模态中的数据对分类器1进行训练。 
3 多模态深度学习方法 
3.1 卷积神经网络与图像处理 3.2 循环神经网络与自然语言理解 3.3 面向多模态数据的深度学习 始于《Multimodal Deep Learning》，发展过程参看几篇文献（原文中） 
4 多模态对抗学习方法 
跨模态迁移和跨模态生成是多模态学习的常见任务。 4.1 基于对抗学习的跨模态迁移与域适应 4.2 基于对抗学习的跨模态生成 Q&A 1 表意符号与信息指什么？ 2 什么是图像文本匹配任务？度量一幅图像和一段文本的相似性，现有方法按照对图像文本对应关系建模方式的不同主要可以被分为两大类：1)一对一匹配和2)多对多匹配 
Part3 与类脑智能技术相关的学科 
1 生物脑科学、计算机科学 
2 神经科学neuroscience 
（又称为神经生物学，是专门研究神经系统的结构、功能、发育、演化、遗传学、生物化学、生理学、药理学及病理学的一门科学） 
3 认知科学cognitive science 
（对于什么是认知科学，还存在着很大的分歧。认知科学定义为“关于智能实体与它们的环境相互作用的原理的研究”。沿着两个方向展开这一定义：第一个是外延的，列举了人认知科学的分支领域以及它们之间的交叉联系，列举的分支领域有计算机科学、心理学、哲学、语言学、人类学和神经科学。第二种展开是内涵的，指出共同的研究目标是“发现心智的表征和计算能力以及它们在人脑中的结构和功能表示”。）wiki 
4 认知神经科学cognitive neuroscience 
（旨在探讨认知历程的生物学基础。主要的目标为阐明心理历程/认知功能的神经机制，也就是大脑的运作如何造就心理或认知功能。认知神经科学以认知科学的理论以及神经心理学、神经科学及计算机模型的实验证据为基础）wiki 
5 计算神经科学 
（计算神经科学是使用数学分析和计算机模拟的方法在不同水平上对神经系统进行模拟和研究，为一种跨领域科学，包含神经科学、认知科学、信息工程、计算机科学、物理学及数学。旨在从计算的角度解析脑的认知功能，以计算建模为手段，研究脑神经信息处理原理，通过多尺度计算模型的方法验证各种认知功能的脑信息处理模型。）来自百度百科 没有一流的计算神经科学， 就不可能获得一流的类脑智能技术创新。计算神经科学侧重对脑系统的理论建模和计算仿真， 通过该学科严谨的理论、建模和统计分析， 从而对复杂的、 非线性的大脑系统进行深入剖析和建模分析， 尤其是大尺度的认知功能脑系统计算模拟、 跨层次机制分析， 计算神经科学搭建起了计算机科学和脑科学之间的桥梁， 这将为类脑智能、 脑机接口和人工智能的发展起到关键作用。 
6 认知心理学cognitive pyschology 
（是心理学的分支。研究人的高级心理过程，主要是认知过程，如注意、知觉、表象、记忆、思维和语言等。与行为主义心理学家相反，认知心理学家研究那些不能观察的内部机制和过程，以信息加工观点研究认知过程是现代认知心理学的主流，可以说认知心理学相当于信息加工心理学。）博客：初试认识心理学 其中计算神经科学与认知神经科学最为重要。 
7计算神经科学在脑认知与脑疾病中的研究"
AI芯片市场现状及企业竞争状况,2019-03-26,"https://www.toutiao.com/a6672168138810851853/ 
  
随着大数据的发展，计算能力的提升，AI算法进一步成熟，谁抢占了AI芯片高地，谁就拥有了市场主导权。纵观AI芯片市场，可谓战火纷纷，群雄逐鹿。 
一、AI芯片整体规模 
据前瞻产业研究院发布的《中国人工智能行业市场前瞻与投资战略规划分析报告》统计数据显示，2015中国人工智能市场规模已突破100亿元，到了2016年人工智能市场规模达到142亿元，截止到2017年人工智能市场规模达到了217亿元，预计2018年人工智能市场规模将达339亿元，并预测在2019、2020年人工智能市场规模将达500亿元、710亿元。2015-2020年复合年均增长率为44.5%。 
麦肯锡(McKinsey)的预测显示出惊人的数据：2017年至2025年，人工智能类半导体将成为半导体市场的领头羊，其年复合增长率将比其它所有半导体的总和高出5倍。Tractica的一项调查将进一步解读了这种增长：将中央处理器（CPU）与图形处理器、现场可编程门阵列、特定用途集成电路进行比较。到2019年，基于中央处理器的营业额将从30亿美元左右起步，到2025年将增长到120亿美元左右。基于图形处理器的系统(GPU)的收入将在2019年接近60亿美元，到2025年将增长到约200亿美元。现场可编程门阵列(FPGA)的贡献非常小，到2025年可能只有10亿美元左右。但特定用途集成电路(ASIC)市场份额将从2019年的约20亿美元增长到2025年的约300亿美元。到2022年左右，基于特定用途集成电路的人工智能将在份额上超过基于图形处理器的人工智能。 
二、市场发展环境 
我国人工智能新一轮利好政策正密集落地。《经济参考报》记者获悉，2019年年初，包括成都、浙江在内的多个省市相继出台人工智能产业发展规划，着力加大智能芯片、智能传感器等基础层技术攻关与资金支持，加快培育人工智能产业集聚区和领军企业。业内预计，以基础层为核心的AI芯片投资空间广阔。 
三、AI芯片市场需求及企业竞争状况 
目前AI芯片的市场需求主要是三类： 
1、面向于各大人工智能企业及实验室研发阶段的Training需求(主要是云端，设备端Training需求尚不明确)； 
2、InferenceOnCloud，Face++、出门问问、Siri等主流人工智能应用均通过云端提供服务； 
3、InferenceOnDevice，面向智能手机、智能摄像头、机器人/无人机、自动驾驶、VR等设备的设备端推理市场，需要高度定制化、低功耗的AI芯片产品。如华为麒麟970搭载了""神经网络处理单元(NPU，实际为寒武纪的IP)""、苹果A11搭载了""神经网络引擎(NeuralEngine)""。 
  
 
  
  
（一）Training训练 
2007年以前，人工智能研究受限于当时算法、数据等因素，对于芯片并没有特别强烈的需求，通用的CPU芯片即可提供足够的计算能力。AndrewNg和JeffDean打造的GoogleBrain项目，使用包含16000个CPU核的并行计算平台，训练超过10亿个神经元的深度神经网络。但CPU的串行结构并不适用于深度学习所需的海量数据运算需求，用CPU做深度学习训练效率很低，在早期使用深度学习算法进行语音识别的模型中，拥有429个神经元的输入层，整个网络拥有156M个参数，训练时间超过75天。 
与CPU少量的逻辑运算单元相比，GPU整个就是一个庞大的计算矩阵，GPU具有数以千计的计算核心、可实现10-100倍应用吞吐量，而且它还支持对深度学习至关重要的并行计算能力，可以比传统处理器更加快速，大大加快了训练过程。 
  
 
  
  
从上图对比来看，在内部结构上，CPU中70%晶体管都是用来构建Cache(高速缓冲存储器)和一部分控制单元，负责逻辑运算的部分(ALU模块)并不多，指令执行是一条接一条的串行过程。GPU由并行计算单元和控制单元以及存储单元构成，拥有大量的核(多达几千个)和大量的高速内存，擅长做类似图像处理的并行计算，以矩阵的分布式形式来实现计算。同CPU不同的是，GPU的计算单元明显增多，特别适合大规模并行计算。 
人工智能的通用计算GPU市场，NVIDIA现在一家独大。2010年NVIDIA就开始布局人工智能产品，2014年发布了新一代PASCALGPU芯片架构，这是NVIDIA的第五代GPU架构，也是首个为深度学习而设计的GPU，它支持所有主流的深度学习计算框架。2016年上半年，NVIDIA又针对神经网络训练过程推出了基于PASCAL架构的TESLAP100芯片以及相应的超级计算机DGX-1。DGX-1包含TESLAP100GPU加速器，采用NVLINK互联技术，软件堆栈包含主要深度学习框架、深度学习SDK、DIGITSGPU训练系统、驱动程序和CUDA，能够快速设计深度神经网络(DNN)，拥有高达170TFLOPS的半精度浮点运算能力，相当于250台传统服务器，可以将深度学习的训练速度加快75倍，将CPU性能提升56倍。 
Training市场目前能与NVIDIA竞争的就是Google。今年5月份Google发布了TPU2.0，TPU(TensorProcessingUnit)是Google研发的一款针对深度学习加速的ASIC芯片，第一代TPU仅能用于推理，而目前发布的TPU2.0既可以用于训练神经网络，又可以用于推理。据介绍，TPU2.0包括了四个芯片，每秒可处理180万亿次浮点运算。Google还找到一种方法，使用新的计算机网络将64个TPU组合到一起，升级为所谓的TPUPods，可提供大约11500万亿次浮点运算能力。Google表示，公司新的深度学习翻译模型如果在32块性能最好的GPU上训练，需要一整天的时间，而八分之一个TPUPod就能在6个小时内完成同样的任务。目前Google并不直接出售TPU芯片，而是结合其开源深度学习框架TensorFlow为AI开发者提供TPU云加速的服务，以此发展TPU2的应用和生态，比如TPU2同时发布的TensorFlowResearchCloud(TFRC)。 
上述两家以外，传统CPU/GPU厂家Intel和AMD也在努力进入这Training市场，如Intel推出的XeonPhi+Nervana方案，AMD的下一代VEGA架构GPU芯片等，但从目前市场进展来看很难对NVIDIA构成威胁。初创公司中，Graphcore的IPU处理器(IntelligenceProcessingUnit)据介绍也同时支持Training和Inference。该IPU采用同构多核架构，有超过1000个独立的处理器；支持All-to-All的核间通信，采用BulkSynchronousParallel的同步计算模型；采用大量片上Memory，不直接连接DRAM。 
总之，对于云端的Training(也包括Inference)系统来说，业界比较一致的观点是竞争的核心不是在单一芯片的层面，而是整个软硬件生态的搭建。NVIDIA的CUDA+GPU、Google的TensorFlow+TPU2.0，巨头的竞争也才刚刚开始。 
（二）InferenceOnCloud云端推理 
相对于Training市场上NVIDIA的一家独大，Inference市场竞争则更为分散。若像业界所说的深度学习市场占比(Training占5%，Inference占95%)，Inference市场竞争必然会更为激烈。 
在云端推理环节，虽然GPU仍有应用，但并不是最优选择，更多的是采用异构计算方案(CPU/GPU+FPGA/ASIC)来完成云端推理任务。FPGA领域，四大厂商(Xilinx/Altera/Lattice/Microsemi)中的Xilinx和Altera（被Intel收购）在云端加速领域优势明显。Altera在2015年12月被Intel收购，随后推出了Xeon+FPGA的云端方案，同时与Azure、腾讯云、阿里云等均有合作；Xilinx则与IBM、百度云、AWS、腾讯云合作较深入，另外Xilinx还战略投资了国内AI芯片初创公司深鉴科技。目前来看，云端加速领域其他FPGA厂商与Xilinx和Altera还有很大差距。 
  
 
  
  
ASIC领域，应用于云端推理的商用AI芯片目前主要是Google的TPU1.0/2.0。其中，TPU1.0仅用于DatacenterInference应用。它的核心是由65,536个8-bitMAC组成的矩阵乘法单元，峰值可以达到92TeraOps/second(TOPS)。有一个很大的片上存储器，一共28MiB。它可以支持MLP，CNN和LSTM这些常见的神经网络，并且支持TensorFLow框架。它的平均性能(TOPS)可以达到CPU和GPU的15到30倍，能耗效率(TOPS/W)能到30到80倍。如果使用GPU的DDR5memory，这两个数值可以达到大约GPU的70倍和CPU的200倍。TPU2.0既用于训练，也用于推理，上一节已经做过介绍。 
国内AI芯片公司寒武纪科技据报道也在自主研发云端高性能AI芯片，目前与科大讯飞、曙光等均有合作，但目前还没有详细的产品介绍。 
（三）InferenceOnDevice设备端推理 
设备端推理的应用场景更为多样化，智能手机、ADAS、智能摄像头、语音交互、VR/AR等设备需求各异，需要更为定制化、低功耗、低成本的嵌入式解决方案，这就给了创业公司更多机会，市场竞争生态也会更加多样化。 
1）智能手机 
华为9月初发布的麒麟970AI芯片就搭载了神经网络处理器NPU(寒武纪IP)。麒麟970采用了TSMC10nm工艺制程，拥有55亿个晶体管，功耗相比上一代芯片降低20%。CPU架构方面为4核A73+4核A53组成8核心，能耗同比上一代芯片得到20%的提升；GPU方面采用了12核MaliG72MP12GPU，在图形处理以及能效两项关键指标方面分别提升20%和50%；NPU采用HiAI移动计算架构，在FP16下提供的运算性能可以达到1.92TFLOPs，相比四个Cortex-A73核心，处理同样的AI任务，有大约50倍能效和25倍性能优势。 
苹果最新发布的A11仿生芯片也搭载了神经网络单元。据介绍，A11仿生芯片有43亿个晶体管，采用TSMC10纳米FinFET工艺制程。CPU采用了六核心设计，由2个高性能核心与4个高能效核心组成。相比A10Fusion，其中两个性能核心的速度提升了25%，四个能效核心的速度提升了70%；GPU采用了苹果自主设计的三核心GPU图形处理单元，图形处理速度与上一代相比最高提升可达30%之多；神经网络引擎NPU采用双核设计，每秒运算次数最高可达6000亿次，主要用于胜任机器学习任务，能够识别人物、地点和物体等，能够分担CPU和GPU的任务，大幅提升芯片的运算效率。 
另外，高通从2014年开始也公开了NPU的研发，并且在最新两代骁龙8xx芯片上都有所体现，例如骁龙835就集成了""骁龙神经处理引擎软件框架""，提供对定制神经网络层的支持，OEM厂商和软件开发商都可以基于此打造自己的神经网络单元。ARM在今年所发布的Cortex-A75和Cortex-A55中也融入了自家的AI神经网络DynamIQ技术，据介绍，DynamIQ技术在未来3-5年内可实现比当前设备高50倍的AI性能，可将特定硬件加速器的反应速度提升10倍。总体来看，智能手机未来AI芯片的生态基本可以断定仍会掌握在传统SoC商手中。 
2）自动驾驶 
NVIDIA去年发布自动驾驶开发平台DRIVEPX2，基于16nmFinFET工艺，功耗高达250W，采用水冷散热设计；支持12路摄像头输入、激光定位、雷达和超声波传感器；CPU采用两颗新一代NVIDIATegra处理器，当中包括了8个A57核心和4个Denver核心；GPU采用新一代Pascal架构，单精度计算能力达到8TFlops，超越TITANX，有后者10倍以上的深度学习计算能力。Intel收购的Mobileye、高通收购的NXP、英飞凌、瑞萨等汽车电子巨头也提供ADAS芯片和算法。初创公司中，地平线的深度学习处理器(BPU，BrainProcessorUnit)IP及其自研雨果(Hugo)平台也是重点面向自动驾驶领域。 
3）计算机视觉领域 
Intel收购的Movidius是主要的芯片提供商，大疆无人机、海康威视和大华股份的智能监控摄像头均使用了Movidius的Myriad系列芯片。目前国内做计算机视觉技术的公司中，商汤科技、Face++、云从、依图等，未来有可能随着其自身计算机视觉技术的积累渐深，部分公司向上游延伸去做CV芯片研发。另外，国内还有如人人智能、智芯原动等创业公司提供摄像头端的AI加速IP及芯片解决方案。 
4）其他 
VR设备芯片的代表为微软为自身VR设备Hololens而研发的HPU芯片，这颗由台积电代工的芯片能同时处理来自5个摄像头、一个深度传感器以及运动传感器的数据，并具备计算机视觉的矩阵运算和CNN运算的加速功能；语音交互设备芯片方面，国内有启英泰伦以及云知声两家公司，其提供的芯片方案均内置了为语音识别而优化的深度神经网络加速方案，实现设备的语音离线识别；在泛IOT领域，NovuMind设计了一种仅使用3×3卷积过滤器的AI芯片，第一款芯片原型预计今年底推出，预计可实现耗能不超过5瓦进行15万亿次浮点运算，可以广泛应用于各类小型的互联网""边缘""设备。 
（四）新架构-类脑计算芯片 
""类脑芯片""是指参考人脑神经元结构和人脑感知认知方式来设计的芯片，其目标是开发出打破冯·诺依曼架构体系的芯片。这一领域目前仍处于探索阶段，如欧盟支持的SpiNNaker和BrainScaleS、斯坦福大学的Neurogrid、IBM公司的TrueNorth以及高通公司的Zeroth等；国内Westwell、清华大学、浙江大学、电子科技大学等也有相关研究。 
IBM的TrueNorth，2014年公布。在一颗芯片上集成了4096个内核，100万个神经元、2.56亿个可编程突触，使用了三星的28nm的工艺，共540万个晶体管；每秒可执行460亿次突触运算，总功耗为70mW，每平方厘米功耗20mW。IBM的最终目标就是希望建立一台包含100亿个神经元和100万亿个突触的计算机，这样的计算机要比人类大脑的功都强大10倍，而功耗只有一千瓦，而且重量不到两升。 
国内AI初创公司西井科技Westwell是用FPGA模拟神经元以实现SNN的工作方式，有两款产品： 
1、仿生类脑神经元芯片DeepSouth(深南)，第三代脉冲神经网络芯片SNN，基于STDP(spike-time-dependentplasticity)的算法构建完整的突触神经网络，由电路模拟真实生物神经元产生脉冲的仿生学芯片，通过动态分配的方法能模拟出高达5000万级别的""神经元""，功耗为传统芯片在同一任务下的几十分之一到几百分之一。 
2、深度学习类脑神经元芯片DeepWell(深井)，处理模式识别问题的通用智能芯片，基于在线伪逆矩阵求解算法(OPIUMlite)对芯片中神经元间的连接权重进行学习和调整；拥12800万个神经元，通过专属指令集调整芯片中神经元资源的分配；学习与识别速度远远高于运行在通用硬件(如CPU,GPU)上的传统方法(如CNN)，且功耗更低。 
总体来看，类脑计算芯片领域仍处于探索阶段，距离规模化商用仍有比较远的距离。"
人类的 POWER IBM 的芯片制造历史,2011-03-19,"人类的 POWER 
IBM 的芯片制造历史 
 

 
 Nora Mikes ( 
 jskelly@jskelly.com), 自由作家 
 

Nora Mikes 在 1992 年就爱上了计算机这个行业，在 1993 年开始涉足 Internet 领域，1995 年陷入了 UNIX 的爱河。她对写作方面的恋情从 1974 年开始一直延续至今。您可以通过 
 jskelly@jskelly.com 与她联系。 
 
  
简介： 在最近 10 年中，IBM 在半导体领域实现了一个又一个的突破：铜技术，绝缘硅，硅锗合金，应变硅和 low-k 绝缘体。所有这些技术都滋生于 IBM 肥沃的研究土壤。这种对现代芯片制造知根知底的能力并非是空穴来风，而是来自于半导体工业中最先进的 R & D 部门的封闭且洁净的实验室。 
 
  
 
 标记本文！ 
 
 

 
 


发布日期： 2004 年 4 月 01 日 级别： 初级 访问情况 619 次浏览 建议: 0 (添加评论) 


 





平均分 （共 3 个评分 ） 
 
 
  

 
 
 
 


 
 
 

 最初，每个计算机的中央处理单元（或称为 CPU）都是惟一的。每个 CPU 都有自己的指令集，且与其他 CPU 的指令集不兼容。这种情况的变化要回溯到 1964 年引入 IBM S/360 产品线的电子管年代：突然之间，我们不需要每次新买一台计算机时都丢弃原来的代码并重新设计代码了。现在的 IBM 大型机依然保持着对 1962 年这种革命性的指令集的向后兼容性。IBM 其他的 CPU 产品线也保持着相同的兼容性。 
 从用户模式来看，PowerPC? 系列处理器对应用提供了完全的兼容性：从最底层的自动交通指示灯到功能强大的 Apple Xserve G5。另外，PowerPC 微处理器还与 IBM 的其他 RISC 处理器产品线 POWER? 和 Star 共用一大组通用指令集，这样就使得这三条产品线保持着“近似”的兼容性。在很多情况中，这等价于二进制兼容；在有些情况中，则意味着需要进行简单的重编译；不论如何，这都意味着程序移植已经是小菜一碟了。 
 IBM 的四条处理器产品线 -- POWER 体系结构，PowerPC 系列的处理器，Star 系列，以及 IBM 大型机上所采用的芯片 -- 都有一个共同的祖先：IBM 801。 
 CPU 的族谱 
 IBM 801 的目标是解决 20 世纪 70 年代很多计算机面临的相同问题：电话呼叫的转换。设计小组的目标是在每个时钟周期内完成一条指令，从而每分钟可以处理 300 个电话。
 当时的大部分计算机，例如 IBM S/360 大型机，都有复杂的冗余指令集，我们今天称之为 CISC（复杂指令集计算机）。计算机的趋势是日益小型化，而 1947 年开始的晶体管的变革更加加剧了这一趋势的变化。随着集成电路越来越小，设计人员可以利用更多的空间在芯片中实现更多指令。到 20 世纪 70 年代，芯片的复杂度已经发展到芯片可以实现非常不可思议的事情（例如日益复杂的数字钟表）。但是另外一个事实是芯片执行指令时需要更多的机器时间，这使得 801 项目组无法实现自己的性能指标。 
 IBM 的 John Cocke 在实现复杂性领域并不是生手，他早就从事于 IBM Stretch 计算机的研究，IBM Stretch 计算机是 IBM 704大型机的一个竞争对手，它的继承者是 ACS （高级计算机系统）；而 704 的继承者是 S/360，二者也是竞争对手。 

 
兼容性设计 
 
 PowerPC 体系结构的指令集分为三级，称为“books”。Book I 是基本的寄存器和指令集，所有的 PowerPC 实现都通用。Book II 定义了其他一些用户级的功能，这些功能超出了应用软件通常的一些要求。Book III 定义了一些特权操作，特别是操作系统需要的一些特权指令。要了解 PowerPC 体系结构的更详细介绍以及程序员参考手册，请参阅“ PowerPC 体系结构开发者指南”。
 

 他对指令集进行了拆分，并设计了一台只有当时同类计算机一半电路的机器，但是其运行速度却是同类计算机的两倍。快速的核心和更少的电路带来的不仅仅是性能的提升，而且功耗也更少（这对于现在的很多用户来说都是最重要的一个问题），成本也更低。这种体系结构称为 RISC（精简指令集计算机）。有些人喜欢称 RISC 为“load-store”，意思是想强调 RISC 计算机只有 100 多条指令（POWER 体系结构也是如此）。其他的 RISC 计算机并没有采用一套精简指令集，而是采用一组精简过的指令：CISC 的每条复杂指令都被拆分成更小的基本单元，然后再组合起来实现复杂指令的功能。 
 无论如何，CPU 中的复杂性并没有简单地消失，而是转嫁到编译器中了。为了更好地实现这种功能，John Cocke 不仅成为编译器领域的专家，而且在编译器的优化方面特有专长。他在 RISC 和编译器优化方面的工作为他赢得了很多荣誉，其中包括 1987 年的图灵奖。 
 就 IBM 801 而言，它从来都没有成为一个电话转换器。相反，IBM 801 后来成为第一个 RISC 芯片，在很多 IBM 的硬件产品中广泛采用 -- 曾经有一段时间，它作为一个微控制器和处理器甚至限制了其竞争对手 IBM 大型机的发展。 
 RISC 体系结构在工作站和嵌入式市场中很快占据了主导地位，John Cocke 之后又转入其他项目的研究之中。在 20 世纪 80 年代，他有机会在一个后来被命名为“America”的项目中重新提炼 801 的设计，这个项目后来就成为 POWER 系列的芯片。之后的几年中，他甚至在 PowerPC 体系结构的开发中贡献了很大的力量。与 801 类似，PowerPC 也被设计成为可以在任何机器上运行的一个通用微处理器，它在从最高端到最低端的机器上都可以运行。 
 现在，RISC 体系结构是惟一一种最通用的 CPU，它是很多平台的基础：从工作站到蜂窝电话，从视频游戏终端到超级计算机，从交通指示灯到桌面系统，从宽带调制解调器到自动加油站和防撞系统。甚至 x86 的制造商（他们在相当长的一段时间内都生产 CISC 芯片）也基于 RISC 体系结构研制了他们的第五代和第六代芯片，并将 x86 的操作码转换成 RISC 操作，以保持向后兼容。 

 

 回页首 
 POWER 
 POWER 是 Power Optimization With Enhanced RISC 的缩写，是 IBM 的很多服务器、工作站和超级计算机的主要处理器。POWER 芯片起源于 801 CPU，是第二代 RISC 处理器。POWER 芯片在 1990 年被 RS 或 RISC System/6000 UNIX 工作站（现在称为 eServer 和 pSeries）采用，POWER 的产品有 POWER1、POWER2、POWER3，现在最高端的是 POWER4。POWER4 处理器是目前单个芯片中性能最好的芯片。
 801 的设计非常简单。但是由于所有的指令都必须在一个时钟周期内完成，因此其浮点运算和超量计算（并行处理）能力很差。POWER 体系结构就着重于解决这个问题。POWER 芯片采用了 100 多条指令，是非常优秀的一个 RISC 体系结构。 
 以下对每种 POWER 芯片简单进行一下介绍；更详细的内容请参考 参考资料中的链接。
 POWER1 发布于 1990 年：每个芯片中集成了 800,000 个晶体管。 与当时其他的 RISC 处理器不同，POWER1 进行了功能划分，这为这种功能强大的芯片赋予了超量计算的能力。它还有单独的浮点寄存器，可以适应从低端到高端的 UNIX 工作站。最初的 POWER1 芯片实际上是在一个主板上的几个芯片；后来很快就变成一个 RSC（RISC 单一芯片），其中集成了 100 多万个晶体管。POWER1 微处理器的 RSC 实现被火星探险任务用作中央处理器，它也是后来 PowerPC 产品线的先驱。 POWER2 发布于 1993 年，一直使用到 1998 年：每个芯片中集成了 1500 万个晶体管。 POWER2 芯片中新加了第二个浮点处理单元（FPU）和更多缓存。PSSC 超级芯片是 POWER2 这种 8 芯片体系结构的一种单片实现，使用这种芯片配置的一个 32 节点的 IBM 深蓝超级计算机在 1997 年击败了国际象棋冠军 Garry Kasparov。 POWER3 发布于 1998 年：每个芯片中集成了 1500 万个晶体管。 第一个 64 位对称多处理器（SMP），POWER3 完全兼容原来的 POWER 指令集，也可以与 PowerPC 指令集很好地兼容。POWER3 设计用来从事从太空探测到天气预报方面的科技计算应用。它特有一个数据预取引擎，无阻塞的交叉数据缓存，双浮点执行单元，以及其他一些很好的设计。POWER3-II 使用铜作为连接介质重新实现了 POWER3，这样以相同的价格可以获得两倍的性能。 POWER4 发布于 2001 年：每个芯片中集成了 1 亿 7400 万个晶体管。 采用 0.18 微米的铜和 SoI（绝缘硅）技术，POWER4 是目前市场上单个芯片功能最强大的芯片。POWER4 继承了 POWER3 芯片的所有优点（包括与 PowerPC 指令集的兼容性），但是采用的却是全新的设计。每个处理器都有 2 个 64 位的 1GHz+ 的PowerPC 核心，这是第一个单板上具有多核心设计的服务器处理器（也称为“片上 CMP”或“片上服务器”）。每个处理器都可以并行执行 200 条指令。POWER4 芯片取代了 Star 系列的处理器，是 IBM Regatta 服务器强大的动力之源，也是 PowerPC 970 处理器（也称为 Apple G5）的祖先。POWER4+?（也称为 POWER4-II）功能与之类似，但是主频更高，功耗更低。 POWER5? 计划 2004 年发布。 与 POWER3 和 POWER4 芯片类似，POWER5 是 POWER 和 PowerPC 体系结构的一种综合体。这种芯片具有很多特性，例如通信加速、芯片多处理器、同步多线程，等等，据报告声称其性能超过 POWER4 芯片 50% 到 100%。POWER5 芯片将装备一条新的服务器产品线，这些服务器代号为“Squadron”，将于 2004 年下半年在劳伦斯利物浦发布。 POWER6? 计划 2006 年发布。 资料保密中。
   

 

 回页首 
 Star 系列 
 RS64 芯片首次于 1998 年面世，在 IBM 内部称之为 Star 系列，这是因为大部分代码字节中都包含了单词“star”或类似的单词（其中一个比较出名的例外是最初的 RS64，其代号是“Apache”）。
 Star 系列芯片源自于对 PowerPC 体系结构的修改，同时还从 POWER 产品线中继承了很多特点。从一开始起，这些芯片就只针对于一种应用进行优化：商业应用。这种专用化使其在 UNIX 服务器领域几乎在 10 年的时间中都牢居霸主地位。 
 RS64 系列将诸如分支预测、浮点处理以及硬件预取之类的问题留给其兄弟 POWER3 芯片来解决，自己则专注于整数运算性能和大型复杂的片上、片外缓存的处理。RS64 系列从面世以来就一直是 64 位的，2000 年在 RS64 IV 中引入了多线程的设计。RS64 可以在一台机器内扩展到多达 24 个处理器，功耗则只需要每个处理器 15 瓦即可，这一点与其兄弟 POWER 芯片有很大的区别。 
 这些特性使 RS64 芯片非常适合一些系统，例如联机事务处理（OLTP）、商业智能、企业资源计划（ERP）以及其他一些大型的、功能强大的、具有多用户和多任务而缓存命中率很低的系统，其中包括 Web 服务。RS64 芯片只装备在 IBM 的 eServer iSeries（RS 系列）和 pSeries（AS 系列）服务器中。
 RS64 发布于 1997 年，代号名：Apache。 第一个 RS64 芯片，也是世界上第一个 64 位的 PowerPC RISC 芯片。其超量运算能力和可扩展性都很好，与 POWER1 芯片的兼容性比后来的 RS64 芯片都要好。由于对商业应用的专著，原来需要使用 7 个芯片的功能现在只需要一个芯片就可以实现。RS64 装备在 AS/400（后来称为 A35）和 RS/6000 中。 RS64 II 发布于 1998 年，代号名：Northstar。 第二代 RS64 芯片，其特点是每个卡上有 4 个处理器，每个 RS/6000 可以支持多达 3 个卡，从而可以装备 4 路、8 路或 12 路的 SMP 系统。 RS64 III 发布于 1999 年，代号名：Pulsar。 第一个使用 IBM 的铜和绝缘硅技术的 RS64 芯片，现在可以支持 6 个处理器卡，可以扩展为 24 路的 SMP 机器。 RS64 IV 发布于 2001 年，代号名：IStar, SStar。 第一个市场份额很大的实现多线程的处理器，RS64 IV 比之前的处理器的速度更快，体积也更小。
   
 现在，商业计算和科学计算的聚合对处理器的设计提出了一个要求：在一个处理器上解决两方面的市场需求。因此 Star 系列正被全新设计的 POWER4 芯片所取代。 

 

 回页首 
 PowerPC 
 PowerPC 中的 PC 代表 performance computing。PowerPC 源自于 POWER 体系结构，在 1993 年首次引入。与 IBM 801 类似，PowerPC 从一开始设计就是要在各种计算机上运行：从靠电池驱动的手持设备到超级计算机和大型机。但是其第一个商业应用却是在桌面系统中，即 Power Macintosh 6100。
 PowerPC 是 Apple、IBM 和摩托罗拉（Motorola）联盟（也称为 AIM 联盟）的产物，它基于 POWER 体系结构，但是与 POWER 又有很多的不同。例如，PowerPC 是开放的，它既支持高端的内存模型，也支持低端的内存模型，而 POWER 芯片是高端的。最初的 PowerPC 设计也着重于浮点性能和多处理能力的研究。当然，它也包含了大部分 POWER 指令。很多应用程序都能在 PowerPC 上正常工作，这可能需要重新编译以进行一些转换。 
 尽管 IBM 和摩托罗拉分别独自开发了自己的芯片，但是从用户层来讲，所有的 PowerPC 处理器都运行相同的关键 PowerPC 指令集，这样可以确保在之上运行的所有软件产品都保持 ABI 兼容性。从 2000 年开始，摩托罗拉和 IBM 的 PowerPC 芯片都开始遵循 Book E 规范，这样可以提供一些增强特性，从而使得 PowerPC 对嵌入式处理器应用（例如网络和存储设备，以及消费者设备）更具有吸引力。 
 除了兼容性之外，关于 PowerPC 体系结构的最大一个优点是它是开放的：它定义了一个指令集（ISA），并且允许任何人来设计和制造与 PowerPC 兼容的处理器；为了支持 PowerPC 而开发的软件模块的源代码都可以自由使用。最后，PowerPC 核心的精简为其他部件预留了很大的空间，从新添加缓存到协处理都是如此，这样可以实现任意的设计复杂度。 
 IBM 的 4 条服务器产品线中有两条与 Apple 计算机的桌面和服务器产品线同样基于 PowerPC 体系结构，分别是 Nintendo GameCube 和 IBM 的“蓝色基因（Blue Gene）”超级计算机。 
 现在，三种主要的 PowerPC 系列是嵌入式 PowerPC 400 系列以及独立的 PowerPC 700 和 PowerPC 900 系列。由于历史的原因，我们介绍独立的 PowerPC 600 系列，因为它是第一个 PowerPC 芯片。
 PowerPC 600 系列 PowerPC 601 是第一代 PowerPC 系列中的第一个芯片。它是 POWER 和 PowerPC 体系结构之间的桥梁，其与 POWER1 的兼容性比以后的 PowerPC 都要好（甚至比 POWER 同一系列的芯片还要好），同时它还兼容 Motorola 88110 总线。PowerPC 601 的首次面世是在 1994 年最早的 PowerMac 6100 中，其主频为 66 Mhz。这条产品线中的下一个芯片是 603，它是一个低端的核心，通常在汽车中可以找到。它与 PowerPC 603 同时发布，当时 PowerPC 604 是业界最高端的芯片。603 和 604 都有一个“e”版本（603e 和 604e），该版本中对性能进行了改善。最后，第一个 64 位的 PowerPC 芯片，也是很高端的 PowerPC 620 于 1995 年发布。 PowerPC 700 系列 首次面世是在 1998 年，PowerPC 740 和 PowerPC 750 与 604e 非常类似 -- 有些人会说他们是同一个 600/700 系列的成员。PowerPC 750 是世界上第一个基于铜的微处理器，当它用于 Apple 计算机时，通常称为 G3。它很快就被 G4（或称为 Motorola 7400）所取代了。32 位的 PowerPC 750FX 在 2002 年发布时其速度就达到了 1GHz，这在业界引起一片哗然。IBM 随之在 2003 年又发布了 750GX，它带有 1MB 的 L2 缓存，速度是 1GHz，功耗大约是 7 瓦。 PowerPC 900 系列 64 位的 PowerPC 970，这是 POWER4 的一个单核心版本，可以同时处理 200 条指令，其速度可以超过 2GHz，而功耗不过几十瓦。低功耗的优势使其一方面成为笔记本和其他便携式系统的宠儿，另一方面又成为大型服务器和存储设备的首选品。它 64 位的处理能力和单指令多数据（SIMD）单元可以加速计算密集型的应用，例如多媒体和图形。这种芯片用于 Apple 的桌面系统、Xserve 服务器、图像系统以及日益增长的网络系统中。Apple Xerve G5 是第一个装备 PowerPC 970FX 的机器，这是第一个采用应变硅和绝缘硅技术制造的芯片，可以只需更低的功耗就实现更高的速度。 PowerPC 400 这是 PowerPC 处理器中的嵌入式系列产品。PowerPC 的灵活性体系结构可以实现很多的专用系统，但是从来没有其他地方会像 400 系列一样灵活。从机顶盒到 IBM 的“蓝色基因”超级计算机，到处都可以看到它的身影。在这个系列的一端是 PowerPC 405EP，每个嵌入式处理器只需要 1 瓦的功耗就可以实现 200 MHz 的主频；而另一端是基于铜技术的 800 MHz 的 PowerPC 440 系列，它可以提供业界最高端的嵌入式处理器。每个子系列都可以专用，例如，PowerPC 440GX 的双千兆以太网和 TCP/IP 负载加速可以减少报文密集型应用对 CPU 的占用率 50% 以上。大量的产品都是在对 PowerPC 400 系列的核心进行高度修改而构建的，其中“蓝色基因”超级计算机就在每个芯片中采用了两个 PowerPC 440 处理器和两个 FP（浮点）核心。
   
 虽然最初考虑用作一个桌面系统的芯片，但是 PowerPC 的低功耗使其成为嵌入式领域中很好的一个替代品，其高性能又对高级应用很有吸引力。现在，PowerPC 已经是很多东西的大脑：从视频游戏终端、多媒体娱乐系统，到数字助手和蜂窝电话，再到基站和 PBX 开关。我们家中的宽带的调制解调器、hub 和路由器、自动化子系统、打印机、复印机以及传真中也都可以找到 PowerPC。当然，桌面系统中也会有 PowerPC。 

 

 回页首 
 CMOS 
 您或许还记得 801 项目是为了解决 CISC 系统的复杂性和专用性的问题而进行的，它是 IBM 大型机中顶端的 CISC 系统。然而，IBM 大型机也从 801 项目中获益颇丰，它与 IBM 的三条 RISC 处理器的产品线都有关系。大型机芯片是 IBM 的第四个系列的处理器，它有一个非常复杂的历史。

 
打破摩尔定律的速度限制 
 
 每个人都知道 IBM 发明了 FORTRAN 和磁盘驱动器；但是您知道 IBM 还生产硅晶片和单颗粒内存单元（DRAM）吗？下面是 IBM 在半导体领域所取得的最新突破：
 铜介质 半导体业界一直有梦想能使用铜作为介质，这样可以获得比铝好 40% 以上的电流传输效率。但是直到最近制造流程才实现了这个目标。让我们从 Edison 的笔记本中翻出一页：IBM 的研究人员使用钨来生产基于铜的芯片，其速度比铝快 25 倍到 30 倍。科技界采用了这种技术，通常称之为 CMOS XS （其中 X 是一个数字）。 low-k 绝缘体 这种技术使用 SiLK 来防止铜线“串扰”，SiLK 是来自 Dow Chemical 的一种商业材料。 硅锗合金（SiGe） 在二极管芯片制造中用来代替功耗更高的砷化镓，SiGe 可以显著地改善操作频率、电流、噪音和电源容量。 绝缘硅（SoI） 在硅表面之间放上很薄的一层绝缘体，可以防止晶体管的“电子效应”，这样可以实现更高的性能和更低的功耗。 应变硅 这种技术对硅进行拉伸，从而加速电子在芯片内的流动，不用进行小型化就可以提高性能和降低功耗。如果与绝缘硅技术一起使用，应变硅技术可以更大程度地提高性能并降低功耗。
   
 

 原因之一是与其他类型的机器相比，大型机对于 CPU 的依赖程度更少，而对于系统体系结构和 I/O 通道的依赖程度则更多。S/360 系列大型机是一个创新，它引入了对业界机器的兼容性的概念，现在有些磁心仍然支持当时的功能。当它在 1971 年名字换成 S/370 时，就成为业界第一个切换为芯片的大型机。当然，它们使用的是 CISC 芯片：具体地讲，是具有 CISC 体系结构的二极管。大约 10 年之后，当 RISC 体系结构出现时，它又切换为 RISC 芯片，令人惊奇的是，其性能获得了显著的提升。然而更重要的一次变化是采用 CMOS 来替代二极管；第一代（G1）CMOS 大型机芯片于 1985 年面世，直到 1997 年 IBM 宣布此后所有的大型机都将只配备 CMOS 而不再采用二极管。实际上不只大型机切换到了 CMOS：尽管二极管统治了早期的芯片制造世界，但是现在大部分处理器都是使用 CMOS。 
 那么这些 CMOS 芯片究竟是什么呢？哦，CMOS（互补金属氧化物半导体）芯片使用金属氧化物半导体场效应晶体管（MOSFET），显然它是一种 FEI（场效应晶体管）。这是与二极管的根本区别，有关这些区别的影响的详细信息请参阅 参考资料。
 虽然二极管的速度很快，但是功耗也高，即使处于备用状态或恒态时也是如此。相反，FET 的速度较慢，但是在恒态下根本不消耗能源。因此，对于那些需要长期供电（而系统性能并不重要）的系统来说，FET 才是正确的选择。因此，当计算还很原始时，人们想出数字钟表来就是很好的一种思路，现在是 CMOS 芯片来驱动。它们还需要满足其他需要更少功耗和并不要求太高性能的系统（例如存放个人计算机的 BIOS 信息）的要求。 这就是为什么它这么慢!
 现在，二极管和 FET 之间另外一个重要的区别是拓扑结构：二极管采用的是纵向布线，而基于 FET 的芯片则采用横向布线。因此，在基于 FET 的芯片上空间就更大。在 20 世纪 80 年代和 90 年代的早期，芯片的小型化已经使得芯片的尺寸非常之小，以至于更小的基于 FET 的芯片上可以留出更多的空间，可以放置更多的晶体管，从而实现远远高出二极管模型的性能。基于 FET 的芯片还有最后一个优点：与临近晶体管之间的电子干扰远远小于二极管。因此，尽管二极管也在进行一场日益小型化的战争，但是每次更小一点时，就会加剧了电子干扰的程度，甚至到不可接受的程度；而基于 FET 的芯片则可以变得更小，甚至可以在更大的表面区域中实现更高的布线密度。因此，大部分最新的高级纳米级的芯片都是 CMOS 芯片。 
 关于大型机芯片另外一个真正有意义的指标是芯片的冗余度。大型机芯片通常会将 20 或 30 甚至更多个芯片封装在一个多芯片模型（MCM）中：其中一半用作备用芯片，以便在活动芯片失效时立即接管这些芯片的工作。更详细地说，大型机会将自己接收到的每条指令都在独立的芯片上执行两次，并在返回结果之前对其进行校验。每个芯片集成 10 亿个晶体管是芯片制造业中的一个里程碑，当我们达到这个水平时，可以发现这种稳定性对于消费用的处理器也同样适用。 

 

 回页首 
 定制芯片 
 任天堂游戏机的 Gekko、Transmeta 的第一个 Crusoe 芯片、Cray 的 X1 超级计算机芯片、Xilinx Virtex-II Pro 处理器、Agilent Tachyon 芯片以及下一代的微软 XBox 处理器等有什么共性吗？这些芯片要么是由 IBM 制造的，要么也将由 IBM 制造。
 在最近几年中，IBM 逐渐向外界开始开放自己的制造工厂和研发中心，这在以前是从未有过的事情。芯片用户包括 Sony、Qualcomm 甚至 AMD。 

 图 1. 薄晶片：300mm 的晶片生产出了更多芯片 



 原因之一是 IBM 在纽约的顶级 Fishkill 工厂。Fishkill 工厂如此先进，可以生产几乎所有最新的芯片：从铜 CMOS XS 到绝缘硅、硅锗合金以及low-k 绝缘体，所有这些都可以在 300mm 的晶片上生产。Fishkill 工艺是如此先进，以至于工人甚至无需穿“兔形套装”，因为晶片从头到尾都是在封闭的 FOUP 中生产的。最后，Fishkill 操作非常优秀，服务器中只运行 Linux。 
 Fishkill 以及 IBM 的大部分其他工艺都花费了大量的时间来使用 PowerPC 核心生产芯片。这是因为 PowerPC 核心速度非常快，而且非常小（这样可以在芯片上为定制留出更多空间）；另外一个原因是由于 PowerPC 体系结构可以与多个协处理器组合使用。这也是为什么 PowerPC 会在诸如机顶盒、游戏机和 Playstation2 视频游戏控制台等高度专用化的环境中如此成功的原因。 
 IBM 工厂也是世界上领先的 ASIC（专用集成电路）的供应商，业务范围从可定制控制处理器（CCP，这种芯片中很大一部分设计是固定的，但是要留下足够的空间进行定制）到 IBM 专家对现有产品进行裁减从而为新应用制造芯片，到对其他供应商的处理器和协处理器提供技术支持。简而言之，他们任何事都可以做。 

 

 回页首 
 Fab 的前景 
 就在 20 年前，芯片组件还是微米或几千纳米级的。现在，使用 300mm 晶片生产的芯片中包含的组件的平均尺寸也不过几十纳米。您要知道纳米是百万分之一毫米，人的一根头发的直径大约是 100,000 纳米。按照这种发展速度，我们很快就要用埃（Angstrom）作为单位来度量组件的尺寸了。
 每个芯片集成了 10 亿个晶体管的芯片现在已经不入主流，业界观察家认为到 2010 年，芯片主频将达到 100 GHz。 
 同时，我们期望看到 POWER5 和细胞超级芯片的发布，后者是由 Sony、Toshiba 和 IBM 共同开发的。 




 参考资料
 John Cocke 在 1950 年加入 IBM 时的第一个任务是从事 Stretch 计算机 的研制。虽然这个系统的性能没有达到他当时所承诺的超过 IBM 704 大型机的 100 倍以上，但是其性能的确超过它 30 倍以上。这个系统倡导了很多方面的技术，例如：流水线，程序分支预测，多线程编程，内存保护，通用中断，8 位字节，等等。这些技术后来都在 IBM 的 System/360 产品线中采用，而且一直影响了目前市场上大部分芯片的发展。 704 的后继者称为 Project X，它是 Stretch 的后继者 Project Y 的竞争对手。尽管 Project X 后来成为 IBM S/360 系列的大型机，而 Project Y 后来成为 ACS （高级计算系统），但是 IBM 的最初目的是要制造一台超级计算机。ACS 是 John 在 Stretch 之后又从事的项目，它也是 John 的下一个项目 801 的祖先。 John Cocke是世界上一个真正的 黑客，他永远改变了芯片设计和计算机世界。由于这一点，他获得了很多业界和国家的奖项，其中包括 Franklin 学院奖和 1987 年的图灵奖。 S/360 大型机差不多是刚好 40 年前发布的，其基本配置的价格现在已经降低到 $133,000 左右。这里有一份 1964 年 4 月 7 日的 新闻稿 的拷贝。 在 20 世纪 80 年代中期，IBM 发布了第一个使用 RISC 芯片的工作站。这款工作站被命名为 RT（这是为了与当时 IBM AT 和 XT 系列的机器一致），但是并没有取得很大的成功。如果您对 ROMP 处理器的技术背景感兴趣，请阅读“ The IBM RT PC ROMP processor and memory management unit architecture”。在 Wikipedia 中的 RISC 中您还可以找到更多有关 RT 和 ROMP 的资料。 IBM Microelectronics上有很多 照片分类，在 科技与创新 页面上有很多很好的资源。特别是您应当浏览一下 PowerPC 页面。 除了身着兔形套装的工作人员的照片之外，IBM Microelectronics 还提供了对商业提供最广泛支持的 定制芯片解决方案 的照片；内容从 PowerPC 核心到 Blue Logic 核心以及其他供应商的核心都有。 IBM Microelectronics 还提供了针对 PowerPC 核心的 评测工具，这些工具包括一些图表、源代码、设计细节以及一组复杂的工具，使用这些工具可以开发基于 PowerPC 的应用程序。这个页面介绍了 您可以从 PowerPC 处理器评测工具中期望获得什么。 IBM Journal of Research and Development Volume 34, Issue 1 中的大部分都是专门介绍最早的 POWER 体系结构的，很多文章在当时都会经常称 POWER 芯片为“RS/6000 处理器”，这是因为 POWER 就是用来装备 RS/6000 机器的芯片。这一期杂志中刊登了 John Cocke 自己（与 Victoria Markstein 一起撰写的）有关“ IBM RISC 技术创新”的文章。（ IBM Journal of Research and Development，1990）。 在 IBM Web 站点中的 IBM POWER2 体系结构、 POWER3 和 POWER4 系统栏目中，您可以找到很多有用的参考资料。 关于革命性的 POWER4 系统有很多话题可以探讨。您可以参阅 What Extreme Tech thinks of the POWER4 processor。 POWER5 将出现在代号为 Squadron 的新服务器和 ASCI Purple 超级计算机中 —— 后者将在 197 个电冰箱大小的节点中装备 12,000 个 POWER5 处理器，其占地面积将达到两个篮球场大小，这比 ENIAC 要高出几个数量级，ENIAC 的体积一直是某些人的笑柄。ASCI Purple 有望在 2004 年下半年运送到劳伦斯利物浦。 基于 POWER5 的服务器早就已经生产出来了，并且从 2003 年 6 月至今一直在 IBM 的 Poughkeepsie 实验室运行。 最初的 Star 或 RS64 IV 体系结构在“ A multithreaded PowerPC processor for commercial servers”一文中有介绍（ IBM Journal of Research and Development, 2000 年）。 正如免费的百科全书 Wikipedia 中解释的一样， 二极管（BJT） 与 CMOS 有很多本质的不同。只要我们能成功将现在 CMOS 制造中使用的相同高级处理器应用到二极管制造中，那就可以实现芯片制造的跃变，实现绝对梦想不到的性能。这个 CMOS 门的演示可以让您理解下一代 CMOS 技术。 E. J. Nowak 的“ Maintaining the benefits of CMOS scaling when scaling bogs down”文章（ IBM Journal of Research and Development, 2002 年）试图回答“当我们可以实现 5 纳米工艺时将会发生什么情况”的问题，而 R.D. Isaac 的“ The future of CMOS technology”文章（ IBM Journal of Research and Development, 2000 年）则对芯片设计以及 CMOS 如何取代和为什么要取代二极管的问题给出了大量的背景知识（特别要注意其中的表 1）。 由 Sony、Toshiba 和 IBM 合作开发的“一个芯片上的超级计算机”的 细胞 芯片将在明年发布；由 IBM 和 微软合作开发的 下一代 XBox 芯片也将于明年发布。 IBM developerWorks 站点上的POWER 芯片上的 Linux 的开发资源和下载工具页面是有关 iSeries、pSeries 以及其他基于 POWER 的芯片的软件和文档的一个很好的入门点。 在 developerWorks 的 Linux 专区可以找到很多有关 Linux 的开发者资源。 在 Developer Bookstore 的 Linux 区，可以找到很多 Linux 方面的书籍。
 关于作者 

 
 Nora Mikes 在 1992 年就爱上了计算机这个行业，在 1993 年开始涉足 Internet 领域，1995 年陷入了 UNIX 的爱河。她对写作方面的恋情从 1974 年开始一直延续至今。您可以通过 jskelly@jskelly.com 与她联系。"
浅层次芯片分类,2022-03-29,"芯片的分类方式可以有很多种，如果我们随机采访10位业内大佬，也许会得到10种不同的答案。 举个例子，按照处理信号方式可分为模拟芯片和数字芯片。 按照设计理念可分为可分为通用芯片和专用芯片。 按照应用领域可分为航天级芯片，汽车级芯片，工业级芯片和商业级芯片。 按照制程的话还可以分为7nm芯片、14nm芯片、28nm芯片…… 还有企业经常说的：我司的主营业务是 CPU芯片/存储芯片。其实这是从应用功能角度来分的。 今天我们就以钢铁侠为例，从应用功能的角度出发、以人做类比，进行一个详细的分类。
1.处理器芯片 
处理器芯片就相当于人类的大脑，是用来思考、分析和计算的。 
中央处理器（CPU），是系统的运算和控制核心，是信息处理、程序运行的最终执行单元。一般提到CPU，很难不第一时间想到Intel，世界上第一款商业化的处理器就是Intel 4004。除了Intel之外，AMD、IBM、高通等都是设计CPU芯片的代表性企业。国产不错的CPU公司有华为、龙芯、海光、北京君正等。 
图形处理器（GPU），也叫视觉处理器或显示芯片，它一般是在电脑、手机这类移动设备上做图像和图形相关运算工作的微处理器。Intel、英伟达、AMD都是GPU领域的代表企业。这两年国产GPU市场也是全面开花，壁仞、摩尔线程、芯动科技、景嘉微等企业都纷纷发布了国产GPU。 
数字信号处理器（DSP），是一种独特的微处理器,是以数字信号来处理大量信息的器件。除了音视频领域，在自动控制、雷达、军事、航空航天、医疗、家用电器等领域都能看到DSP的身影。Microchip、TI、ADI、ARM等公司都是DSP芯片领域的代表性企业。 
微控制器（MCU），在一颗芯片上将计算、存储、接口等集成在一起，形成芯片级的计算机。MCU的用途比较广泛，但是它最大的应用场景还得是汽车，一辆汽车里就包含70多块MCU芯片。MCU芯片领域的国内外代表企业有：瑞萨电子、恩智浦、ST、华大半导体、士兰微、复旦微电子等。 
2. 存储芯片 
存储芯片相当于人类的大脑皮层，用来储存信息和数据。心理学上说，人脑记忆可以分为瞬时记忆、短时记忆、长时记忆。同样的，存储芯片也有不同的类型，起到不同的作用。 
静态随机存取存储器（SRAM），只要保持通电，里面储存的数据就可以恒常保持；但是只要断电，数据就会丢失。属于易失性存储器。 
动态随机存取存储器（DRAM），只能将数据保持很短的时间，所以需要在通电状态下定时刷新，才能保持数据不变；断电后，数据就会丢失。同样属于易失性存储器。 
只读存储器（ROM），信息一旦写入后就固定下来，即使切断电源，信息也不会丢失。属于非易失性存储器。 
闪存（Flash），无论是否通电、是否刷新数据，都能够长久地保持数据。和ROM一样属于非易失性存储器。 
我国作为世界上最大的存储芯片市场，但长期以来的存储芯片都是依赖进口。当下存储器市场可以说是被国外巨头垄断的状态，尤其是DRAM，光三星、海力士、美光这三家的市场占有率就在94%以上。为了突破存储芯片市场，现在国内也有多家存储芯片企业突围，比如长江存储、长鑫存储、兆易创新、华天科技、复旦微电子等。 
3. 传感器 
传感器就相当于人类的五官，可以感知/测量生活中的声音、图像、温度、湿度、压力和光信号等。 
CMOS图像传感器（CIS），是将光学图像转换成电子信号的电子设备，相当于人的视网膜，手机、相机乃至卫星的拍摄成像都缺不了它。CIS领域的国内外代表企业有：索尼、三星、豪威科技、格科微等。 
微机电系统（MEMS），是按功能要求在芯片上把微电路和微机械集成于一体的系统。在手机中的应用尤其广泛，比如磁传感器、指纹传感器、环境传感器等。MEMS领域的国内外代表企业有：博通、博世、歌尔微电子、敏芯微、士兰微等。 
触摸/触控芯片（Touch），大家日常使用的手机和平板电脑，它们就内含触控芯片，这种芯片对环境变化具有灵敏的自动识别和跟踪功能。Touch芯片领域的国内外代表企业有：Atmel、比亚迪、晨星半导体、汇顶科技。 
4. 电源芯片 
电源芯片相当于人类的心脏，可以给身体其他器官提供营养物质。电源芯片可以给ASIC、DSP、CPU、Memory、FPGA及其他数字或模拟负载提供供电。 
DC/DC电源芯片（DCDC），指直流转换为直流的电源，是开关电源芯片的一种（另一种是AC/DC）。相较于AC/DC，DC/DC的设计技术及生产工艺在国内外均已成熟和标准化。 
LDO低压差线性稳压器（LDO），是一种稳压器。在电子设计中，经常需要用不同的直流电压给不同器件供电，就可以通过LDO稳压芯片来实现。 
电源管理单元（PMU），是一种高度集成的、更加便携式的电源管理方案，可以获得更高的电源转换效率和更低的功耗。 
在电源芯片领域比较有代表性的国内外企业有：芯源系统、TI、英飞凌、东芝、瑞芯微、圣邦微电子、上海贝岭等。 
5. 通信芯片 
通信芯片相当于人类的神经，四通八达，传递信息和数据。 
蓝牙芯片（Bluetooth），蓝牙大家经常会用到，它是一种近距离无线连接技术，支持声音、图像的传输。在蓝牙芯片领域有代表性的企业有：高通、赛普拉斯、Dialog、ST、紫光展锐等。 
**无线芯片（WIFI）**就不用多说了，上网冲浪必备，也是一种近距离无线通信。在WIFI芯片领域有代表性的企业有：高通、博通、Marvell、联发科、瑞昱、乐鑫等。 
窄带物联网芯片（NB-loT），是一种远距离无线通信，属于广域网。可以应用到智能抄表、智慧路灯、智慧停车、共享单车等多种领域。在NB-loT芯片领域有代表性的企业有：高通、华为、中兴微、联发科、紫光展锐等。 
6. 接口芯片 
接口芯片相当于人的“四肢”。同样可以用于传递信息和数据。 
通用串行总线芯片（USB），是一个外部总线标准，是应用在PC和手机领域的一种接口技术。它也是我们日常接触比较多的一种，比如电脑上的USB插口、手机的USB充电线，都和我们的生活息息相关。在USB芯片领域有代表性的企业有：赛普拉斯、恩智浦和全志科技。 
高清多媒体接口芯片（HDMI），是一种全数字化视频和声音发送接口，可以发送未压缩的音频及视频信号。生活中的笔记本电脑、电视、机顶盒就有HDMI接口。在HDMI芯片领域有代表性的企业有：东芝、ADI和全志科技。 
结 语 
在芯片产业“国产替代化”盛行的当下，已经入行的工程师，在专业知识方面是信手拈来，但对于芯片分类却是不甚了解。而打算入行的工程师在参加企业面试时，甚至不清楚公司的主营芯片产品。 
写下本文的初衷也正在于此，在IC学习之路上，IC修真院愿与你同行。 


 

 
原文URL：https://blog.csdn.net/coachip/article/details/121769365"
IBM计划利用新的编程语言模拟大脑电脑芯片,2013-08-10,"IBM计划利用新的编程语言模拟大脑电脑芯片 
原文作者：Michael Passingham 
  
硬件巨头IBM揭露了最新的战略，致力于生产一个和人脑大小、能力相仿的电脑系统。 
  
神经元（神经形态自适应可伸缩电子系统）计划，IBM在康奈尔大学工作的研究者从美国国防部高级研究局获得5300万美元研究资金。他们打算传统电脑模式处理大量数据时所带来的问题。 
  
Dharmendra Modha博士是IBM研究项目组负责人，他表示说现在是该向下一代IT迈进的时刻了。现在我们正处于信息技术的另一个历史转折点上。Backus时代和他同时代所创造的的可编程电脑时代正在被认知电脑时代所代替。 
  
另外，计算机将会获取庞大的数据，数据推理，并且从和人们互动信息中学习。这些信的特性将会帮助我们洞悉复杂的事物并作出更好的决定，像是如何管理城市，如何解决商业合作等复杂问题。 
  
新的编程模式工作是靠一种新型的芯片架构。这种架构的特性是处理器和存储芯片工作时候彼此更接近，在一个事件驱动模式中活动。IBM表示该结构模仿和扩展了大脑对事物改变的反应能力和一次从很多数据源中分析大量数据的能力。 
  
新的编程模式是基于'corelets'的，他是被描述为可以重复使用的构造模块，每一个模块都有他们自己的功能。在这个项目网站上还可以找到更多关于此项语言的技术细节。 
  
IBM建议这项技术的潜在用途包括一副眼镜，可以在危险的环境中协助视觉障碍者。考虑到在船上有大量的视觉和声音数据，现实增强眼镜将会标记比如路肩和汽车等障碍，引导用户更清晰。 
  
其他使用包括智能麦克风，保持通话这的追踪，对任何对话增加了精确文字记录。 
  
IBM的长期目标是要创造拥有10亿神经元和100万亿神经触突的神经元芯片。并且占用内部空间不到两升，消耗能源不超过1千万。 

原文地址：http://www.v3.co.uk/v3-uk/news/2287861/ibm-plans-for-brainlike-computer-chips-with-new-programming-language"
